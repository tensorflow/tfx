// Copyright 2019 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     https://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

syntax = "proto3";

package tfx.components.infra_validator;

// ServingSpec defines an environment of the validating infrastructure: what
// (serving binary) and where (serving platform) to run model server.
message ServingSpec {
  // The kind of binary used to serve your model.This should be the same as the
  // one that is used in the pushing environment.
  //
  // It is often useful to validate multiple versions of the same serving binary
  // before pushing, and infra validator allows it. For example, you can specify
  // multiple versions of TensorFlow Serving image by setting:
  //
  //   {
  //     "tensorflow_serving": {
  //       "tags": ["1.15.0-gpu", "latest-gpu"]
  //     }
  //   }
  //
  // Infra validator will validate your model against both versions.
  oneof serving_binary {
    TensorFlowServing tensorflow_serving = 1;
  }

  // The kind of runtime platform for running your model server and
  // corresponding configuration.
  oneof serving_platform {
    LocalDockerConfig local_docker = 2;
  }

  // Optional.
  // `model_name` is the identifier of the model in the serving platform. If
  // your infra validation depends on the name heavily (e.g. TensorFlow Serving
  // requires your model to be stored under the directory structure
  // `model_name/version`), use this field to set it accordingly. If you're
  // using TensorFlow Estimator API in the trainer, this is the name of your
  // exporter.
  // If not specified, infra validator will assign some default value and use it
  // consistently throughout the validation.
  //
  // For TensorFlow Serving: if `model_name` is not specified or your exported
  // model path does not comply with TensorFlow Serving directory structure
  // (i.e. stored under `model_name/version`), your model will be copied to
  // a temporary directory during validation.
  string model_name = 4;
}

// TensorFlow Serving docker image (tensorflow/serving) for serving binary.
message TensorFlowServing {
  // Docker image tags to use such as "latest", "1.15.0-gpu", etc..
  repeated string tags = 1;

  // Alternative to `tags`, you can specify docker image `digests`, or even
  // both.
  repeated string digests = 2;
}

// Docker runtime in a local machine. This is useful when you're running
// pipeline with infra validator component in your your local machine. You need
// to install docker in advance.
message LocalDockerConfig {
  // Optional docker client base URL where docker server is hosted. Default to
  // your running platform's OS, e.g. "unix://var/run/docker.sock"
  string client_base_url = 1;
  // Optional docker API version to specify. If unspecified, it will use the
  // installed docker client version.
  string client_api_version = 2;
  // Optional docker client timeout seconds. Default to the constant defined in
  // docker-py library.
  int32 client_timeout_seconds = 3;
}

// Specification for validation criteria and thresholds.
message ValidationSpec {
  // If the model is not loaded until this time, validation will fail. Should
  // be a positive number.
  int32 max_loading_time_seconds = 1;

  // Optional.
  // Number of infra validation tries. Infra validation will be retried until
  // infra validation succeed, or failed `num_tries` times. Default to 3.
  int32 num_tries = 2;
}

// InfraValidator can optionally send sample requests to the loaded model to
// check model is truly servable. Model should return successful responses to be
// infra validated.
message RequestSpec {
  // Serving binary specific request configuration. Should match the
  // serving_binary specified from the serving spec.
  oneof serving_binary {
    TensorFlowServingRequestSpec tensorflow_serving = 1;
  }

  // Optional.
  // InfraValidator consumes "examples" artifact to generate a request. You can
  // choose the split to be used for request generation. If not specified, any
  // available split will be chosen automatically.
  string split_name = 2;

  // Optional.
  // Maximum number of requests to issue against a model for infra validation.
  // Default to 1.
  int32 max_examples = 3;
}

// TensorFlow Serving specific request configuration.
message TensorFlowServingRequestSpec {
  // Based on the rpc_kind, one of Classify(), Regress(), or Predict() request
  // will be sent.
  TensorFlowServingRpcKind rpc_kind = 1;
  // Optional.
  // If you serve a model with multiple signatures, or your model is exported
  // with a non-default method name, you can directly specify a signature name
  // to be set in a
  // [`ModelSpec`](https://github.com/tensorflow/serving/blob/master/tensorflow_serving/apis/model.proto#L9).
  // Signature name is defined in
  // [`SignatureDef`](https://www.tensorflow.org/tfx/serving/signature_defs)
  // of a saved model.
  // Default value is empty, where server will choose an appropriate default
  // signature name depending on `rpc_kind`, e.g. "tensorflow/serving/classify".
  string signature_name = 2;
}

// List of RPCs of TensorFlow Serving Prediction service.
// https://github.com/tensorflow/serving/blob/master/tensorflow_serving/apis/prediction_service.proto
enum TensorFlowServingRpcKind {
  TF_SERVING_RPC_KIND_UNSPECIFIED = 0;
  CLASSIFY = 1;
  REGRESS = 2;
}
