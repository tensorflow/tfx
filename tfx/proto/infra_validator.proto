// Copyright 2019 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     https://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

syntax = "proto3";

package tfx.components.infra_validator;

// ServingSpec defines an environment of the validating infrastructure: what
// (serving binary) and where (serving platform) to run model server.
message ServingSpec {
  // The kind of binary used to serve your model.This should be the same as the
  // one that is used in the pushing environment.
  //
  // It is often useful to validate multiple versions of the same serving binary
  // before pushing, and infra validator allows it. For example, you can specify
  // multiple versions of TensorFlow Serving image by setting:
  //
  //   {
  //     "tensorflow_serving": {
  //       "tags": ["1.15.0-gpu", "latest-gpu"]
  //     }
  //   }
  //
  // Infra validator will validate your model against both versions.
  oneof serving_binary {
    TensorFlowServing tensorflow_serving = 1;
  }

  // The kind of runtime platform for running your model server and
  // corresponding configuration.
  oneof serving_platform {
    LocalDockerConfig local_docker = 2;
  }

  // Optional.
  // `model_name` is the identifier of the model in the serving platform. If
  // your infra validation depends on the name heavily (e.g. TensorFlow Serving
  // requires your model to be stored under the directory structure
  // `model_name/version`), use this field to set it accordingly. If you're
  // using TensorFlow Estimator API in the trainer, this is the name of your
  // exporter.
  // If not specified, infra validator will assign some default value and use it
  // consistently throughout the validation.
  //
  // For TensorFlow Serving: if `model_name` is not specified or your exported
  // model path does not comply with TensorFlow Serving directory structure
  // (i.e. stored under `model_name/version`), your model will be copied to
  // a temporary directory during validation.
  string model_name = 4;
}

// TensorFlow Serving docker image (tensorflow/serving) for serving binary.
message TensorFlowServing {
  // Docker image tags to use such as "latest", "1.15.0-gpu", etc..
  repeated string tags = 1;

  // Alternative to `tags`, you can specify docker image `digests`, or even
  // both.
  repeated string digests = 2;
}

// Docker runtime in a local machine. This is useful when you're running
// pipeline with infra validator component in your your local machine. You need
// to install docker in advance.
message LocalDockerConfig {
  // Optional docker client base URL where docker server is hosted. Default to
  // your running platform's OS, e.g. "unix://var/run/docker.sock"
  string client_base_url = 1;
  // Optional docker API version to specify. If unspecified, it will use the
  // installed docker client version.
  string client_api_version = 2;
  // Optional docker client timeout seconds. Default to the constant defined in
  // docker-py library.
  int32 client_timeout_seconds = 3;
}

// Specification for validation criteria and thresholds.
message ValidationSpec {
  // Optional.
  // If the model is not loaded until this time, validation will fail. Should
  // be a positive number. Default to 300 seconds (5 minutes)
  int32 max_loading_time_seconds = 1;

  // Optional.
  // Number of infra validation tries. Infra validation will be retried until
  // it fails `num_tries` times to mark model as not blessed. Default to 5.
  int32 num_tries = 2;
}

// InfraValidator can optionally send sample requests to the loaded model to
// check model is truly servable. Model should return successful responses to be
// infra validated.
message RequestSpec {
  // Required.
  // Request kind should be compatible with the serving_binary specified in the
  // ServingSpec.
  oneof kind {
    // Generates TF Serving RPC requests. This is compatible with
    // TensorFlowServing.
    TensorFlowServingRequestSpec tensorflow_serving = 1;
  }

  // Optional.
  // InfraValidator consumes "examples" artifact to generate a request. You can
  // choose the split to be used for request generation. If not specified, any
  // available split will be chosen automatically.
  string split_name = 2;

  // Optional.
  // Number of examples to use for building model server requests. Actual number
  // of requests will be multiplied by the number of request type to make. For
  // example the following request_spec:
  //
  // request_spec: {
  //   tensorflow_serving: {
  //     signature_names: ['serving_default', 'classification']
  //   }
  //   num_examples: 3
  // }
  //
  // will send total 6 requests (3 for each signature) to a model server.
  // Default to 1.
  int32 num_examples = 3;
}

// Request spec for building TF Serving requests.
message TensorFlowServingRequestSpec {
  // Optional.
  // Exact set of tags that MetaGraphDef should have. If SavedModel contains
  // multiple MetaGraphDef with different tag set, use this field to find the
  // right one.
  // Note that you must specify the exact set of tags, not a subset or a
  // superset of tags.
  // Default to [tf.saved_model.SERVING] (i.e. ['serve']).
  repeated string tag_set = 1;

  // Optional.
  // List of signatures to make request of.
  // Default to [tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY] (i.e.
  // ['serving_default']).
  repeated string signature_names = 2;
}
