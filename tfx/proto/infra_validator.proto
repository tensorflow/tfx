// Copyright 2019 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     https://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

syntax = "proto3";

package tfx.components.infra_validator;

// ServingSpec defines an environment of the validating infrastructure: what
// (serving binary) and where (serving platform) to run model server.
message ServingSpec {
  // The kind of binary used to serve your model.This should be the same as the
  // one that is used in the pushing environment.
  //
  // It is often useful to validate multiple versions of the same serving binary
  // before pushing, and infra validator allows it. For example, you can specify
  // multiple versions of TensorFlow Serving image by setting:
  //
  //   {
  //     "tensorflow_serving": {
  //       "tags": ["1.15.0-gpu", "latest-gpu"]
  //     }
  //   }
  //
  // Infra validator will validate your model against both versions.
  oneof serving_binary {
    TensorFlowServing tensorflow_serving = 1;
  }

  // The kind of runtime platform for running your model server and
  // corresponding configuration.
  oneof serving_platform {
    LocalDockerConfig local_docker = 2;
    KubernetesConfig kubernetes = 3;
  }

  // Required.
  // Model name is the second last segment of the exported path of the
  // tensorflow SavedModel. (Note the last segment should be the version.)
  // If you're using model exporter, `model_name` is the name of your exporter.
  // The `model_name` should match the Model artifact's directory structure,
  // which has the format of
  // `{export_path}/serving_model_dir/export/{model_name}/{model_version}`. (See
  // https://github.com/tensorflow/tfx/blob/master/tfx/utils/path_utils.py for
  // detail.)
  string model_name = 4;
}

// TensorFlow Serving docker image (tensorflow/serving) for serving binary.
message TensorFlowServing {
  // Docker image tags to use such as "latest", "1.15.0-gpu", etc..
  repeated string tags = 1;

  // Alternative to `tags`, you can specify docker image `digests`, or even
  // both.
  repeated string digests = 2;
}

// Docker runtime in a local machine. This is useful when you're running
// pipeline with infra validator component in your your local machine. You need
// to install docker in advance.
message LocalDockerConfig {
  // Optional docker client base URL where docker server is hosted. Default to
  // your running platform's OS, e.g. "unix://var/run/docker.sock"
  string client_base_url = 1;
  // Optional docker API version to specify. If unspecified, it will use the
  // installed docker client version.
  string client_api_version = 2;
  // Optional docker client timeout seconds. Default to the constant defined in
  // docker-py library.
  int32 client_timeout_seconds = 3;
}

// Kubernetes configuration. We currently only support the use case when infra
// validator is run by KFP engine. Model server will be launched in the same
// namespace KFP is running on, as well as same service account will be used
// (unless specified).
//
// Model server will have ownerReferences to the infra validator, which
// delegates the strict cleanup guarantee to the kubernetes cluster.
message KubernetesConfig {
  // Optional.
  // Name of the ServiceAccount to use to run this pod. Default to the same
  // service account that KFP uses.
  string service_account_name = 1;
  // Optional.
  // Names of Secret that contain docker config json data. This will be used to
  // authenticate and pull image from your private container registry. See
  // https://kubernetes.io/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod
  repeated string image_pull_secrets = 2;
  // Optional.
  // Specify compute resources required by the running container.
  KubernetesResourceRequirements resources = 3;
}

// Kubernetes resource requirements.
// Map can have a resource type (one of `cpu`, `memory`) and its quantity (cpu
// unit for a cpu, bytes for a memory) in a string format. For example,
//
//   requests:
//     memory: "64Mi"  # 64 * 1024 * 1024 bytes.
//     cpu: "250m"     # 250 milli-cpu-unit (1/4 cpu)
//   limits:
//     memory: "128Mi"
//     cpu: "500m"
//
// See more information at
// https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/
message KubernetesResourceRequirements {
  // Optional.
  // Maximum amount of compute resources allowed.
  map<string, string> limits = 1;
  // Optional.
  // Minimum amount of compute resources allowed.
  map<string, string> requests = 2;
}

// Specification for validation criteria and thresholds.
message ValidationSpec {
  // If the model is not loaded until this time, validation will fail. Should
  // be a positive number.
  int32 max_loading_time_seconds = 1;

  // Optional.
  // Number of infra validation tries. Infra validation will be retried until
  // infra validation succeed, or failed `num_tries` times. Default to 3.
  int32 num_tries = 2;
}

// InfraValidator can optionally send sample requests to the loaded model to
// check model is truly servable. Model should return successful responses to be
// infra validated.
message RequestSpec {
  // Serving binary specific request configuration. Should match the
  // serving_binary specified from the serving spec.
  oneof serving_binary {
    TensorFlowServingRequestSpec tensorflow_serving = 1;
  }

  // Optional.
  // InfraValidator consumes "examples" artifact to generate a request. You can
  // choose the split to be used for request generation. If not specified, any
  // available split will be chosen automatically.
  string split_name = 2;

  // Optional.
  // Maximum number of requests to issue against a model for infra validation.
  // Default to 1.
  int32 max_examples = 3;
}

// TensorFlow Serving specific request configuration.
message TensorFlowServingRequestSpec {
  // Based on the rpc_kind, one of Classify(), Regress(), or Predict() request
  // will be sent.
  TensorFlowServingRpcKind rpc_kind = 1;
  // Optional.
  // If you serve a model with multiple signatures, or your model is exported
  // with a non-default method name, you can directly specify a signature name
  // to be set in a
  // [`ModelSpec`](https://github.com/tensorflow/serving/blob/master/tensorflow_serving/apis/model.proto#L9).
  // Signature name is defined in
  // [`SignatureDef`](https://www.tensorflow.org/tfx/serving/signature_defs)
  // of a saved model.
  // Default value is empty, where server will choose an appropriate default
  // signature name depending on `rpc_kind`, e.g. "tensorflow/serving/classify".
  string signature_name = 2;
}

// List of RPCs of TensorFlow Serving Prediction service.
// https://github.com/tensorflow/serving/blob/master/tensorflow_serving/apis/prediction_service.proto
enum TensorFlowServingRpcKind {
  TF_SERVING_RPC_KIND_UNSPECIFIED = 0;
  CLASSIFY = 1;
  REGRESS = 2;
}
