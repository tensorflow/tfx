{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "23R0Z9RojXYW"
      },
      "source": [
        "\u003c!-- TODO(ccy): split this notebook into (1) a beginner friendly notebook that\n",
        "     avoids directly calling into TFX libraries and (2) an advanced notebook\n",
        "     or notebooks that delve more deeply into each component, along with their\n",
        "     underlying libraries. --\u003e\n",
        "# TFX Iterative Development Example\n",
        "This notebook demonstrates how to use Jupyter notebooks for TFX iterative development.  Here, we walk through the Chicago Taxi example in an interactive Jupyter notebook.\n",
        "\n",
        "Note: this notebook along with its associated APIs are **experimental** and are\n",
        "in active development.  Major changes in functionality, behavior and\n",
        "presentation are expected."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2GivNBNYjb3b"
      },
      "source": [
        "## Setup\n",
        "First, download data, import modules and set up paths."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "N-ePgV0Lj68Q"
      },
      "source": [
        "### Import packages\n",
        "We import necessary packages, including standard TFX component classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "YIqpWK9efviJ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tempfile\n",
        "import urllib\n",
        "\n",
        "import tensorflow as tf\n",
        "import tfx\n",
        "from tfx.components.evaluator.component import Evaluator\n",
        "from tfx.components.example_gen.csv_example_gen.component import CsvExampleGen\n",
        "from tfx.components.example_validator.component import ExampleValidator\n",
        "from tfx.components.model_validator.component import ModelValidator\n",
        "from tfx.components.pusher.component import Pusher\n",
        "from tfx.components.schema_gen.component import SchemaGen\n",
        "from tfx.components.statistics_gen.component import StatisticsGen\n",
        "from tfx.components.trainer.component import Trainer\n",
        "from tfx.components.transform.component import Transform\n",
        "from tfx.orchestration import metadata\n",
        "from tfx.orchestration import pipeline\n",
        "from tfx.orchestration.interactive.interactive_context import check_ipython\n",
        "from tfx.orchestration.interactive.interactive_context import InteractiveContext\n",
        "from tfx.proto import evaluator_pb2\n",
        "from tfx.proto import pusher_pb2\n",
        "from tfx.proto import trainer_pb2\n",
        "from tfx.proto.evaluator_pb2 import SingleSlicingSpec\n",
        "from tfx.utils.dsl_utils import csv_input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-ChG4HSd1Tgq"
      },
      "source": [
        "### Load custom extensions.\n",
        "Use cell magic `%%skip_for_export` to denote cells to skip during export to pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "SgxtWZ581Tgr"
      },
      "outputs": [],
      "source": [
        "%load_ext tfx.orchestration.interactive.notebook_extensions.skip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ufJKQ6OvkJlY"
      },
      "source": [
        "### Set up pipeline paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "ad5JLpKbf6sN"
      },
      "outputs": [],
      "source": [
        "# Set up paths.\n",
        "\n",
        "if check_ipython():\n",
        "  _tfx_root = tfx.__path__[0]\n",
        "  _taxi_root = os.path.join(_tfx_root, 'examples/chicago_taxi_pipeline')\n",
        "  # Path which can be listened to by the model server.  Pusher will output the\n",
        "  # trained model here.\n",
        "  _serving_model_dir = os.path.join(\n",
        "      tempfile.mkdtemp(), 'serving_model/taxi_simple')\n",
        "else:\n",
        "  # Paths for exported pipeline.\n",
        "  _tfx_root = os.path.join(os.environ['HOME'], 'tfx')\n",
        "  _taxi_root = os.path.join(os.environ['HOME'], 'taxi')\n",
        "  _serving_model_dir = os.path.join(_taxi_root, 'serving_model')\n",
        "\n",
        "# Python module file to inject customized logic into the TFX components. The\n",
        "# Transform and Trainer both require user-defined functions to run successfully.\n",
        "_taxi_module_file = os.path.join(_taxi_root, 'taxi_utils.py')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "n2cMMAbSkGfX"
      },
      "source": [
        "### Download example data\n",
        "We download the sample dataset for use in our TFX pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "BywX6OUEhAqn"
      },
      "outputs": [],
      "source": [
        "# Download the example data.\n",
        "if check_ipython():\n",
        "  _data_root = tempfile.mkdtemp(prefix='tfx-data')\n",
        "  DATA_PATH = 'https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/chicago_taxi_pipeline/data/simple/data.csv'\n",
        "  with open(os.path.join(_data_root, 'data.csv'), 'wb') as f:\n",
        "      contents = urllib.request.urlopen(DATA_PATH).read()\n",
        "      f.write(contents)\n",
        "else:\n",
        "  _data_root = os.path.join(_taxi_root, 'data', 'simple')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8ONIE_hdkPS4"
      },
      "source": [
        "## Create the InteractiveContext\n",
        "We now create the interactive context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "0Rh6K5sUf9dd"
      },
      "outputs": [],
      "source": [
        "# Here, we create an InteractiveContext using default parameters. This will\n",
        "# use a temporary directory with an ephemeral ML Metadata database instance.\n",
        "# To use your own pipeline root or database, the optional properties\n",
        "# `pipeline_root` and `metadata_connection_config` may be passed to\n",
        "# InteractiveContext. Calls to InteractiveContext are no-ops outside of the\n",
        "# notebook.\n",
        "context = InteractiveContext()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HdQWxfsVkzdJ"
      },
      "source": [
        "## Run TFX components interactively\n",
        "Next, we construct TFX components and run each one interactively using within the interactive session to obtain `ExecutionResult` objects."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "L9fwt9gQk3BR"
      },
      "source": [
        "### ExampleGen\n",
        "`ExampleGen` brings data into the TFX pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "PyXjuMt8f-9u"
      },
      "outputs": [],
      "source": [
        "# Use the packaged CSV input data.\n",
        "examples = csv_input(_data_root)\n",
        "\n",
        "# Brings data into the pipeline or otherwise joins/converts training data.\n",
        "example_gen = CsvExampleGen(input_base=examples)\n",
        "context.run(example_gen)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "csM6BFhtk5Aa"
      },
      "source": [
        "### StatisticsGen (using Tensorflow Data Validation)\n",
        "`StatisticsGen` computes statistics for visualization and example validation. This uses the [Tensorflow Data Validation](https://www.tensorflow.org/tfx/data_validation/get_started) library."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "W0tl75TMW3DR"
      },
      "source": [
        "#### Run TFDV statistics computation using the StatisticsGen component"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "MAscCCYWgA-9"
      },
      "outputs": [],
      "source": [
        "# Computes statistics over data for visualization and example validation.\n",
        "statistics_gen = StatisticsGen(\n",
        "    input_data=example_gen.outputs['examples'])\n",
        "context.run(statistics_gen)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HLI6cb_5WugZ"
      },
      "source": [
        "#### Import TFDV and visualize the statistics result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "tLjXy7K6Tp_G"
      },
      "outputs": [],
      "source": [
        "%%skip_for_export\n",
        "\n",
        "# Import TFDV and get the train statistics path.\n",
        "import tensorflow_data_validation as tfdv\n",
        "from tfx.types.artifact_utils import get_split_uri\n",
        "\n",
        "artifact_list = statistics_gen.outputs['output'].get()\n",
        "train_artifact_uri = get_split_uri(artifact_list, 'train')\n",
        "train_stats_path = os.path.join(train_artifact_uri, 'stats_tfrecord')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "bKicTZkPTUY8"
      },
      "outputs": [],
      "source": [
        "%%skip_for_export\n",
        "\n",
        "# Load statistics and visualize training data stats.\n",
        "\n",
        "train_stats = tfdv.load_statistics(train_stats_path)\n",
        "tfdv.visualize_statistics(train_stats)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HLKLTO9Nk60p"
      },
      "source": [
        "### SchemaGen (using Tensorflow Data Validation)\n",
        "`SchemaGen` generates a schema for your data based on computed statistics. This component also uses the [Tensorflow Data Validation](https://www.tensorflow.org/tfx/data_validation/get_started) library."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "L0xvulenXSK3"
      },
      "source": [
        "#### Run TFDV schema inference using the SchemaGen component"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "ygQvZ6hsiQ_J"
      },
      "outputs": [],
      "source": [
        "# Generates schema based on statistics files.\n",
        "infer_schema = SchemaGen(\n",
        "    stats=statistics_gen.outputs['output'],\n",
        "    infer_feature_shape=False)\n",
        "context.run(infer_schema)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zi6TxTUKXM6b"
      },
      "source": [
        "#### Visualize the inferred schema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Ec9vqDXpXeMb"
      },
      "outputs": [],
      "source": [
        "%%skip_for_export\n",
        "\n",
        "# Get the schema path.\n",
        "schema_dir = infer_schema.outputs['output'].get()[0].uri\n",
        "schema_path = os.path.join(schema_dir, 'schema.pbtxt')\n",
        "\n",
        "# Load and visualize the generated schema.\n",
        "schema = tfdv.load_schema_text(schema_path)\n",
        "tfdv.display_schema(schema)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "V1qcUuO9k9f8"
      },
      "source": [
        "### ExampleValidator\n",
        "`ExampleValidator` performs anomaly detection based on computed statistics and your data schema."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ONg1bt57gVna"
      },
      "source": [
        "#### Run TFDV data validation using the ExampleValidation component"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "XRlRUuGgiXks"
      },
      "outputs": [],
      "source": [
        "# Performs anomaly detection based on statistics and data schema.\n",
        "validate_stats = ExampleValidator(\n",
        "    stats=statistics_gen.outputs['output'],\n",
        "    schema=infer_schema.outputs['output'])\n",
        "context.run(validate_stats)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "855mrHgJcoer"
      },
      "source": [
        "#### Visualize the detected anomalies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "TDyAAozQcrk3"
      },
      "outputs": [],
      "source": [
        "%%skip_for_export\n",
        "\n",
        "# Get the validation path.\n",
        "validation_dir = validate_stats.outputs['output'].get()[0].uri\n",
        "anomalies_path = os.path.join(validation_dir, 'anomalies.pbtxt')\n",
        "\n",
        "# Load and visualize the anomalies.\n",
        "anomalies = tfdv.load_anomalies_text(anomalies_path)\n",
        "tfdv.display_anomalies(anomalies)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JPViEz5RlA36"
      },
      "source": [
        "### Transform\n",
        "`Transform` performs data transformations and feature engineering which are kept in sync for training and serving."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wgbmZr3sgbWW"
      },
      "source": [
        "#### Run the Transform component"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "jHfhth_GiZI9"
      },
      "outputs": [],
      "source": [
        "# Performs transformations and feature engineering in training and serving.\n",
        "transform = Transform(\n",
        "    input_data=example_gen.outputs['examples'],\n",
        "    schema=infer_schema.outputs['output'],\n",
        "    module_file=_taxi_module_file)\n",
        "context.run(transform)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OBJFtnl6lCg9"
      },
      "source": [
        "### Trainer\n",
        "`Trainer` trains your custom model using TF-Learn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "429-vvCWibO0"
      },
      "outputs": [],
      "source": [
        "# Uses user-provided Python function that implements a model using TF-Learn.\n",
        "trainer = Trainer(\n",
        "    module_file=_taxi_module_file,\n",
        "    transformed_examples=transform.outputs['transformed_examples'],\n",
        "    schema=infer_schema.outputs['output'],\n",
        "    transform_output=transform.outputs['transform_output'],\n",
        "    train_args=trainer_pb2.TrainArgs(num_steps=10000),\n",
        "    eval_args=trainer_pb2.EvalArgs(num_steps=5000))\n",
        "context.run(trainer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FmPftrv0lEQy"
      },
      "source": [
        "### Evaluator (using Tensorflow Model Analysis)\n",
        "The `Evaluator` computes evaluation statistics over features of your model using [Tensorflow Model Analysis](https://www.tensorflow.org/tfx/model_analysis/get_started). In this section, we run TFMA in our TFX pipeline and then visualize the results to analyze the performance of our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kGcid3lXJsBf"
      },
      "source": [
        "#### Run TFMA using the Evaluator component\n",
        "\n",
        "Here, we first define slicing specs for analyzing our data. Next, we run TFMA using these specs to generate results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "fVhfzzh9PDEx"
      },
      "outputs": [],
      "source": [
        "# An empty slice spec means the overall slice, that is, the whole dataset.\n",
        "OVERALL_SLICE_SPEC = evaluator_pb2.SingleSlicingSpec()\n",
        "\n",
        "# Data can be sliced along a feature column\n",
        "# In this case, data is sliced along feature column trip_start_hour.\n",
        "FEATURE_COLUMN_SLICE_SPEC = evaluator_pb2.SingleSlicingSpec(\n",
        "    column_for_slicing=['trip_start_hour'])\n",
        "\n",
        "# Data can be sliced by crossing feature columns\n",
        "# In this case, slices are computed for trip_start_day x trip_start_month.\n",
        "FEATURE_COLUMN_CROSS_SPEC = evaluator_pb2.SingleSlicingSpec(\n",
        "    column_for_slicing=['trip_start_day', 'trip_start_month'])\n",
        "\n",
        "ALL_SPECS = [\n",
        "    OVERALL_SLICE_SPEC,\n",
        "    FEATURE_COLUMN_SLICE_SPEC,\n",
        "    FEATURE_COLUMN_CROSS_SPEC,\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Zjcx8g6mihSt"
      },
      "outputs": [],
      "source": [
        "# Use TFMA to compute a evaluation statistics over features of a model.\n",
        "model_analyzer = Evaluator(\n",
        "    examples=example_gen.outputs['examples'],\n",
        "    model_exports=trainer.outputs['output'],\n",
        "    feature_slicing_spec=evaluator_pb2.FeatureSlicingSpec(\n",
        "        specs=ALL_SPECS\n",
        "    ))\n",
        "context.run(model_analyzer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "X2VKvr3NJwii"
      },
      "source": [
        "#### Get the TFMA output result path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "pyis6iy0HLdi"
      },
      "outputs": [],
      "source": [
        "PATH_TO_RESULT = model_analyzer.outputs['output'].get()[0].uri"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "L9YNBXsQKW63"
      },
      "source": [
        "#### Import TFMA and load the result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "7SeuBh8aHZ9S"
      },
      "outputs": [],
      "source": [
        "%%skip_for_export\n",
        "\n",
        "import tensorflow_model_analysis as tfma\n",
        "\n",
        "tfma_result = tfma.load_eval_result(PATH_TO_RESULT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LdLc25OnKs0G"
      },
      "source": [
        "#### Visualization: Slicing Metrics\n",
        "\n",
        "To see the slices, either use the name of the column (by setting slicing_column) or provide a tfma.slicer.SingleSliceSpec (by setting slicing_spec). If neither is provided, an overall visualization will be displayed.\n",
        "\n",
        "The default visualization is the **slice overview** when the number of slices is small. It shows the value of a metric for each slice, sorted by the another metric. It is also possible to set a threshold to filter out slices with smaller weights.\n",
        "\n",
        "This view also supports the **metrics histogram** as an alternative visualization. It is also the default view when the number of slices is large. The results will be divided into buckets and the number of slices / total weights / both can be visualized. Slices with small weights can be filtered out by setting the threshold. Further filtering can be applied by dragging the grey band. To reset the range, double click the band. Filtering can be used to remove outliers in the visualization and the metrics table below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "SYvXSBANHdmZ"
      },
      "outputs": [],
      "source": [
        "%%skip_for_export\n",
        "\n",
        "# Show data sliced along feature column trip_start_hour.\n",
        "tfma.view.render_slicing_metrics(\n",
        "    tfma_result, slicing_column='trip_start_hour')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "IGADc5R7Q8yo"
      },
      "outputs": [],
      "source": [
        "%%skip_for_export\n",
        "\n",
        "# Show metrics sliced by 'trip_start_day' x 'trip_start_month'.\n",
        "tfma.view.render_slicing_metrics(\n",
        "    tfma_result,\n",
        "    slicing_spec=tfma.slicer.SingleSliceSpec(\n",
        "        columns=['trip_start_day','trip_start_month']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "WFvfzTTZL_0a"
      },
      "outputs": [],
      "source": [
        "%%skip_for_export\n",
        "\n",
        "# Show overall metrics.\n",
        "tfma.view.render_slicing_metrics(tfma_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "76Mil-7FlF_y"
      },
      "source": [
        "### ModelValidator\n",
        "`ModelValidator` performs validation of your candidate model compared to a baseline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "FXk1MA7sijCr"
      },
      "outputs": [],
      "source": [
        "# Performs quality validation of a candidate model (compared to a baseline).\n",
        "model_validator = ModelValidator(\n",
        "    examples=example_gen.outputs['examples'],\n",
        "    model=trainer.outputs['output'])\n",
        "context.run(model_validator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "T8DYekCZlHfj"
      },
      "source": [
        "### Pusher\n",
        "`Pusher` checks whether a model has passed validation, and if so, pushes the model to a file destination."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "r45nQ69eikc9"
      },
      "outputs": [],
      "source": [
        "# Checks whether the model passed the validation steps and pushes the model\n",
        "# to a file destination if check passed.\n",
        "pusher = Pusher(\n",
        "    model_export=trainer.outputs['output'],\n",
        "    model_blessing=model_validator.outputs['blessing'],\n",
        "    push_destination=pusher_pb2.PushDestination(\n",
        "        filesystem=pusher_pb2.PushDestination.Filesystem(\n",
        "            base_directory=_serving_model_dir)))\n",
        "context.run(pusher)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qGNDOG1o1Tht"
      },
      "source": [
        "## Export to pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Obxof4O71Thu"
      },
      "source": [
        "### Mount Google Drive (Colab).\n",
        "Save a copy of your notebook in Google Drive if you haven't already, then run the following cell to mount drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "CH8yu7Un1Thu"
      },
      "outputs": [],
      "source": [
        "%%skip_for_export\n",
        "\n",
        "import sys\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "  # Colab.\n",
        "  from google.colab import drive\n",
        "\n",
        "  drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "d64gdS2u1Thw"
      },
      "source": [
        "### Set up notebook/export paths."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "X_dZL1lS1Thx"
      },
      "outputs": [],
      "source": [
        "# TODO(USER).\n",
        "notebook_filename = 'taxi_pipeline_interactive.ipynb'\n",
        "\n",
        "# Colab example.\n",
        "# TODO(USER): Update notebook directory path.\n",
        "_notebook_filepath = os.path.join('/content/drive/My Drive/Colab Notebooks',\n",
        "                                  notebook_filename)\n",
        "_export_filepath_template = '/content/export_%s.py'\n",
        "\n",
        "# Jupyter example.\n",
        "# _notebook_filepath = os.path.join(os.getcwd(), notebook_filename)\n",
        "# _export_filepath_template = os.path.join(os.getcwd(), 'export_%s.py')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6nATTNYZ1Thy"
      },
      "source": [
        "### Save your notebook, then run the appropriate cell below to export a pipeline to run on Beam/Airflow/etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "SsfNFi6iHMSp"
      },
      "outputs": [],
      "source": [
        "# Beam pipeline export template.\n",
        "\n",
        "from tfx.orchestration.beam.beam_dag_runner import BeamDagRunner\n",
        "\n",
        "\n",
        "# Components to be part of the exported pipeline.\n",
        "components = [\n",
        "    example_gen, statistics_gen, infer_schema, validate_stats, transform,\n",
        "    trainer, model_analyzer, model_validator, pusher\n",
        "]\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "_pipeline_name = 'chicago_taxi_beam'\n",
        "_pipeline_root = os.path.join(_tfx_root, 'pipelines', _pipeline_name)\n",
        "# Sqlite ML-metadata db path.\n",
        "_metadata_path = os.path.join(_tfx_root, 'metadata', _pipeline_name,\n",
        "                              'metadata.db')\n",
        "\n",
        "tfx_pipeline = pipeline.Pipeline(\n",
        "    pipeline_name=_pipeline_name,\n",
        "    pipeline_root=_pipeline_root,\n",
        "    components=components,\n",
        "    enable_cache=True,\n",
        "    metadata_connection_config=(\n",
        "        metadata.sqlite_metadata_connection_config(_metadata_path)),\n",
        "    additional_pipeline_args={})\n",
        "\n",
        "if not check_ipython():\n",
        "  BeamDagRunner().run(tfx_pipeline)\n",
        "\n",
        "export_filepath = _export_filepath_template % _pipeline_name\n",
        "context.export_to_pipeline(notebook_filepath=_notebook_filepath,\n",
        "                           export_filepath=export_filepath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "-sqEac0otnnt"
      },
      "outputs": [],
      "source": [
        "%%skip_for_export\n",
        "\n",
        "# # Airflow pipeline export template.\n",
        "\n",
        "# import datetime\n",
        "# from tfx.orchestration.airflow.airflow_dag_runner import AirflowDagRunner\n",
        "\n",
        "# # Components to be part of the exported pipeline.\n",
        "# components = [\n",
        "#     example_gen, statistics_gen, infer_schema, validate_stats, transform,\n",
        "#     trainer, model_analyzer, model_validator, pusher\n",
        "# ]\n",
        "\n",
        "# tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "# _pipeline_name = 'chicago_taxi_airflow'\n",
        "# _pipeline_root = os.path.join(_tfx_root, 'pipelines', _pipeline_name)\n",
        "# # Sqlite ML-metadata db path.\n",
        "# _metadata_path = os.path.join(_tfx_root, 'metadata', _pipeline_name,\n",
        "#                               'metadata.db')\n",
        "\n",
        "# # Airflow-specific configs; these will be passed directly to airflow\n",
        "# _airflow_config = {\n",
        "#     'schedule_interval': None,\n",
        "#     'start_date': datetime.datetime(2019, 1, 1),\n",
        "# }\n",
        "\n",
        "# tfx_pipeline = pipeline.Pipeline(\n",
        "#     pipeline_name=_pipeline_name,\n",
        "#     pipeline_root=_pipeline_root,\n",
        "#     components=components,\n",
        "#     enable_cache=True,\n",
        "#     metadata_connection_config=(\n",
        "#         metadata.sqlite_metadata_connection_config(_metadata_path)),\n",
        "#     additional_pipeline_args={})\n",
        "\n",
        "# if not check_ipython():\n",
        "#   # 'DAG' below needs to be kept for Airflow to detect dag.\n",
        "#   DAG = AirflowDagRunner(_airflow_config).run(tfx_pipeline)\n",
        "\n",
        "# export_filepath = _export_filepath_template % _pipeline_name\n",
        "# context.export_to_pipeline(\n",
        "#     notebook_filepath=_notebook_filepath,\n",
        "#     export_filepath=export_filepath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AL-eHsdV1Th3"
      },
      "source": [
        "### Download exported script from Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "FeRJyHly1Th3"
      },
      "outputs": [],
      "source": [
        "%%skip_for_export\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "  from google.colab import files\n",
        "\n",
        "  files.download(export_filepath)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "taxi_pipeline_interactive.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 2",
      "name": "python2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
