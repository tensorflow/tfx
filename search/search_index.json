{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"TFX","text":"<p>TFX is an end-to-end platform for deploying production ML pipelines.</p> <p>When you're ready to move your models from research to production, use TFX to create and manage a production pipeline.</p> <p> </p>"},{"location":"#how-it-works","title":"How it works","text":"<p>A TFX pipeline is a sequence of components that implement an ML pipeline which is specifically designed for scalable, high-performance machine learning tasks. Components are built using TFX libraries which can also be used individually.</p> <ul> <li> <p> Install TFX</p> <p>Install <code>tfx</code> with <code>pip</code>:</p> <pre><code>pip install tfx\n</code></pre> <p> Getting started</p> </li> <li> <p> User Guide</p> <p>Learn more about how to get started with TFX in the user guide.</p> <p> User Guide</p> </li> <li> <p> View The Tutorials</p> <p>Learn from real world examples that use TFX.</p> <p> Tutorials</p> </li> <li> <p> API Reference</p> <p>The API reference contains details about functions, classes, and modules that are part of TFX.</p> <p> API Reference</p> </li> </ul>"},{"location":"api_overview/","title":"TFX API Reference","text":""},{"location":"api_overview/#tensorflow-extended","title":"TensorFlow Extended","text":"<ul> <li>TensorFlow Extended</li> </ul>"},{"location":"api_overview/#data-validation","title":"Data Validation","text":"<ul> <li>Data Validation</li> </ul>"},{"location":"api_overview/#transform","title":"Transform","text":"<ul> <li>Transform</li> <li>Transform.beam</li> </ul>"},{"location":"api_overview/#model-analysis","title":"Model Analysis","text":"<ul> <li>Model Analysis</li> </ul>"},{"location":"api_overview/#serving","title":"Serving","text":"<ul> <li>Client API (REST)</li> <li>Server API (C++)</li> </ul>"},{"location":"api_overview/#ml-metadata","title":"ML Metadata","text":"<ul> <li>ML Metadata</li> </ul>"},{"location":"api_overview/#tf-metadata","title":"TF Metadata","text":"<ul> <li>ML Metadata</li> </ul>"},{"location":"api_overview/#tfx-basic-shared-libraries-tfx_bsl","title":"TFX Basic Shared Libraries (<code>tfx_bsl</code>)","text":"<ul> <li>tfx_bsl</li> </ul>"},{"location":"api/v1/","title":"Modules","text":"<p>components module: TFX components module.</p> <p>dsl module: TFX DSL module.</p> <p>extensions module: TFX extensions module.</p> <p>orchestration module: TFX orchestration module.</p> <p>proto module: TFX proto module.</p> <p>testing module: Public testing modules for TFX.</p> <p>types module: TFX types module.</p> <p>utils module: TFX utils module.</p>"},{"location":"api/v1/components/","title":"Components","text":""},{"location":"api/v1/components/#tfx.v1.components","title":"tfx.v1.components","text":"<p>TFX components module.</p> CLASS DESCRIPTION <code>BulkInferrer</code> <p>A TFX component to do batch inference on a model with unlabelled examples.</p> <code>CsvExampleGen</code> <p>Official TFX CsvExampleGen component.</p> <code>Evaluator</code> <p>A TFX component to evaluate models trained by a TFX Trainer component.</p> <code>ExampleDiff</code> <p>TFX ExampleDiff component.</p> <code>ExampleValidator</code> <p>A TFX component to validate input examples.</p> <code>FnArgs</code> <p>Args to pass to user defined training/tuning function(s).</p> <code>ImportExampleGen</code> <p>Official TFX ImportExampleGen component.</p> <code>ImportSchemaGen</code> <p>A TFX ImportSchemaGen component to import a schema file into the pipeline.</p> <code>InfraValidator</code> <p>A TFX component to validate the model against the serving infrastructure.</p> <code>Pusher</code> <p>A TFX component to push validated TensorFlow models to a model serving platform.</p> <code>SchemaGen</code> <p>A TFX SchemaGen component to generate a schema from the training data.</p> <code>StatisticsGen</code> <p>Official TFX StatisticsGen component.</p> <code>Trainer</code> <p>A TFX component to train a TensorFlow model.</p> <code>Transform</code> <p>A TFX component to transform the input examples.</p> <code>Tuner</code> <p>A TFX component for model hyperparameter tuning.</p> ATTRIBUTE DESCRIPTION <code>DataAccessor</code> <p>For accessing the data on disk.</p> <p> </p> <code>TunerFnResult</code> <p>Return type of tuner_fn.</p> <p> </p>"},{"location":"api/v1/components/#tfx.v1.components-attributes","title":"Attributes","text":""},{"location":"api/v1/components/#tfx.v1.components.DataAccessor","title":"DataAccessor  <code>module-attribute</code>","text":"<pre><code>DataAccessor = NamedTuple('DataAccessor', [('tf_dataset_factory', Callable[[List[str], TensorFlowDatasetOptions, Optional[Schema]], Dataset]), ('record_batch_factory', Callable[[List[str], RecordBatchesOptions, Optional[Schema]], Iterator[RecordBatch]]), ('data_view_decode_fn', Optional[Callable[[Tensor], Dict[str, Any]]])])\n</code></pre> <p>For accessing the data on disk.</p> <p>Contains factories that can create tf.data.Datasets or other means to access the train/eval data. They provide a uniform way of accessing data, regardless of how the data is stored on disk.</p>"},{"location":"api/v1/components/#tfx.v1.components.TunerFnResult","title":"TunerFnResult  <code>module-attribute</code>","text":"<pre><code>TunerFnResult = NamedTuple('TunerFnResult', [('tuner', BaseTuner), ('fit_kwargs', Dict[str, Any])])\n</code></pre> <p>Return type of tuner_fn.</p> <p>tuner_fn returns a TunerFnResult that contains: - tuner: A BaseTuner that will be used for tuning. - fit_kwargs: Args to pass to tuner's run_trial function for fitting the               model , e.g., the training and validation dataset. Required               args depend on the tuner's implementation.</p>"},{"location":"api/v1/components/#tfx.v1.components-classes","title":"Classes","text":""},{"location":"api/v1/components/#tfx.v1.components.BulkInferrer","title":"BulkInferrer","text":"<pre><code>BulkInferrer(examples: BaseChannel, model: Optional[BaseChannel] = None, model_blessing: Optional[BaseChannel] = None, data_spec: Optional[Union[DataSpec, RuntimeParameter]] = None, model_spec: Optional[Union[ModelSpec, RuntimeParameter]] = None, output_example_spec: Optional[Union[OutputExampleSpec, RuntimeParameter]] = None)\n</code></pre> <p>               Bases: <code>BaseBeamComponent</code></p> <p>A TFX component to do batch inference on a model with unlabelled examples.</p> <p>BulkInferrer consumes examples data and a model, and produces the inference results to an external location as PredictionLog proto.</p> <p>BulkInferrer will infer on validated model.</p>"},{"location":"api/v1/components/#tfx.v1.components.BulkInferrer--example","title":"Example","text":"<pre><code>  # Uses BulkInferrer to inference on examples.\n  bulk_inferrer = BulkInferrer(\n      examples=example_gen.outputs['examples'],\n      model=trainer.outputs['model'])\n</code></pre> <p>Component <code>outputs</code> contains:</p> <ul> <li><code>inference_result</code>: Channel of type <code>standard_artifacts.InferenceResult</code>                        to store the inference results.</li> <li><code>output_examples</code>: Channel of type <code>standard_artifacts.Examples</code>                       to store the output examples. This is optional                       controlled by <code>output_example_spec</code>.</li> </ul> <p>See the BulkInferrer guide for more details.</p> <p>Construct an BulkInferrer component.</p> PARAMETER DESCRIPTION <code>examples</code> <p>A BaseChannel of type <code>standard_artifacts.Examples</code>, usually produced by an ExampleGen component. required</p> <p> TYPE: <code>BaseChannel</code> </p> <code>model</code> <p>A BaseChannel of type <code>standard_artifacts.Model</code>, usually produced by a Trainer component.</p> <p> TYPE: <code>Optional[BaseChannel]</code> DEFAULT: <code>None</code> </p> <code>model_blessing</code> <p>A BaseChannel of type <code>standard_artifacts.ModelBlessing</code>, usually produced by a ModelValidator component.</p> <p> TYPE: <code>Optional[BaseChannel]</code> DEFAULT: <code>None</code> </p> <code>data_spec</code> <p>bulk_inferrer_pb2.DataSpec instance that describes data selection.</p> <p> TYPE: <code>Optional[Union[DataSpec, RuntimeParameter]]</code> DEFAULT: <code>None</code> </p> <code>model_spec</code> <p>bulk_inferrer_pb2.ModelSpec instance that describes model specification.</p> <p> TYPE: <code>Optional[Union[ModelSpec, RuntimeParameter]]</code> DEFAULT: <code>None</code> </p> <code>output_example_spec</code> <p>bulk_inferrer_pb2.OutputExampleSpec instance, specify if you want BulkInferrer to output examples instead of inference result.</p> <p> TYPE: <code>Optional[Union[OutputExampleSpec, RuntimeParameter]]</code> DEFAULT: <code>None</code> </p> METHOD DESCRIPTION <code>add_downstream_node</code> <p>Experimental: Add another component that must run after this one.</p> <code>add_downstream_nodes</code> <p>Experimental: Add another component that must run after this one.</p> <code>add_upstream_node</code> <p>Experimental: Add another component that must run before this one.</p> <code>add_upstream_nodes</code> <p>Experimental: Add components that must run before this one.</p> <code>from_json_dict</code> <p>Convert from dictionary data to an object.</p> <code>to_json_dict</code> <p>Convert from an object to a JSON serializable dictionary.</p> <code>with_beam_pipeline_args</code> <p>Add per component Beam pipeline args.</p> <code>with_platform_config</code> <p>Attaches a proto-form platform config to a component.</p> ATTRIBUTE DESCRIPTION <code>id</code> <p>Node id, unique across all TFX nodes in a pipeline.</p> <p> TYPE: <code>str</code> </p> <code>outputs</code> <p>Component's output channel dict.</p> <p> TYPE: <code>Dict[str, OutputChannel]</code> </p> Source code in <code>tfx/components/bulk_inferrer/component.py</code> <pre><code>def __init__(\n    self,\n    examples: types.BaseChannel,\n    model: Optional[types.BaseChannel] = None,\n    model_blessing: Optional[types.BaseChannel] = None,\n    data_spec: Optional[Union[bulk_inferrer_pb2.DataSpec,\n                              data_types.RuntimeParameter]] = None,\n    model_spec: Optional[Union[bulk_inferrer_pb2.ModelSpec,\n                               data_types.RuntimeParameter]] = None,\n    output_example_spec: Optional[Union[bulk_inferrer_pb2.OutputExampleSpec,\n                                        data_types.RuntimeParameter]] = None):\n  \"\"\"Construct an BulkInferrer component.\n\n  Args:\n    examples: A [BaseChannel][tfx.v1.types.BaseChannel] of type [`standard_artifacts.Examples`][tfx.v1.types.standard_artifacts.Examples], usually\n      produced by an ExampleGen component. _required_\n    model: A [BaseChannel][tfx.v1.types.BaseChannel] of type [`standard_artifacts.Model`][tfx.v1.types.standard_artifacts.Model], usually produced\n      by a [Trainer][tfx.v1.components.Trainer] component.\n    model_blessing: A [BaseChannel][tfx.v1.types.BaseChannel] of type [`standard_artifacts.ModelBlessing`][tfx.v1.types.standard_artifacts.ModelBlessing],\n      usually produced by a ModelValidator component.\n    data_spec: bulk_inferrer_pb2.DataSpec instance that describes data\n      selection.\n    model_spec: bulk_inferrer_pb2.ModelSpec instance that describes model\n      specification.\n    output_example_spec: bulk_inferrer_pb2.OutputExampleSpec instance, specify\n      if you want BulkInferrer to output examples instead of inference result.\n  \"\"\"\n  if output_example_spec:\n    output_examples = types.Channel(type=standard_artifacts.Examples)\n    inference_result = None\n  else:\n    inference_result = types.Channel(type=standard_artifacts.InferenceResult)\n    output_examples = None\n\n  spec = standard_component_specs.BulkInferrerSpec(\n      examples=examples,\n      model=model,\n      model_blessing=model_blessing,\n      data_spec=data_spec or bulk_inferrer_pb2.DataSpec(),\n      model_spec=model_spec or bulk_inferrer_pb2.ModelSpec(),\n      output_example_spec=output_example_spec,\n      inference_result=inference_result,\n      output_examples=output_examples)\n  super().__init__(spec=spec)\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.BulkInferrer-attributes","title":"Attributes","text":""},{"location":"api/v1/components/#tfx.v1.components.BulkInferrer.id","title":"id  <code>property</code> <code>writable</code>","text":"<pre><code>id: str\n</code></pre> <p>Node id, unique across all TFX nodes in a pipeline.</p> <p>If <code>id</code> is set by the user, return it directly. Otherwise, return .</p> RETURNS DESCRIPTION <code>str</code> <p>node id.</p>"},{"location":"api/v1/components/#tfx.v1.components.BulkInferrer.outputs","title":"outputs  <code>property</code>","text":"<pre><code>outputs: Dict[str, OutputChannel]\n</code></pre> <p>Component's output channel dict.</p>"},{"location":"api/v1/components/#tfx.v1.components.BulkInferrer-functions","title":"Functions","text":""},{"location":"api/v1/components/#tfx.v1.components.BulkInferrer.add_downstream_node","title":"add_downstream_node","text":"<pre><code>add_downstream_node(downstream_node)\n</code></pre> <p>Experimental: Add another component that must run after this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> <p>It is symmetric with <code>add_upstream_node</code>.</p> PARAMETER DESCRIPTION <code>downstream_node</code> <p>a component that must run after this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_downstream_node(self, downstream_node):\n  \"\"\"Experimental: Add another component that must run after this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n  It is symmetric with `add_upstream_node`.\n\n  Args:\n    downstream_node: a component that must run after this node.\n  \"\"\"\n  self._downstream_nodes.add(downstream_node)\n  if self not in downstream_node.upstream_nodes:\n    downstream_node.add_upstream_node(self)\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.BulkInferrer.add_downstream_nodes","title":"add_downstream_nodes","text":"<pre><code>add_downstream_nodes(downstream_nodes)\n</code></pre> <p>Experimental: Add another component that must run after this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> <p>It is symmetric with <code>add_upstream_nodes</code>.</p> PARAMETER DESCRIPTION <code>downstream_nodes</code> <p>a list of components that must run after this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_downstream_nodes(self, downstream_nodes):\n  \"\"\"Experimental: Add another component that must run after this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n  It is symmetric with `add_upstream_nodes`.\n\n  Args:\n    downstream_nodes: a list of components that must run after this node.\n  \"\"\"\n  self._downstream_nodes.update(downstream_nodes)\n  for downstream_node in downstream_nodes:\n    if self not in downstream_node.upstream_nodes:\n      downstream_node.add_upstream_node(self)\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.BulkInferrer.add_upstream_node","title":"add_upstream_node","text":"<pre><code>add_upstream_node(upstream_node)\n</code></pre> <p>Experimental: Add another component that must run before this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> <p>It is symmetric with <code>add_downstream_node</code>.</p> PARAMETER DESCRIPTION <code>upstream_node</code> <p>a component that must run before this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_upstream_node(self, upstream_node):\n  \"\"\"Experimental: Add another component that must run before this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n  It is symmetric with `add_downstream_node`.\n\n  Args:\n    upstream_node: a component that must run before this node.\n  \"\"\"\n  self._upstream_nodes.add(upstream_node)\n  if self not in upstream_node.downstream_nodes:\n    upstream_node.add_downstream_node(self)\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.BulkInferrer.add_upstream_nodes","title":"add_upstream_nodes","text":"<pre><code>add_upstream_nodes(upstream_nodes)\n</code></pre> <p>Experimental: Add components that must run before this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> PARAMETER DESCRIPTION <code>upstream_nodes</code> <p>a list of components that must run before this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_upstream_nodes(self, upstream_nodes):\n  \"\"\"Experimental: Add components that must run before this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n\n  Args:\n    upstream_nodes: a list of components that must run before this node.\n  \"\"\"\n  self._upstream_nodes.update(upstream_nodes)\n  for upstream_node in upstream_nodes:\n    if self not in upstream_node.downstream_nodes:\n      upstream_node.add_downstream_node(self)\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.BulkInferrer.from_json_dict","title":"from_json_dict  <code>classmethod</code>","text":"<pre><code>from_json_dict(dict_data: Dict[str, Any]) -&gt; Any\n</code></pre> <p>Convert from dictionary data to an object.</p> Source code in <code>tfx/utils/json_utils.py</code> <pre><code>@classmethod\n@doc_controls.do_not_doc_in_subclasses\ndef from_json_dict(cls, dict_data: Dict[str, Any]) -&gt; Any:\n  \"\"\"Convert from dictionary data to an object.\"\"\"\n  instance = cls.__new__(cls)\n  instance.__dict__ = dict_data\n  return instance\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.BulkInferrer.to_json_dict","title":"to_json_dict","text":"<pre><code>to_json_dict() -&gt; Dict[str, Any]\n</code></pre> <p>Convert from an object to a JSON serializable dictionary.</p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef to_json_dict(self) -&gt; Dict[str, Any]:\n  \"\"\"Convert from an object to a JSON serializable dictionary.\"\"\"\n  return dict((k, v)\n              for k, v in self.__dict__.items()\n              if k not in ['_upstream_nodes', '_downstream_nodes'])\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.BulkInferrer.with_beam_pipeline_args","title":"with_beam_pipeline_args","text":"<pre><code>with_beam_pipeline_args(beam_pipeline_args: Iterable[Union[str, Placeholder]]) -&gt; BaseBeamComponent\n</code></pre> <p>Add per component Beam pipeline args.</p> PARAMETER DESCRIPTION <code>beam_pipeline_args</code> <p>List of Beam pipeline args to be added to the Beam executor spec.</p> <p> TYPE: <code>Iterable[Union[str, Placeholder]]</code> </p> RETURNS DESCRIPTION <code>BaseBeamComponent</code> <p>the same component itself.</p> Source code in <code>tfx/dsl/components/base/base_beam_component.py</code> <pre><code>def with_beam_pipeline_args(\n    self, beam_pipeline_args: Iterable[Union[str, placeholder.Placeholder]]\n) -&gt; 'BaseBeamComponent':\n  \"\"\"Add per component Beam pipeline args.\n\n  Args:\n    beam_pipeline_args: List of Beam pipeline args to be added to the Beam\n      executor spec.\n\n  Returns:\n    the same component itself.\n  \"\"\"\n  cast(executor_spec.BeamExecutorSpec,\n       self.executor_spec).add_beam_pipeline_args(beam_pipeline_args)\n  return self\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.BulkInferrer.with_platform_config","title":"with_platform_config","text":"<pre><code>with_platform_config(config: Message) -&gt; Self\n</code></pre> <p>Attaches a proto-form platform config to a component.</p> <p>The config will be a per-node platform-specific config.</p> PARAMETER DESCRIPTION <code>config</code> <p>platform config to attach to the component.</p> <p> TYPE: <code>Message</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>the same component itself.</p> Source code in <code>tfx/dsl/components/base/base_component.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef with_platform_config(\n    self, config: message.Message\n) -&gt; typing_extensions.Self:\n  \"\"\"Attaches a proto-form platform config to a component.\n\n  The config will be a per-node platform-specific config.\n\n  Args:\n    config: platform config to attach to the component.\n\n  Returns:\n    the same component itself.\n  \"\"\"\n  self.platform_config = config\n  return self\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.CsvExampleGen","title":"CsvExampleGen","text":"<pre><code>CsvExampleGen(input_base: Optional[str] = None, input_config: Optional[Union[Input, RuntimeParameter]] = None, output_config: Optional[Union[Output, RuntimeParameter]] = None, range_config: Optional[Union[Placeholder, RangeConfig, RuntimeParameter]] = None)\n</code></pre> <p>               Bases: <code>FileBasedExampleGen</code></p> <p>Official TFX CsvExampleGen component.</p> <p>The csv examplegen component takes csv data, and generates train and eval examples for downstream components.</p> <p>The csv examplegen encodes column values to tf.Example int/float/byte feature. For the case when there's missing cells, the csv examplegen uses:</p> <ul> <li>tf.train.Feature(<code>type</code>_list=tf.train.<code>type</code>List(value=[])), when the    <code>type</code> can be inferred.</li> <li>tf.train.Feature() when it cannot infer the <code>type</code> from the column.</li> </ul> <p>Note that the type inferring will be per input split. If input isn't a single split, users need to ensure the column types align in each pre-splits.</p> <p>For example, given the following csv rows of a split:</p> <pre><code>header:A,B,C,D\nrow1:  1,,x,0.1\nrow2:  2,,y,0.2\nrow3:  3,,,0.3\nrow4:\n</code></pre> <p>The output example will be </p><pre><code>example1: 1(int), empty feature(no type), x(string), 0.1(float)\nexample2: 2(int), empty feature(no type), x(string), 0.2(float)\nexample3: 3(int), empty feature(no type), empty list(string), 0.3(float)\n</code></pre> <p>Note that the empty feature is <code>tf.train.Feature()</code> while empty list string feature is <code>tf.train.Feature(bytes_list=tf.train.BytesList(value=[]))</code>.</p> <p>Component <code>outputs</code> contains:</p> <ul> <li><code>examples</code>: Channel of type <code>standard_artifacts.Examples</code> for output train                and eval examples.</li> </ul> <p>Construct a CsvExampleGen component.</p> PARAMETER DESCRIPTION <code>input_base</code> <p>an external directory containing the CSV files.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>input_config</code> <p>An example_gen_pb2.Input instance, providing input configuration. If unset, the files under input_base will be treated as a single split.</p> <p> TYPE: <code>Optional[Union[Input, RuntimeParameter]]</code> DEFAULT: <code>None</code> </p> <code>output_config</code> <p>An example_gen_pb2.Output instance, providing output configuration. If unset, default splits will be 'train' and 'eval' with size 2:1.</p> <p> TYPE: <code>Optional[Union[Output, RuntimeParameter]]</code> DEFAULT: <code>None</code> </p> <code>range_config</code> <p>An optional range_config_pb2.RangeConfig instance, specifying the range of span values to consider. If unset, driver will default to searching for latest span with no restrictions.</p> <p> TYPE: <code>Optional[Union[Placeholder, RangeConfig, RuntimeParameter]]</code> DEFAULT: <code>None</code> </p> METHOD DESCRIPTION <code>add_downstream_node</code> <p>Experimental: Add another component that must run after this one.</p> <code>add_downstream_nodes</code> <p>Experimental: Add another component that must run after this one.</p> <code>add_upstream_node</code> <p>Experimental: Add another component that must run before this one.</p> <code>add_upstream_nodes</code> <p>Experimental: Add components that must run before this one.</p> <code>from_json_dict</code> <p>Convert from dictionary data to an object.</p> <code>to_json_dict</code> <p>Convert from an object to a JSON serializable dictionary.</p> <code>with_beam_pipeline_args</code> <p>Add per component Beam pipeline args.</p> <code>with_platform_config</code> <p>Attaches a proto-form platform config to a component.</p> ATTRIBUTE DESCRIPTION <code>id</code> <p>Node id, unique across all TFX nodes in a pipeline.</p> <p> TYPE: <code>str</code> </p> <code>outputs</code> <p>Component's output channel dict.</p> <p> TYPE: <code>Dict[str, OutputChannel]</code> </p> Source code in <code>tfx/components/example_gen/csv_example_gen/component.py</code> <pre><code>def __init__(\n    self,\n    input_base: Optional[str] = None,\n    input_config: Optional[Union[example_gen_pb2.Input,\n                                 data_types.RuntimeParameter]] = None,\n    output_config: Optional[Union[example_gen_pb2.Output,\n                                  data_types.RuntimeParameter]] = None,\n    range_config: Optional[Union[placeholder.Placeholder,\n                                 range_config_pb2.RangeConfig,\n                                 data_types.RuntimeParameter]] = None):\n  \"\"\"Construct a CsvExampleGen component.\n\n  Args:\n    input_base: an external directory containing the CSV files.\n    input_config: An example_gen_pb2.Input instance, providing input\n      configuration. If unset, the files under input_base will be treated as a\n      single split.\n    output_config: An example_gen_pb2.Output instance, providing output\n      configuration. If unset, default splits will be 'train' and 'eval' with\n      size 2:1.\n    range_config: An optional range_config_pb2.RangeConfig instance,\n      specifying the range of span values to consider. If unset, driver will\n      default to searching for latest span with no restrictions.\n  \"\"\"\n  super().__init__(\n      input_base=input_base,\n      input_config=input_config,\n      output_config=output_config,\n      range_config=range_config)\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.CsvExampleGen-attributes","title":"Attributes","text":""},{"location":"api/v1/components/#tfx.v1.components.CsvExampleGen.id","title":"id  <code>property</code> <code>writable</code>","text":"<pre><code>id: str\n</code></pre> <p>Node id, unique across all TFX nodes in a pipeline.</p> <p>If <code>id</code> is set by the user, return it directly. Otherwise, return .</p> RETURNS DESCRIPTION <code>str</code> <p>node id.</p>"},{"location":"api/v1/components/#tfx.v1.components.CsvExampleGen.outputs","title":"outputs  <code>property</code>","text":"<pre><code>outputs: Dict[str, OutputChannel]\n</code></pre> <p>Component's output channel dict.</p>"},{"location":"api/v1/components/#tfx.v1.components.CsvExampleGen-functions","title":"Functions","text":""},{"location":"api/v1/components/#tfx.v1.components.CsvExampleGen.add_downstream_node","title":"add_downstream_node","text":"<pre><code>add_downstream_node(downstream_node)\n</code></pre> <p>Experimental: Add another component that must run after this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> <p>It is symmetric with <code>add_upstream_node</code>.</p> PARAMETER DESCRIPTION <code>downstream_node</code> <p>a component that must run after this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_downstream_node(self, downstream_node):\n  \"\"\"Experimental: Add another component that must run after this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n  It is symmetric with `add_upstream_node`.\n\n  Args:\n    downstream_node: a component that must run after this node.\n  \"\"\"\n  self._downstream_nodes.add(downstream_node)\n  if self not in downstream_node.upstream_nodes:\n    downstream_node.add_upstream_node(self)\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.CsvExampleGen.add_downstream_nodes","title":"add_downstream_nodes","text":"<pre><code>add_downstream_nodes(downstream_nodes)\n</code></pre> <p>Experimental: Add another component that must run after this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> <p>It is symmetric with <code>add_upstream_nodes</code>.</p> PARAMETER DESCRIPTION <code>downstream_nodes</code> <p>a list of components that must run after this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_downstream_nodes(self, downstream_nodes):\n  \"\"\"Experimental: Add another component that must run after this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n  It is symmetric with `add_upstream_nodes`.\n\n  Args:\n    downstream_nodes: a list of components that must run after this node.\n  \"\"\"\n  self._downstream_nodes.update(downstream_nodes)\n  for downstream_node in downstream_nodes:\n    if self not in downstream_node.upstream_nodes:\n      downstream_node.add_upstream_node(self)\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.CsvExampleGen.add_upstream_node","title":"add_upstream_node","text":"<pre><code>add_upstream_node(upstream_node)\n</code></pre> <p>Experimental: Add another component that must run before this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> <p>It is symmetric with <code>add_downstream_node</code>.</p> PARAMETER DESCRIPTION <code>upstream_node</code> <p>a component that must run before this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_upstream_node(self, upstream_node):\n  \"\"\"Experimental: Add another component that must run before this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n  It is symmetric with `add_downstream_node`.\n\n  Args:\n    upstream_node: a component that must run before this node.\n  \"\"\"\n  self._upstream_nodes.add(upstream_node)\n  if self not in upstream_node.downstream_nodes:\n    upstream_node.add_downstream_node(self)\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.CsvExampleGen.add_upstream_nodes","title":"add_upstream_nodes","text":"<pre><code>add_upstream_nodes(upstream_nodes)\n</code></pre> <p>Experimental: Add components that must run before this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> PARAMETER DESCRIPTION <code>upstream_nodes</code> <p>a list of components that must run before this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_upstream_nodes(self, upstream_nodes):\n  \"\"\"Experimental: Add components that must run before this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n\n  Args:\n    upstream_nodes: a list of components that must run before this node.\n  \"\"\"\n  self._upstream_nodes.update(upstream_nodes)\n  for upstream_node in upstream_nodes:\n    if self not in upstream_node.downstream_nodes:\n      upstream_node.add_downstream_node(self)\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.CsvExampleGen.from_json_dict","title":"from_json_dict  <code>classmethod</code>","text":"<pre><code>from_json_dict(dict_data: Dict[str, Any]) -&gt; Any\n</code></pre> <p>Convert from dictionary data to an object.</p> Source code in <code>tfx/utils/json_utils.py</code> <pre><code>@classmethod\n@doc_controls.do_not_doc_in_subclasses\ndef from_json_dict(cls, dict_data: Dict[str, Any]) -&gt; Any:\n  \"\"\"Convert from dictionary data to an object.\"\"\"\n  instance = cls.__new__(cls)\n  instance.__dict__ = dict_data\n  return instance\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.CsvExampleGen.to_json_dict","title":"to_json_dict","text":"<pre><code>to_json_dict() -&gt; Dict[str, Any]\n</code></pre> <p>Convert from an object to a JSON serializable dictionary.</p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef to_json_dict(self) -&gt; Dict[str, Any]:\n  \"\"\"Convert from an object to a JSON serializable dictionary.\"\"\"\n  return dict((k, v)\n              for k, v in self.__dict__.items()\n              if k not in ['_upstream_nodes', '_downstream_nodes'])\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.CsvExampleGen.with_beam_pipeline_args","title":"with_beam_pipeline_args","text":"<pre><code>with_beam_pipeline_args(beam_pipeline_args: Iterable[Union[str, Placeholder]]) -&gt; BaseBeamComponent\n</code></pre> <p>Add per component Beam pipeline args.</p> PARAMETER DESCRIPTION <code>beam_pipeline_args</code> <p>List of Beam pipeline args to be added to the Beam executor spec.</p> <p> TYPE: <code>Iterable[Union[str, Placeholder]]</code> </p> RETURNS DESCRIPTION <code>BaseBeamComponent</code> <p>the same component itself.</p> Source code in <code>tfx/dsl/components/base/base_beam_component.py</code> <pre><code>def with_beam_pipeline_args(\n    self, beam_pipeline_args: Iterable[Union[str, placeholder.Placeholder]]\n) -&gt; 'BaseBeamComponent':\n  \"\"\"Add per component Beam pipeline args.\n\n  Args:\n    beam_pipeline_args: List of Beam pipeline args to be added to the Beam\n      executor spec.\n\n  Returns:\n    the same component itself.\n  \"\"\"\n  cast(executor_spec.BeamExecutorSpec,\n       self.executor_spec).add_beam_pipeline_args(beam_pipeline_args)\n  return self\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.CsvExampleGen.with_platform_config","title":"with_platform_config","text":"<pre><code>with_platform_config(config: Message) -&gt; Self\n</code></pre> <p>Attaches a proto-form platform config to a component.</p> <p>The config will be a per-node platform-specific config.</p> PARAMETER DESCRIPTION <code>config</code> <p>platform config to attach to the component.</p> <p> TYPE: <code>Message</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>the same component itself.</p> Source code in <code>tfx/dsl/components/base/base_component.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef with_platform_config(\n    self, config: message.Message\n) -&gt; typing_extensions.Self:\n  \"\"\"Attaches a proto-form platform config to a component.\n\n  The config will be a per-node platform-specific config.\n\n  Args:\n    config: platform config to attach to the component.\n\n  Returns:\n    the same component itself.\n  \"\"\"\n  self.platform_config = config\n  return self\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.Evaluator","title":"Evaluator","text":"<pre><code>Evaluator(examples: BaseChannel, model: Optional[BaseChannel] = None, baseline_model: Optional[BaseChannel] = None, feature_slicing_spec: Optional[Union[FeatureSlicingSpec, RuntimeParameter]] = None, fairness_indicator_thresholds: Optional[Union[List[float], RuntimeParameter]] = None, example_splits: Optional[List[str]] = None, eval_config: Optional[EvalConfig] = None, schema: Optional[BaseChannel] = None, module_file: Optional[str] = None, module_path: Optional[str] = None)\n</code></pre> <p>               Bases: <code>BaseBeamComponent</code></p> <p>A TFX component to evaluate models trained by a TFX Trainer component.</p> <p>Component <code>outputs</code> contains:</p> <ul> <li><code>evaluation</code>: Channel of type <code>standard_artifacts.ModelEvaluation</code> to                  store the evaluation results.</li> <li><code>blessing</code>: Channel of type <code>standard_artifacts.ModelBlessing</code> that                contains the blessing result.</li> </ul> <p>See the Evaluator guide for more details.</p> <p>Construct an Evaluator component.</p> PARAMETER DESCRIPTION <code>examples</code> <p>A BaseChannel of type <code>standard_artifacts.Examples</code>, usually produced by an ExampleGen component. required</p> <p> TYPE: <code>BaseChannel</code> </p> <code>model</code> <p>A BaseChannel of type <code>standard_artifacts.Model</code>, usually produced by a Trainer component.</p> <p> TYPE: <code>Optional[BaseChannel]</code> DEFAULT: <code>None</code> </p> <code>baseline_model</code> <p>An optional channel of type 'standard_artifacts.Model' as the baseline model for model diff and model validation purpose.</p> <p> TYPE: <code>Optional[BaseChannel]</code> DEFAULT: <code>None</code> </p> <code>feature_slicing_spec</code> <p>Deprecated, please use eval_config instead. Only support estimator. evaluator_pb2.FeatureSlicingSpec instance that describes how Evaluator should slice the data.</p> <p> TYPE: <code>Optional[Union[FeatureSlicingSpec, RuntimeParameter]]</code> DEFAULT: <code>None</code> </p> <code>fairness_indicator_thresholds</code> <p>Optional list of float (or RuntimeParameter) threshold values for use with TFMA fairness   indicators. Experimental functionality: this interface and   functionality may change at any time. TODO(b/142653905): add a link   to additional documentation for TFMA fairness indicators here.</p> <p> TYPE: <code>Optional[Union[List[float], RuntimeParameter]]</code> DEFAULT: <code>None</code> </p> <code>example_splits</code> <p>Names of splits on which the metrics are computed. Default behavior (when example_splits is set to None or Empty) is using the 'eval' split.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>eval_config</code> <p>Instance of tfma.EvalConfig containg configuration settings for running the evaluation. This config has options for both estimator and Keras.</p> <p> TYPE: <code>Optional[EvalConfig]</code> DEFAULT: <code>None</code> </p> <code>schema</code> <p>A <code>Schema</code> channel to use for TFXIO.</p> <p> TYPE: <code>Optional[BaseChannel]</code> DEFAULT: <code>None</code> </p> <code>module_file</code> <p>A path to python module file containing UDFs for Evaluator customization. This functionality is experimental and may change at any time. The module_file can implement following functions at its top level.   </p><pre><code>def custom_eval_shared_model(\n   eval_saved_model_path, model_name, eval_config, **kwargs,\n) -&gt; tfma.EvalSharedModel:\n</code></pre> <pre><code>def custom_extractors(\n  eval_shared_model, eval_config, tensor_adapter_config,\n) -&gt; List[tfma.extractors.Extractor]:\n</code></pre> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>module_path</code> <p>A python path to the custom module that contains the UDFs. See 'module_file' for the required signature of UDFs. This functionality is experimental and this API may change at any time. Note this can not be set together with module_file.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> METHOD DESCRIPTION <code>add_downstream_node</code> <p>Experimental: Add another component that must run after this one.</p> <code>add_downstream_nodes</code> <p>Experimental: Add another component that must run after this one.</p> <code>add_upstream_node</code> <p>Experimental: Add another component that must run before this one.</p> <code>add_upstream_nodes</code> <p>Experimental: Add components that must run before this one.</p> <code>from_json_dict</code> <p>Convert from dictionary data to an object.</p> <code>to_json_dict</code> <p>Convert from an object to a JSON serializable dictionary.</p> <code>with_beam_pipeline_args</code> <p>Add per component Beam pipeline args.</p> <code>with_platform_config</code> <p>Attaches a proto-form platform config to a component.</p> ATTRIBUTE DESCRIPTION <code>id</code> <p>Node id, unique across all TFX nodes in a pipeline.</p> <p> TYPE: <code>str</code> </p> <code>outputs</code> <p>Component's output channel dict.</p> <p> TYPE: <code>Dict[str, OutputChannel]</code> </p> Source code in <code>tfx/components/evaluator/component.py</code> <pre><code>def __init__(\n    self,\n    examples: types.BaseChannel,\n    model: Optional[types.BaseChannel] = None,\n    baseline_model: Optional[types.BaseChannel] = None,\n    # TODO(b/148618405): deprecate feature_slicing_spec.\n    feature_slicing_spec: Optional[Union[evaluator_pb2.FeatureSlicingSpec,\n                                         data_types.RuntimeParameter]] = None,\n    fairness_indicator_thresholds: Optional[Union[\n        List[float], data_types.RuntimeParameter]] = None,\n    example_splits: Optional[List[str]] = None,\n    eval_config: Optional[tfma.EvalConfig] = None,\n    schema: Optional[types.BaseChannel] = None,\n    module_file: Optional[str] = None,\n    module_path: Optional[str] = None):\n  \"\"\"Construct an Evaluator component.\n\n  Args:\n    examples: A [BaseChannel][tfx.v1.types.BaseChannel] of type [`standard_artifacts.Examples`][tfx.v1.types.standard_artifacts.Examples], usually\n      produced by an ExampleGen component. _required_\n    model: A [BaseChannel][tfx.v1.types.BaseChannel] of type [`standard_artifacts.Model`][tfx.v1.types.standard_artifacts.Model], usually produced\n      by a [Trainer][tfx.v1.components.Trainer] component.\n    baseline_model: An optional channel of type ['standard_artifacts.Model'][tfx.v1.types.standard_artifacts.Model] as\n      the baseline model for model diff and model validation purpose.\n    feature_slicing_spec: Deprecated, please use eval_config instead. Only\n      support estimator.\n      [evaluator_pb2.FeatureSlicingSpec](https://github.com/tensorflow/tfx/blob/master/tfx/proto/evaluator.proto)\n      instance that describes how Evaluator should slice the data.\n    fairness_indicator_thresholds: Optional list of float (or\n      [RuntimeParameter][tfx.v1.dsl.experimental.RuntimeParameter]) threshold values for use with TFMA fairness\n        indicators. Experimental functionality: this interface and\n        functionality may change at any time. TODO(b/142653905): add a link\n        to additional documentation for TFMA fairness indicators here.\n    example_splits: Names of splits on which the metrics are computed.\n      Default behavior (when example_splits is set to None or Empty) is using\n      the 'eval' split.\n    eval_config: Instance of tfma.EvalConfig containg configuration settings\n      for running the evaluation. This config has options for both estimator\n      and Keras.\n    schema: A `Schema` channel to use for TFXIO.\n    module_file: A path to python module file containing UDFs for Evaluator\n      customization. This functionality is experimental and may change at any\n      time. The module_file can implement following functions at its top\n      level.\n        ``` {.py .no-copy}\n        def custom_eval_shared_model(\n           eval_saved_model_path, model_name, eval_config, **kwargs,\n        ) -&gt; tfma.EvalSharedModel:\n        ```\n        ``` {.py .no-copy}\n        def custom_extractors(\n          eval_shared_model, eval_config, tensor_adapter_config,\n        ) -&gt; List[tfma.extractors.Extractor]:\n        ```\n    module_path: A python path to the custom module that contains the UDFs.\n      See 'module_file' for the required signature of UDFs. This functionality\n      is experimental and this API may change at any time. Note this can not\n      be set together with module_file.\n  \"\"\"\n  if bool(module_file) and bool(module_path):\n    raise ValueError(\n        'Python module path can not be set together with module file path.')\n\n  if eval_config is not None and feature_slicing_spec is not None:\n    raise ValueError(\"Exactly one of 'eval_config' or 'feature_slicing_spec' \"\n                     'must be supplied.')\n  if eval_config is None and feature_slicing_spec is None:\n    feature_slicing_spec = evaluator_pb2.FeatureSlicingSpec()\n    logging.info('Neither eval_config nor feature_slicing_spec is passed, '\n                 'the model is treated as estimator.')\n\n  if feature_slicing_spec:\n    logging.warning('feature_slicing_spec is deprecated, please use '\n                    'eval_config instead.')\n\n  blessing = types.Channel(type=standard_artifacts.ModelBlessing)\n  evaluation = types.Channel(type=standard_artifacts.ModelEvaluation)\n  spec = standard_component_specs.EvaluatorSpec(\n      examples=examples,\n      model=model,\n      baseline_model=baseline_model,\n      feature_slicing_spec=feature_slicing_spec,\n      fairness_indicator_thresholds=(\n          fairness_indicator_thresholds if isinstance(\n              fairness_indicator_thresholds, data_types.RuntimeParameter) else\n          json_utils.dumps(fairness_indicator_thresholds)),\n      example_splits=json_utils.dumps(example_splits),\n      evaluation=evaluation,\n      eval_config=eval_config,\n      blessing=blessing,\n      schema=schema,\n      module_file=module_file,\n      module_path=module_path)\n  super().__init__(spec=spec)\n\n  if udf_utils.should_package_user_modules():\n    # In this case, the `MODULE_PATH_KEY` execution property will be injected\n    # as a reference to the given user module file after packaging, at which\n    # point the `MODULE_FILE_KEY` execution property will be removed.\n    udf_utils.add_user_module_dependency(\n        self, standard_component_specs.MODULE_FILE_KEY,\n        standard_component_specs.MODULE_PATH_KEY)\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.Evaluator-attributes","title":"Attributes","text":""},{"location":"api/v1/components/#tfx.v1.components.Evaluator.id","title":"id  <code>property</code> <code>writable</code>","text":"<pre><code>id: str\n</code></pre> <p>Node id, unique across all TFX nodes in a pipeline.</p> <p>If <code>id</code> is set by the user, return it directly. Otherwise, return .</p> RETURNS DESCRIPTION <code>str</code> <p>node id.</p>"},{"location":"api/v1/components/#tfx.v1.components.Evaluator.outputs","title":"outputs  <code>property</code>","text":"<pre><code>outputs: Dict[str, OutputChannel]\n</code></pre> <p>Component's output channel dict.</p>"},{"location":"api/v1/components/#tfx.v1.components.Evaluator-functions","title":"Functions","text":""},{"location":"api/v1/components/#tfx.v1.components.Evaluator.add_downstream_node","title":"add_downstream_node","text":"<pre><code>add_downstream_node(downstream_node)\n</code></pre> <p>Experimental: Add another component that must run after this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> <p>It is symmetric with <code>add_upstream_node</code>.</p> PARAMETER DESCRIPTION <code>downstream_node</code> <p>a component that must run after this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_downstream_node(self, downstream_node):\n  \"\"\"Experimental: Add another component that must run after this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n  It is symmetric with `add_upstream_node`.\n\n  Args:\n    downstream_node: a component that must run after this node.\n  \"\"\"\n  self._downstream_nodes.add(downstream_node)\n  if self not in downstream_node.upstream_nodes:\n    downstream_node.add_upstream_node(self)\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.Evaluator.add_downstream_nodes","title":"add_downstream_nodes","text":"<pre><code>add_downstream_nodes(downstream_nodes)\n</code></pre> <p>Experimental: Add another component that must run after this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> <p>It is symmetric with <code>add_upstream_nodes</code>.</p> PARAMETER DESCRIPTION <code>downstream_nodes</code> <p>a list of components that must run after this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_downstream_nodes(self, downstream_nodes):\n  \"\"\"Experimental: Add another component that must run after this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n  It is symmetric with `add_upstream_nodes`.\n\n  Args:\n    downstream_nodes: a list of components that must run after this node.\n  \"\"\"\n  self._downstream_nodes.update(downstream_nodes)\n  for downstream_node in downstream_nodes:\n    if self not in downstream_node.upstream_nodes:\n      downstream_node.add_upstream_node(self)\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.Evaluator.add_upstream_node","title":"add_upstream_node","text":"<pre><code>add_upstream_node(upstream_node)\n</code></pre> <p>Experimental: Add another component that must run before this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> <p>It is symmetric with <code>add_downstream_node</code>.</p> PARAMETER DESCRIPTION <code>upstream_node</code> <p>a component that must run before this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_upstream_node(self, upstream_node):\n  \"\"\"Experimental: Add another component that must run before this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n  It is symmetric with `add_downstream_node`.\n\n  Args:\n    upstream_node: a component that must run before this node.\n  \"\"\"\n  self._upstream_nodes.add(upstream_node)\n  if self not in upstream_node.downstream_nodes:\n    upstream_node.add_downstream_node(self)\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.Evaluator.add_upstream_nodes","title":"add_upstream_nodes","text":"<pre><code>add_upstream_nodes(upstream_nodes)\n</code></pre> <p>Experimental: Add components that must run before this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> PARAMETER DESCRIPTION <code>upstream_nodes</code> <p>a list of components that must run before this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_upstream_nodes(self, upstream_nodes):\n  \"\"\"Experimental: Add components that must run before this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n\n  Args:\n    upstream_nodes: a list of components that must run before this node.\n  \"\"\"\n  self._upstream_nodes.update(upstream_nodes)\n  for upstream_node in upstream_nodes:\n    if self not in upstream_node.downstream_nodes:\n      upstream_node.add_downstream_node(self)\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.Evaluator.from_json_dict","title":"from_json_dict  <code>classmethod</code>","text":"<pre><code>from_json_dict(dict_data: Dict[str, Any]) -&gt; Any\n</code></pre> <p>Convert from dictionary data to an object.</p> Source code in <code>tfx/utils/json_utils.py</code> <pre><code>@classmethod\n@doc_controls.do_not_doc_in_subclasses\ndef from_json_dict(cls, dict_data: Dict[str, Any]) -&gt; Any:\n  \"\"\"Convert from dictionary data to an object.\"\"\"\n  instance = cls.__new__(cls)\n  instance.__dict__ = dict_data\n  return instance\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.Evaluator.to_json_dict","title":"to_json_dict","text":"<pre><code>to_json_dict() -&gt; Dict[str, Any]\n</code></pre> <p>Convert from an object to a JSON serializable dictionary.</p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef to_json_dict(self) -&gt; Dict[str, Any]:\n  \"\"\"Convert from an object to a JSON serializable dictionary.\"\"\"\n  return dict((k, v)\n              for k, v in self.__dict__.items()\n              if k not in ['_upstream_nodes', '_downstream_nodes'])\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.Evaluator.with_beam_pipeline_args","title":"with_beam_pipeline_args","text":"<pre><code>with_beam_pipeline_args(beam_pipeline_args: Iterable[Union[str, Placeholder]]) -&gt; BaseBeamComponent\n</code></pre> <p>Add per component Beam pipeline args.</p> PARAMETER DESCRIPTION <code>beam_pipeline_args</code> <p>List of Beam pipeline args to be added to the Beam executor spec.</p> <p> TYPE: <code>Iterable[Union[str, Placeholder]]</code> </p> RETURNS DESCRIPTION <code>BaseBeamComponent</code> <p>the same component itself.</p> Source code in <code>tfx/dsl/components/base/base_beam_component.py</code> <pre><code>def with_beam_pipeline_args(\n    self, beam_pipeline_args: Iterable[Union[str, placeholder.Placeholder]]\n) -&gt; 'BaseBeamComponent':\n  \"\"\"Add per component Beam pipeline args.\n\n  Args:\n    beam_pipeline_args: List of Beam pipeline args to be added to the Beam\n      executor spec.\n\n  Returns:\n    the same component itself.\n  \"\"\"\n  cast(executor_spec.BeamExecutorSpec,\n       self.executor_spec).add_beam_pipeline_args(beam_pipeline_args)\n  return self\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.Evaluator.with_platform_config","title":"with_platform_config","text":"<pre><code>with_platform_config(config: Message) -&gt; Self\n</code></pre> <p>Attaches a proto-form platform config to a component.</p> <p>The config will be a per-node platform-specific config.</p> PARAMETER DESCRIPTION <code>config</code> <p>platform config to attach to the component.</p> <p> TYPE: <code>Message</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>the same component itself.</p> Source code in <code>tfx/dsl/components/base/base_component.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef with_platform_config(\n    self, config: message.Message\n) -&gt; typing_extensions.Self:\n  \"\"\"Attaches a proto-form platform config to a component.\n\n  The config will be a per-node platform-specific config.\n\n  Args:\n    config: platform config to attach to the component.\n\n  Returns:\n    the same component itself.\n  \"\"\"\n  self.platform_config = config\n  return self\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.ExampleDiff","title":"ExampleDiff","text":"<pre><code>ExampleDiff(examples_test: BaseChannel, examples_base: BaseChannel, config: ExampleDiffConfig, include_split_pairs: Optional[List[Tuple[str, str]]] = None)\n</code></pre> <p>               Bases: <code>BaseBeamComponent</code></p> <p>TFX ExampleDiff component.</p> <p>Computes example level diffs according to an ExampleDiffConfig. See TFDV feature_skew_detector.py for more details.</p> <p>This executor is under development and may change.</p> <p>Construct an ExampleDiff component.</p> PARAMETER DESCRIPTION <code>examples_test</code> <p>A BaseChannel of <code>ExamplesPath</code> type, as generated by the ExampleGen component. This needs to contain any splits referenced in <code>include_split_pairs</code>.</p> <p> TYPE: <code>BaseChannel</code> </p> <code>examples_base</code> <p>A second BaseChannel of <code>ExamplesPath</code> type to which <code>examples</code> should be compared. This needs to contain any splits referenced in <code>include_split_pairs</code>.</p> <p> TYPE: <code>BaseChannel</code> </p> <code>config</code> <p>A ExampleDiffConfig that defines configuration for the skew detection pipeline.</p> <p> TYPE: <code>ExampleDiffConfig</code> </p> <code>include_split_pairs</code> <p>Pairs of split names that ExampleDiff should be run on. Default behavior if not supplied is to run on all pairs. Order is (test, base) with respect to examples_test, examples_base.</p> <p> TYPE: <code>Optional[List[Tuple[str, str]]]</code> DEFAULT: <code>None</code> </p> METHOD DESCRIPTION <code>add_downstream_node</code> <p>Experimental: Add another component that must run after this one.</p> <code>add_downstream_nodes</code> <p>Experimental: Add another component that must run after this one.</p> <code>add_upstream_node</code> <p>Experimental: Add another component that must run before this one.</p> <code>add_upstream_nodes</code> <p>Experimental: Add components that must run before this one.</p> <code>from_json_dict</code> <p>Convert from dictionary data to an object.</p> <code>to_json_dict</code> <p>Convert from an object to a JSON serializable dictionary.</p> <code>with_beam_pipeline_args</code> <p>Add per component Beam pipeline args.</p> <code>with_platform_config</code> <p>Attaches a proto-form platform config to a component.</p> ATTRIBUTE DESCRIPTION <code>id</code> <p>Node id, unique across all TFX nodes in a pipeline.</p> <p> TYPE: <code>str</code> </p> <code>outputs</code> <p>Component's output channel dict.</p> <p> TYPE: <code>Dict[str, OutputChannel]</code> </p> Source code in <code>tfx/components/example_diff/component.py</code> <pre><code>def __init__(self,\n             examples_test: types.BaseChannel,\n             examples_base: types.BaseChannel,\n             config: example_diff_pb2.ExampleDiffConfig,\n             include_split_pairs: Optional[List[Tuple[str, str]]] = None):\n  \"\"\"Construct an ExampleDiff component.\n\n  Args:\n    examples_test: A [BaseChannel][tfx.v1.types.BaseChannel] of `ExamplesPath` type, as generated by the\n      [ExampleGen component](../../../guide/examplegen).\n      This needs to contain any splits referenced in `include_split_pairs`.\n    examples_base: A second [BaseChannel][tfx.v1.types.BaseChannel] of `ExamplesPath` type to which\n      `examples` should be compared. This needs to contain any splits\n      referenced in `include_split_pairs`.\n    config: A ExampleDiffConfig that defines configuration for the skew\n      detection pipeline.\n    include_split_pairs: Pairs of split names that ExampleDiff should be run\n      on. Default behavior if not supplied is to run on all pairs. Order is\n      (test, base) with respect to examples_test, examples_base.\n  \"\"\"\n  if include_split_pairs is None:\n    logging.info('Including all split pairs because include_split_pairs is '\n                 'not set.')\n  diffs = types.Channel(type=standard_artifacts.ExamplesDiff)\n  spec = standard_component_specs.ExampleDiffSpec(\n      **{\n          standard_component_specs.EXAMPLES_KEY:\n              examples_test,\n          standard_component_specs.BASELINE_EXAMPLES_KEY:\n              examples_base,\n          standard_component_specs.INCLUDE_SPLIT_PAIRS_KEY:\n              json_utils.dumps(include_split_pairs),\n          standard_component_specs.EXAMPLE_DIFF_RESULT_KEY:\n              diffs,\n          standard_component_specs.EXAMPLE_DIFF_CONFIG_KEY: config\n      })\n  super().__init__(spec=spec)\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.ExampleDiff-attributes","title":"Attributes","text":""},{"location":"api/v1/components/#tfx.v1.components.ExampleDiff.id","title":"id  <code>property</code> <code>writable</code>","text":"<pre><code>id: str\n</code></pre> <p>Node id, unique across all TFX nodes in a pipeline.</p> <p>If <code>id</code> is set by the user, return it directly. Otherwise, return .</p> RETURNS DESCRIPTION <code>str</code> <p>node id.</p>"},{"location":"api/v1/components/#tfx.v1.components.ExampleDiff.outputs","title":"outputs  <code>property</code>","text":"<pre><code>outputs: Dict[str, OutputChannel]\n</code></pre> <p>Component's output channel dict.</p>"},{"location":"api/v1/components/#tfx.v1.components.ExampleDiff-functions","title":"Functions","text":""},{"location":"api/v1/components/#tfx.v1.components.ExampleDiff.add_downstream_node","title":"add_downstream_node","text":"<pre><code>add_downstream_node(downstream_node)\n</code></pre> <p>Experimental: Add another component that must run after this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> <p>It is symmetric with <code>add_upstream_node</code>.</p> PARAMETER DESCRIPTION <code>downstream_node</code> <p>a component that must run after this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_downstream_node(self, downstream_node):\n  \"\"\"Experimental: Add another component that must run after this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n  It is symmetric with `add_upstream_node`.\n\n  Args:\n    downstream_node: a component that must run after this node.\n  \"\"\"\n  self._downstream_nodes.add(downstream_node)\n  if self not in downstream_node.upstream_nodes:\n    downstream_node.add_upstream_node(self)\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.ExampleDiff.add_downstream_nodes","title":"add_downstream_nodes","text":"<pre><code>add_downstream_nodes(downstream_nodes)\n</code></pre> <p>Experimental: Add another component that must run after this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> <p>It is symmetric with <code>add_upstream_nodes</code>.</p> PARAMETER DESCRIPTION <code>downstream_nodes</code> <p>a list of components that must run after this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_downstream_nodes(self, downstream_nodes):\n  \"\"\"Experimental: Add another component that must run after this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n  It is symmetric with `add_upstream_nodes`.\n\n  Args:\n    downstream_nodes: a list of components that must run after this node.\n  \"\"\"\n  self._downstream_nodes.update(downstream_nodes)\n  for downstream_node in downstream_nodes:\n    if self not in downstream_node.upstream_nodes:\n      downstream_node.add_upstream_node(self)\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.ExampleDiff.add_upstream_node","title":"add_upstream_node","text":"<pre><code>add_upstream_node(upstream_node)\n</code></pre> <p>Experimental: Add another component that must run before this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> <p>It is symmetric with <code>add_downstream_node</code>.</p> PARAMETER DESCRIPTION <code>upstream_node</code> <p>a component that must run before this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_upstream_node(self, upstream_node):\n  \"\"\"Experimental: Add another component that must run before this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n  It is symmetric with `add_downstream_node`.\n\n  Args:\n    upstream_node: a component that must run before this node.\n  \"\"\"\n  self._upstream_nodes.add(upstream_node)\n  if self not in upstream_node.downstream_nodes:\n    upstream_node.add_downstream_node(self)\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.ExampleDiff.add_upstream_nodes","title":"add_upstream_nodes","text":"<pre><code>add_upstream_nodes(upstream_nodes)\n</code></pre> <p>Experimental: Add components that must run before this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> PARAMETER DESCRIPTION <code>upstream_nodes</code> <p>a list of components that must run before this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_upstream_nodes(self, upstream_nodes):\n  \"\"\"Experimental: Add components that must run before this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n\n  Args:\n    upstream_nodes: a list of components that must run before this node.\n  \"\"\"\n  self._upstream_nodes.update(upstream_nodes)\n  for upstream_node in upstream_nodes:\n    if self not in upstream_node.downstream_nodes:\n      upstream_node.add_downstream_node(self)\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.ExampleDiff.from_json_dict","title":"from_json_dict  <code>classmethod</code>","text":"<pre><code>from_json_dict(dict_data: Dict[str, Any]) -&gt; Any\n</code></pre> <p>Convert from dictionary data to an object.</p> Source code in <code>tfx/utils/json_utils.py</code> <pre><code>@classmethod\n@doc_controls.do_not_doc_in_subclasses\ndef from_json_dict(cls, dict_data: Dict[str, Any]) -&gt; Any:\n  \"\"\"Convert from dictionary data to an object.\"\"\"\n  instance = cls.__new__(cls)\n  instance.__dict__ = dict_data\n  return instance\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.ExampleDiff.to_json_dict","title":"to_json_dict","text":"<pre><code>to_json_dict() -&gt; Dict[str, Any]\n</code></pre> <p>Convert from an object to a JSON serializable dictionary.</p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef to_json_dict(self) -&gt; Dict[str, Any]:\n  \"\"\"Convert from an object to a JSON serializable dictionary.\"\"\"\n  return dict((k, v)\n              for k, v in self.__dict__.items()\n              if k not in ['_upstream_nodes', '_downstream_nodes'])\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.ExampleDiff.with_beam_pipeline_args","title":"with_beam_pipeline_args","text":"<pre><code>with_beam_pipeline_args(beam_pipeline_args: Iterable[Union[str, Placeholder]]) -&gt; BaseBeamComponent\n</code></pre> <p>Add per component Beam pipeline args.</p> PARAMETER DESCRIPTION <code>beam_pipeline_args</code> <p>List of Beam pipeline args to be added to the Beam executor spec.</p> <p> TYPE: <code>Iterable[Union[str, Placeholder]]</code> </p> RETURNS DESCRIPTION <code>BaseBeamComponent</code> <p>the same component itself.</p> Source code in <code>tfx/dsl/components/base/base_beam_component.py</code> <pre><code>def with_beam_pipeline_args(\n    self, beam_pipeline_args: Iterable[Union[str, placeholder.Placeholder]]\n) -&gt; 'BaseBeamComponent':\n  \"\"\"Add per component Beam pipeline args.\n\n  Args:\n    beam_pipeline_args: List of Beam pipeline args to be added to the Beam\n      executor spec.\n\n  Returns:\n    the same component itself.\n  \"\"\"\n  cast(executor_spec.BeamExecutorSpec,\n       self.executor_spec).add_beam_pipeline_args(beam_pipeline_args)\n  return self\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.ExampleDiff.with_platform_config","title":"with_platform_config","text":"<pre><code>with_platform_config(config: Message) -&gt; Self\n</code></pre> <p>Attaches a proto-form platform config to a component.</p> <p>The config will be a per-node platform-specific config.</p> PARAMETER DESCRIPTION <code>config</code> <p>platform config to attach to the component.</p> <p> TYPE: <code>Message</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>the same component itself.</p> Source code in <code>tfx/dsl/components/base/base_component.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef with_platform_config(\n    self, config: message.Message\n) -&gt; typing_extensions.Self:\n  \"\"\"Attaches a proto-form platform config to a component.\n\n  The config will be a per-node platform-specific config.\n\n  Args:\n    config: platform config to attach to the component.\n\n  Returns:\n    the same component itself.\n  \"\"\"\n  self.platform_config = config\n  return self\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.ExampleValidator","title":"ExampleValidator","text":"<pre><code>ExampleValidator(statistics: BaseChannel, schema: BaseChannel, exclude_splits: Optional[List[str]] = None, custom_validation_config: Optional[CustomValidationConfig] = None)\n</code></pre> <p>               Bases: <code>BaseComponent</code></p> <p>A TFX component to validate input examples.</p> <p>The ExampleValidator component uses Tensorflow Data Validation to validate the statistics of some splits on input examples against a schema.</p> <p>The ExampleValidator component identifies anomalies in training and serving data. The component can be configured to detect different classes of anomalies in the data. It can:</p> <ul> <li>perform validity checks by comparing data statistics against a schema that   codifies expectations of the user.</li> <li>run custom validations based on an optional SQL-based config.</li> </ul>"},{"location":"api/v1/components/#tfx.v1.components.ExampleValidator--schema-based-example-validation","title":"Schema Based Example Validation","text":"<p>The ExampleValidator component identifies any anomalies in the example data by comparing data statistics computed by the StatisticsGen component against a schema. The schema codifies properties which the input data is expected to satisfy, and is provided and maintained by the user.</p> <p>Example</p> <pre><code># Performs anomaly detection based on statistics and data schema.\nvalidate_stats = ExampleValidator(\n    statistics=statistics_gen.outputs['statistics'],\n    schema=infer_schema.outputs['schema'])\n</code></pre> <p>Component <code>outputs</code> contains:</p> <ul> <li><code>anomalies</code>: Channel of type <code>standard_artifacts.ExampleAnomalies</code>.</li> </ul> <p>See the ExampleValidator guide for more details.</p> <p>Construct an ExampleValidator component.</p> PARAMETER DESCRIPTION <code>statistics</code> <p>A BaseChannel of type <code>standard_artifacts.ExampleStatistics</code>.</p> <p> TYPE: <code>BaseChannel</code> </p> <code>schema</code> <p>A BaseChannel of type [<code>standard_artifacts.Schema</code>]. required</p> <p> TYPE: <code>BaseChannel</code> </p> <code>exclude_splits</code> <p>Names of splits that the example validator should not validate. Default behavior (when exclude_splits is set to None) is excluding no splits.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>custom_validation_config</code> <p>Optional configuration for specifying SQL-based custom validations.</p> <p> TYPE: <code>Optional[CustomValidationConfig]</code> DEFAULT: <code>None</code> </p> METHOD DESCRIPTION <code>add_downstream_node</code> <p>Experimental: Add another component that must run after this one.</p> <code>add_downstream_nodes</code> <p>Experimental: Add another component that must run after this one.</p> <code>add_upstream_node</code> <p>Experimental: Add another component that must run before this one.</p> <code>add_upstream_nodes</code> <p>Experimental: Add components that must run before this one.</p> <code>from_json_dict</code> <p>Convert from dictionary data to an object.</p> <code>to_json_dict</code> <p>Convert from an object to a JSON serializable dictionary.</p> <code>with_platform_config</code> <p>Attaches a proto-form platform config to a component.</p> ATTRIBUTE DESCRIPTION <code>id</code> <p>Node id, unique across all TFX nodes in a pipeline.</p> <p> TYPE: <code>str</code> </p> <code>outputs</code> <p>Component's output channel dict.</p> <p> TYPE: <code>Dict[str, OutputChannel]</code> </p> Source code in <code>tfx/components/example_validator/component.py</code> <pre><code>def __init__(self,\n             statistics: types.BaseChannel,\n             schema: types.BaseChannel,\n             exclude_splits: Optional[List[str]] = None,\n             custom_validation_config: Optional[\n                 custom_validation_config_pb2.CustomValidationConfig] = None):\n  \"\"\"Construct an ExampleValidator component.\n\n  Args:\n    statistics: A [BaseChannel][tfx.v1.types.BaseChannel] of type [`standard_artifacts.ExampleStatistics`][tfx.v1.types.standard_artifacts.ExampleStatistics].\n    schema: A [BaseChannel][tfx.v1.types.BaseChannel] of type [`standard_artifacts.Schema`]. _required_\n    exclude_splits: Names of splits that the example validator should not\n      validate. Default behavior (when exclude_splits is set to None) is\n      excluding no splits.\n    custom_validation_config: Optional configuration for specifying SQL-based\n      custom validations.\n  \"\"\"\n  if exclude_splits is None:\n    exclude_splits = []\n    logging.info('Excluding no splits because exclude_splits is not set.')\n  anomalies = types.Channel(type=standard_artifacts.ExampleAnomalies)\n  spec = standard_component_specs.ExampleValidatorSpec(\n      statistics=statistics,\n      schema=schema,\n      exclude_splits=json_utils.dumps(exclude_splits),\n      custom_validation_config=custom_validation_config,\n      anomalies=anomalies)\n  super().__init__(spec=spec)\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.ExampleValidator-attributes","title":"Attributes","text":""},{"location":"api/v1/components/#tfx.v1.components.ExampleValidator.id","title":"id  <code>property</code> <code>writable</code>","text":"<pre><code>id: str\n</code></pre> <p>Node id, unique across all TFX nodes in a pipeline.</p> <p>If <code>id</code> is set by the user, return it directly. Otherwise, return .</p> RETURNS DESCRIPTION <code>str</code> <p>node id.</p>"},{"location":"api/v1/components/#tfx.v1.components.ExampleValidator.outputs","title":"outputs  <code>property</code>","text":"<pre><code>outputs: Dict[str, OutputChannel]\n</code></pre> <p>Component's output channel dict.</p>"},{"location":"api/v1/components/#tfx.v1.components.ExampleValidator-functions","title":"Functions","text":""},{"location":"api/v1/components/#tfx.v1.components.ExampleValidator.add_downstream_node","title":"add_downstream_node","text":"<pre><code>add_downstream_node(downstream_node)\n</code></pre> <p>Experimental: Add another component that must run after this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> <p>It is symmetric with <code>add_upstream_node</code>.</p> PARAMETER DESCRIPTION <code>downstream_node</code> <p>a component that must run after this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_downstream_node(self, downstream_node):\n  \"\"\"Experimental: Add another component that must run after this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n  It is symmetric with `add_upstream_node`.\n\n  Args:\n    downstream_node: a component that must run after this node.\n  \"\"\"\n  self._downstream_nodes.add(downstream_node)\n  if self not in downstream_node.upstream_nodes:\n    downstream_node.add_upstream_node(self)\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.ExampleValidator.add_downstream_nodes","title":"add_downstream_nodes","text":"<pre><code>add_downstream_nodes(downstream_nodes)\n</code></pre> <p>Experimental: Add another component that must run after this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> <p>It is symmetric with <code>add_upstream_nodes</code>.</p> PARAMETER DESCRIPTION <code>downstream_nodes</code> <p>a list of components that must run after this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_downstream_nodes(self, downstream_nodes):\n  \"\"\"Experimental: Add another component that must run after this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n  It is symmetric with `add_upstream_nodes`.\n\n  Args:\n    downstream_nodes: a list of components that must run after this node.\n  \"\"\"\n  self._downstream_nodes.update(downstream_nodes)\n  for downstream_node in downstream_nodes:\n    if self not in downstream_node.upstream_nodes:\n      downstream_node.add_upstream_node(self)\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.ExampleValidator.add_upstream_node","title":"add_upstream_node","text":"<pre><code>add_upstream_node(upstream_node)\n</code></pre> <p>Experimental: Add another component that must run before this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> <p>It is symmetric with <code>add_downstream_node</code>.</p> PARAMETER DESCRIPTION <code>upstream_node</code> <p>a component that must run before this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_upstream_node(self, upstream_node):\n  \"\"\"Experimental: Add another component that must run before this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n  It is symmetric with `add_downstream_node`.\n\n  Args:\n    upstream_node: a component that must run before this node.\n  \"\"\"\n  self._upstream_nodes.add(upstream_node)\n  if self not in upstream_node.downstream_nodes:\n    upstream_node.add_downstream_node(self)\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.ExampleValidator.add_upstream_nodes","title":"add_upstream_nodes","text":"<pre><code>add_upstream_nodes(upstream_nodes)\n</code></pre> <p>Experimental: Add components that must run before this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> PARAMETER DESCRIPTION <code>upstream_nodes</code> <p>a list of components that must run before this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_upstream_nodes(self, upstream_nodes):\n  \"\"\"Experimental: Add components that must run before this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n\n  Args:\n    upstream_nodes: a list of components that must run before this node.\n  \"\"\"\n  self._upstream_nodes.update(upstream_nodes)\n  for upstream_node in upstream_nodes:\n    if self not in upstream_node.downstream_nodes:\n      upstream_node.add_downstream_node(self)\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.ExampleValidator.from_json_dict","title":"from_json_dict  <code>classmethod</code>","text":"<pre><code>from_json_dict(dict_data: Dict[str, Any]) -&gt; Any\n</code></pre> <p>Convert from dictionary data to an object.</p> Source code in <code>tfx/utils/json_utils.py</code> <pre><code>@classmethod\n@doc_controls.do_not_doc_in_subclasses\ndef from_json_dict(cls, dict_data: Dict[str, Any]) -&gt; Any:\n  \"\"\"Convert from dictionary data to an object.\"\"\"\n  instance = cls.__new__(cls)\n  instance.__dict__ = dict_data\n  return instance\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.ExampleValidator.to_json_dict","title":"to_json_dict","text":"<pre><code>to_json_dict() -&gt; Dict[str, Any]\n</code></pre> <p>Convert from an object to a JSON serializable dictionary.</p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef to_json_dict(self) -&gt; Dict[str, Any]:\n  \"\"\"Convert from an object to a JSON serializable dictionary.\"\"\"\n  return dict((k, v)\n              for k, v in self.__dict__.items()\n              if k not in ['_upstream_nodes', '_downstream_nodes'])\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.ExampleValidator.with_platform_config","title":"with_platform_config","text":"<pre><code>with_platform_config(config: Message) -&gt; Self\n</code></pre> <p>Attaches a proto-form platform config to a component.</p> <p>The config will be a per-node platform-specific config.</p> PARAMETER DESCRIPTION <code>config</code> <p>platform config to attach to the component.</p> <p> TYPE: <code>Message</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>the same component itself.</p> Source code in <code>tfx/dsl/components/base/base_component.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef with_platform_config(\n    self, config: message.Message\n) -&gt; typing_extensions.Self:\n  \"\"\"Attaches a proto-form platform config to a component.\n\n  The config will be a per-node platform-specific config.\n\n  Args:\n    config: platform config to attach to the component.\n\n  Returns:\n    the same component itself.\n  \"\"\"\n  self.platform_config = config\n  return self\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.FnArgs","title":"FnArgs","text":"<p>Args to pass to user defined training/tuning function(s).</p> ATTRIBUTE DESCRIPTION <code>working_dir</code> <p>Working dir.</p> <p> </p> <code>train_files</code> <p>A list of patterns for train files.</p> <p> </p> <code>eval_files</code> <p>A list of patterns for eval files.</p> <p> </p> <code>train_steps</code> <p>Number of train steps.</p> <p> </p> <code>eval_steps</code> <p>Number of eval steps.</p> <p> </p> <code>schema_path</code> <p>A single uri for schema file. Will be None if not specified.</p> <p> </p> <code>schema_file</code> <p>Deprecated, use <code>schema_path</code> instead.</p> <p> </p> <code>transform_graph_path</code> <p>An optional single uri for transform graph produced by TFT. Will be None if not specified.</p> <p> </p> <code>transform_output</code> <p>Deprecated, use <code>transform_graph_path</code> instead.</p> <p> </p> <code>data_accessor</code> <p>Contains factories that can create tf.data.Datasets or other means to access the train/eval data. They provide a uniform way of accessing data, regardless of how the data is stored on disk.</p> <p> </p> <code>serving_model_dir</code> <p>A single uri for the output directory of the serving model.</p> <p> </p> <code>eval_model_dir</code> <p>A single uri for the output directory of the eval model. Note that this is estimator only, Keras doesn't require it for TFMA.</p> <p> </p> <code>model_run_dir</code> <p>A single uri for the output directory of model training related files.</p> <p> </p> <code>base_model</code> <p>An optional base model path that will be used for this training.</p> <p> </p> <code>hyperparameters</code> <p>An optional keras_tuner.HyperParameters config.</p> <p> </p> <code>custom_config</code> <p>An optional dictionary passed to the component.</p> <p> </p>"},{"location":"api/v1/components/#tfx.v1.components.ImportExampleGen","title":"ImportExampleGen","text":"<pre><code>ImportExampleGen(input_base: Optional[str] = None, input_config: Optional[Union[Input, RuntimeParameter]] = None, output_config: Optional[Union[Output, RuntimeParameter]] = None, range_config: Optional[Union[RangeConfig, RuntimeParameter]] = None, payload_format: Optional[int] = FORMAT_TF_EXAMPLE)\n</code></pre> <p>               Bases: <code>FileBasedExampleGen</code></p> <p>Official TFX ImportExampleGen component.</p> <p>The ImportExampleGen component takes TFRecord files with TF Example data format, and generates train and eval examples for downstream components. This component provides consistent and configurable partition, and it also shuffle the dataset for ML best practice.</p> <p>Component <code>outputs</code> contains:</p> <ul> <li><code>examples</code>: Channel of type <code>standard_artifacts.Examples</code> for output    train and eval examples.</li> </ul> <p>Construct an ImportExampleGen component.</p> PARAMETER DESCRIPTION <code>input_base</code> <p>an external directory containing the TFRecord files.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>input_config</code> <p>An example_gen_pb2.Input instance, providing input configuration. If unset, the files under input_base will be treated as a single split.</p> <p> TYPE: <code>Optional[Union[Input, RuntimeParameter]]</code> DEFAULT: <code>None</code> </p> <code>output_config</code> <p>An example_gen_pb2.Output instance, providing output configuration. If unset, default splits will be 'train' and 'eval' with size 2:1.</p> <p> TYPE: <code>Optional[Union[Output, RuntimeParameter]]</code> DEFAULT: <code>None</code> </p> <code>range_config</code> <p>An optional range_config_pb2.RangeConfig instance, specifying the range of span values to consider. If unset, driver will default to searching for latest span with no restrictions.</p> <p> TYPE: <code>Optional[Union[RangeConfig, RuntimeParameter]]</code> DEFAULT: <code>None</code> </p> <code>payload_format</code> <p>Payload format of input data. Should be one of example_gen_pb2.PayloadFormat enum. Note that payload format of output data is the same as input.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>FORMAT_TF_EXAMPLE</code> </p> METHOD DESCRIPTION <code>add_downstream_node</code> <p>Experimental: Add another component that must run after this one.</p> <code>add_downstream_nodes</code> <p>Experimental: Add another component that must run after this one.</p> <code>add_upstream_node</code> <p>Experimental: Add another component that must run before this one.</p> <code>add_upstream_nodes</code> <p>Experimental: Add components that must run before this one.</p> <code>from_json_dict</code> <p>Convert from dictionary data to an object.</p> <code>to_json_dict</code> <p>Convert from an object to a JSON serializable dictionary.</p> <code>with_beam_pipeline_args</code> <p>Add per component Beam pipeline args.</p> <code>with_platform_config</code> <p>Attaches a proto-form platform config to a component.</p> ATTRIBUTE DESCRIPTION <code>id</code> <p>Node id, unique across all TFX nodes in a pipeline.</p> <p> TYPE: <code>str</code> </p> <code>outputs</code> <p>Component's output channel dict.</p> <p> TYPE: <code>Dict[str, OutputChannel]</code> </p> Source code in <code>tfx/components/example_gen/import_example_gen/component.py</code> <pre><code>def __init__(\n    self,\n    input_base: Optional[str] = None,\n    input_config: Optional[Union[example_gen_pb2.Input,\n                                 data_types.RuntimeParameter]] = None,\n    output_config: Optional[Union[example_gen_pb2.Output,\n                                  data_types.RuntimeParameter]] = None,\n    range_config: Optional[Union[range_config_pb2.RangeConfig,\n                                 data_types.RuntimeParameter]] = None,\n    payload_format: Optional[int] = example_gen_pb2.FORMAT_TF_EXAMPLE):\n  \"\"\"Construct an ImportExampleGen component.\n\n  Args:\n    input_base: an external directory containing the TFRecord files.\n    input_config: An example_gen_pb2.Input instance, providing input\n      configuration. If unset, the files under input_base will be treated as a\n      single split.\n    output_config: An example_gen_pb2.Output instance, providing output\n      configuration. If unset, default splits will be 'train' and 'eval' with\n      size 2:1.\n    range_config: An optional range_config_pb2.RangeConfig instance,\n      specifying the range of span values to consider. If unset, driver will\n      default to searching for latest span with no restrictions.\n    payload_format: Payload format of input data. Should be one of\n      example_gen_pb2.PayloadFormat enum. Note that payload format of output\n      data is the same as input.\n  \"\"\"\n  super().__init__(\n      input_base=input_base,\n      input_config=input_config,\n      output_config=output_config,\n      range_config=range_config,\n      output_data_format=payload_format)\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.ImportExampleGen-attributes","title":"Attributes","text":""},{"location":"api/v1/components/#tfx.v1.components.ImportExampleGen.id","title":"id  <code>property</code> <code>writable</code>","text":"<pre><code>id: str\n</code></pre> <p>Node id, unique across all TFX nodes in a pipeline.</p> <p>If <code>id</code> is set by the user, return it directly. Otherwise, return .</p> RETURNS DESCRIPTION <code>str</code> <p>node id.</p>"},{"location":"api/v1/components/#tfx.v1.components.ImportExampleGen.outputs","title":"outputs  <code>property</code>","text":"<pre><code>outputs: Dict[str, OutputChannel]\n</code></pre> <p>Component's output channel dict.</p>"},{"location":"api/v1/components/#tfx.v1.components.ImportExampleGen-functions","title":"Functions","text":""},{"location":"api/v1/components/#tfx.v1.components.ImportExampleGen.add_downstream_node","title":"add_downstream_node","text":"<pre><code>add_downstream_node(downstream_node)\n</code></pre> <p>Experimental: Add another component that must run after this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> <p>It is symmetric with <code>add_upstream_node</code>.</p> PARAMETER DESCRIPTION <code>downstream_node</code> <p>a component that must run after this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_downstream_node(self, downstream_node):\n  \"\"\"Experimental: Add another component that must run after this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n  It is symmetric with `add_upstream_node`.\n\n  Args:\n    downstream_node: a component that must run after this node.\n  \"\"\"\n  self._downstream_nodes.add(downstream_node)\n  if self not in downstream_node.upstream_nodes:\n    downstream_node.add_upstream_node(self)\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.ImportExampleGen.add_downstream_nodes","title":"add_downstream_nodes","text":"<pre><code>add_downstream_nodes(downstream_nodes)\n</code></pre> <p>Experimental: Add another component that must run after this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> <p>It is symmetric with <code>add_upstream_nodes</code>.</p> PARAMETER DESCRIPTION <code>downstream_nodes</code> <p>a list of components that must run after this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_downstream_nodes(self, downstream_nodes):\n  \"\"\"Experimental: Add another component that must run after this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n  It is symmetric with `add_upstream_nodes`.\n\n  Args:\n    downstream_nodes: a list of components that must run after this node.\n  \"\"\"\n  self._downstream_nodes.update(downstream_nodes)\n  for downstream_node in downstream_nodes:\n    if self not in downstream_node.upstream_nodes:\n      downstream_node.add_upstream_node(self)\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.ImportExampleGen.add_upstream_node","title":"add_upstream_node","text":"<pre><code>add_upstream_node(upstream_node)\n</code></pre> <p>Experimental: Add another component that must run before this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> <p>It is symmetric with <code>add_downstream_node</code>.</p> PARAMETER DESCRIPTION <code>upstream_node</code> <p>a component that must run before this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_upstream_node(self, upstream_node):\n  \"\"\"Experimental: Add another component that must run before this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n  It is symmetric with `add_downstream_node`.\n\n  Args:\n    upstream_node: a component that must run before this node.\n  \"\"\"\n  self._upstream_nodes.add(upstream_node)\n  if self not in upstream_node.downstream_nodes:\n    upstream_node.add_downstream_node(self)\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.ImportExampleGen.add_upstream_nodes","title":"add_upstream_nodes","text":"<pre><code>add_upstream_nodes(upstream_nodes)\n</code></pre> <p>Experimental: Add components that must run before this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> PARAMETER DESCRIPTION <code>upstream_nodes</code> <p>a list of components that must run before this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_upstream_nodes(self, upstream_nodes):\n  \"\"\"Experimental: Add components that must run before this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n\n  Args:\n    upstream_nodes: a list of components that must run before this node.\n  \"\"\"\n  self._upstream_nodes.update(upstream_nodes)\n  for upstream_node in upstream_nodes:\n    if self not in upstream_node.downstream_nodes:\n      upstream_node.add_downstream_node(self)\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.ImportExampleGen.from_json_dict","title":"from_json_dict  <code>classmethod</code>","text":"<pre><code>from_json_dict(dict_data: Dict[str, Any]) -&gt; Any\n</code></pre> <p>Convert from dictionary data to an object.</p> Source code in <code>tfx/utils/json_utils.py</code> <pre><code>@classmethod\n@doc_controls.do_not_doc_in_subclasses\ndef from_json_dict(cls, dict_data: Dict[str, Any]) -&gt; Any:\n  \"\"\"Convert from dictionary data to an object.\"\"\"\n  instance = cls.__new__(cls)\n  instance.__dict__ = dict_data\n  return instance\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.ImportExampleGen.to_json_dict","title":"to_json_dict","text":"<pre><code>to_json_dict() -&gt; Dict[str, Any]\n</code></pre> <p>Convert from an object to a JSON serializable dictionary.</p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef to_json_dict(self) -&gt; Dict[str, Any]:\n  \"\"\"Convert from an object to a JSON serializable dictionary.\"\"\"\n  return dict((k, v)\n              for k, v in self.__dict__.items()\n              if k not in ['_upstream_nodes', '_downstream_nodes'])\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.ImportExampleGen.with_beam_pipeline_args","title":"with_beam_pipeline_args","text":"<pre><code>with_beam_pipeline_args(beam_pipeline_args: Iterable[Union[str, Placeholder]]) -&gt; BaseBeamComponent\n</code></pre> <p>Add per component Beam pipeline args.</p> PARAMETER DESCRIPTION <code>beam_pipeline_args</code> <p>List of Beam pipeline args to be added to the Beam executor spec.</p> <p> TYPE: <code>Iterable[Union[str, Placeholder]]</code> </p> RETURNS DESCRIPTION <code>BaseBeamComponent</code> <p>the same component itself.</p> Source code in <code>tfx/dsl/components/base/base_beam_component.py</code> <pre><code>def with_beam_pipeline_args(\n    self, beam_pipeline_args: Iterable[Union[str, placeholder.Placeholder]]\n) -&gt; 'BaseBeamComponent':\n  \"\"\"Add per component Beam pipeline args.\n\n  Args:\n    beam_pipeline_args: List of Beam pipeline args to be added to the Beam\n      executor spec.\n\n  Returns:\n    the same component itself.\n  \"\"\"\n  cast(executor_spec.BeamExecutorSpec,\n       self.executor_spec).add_beam_pipeline_args(beam_pipeline_args)\n  return self\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.ImportExampleGen.with_platform_config","title":"with_platform_config","text":"<pre><code>with_platform_config(config: Message) -&gt; Self\n</code></pre> <p>Attaches a proto-form platform config to a component.</p> <p>The config will be a per-node platform-specific config.</p> PARAMETER DESCRIPTION <code>config</code> <p>platform config to attach to the component.</p> <p> TYPE: <code>Message</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>the same component itself.</p> Source code in <code>tfx/dsl/components/base/base_component.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef with_platform_config(\n    self, config: message.Message\n) -&gt; typing_extensions.Self:\n  \"\"\"Attaches a proto-form platform config to a component.\n\n  The config will be a per-node platform-specific config.\n\n  Args:\n    config: platform config to attach to the component.\n\n  Returns:\n    the same component itself.\n  \"\"\"\n  self.platform_config = config\n  return self\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.ImportSchemaGen","title":"ImportSchemaGen","text":"<pre><code>ImportSchemaGen(schema_file: str)\n</code></pre> <p>               Bases: <code>BaseComponent</code></p> <p>A TFX ImportSchemaGen component to import a schema file into the pipeline.</p> <p>ImportSchemaGen is a specialized SchemaGen which imports a pre-defined schema file into the pipeline.</p> <p>In a typical TFX pipeline, users are expected to review the schemas generated with <code>SchemaGen</code> and store them in SCM or equivalent. Those schema files can be brought back to pipelines using ImportSchemaGen.</p> <p>Here is an example to use the ImportSchemaGen:</p> <pre><code>schema_gen = ImportSchemaGen(schema_file=schema_path)\n</code></pre> <p>Component <code>outputs</code> contains:</p> <ul> <li><code>schema</code>: Channel of type <code>standard_artifacts.Schema</code> for schema result.</li> </ul> <p>See the SchemaGen guide for more details.</p> <p>ImportSchemaGen works almost similar to <code>Importer</code> except following:</p> <ul> <li><code>schema_file</code> should be the full file path instead of directory holding it.</li> <li><code>schema_file</code> is copied to the output artifact. This is different from   <code>Importer</code> that loads an \"Artifact\" by setting its URI to the given path.</li> </ul> <p>Init function for the ImportSchemaGen.</p> PARAMETER DESCRIPTION <code>schema_file</code> <p>File path to the input schema file. This file will be copied to the output artifact which is generated inside the pipeline root directory.</p> <p> TYPE: <code>str</code> </p> METHOD DESCRIPTION <code>add_downstream_node</code> <p>Experimental: Add another component that must run after this one.</p> <code>add_downstream_nodes</code> <p>Experimental: Add another component that must run after this one.</p> <code>add_upstream_node</code> <p>Experimental: Add another component that must run before this one.</p> <code>add_upstream_nodes</code> <p>Experimental: Add components that must run before this one.</p> <code>from_json_dict</code> <p>Convert from dictionary data to an object.</p> <code>to_json_dict</code> <p>Convert from an object to a JSON serializable dictionary.</p> <code>with_platform_config</code> <p>Attaches a proto-form platform config to a component.</p> ATTRIBUTE DESCRIPTION <code>id</code> <p>Node id, unique across all TFX nodes in a pipeline.</p> <p> TYPE: <code>str</code> </p> <code>outputs</code> <p>Component's output channel dict.</p> <p> TYPE: <code>Dict[str, OutputChannel]</code> </p> Source code in <code>tfx/components/schema_gen/import_schema_gen/component.py</code> <pre><code>def __init__(self, schema_file: str):\n  \"\"\"Init function for the ImportSchemaGen.\n\n  Args:\n    schema_file: File path to the input schema file. This file will be copied\n      to the output artifact which is generated inside the pipeline root\n      directory.\n  \"\"\"\n  spec = standard_component_specs.ImportSchemaGenSpec(\n      schema_file=schema_file,\n      schema=types.Channel(type=standard_artifacts.Schema))\n  super().__init__(spec=spec)\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.ImportSchemaGen-attributes","title":"Attributes","text":""},{"location":"api/v1/components/#tfx.v1.components.ImportSchemaGen.id","title":"id  <code>property</code> <code>writable</code>","text":"<pre><code>id: str\n</code></pre> <p>Node id, unique across all TFX nodes in a pipeline.</p> <p>If <code>id</code> is set by the user, return it directly. Otherwise, return .</p> RETURNS DESCRIPTION <code>str</code> <p>node id.</p>"},{"location":"api/v1/components/#tfx.v1.components.ImportSchemaGen.outputs","title":"outputs  <code>property</code>","text":"<pre><code>outputs: Dict[str, OutputChannel]\n</code></pre> <p>Component's output channel dict.</p>"},{"location":"api/v1/components/#tfx.v1.components.ImportSchemaGen-functions","title":"Functions","text":""},{"location":"api/v1/components/#tfx.v1.components.ImportSchemaGen.add_downstream_node","title":"add_downstream_node","text":"<pre><code>add_downstream_node(downstream_node)\n</code></pre> <p>Experimental: Add another component that must run after this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> <p>It is symmetric with <code>add_upstream_node</code>.</p> PARAMETER DESCRIPTION <code>downstream_node</code> <p>a component that must run after this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_downstream_node(self, downstream_node):\n  \"\"\"Experimental: Add another component that must run after this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n  It is symmetric with `add_upstream_node`.\n\n  Args:\n    downstream_node: a component that must run after this node.\n  \"\"\"\n  self._downstream_nodes.add(downstream_node)\n  if self not in downstream_node.upstream_nodes:\n    downstream_node.add_upstream_node(self)\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.ImportSchemaGen.add_downstream_nodes","title":"add_downstream_nodes","text":"<pre><code>add_downstream_nodes(downstream_nodes)\n</code></pre> <p>Experimental: Add another component that must run after this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> <p>It is symmetric with <code>add_upstream_nodes</code>.</p> PARAMETER DESCRIPTION <code>downstream_nodes</code> <p>a list of components that must run after this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_downstream_nodes(self, downstream_nodes):\n  \"\"\"Experimental: Add another component that must run after this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n  It is symmetric with `add_upstream_nodes`.\n\n  Args:\n    downstream_nodes: a list of components that must run after this node.\n  \"\"\"\n  self._downstream_nodes.update(downstream_nodes)\n  for downstream_node in downstream_nodes:\n    if self not in downstream_node.upstream_nodes:\n      downstream_node.add_upstream_node(self)\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.ImportSchemaGen.add_upstream_node","title":"add_upstream_node","text":"<pre><code>add_upstream_node(upstream_node)\n</code></pre> <p>Experimental: Add another component that must run before this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> <p>It is symmetric with <code>add_downstream_node</code>.</p> PARAMETER DESCRIPTION <code>upstream_node</code> <p>a component that must run before this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_upstream_node(self, upstream_node):\n  \"\"\"Experimental: Add another component that must run before this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n  It is symmetric with `add_downstream_node`.\n\n  Args:\n    upstream_node: a component that must run before this node.\n  \"\"\"\n  self._upstream_nodes.add(upstream_node)\n  if self not in upstream_node.downstream_nodes:\n    upstream_node.add_downstream_node(self)\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.ImportSchemaGen.add_upstream_nodes","title":"add_upstream_nodes","text":"<pre><code>add_upstream_nodes(upstream_nodes)\n</code></pre> <p>Experimental: Add components that must run before this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> PARAMETER DESCRIPTION <code>upstream_nodes</code> <p>a list of components that must run before this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_upstream_nodes(self, upstream_nodes):\n  \"\"\"Experimental: Add components that must run before this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n\n  Args:\n    upstream_nodes: a list of components that must run before this node.\n  \"\"\"\n  self._upstream_nodes.update(upstream_nodes)\n  for upstream_node in upstream_nodes:\n    if self not in upstream_node.downstream_nodes:\n      upstream_node.add_downstream_node(self)\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.ImportSchemaGen.from_json_dict","title":"from_json_dict  <code>classmethod</code>","text":"<pre><code>from_json_dict(dict_data: Dict[str, Any]) -&gt; Any\n</code></pre> <p>Convert from dictionary data to an object.</p> Source code in <code>tfx/utils/json_utils.py</code> <pre><code>@classmethod\n@doc_controls.do_not_doc_in_subclasses\ndef from_json_dict(cls, dict_data: Dict[str, Any]) -&gt; Any:\n  \"\"\"Convert from dictionary data to an object.\"\"\"\n  instance = cls.__new__(cls)\n  instance.__dict__ = dict_data\n  return instance\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.ImportSchemaGen.to_json_dict","title":"to_json_dict","text":"<pre><code>to_json_dict() -&gt; Dict[str, Any]\n</code></pre> <p>Convert from an object to a JSON serializable dictionary.</p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef to_json_dict(self) -&gt; Dict[str, Any]:\n  \"\"\"Convert from an object to a JSON serializable dictionary.\"\"\"\n  return dict((k, v)\n              for k, v in self.__dict__.items()\n              if k not in ['_upstream_nodes', '_downstream_nodes'])\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.ImportSchemaGen.with_platform_config","title":"with_platform_config","text":"<pre><code>with_platform_config(config: Message) -&gt; Self\n</code></pre> <p>Attaches a proto-form platform config to a component.</p> <p>The config will be a per-node platform-specific config.</p> PARAMETER DESCRIPTION <code>config</code> <p>platform config to attach to the component.</p> <p> TYPE: <code>Message</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>the same component itself.</p> Source code in <code>tfx/dsl/components/base/base_component.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef with_platform_config(\n    self, config: message.Message\n) -&gt; typing_extensions.Self:\n  \"\"\"Attaches a proto-form platform config to a component.\n\n  The config will be a per-node platform-specific config.\n\n  Args:\n    config: platform config to attach to the component.\n\n  Returns:\n    the same component itself.\n  \"\"\"\n  self.platform_config = config\n  return self\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.InfraValidator","title":"InfraValidator","text":"<pre><code>InfraValidator(model: BaseChannel, serving_spec: ServingSpec, examples: Optional[BaseChannel] = None, request_spec: Optional[RequestSpec] = None, validation_spec: Optional[ValidationSpec] = None)\n</code></pre> <p>               Bases: <code>BaseComponent</code></p> <p>A TFX component to validate the model against the serving infrastructure.</p> <p>An infra validation is done by loading the model to the exactly same serving binary that is used in production, and additionaly sending some requests to the model server. Such requests can be specified from Examples artifact.</p>"},{"location":"api/v1/components/#tfx.v1.components.InfraValidator--examples","title":"Examples","text":"<p>Full example using TensorFlowServing binary running on local docker.</p> <pre><code>infra_validator = InfraValidator(\n    model=trainer.outputs['model'],\n    examples=test_example_gen.outputs['examples'],\n    serving_spec=ServingSpec(\n        tensorflow_serving=TensorFlowServing(  # Using TF Serving.\n            tags=['latest']\n        ),\n        local_docker=LocalDockerConfig(),  # Running on local docker.\n    ),\n    validation_spec=ValidationSpec(\n        max_loading_time_seconds=60,\n        num_tries=5,\n    ),\n    request_spec=RequestSpec(\n        tensorflow_serving=TensorFlowServingRequestSpec(),\n        num_examples=1,\n    )\n)\n</code></pre> <p>Minimal example when running on Kubernetes.</p> <pre><code>infra_validator = InfraValidator(\n    model=trainer.outputs['model'],\n    examples=test_example_gen.outputs['examples'],\n    serving_spec=ServingSpec(\n        tensorflow_serving=TensorFlowServing(\n            tags=['latest']\n        ),\n        kubernetes=KubernetesConfig(),  # Running on Kubernetes.\n    ),\n)\n</code></pre> <p>Component <code>outputs</code> contains:</p> <ul> <li><code>blessing</code>: Channel of type <code>standard_artifacts.InfraBlessing</code> that                contains the validation result.</li> </ul> <p>See the InfraValidator guide for more details.</p> <p>Construct a InfraValidator component.</p> PARAMETER DESCRIPTION <code>model</code> <p>A <code>BaseChannel</code> of <code>ModelExportPath</code> type, usually produced by Trainer component.   required</p> <p> TYPE: <code>BaseChannel</code> </p> <code>serving_spec</code> <p>A <code>ServingSpec</code> configuration about serving binary and test platform config to launch model server for validation. required</p> <p> TYPE: <code>ServingSpec</code> </p> <code>examples</code> <p>A <code>BaseChannel</code> of <code>ExamplesPath</code> type, usually produced by ExampleGen component.   If not specified, InfraValidator does not issue requests for   validation.</p> <p> TYPE: <code>Optional[BaseChannel]</code> DEFAULT: <code>None</code> </p> <code>request_spec</code> <p>Optional <code>RequestSpec</code> configuration about making requests from <code>examples</code> input. If not specified, InfraValidator does not issue requests for validation.</p> <p> TYPE: <code>Optional[RequestSpec]</code> DEFAULT: <code>None</code> </p> <code>validation_spec</code> <p>Optional <code>ValidationSpec</code> configuration.</p> <p> TYPE: <code>Optional[ValidationSpec]</code> DEFAULT: <code>None</code> </p> METHOD DESCRIPTION <code>add_downstream_node</code> <p>Experimental: Add another component that must run after this one.</p> <code>add_downstream_nodes</code> <p>Experimental: Add another component that must run after this one.</p> <code>add_upstream_node</code> <p>Experimental: Add another component that must run before this one.</p> <code>add_upstream_nodes</code> <p>Experimental: Add components that must run before this one.</p> <code>from_json_dict</code> <p>Convert from dictionary data to an object.</p> <code>to_json_dict</code> <p>Convert from an object to a JSON serializable dictionary.</p> <code>with_platform_config</code> <p>Attaches a proto-form platform config to a component.</p> ATTRIBUTE DESCRIPTION <code>id</code> <p>Node id, unique across all TFX nodes in a pipeline.</p> <p> TYPE: <code>str</code> </p> <code>outputs</code> <p>Component's output channel dict.</p> <p> TYPE: <code>Dict[str, OutputChannel]</code> </p> Source code in <code>tfx/components/infra_validator/component.py</code> <pre><code>def __init__(\n    self,\n    model: types.BaseChannel,\n    serving_spec: infra_validator_pb2.ServingSpec,\n    examples: Optional[types.BaseChannel] = None,\n    request_spec: Optional[infra_validator_pb2.RequestSpec] = None,\n    validation_spec: Optional[infra_validator_pb2.ValidationSpec] = None):\n  \"\"\"Construct a InfraValidator component.\n\n  Args:\n    model: A [`BaseChannel`][tfx.v1.types.BaseChannel] of `ModelExportPath` type, usually produced by\n      [Trainer](../../../guide/trainer) component.\n        _required_\n    serving_spec: A `ServingSpec` configuration about serving binary and test\n      platform config to launch model server for validation. _required_\n    examples: A [`BaseChannel`][tfx.v1.types.BaseChannel] of `ExamplesPath` type, usually produced by\n      [ExampleGen](../../../guide/examplegen) component.\n        If not specified, InfraValidator does not issue requests for\n        validation.\n    request_spec: Optional `RequestSpec` configuration about making requests\n      from `examples` input. If not specified, InfraValidator does not issue\n      requests for validation.\n    validation_spec: Optional `ValidationSpec` configuration.\n  \"\"\"\n  blessing = types.Channel(type=standard_artifacts.InfraBlessing)\n  spec = standard_component_specs.InfraValidatorSpec(\n      model=model,\n      examples=examples,\n      blessing=blessing,\n      serving_spec=serving_spec,\n      validation_spec=validation_spec,\n      request_spec=request_spec)\n  super().__init__(spec=spec)\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.InfraValidator-attributes","title":"Attributes","text":""},{"location":"api/v1/components/#tfx.v1.components.InfraValidator.id","title":"id  <code>property</code> <code>writable</code>","text":"<pre><code>id: str\n</code></pre> <p>Node id, unique across all TFX nodes in a pipeline.</p> <p>If <code>id</code> is set by the user, return it directly. Otherwise, return .</p> RETURNS DESCRIPTION <code>str</code> <p>node id.</p>"},{"location":"api/v1/components/#tfx.v1.components.InfraValidator.outputs","title":"outputs  <code>property</code>","text":"<pre><code>outputs: Dict[str, OutputChannel]\n</code></pre> <p>Component's output channel dict.</p>"},{"location":"api/v1/components/#tfx.v1.components.InfraValidator-functions","title":"Functions","text":""},{"location":"api/v1/components/#tfx.v1.components.InfraValidator.add_downstream_node","title":"add_downstream_node","text":"<pre><code>add_downstream_node(downstream_node)\n</code></pre> <p>Experimental: Add another component that must run after this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> <p>It is symmetric with <code>add_upstream_node</code>.</p> PARAMETER DESCRIPTION <code>downstream_node</code> <p>a component that must run after this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_downstream_node(self, downstream_node):\n  \"\"\"Experimental: Add another component that must run after this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n  It is symmetric with `add_upstream_node`.\n\n  Args:\n    downstream_node: a component that must run after this node.\n  \"\"\"\n  self._downstream_nodes.add(downstream_node)\n  if self not in downstream_node.upstream_nodes:\n    downstream_node.add_upstream_node(self)\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.InfraValidator.add_downstream_nodes","title":"add_downstream_nodes","text":"<pre><code>add_downstream_nodes(downstream_nodes)\n</code></pre> <p>Experimental: Add another component that must run after this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> <p>It is symmetric with <code>add_upstream_nodes</code>.</p> PARAMETER DESCRIPTION <code>downstream_nodes</code> <p>a list of components that must run after this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_downstream_nodes(self, downstream_nodes):\n  \"\"\"Experimental: Add another component that must run after this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n  It is symmetric with `add_upstream_nodes`.\n\n  Args:\n    downstream_nodes: a list of components that must run after this node.\n  \"\"\"\n  self._downstream_nodes.update(downstream_nodes)\n  for downstream_node in downstream_nodes:\n    if self not in downstream_node.upstream_nodes:\n      downstream_node.add_upstream_node(self)\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.InfraValidator.add_upstream_node","title":"add_upstream_node","text":"<pre><code>add_upstream_node(upstream_node)\n</code></pre> <p>Experimental: Add another component that must run before this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> <p>It is symmetric with <code>add_downstream_node</code>.</p> PARAMETER DESCRIPTION <code>upstream_node</code> <p>a component that must run before this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_upstream_node(self, upstream_node):\n  \"\"\"Experimental: Add another component that must run before this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n  It is symmetric with `add_downstream_node`.\n\n  Args:\n    upstream_node: a component that must run before this node.\n  \"\"\"\n  self._upstream_nodes.add(upstream_node)\n  if self not in upstream_node.downstream_nodes:\n    upstream_node.add_downstream_node(self)\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.InfraValidator.add_upstream_nodes","title":"add_upstream_nodes","text":"<pre><code>add_upstream_nodes(upstream_nodes)\n</code></pre> <p>Experimental: Add components that must run before this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> PARAMETER DESCRIPTION <code>upstream_nodes</code> <p>a list of components that must run before this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_upstream_nodes(self, upstream_nodes):\n  \"\"\"Experimental: Add components that must run before this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n\n  Args:\n    upstream_nodes: a list of components that must run before this node.\n  \"\"\"\n  self._upstream_nodes.update(upstream_nodes)\n  for upstream_node in upstream_nodes:\n    if self not in upstream_node.downstream_nodes:\n      upstream_node.add_downstream_node(self)\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.InfraValidator.from_json_dict","title":"from_json_dict  <code>classmethod</code>","text":"<pre><code>from_json_dict(dict_data: Dict[str, Any]) -&gt; Any\n</code></pre> <p>Convert from dictionary data to an object.</p> Source code in <code>tfx/utils/json_utils.py</code> <pre><code>@classmethod\n@doc_controls.do_not_doc_in_subclasses\ndef from_json_dict(cls, dict_data: Dict[str, Any]) -&gt; Any:\n  \"\"\"Convert from dictionary data to an object.\"\"\"\n  instance = cls.__new__(cls)\n  instance.__dict__ = dict_data\n  return instance\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.InfraValidator.to_json_dict","title":"to_json_dict","text":"<pre><code>to_json_dict() -&gt; Dict[str, Any]\n</code></pre> <p>Convert from an object to a JSON serializable dictionary.</p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef to_json_dict(self) -&gt; Dict[str, Any]:\n  \"\"\"Convert from an object to a JSON serializable dictionary.\"\"\"\n  return dict((k, v)\n              for k, v in self.__dict__.items()\n              if k not in ['_upstream_nodes', '_downstream_nodes'])\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.InfraValidator.with_platform_config","title":"with_platform_config","text":"<pre><code>with_platform_config(config: Message) -&gt; Self\n</code></pre> <p>Attaches a proto-form platform config to a component.</p> <p>The config will be a per-node platform-specific config.</p> PARAMETER DESCRIPTION <code>config</code> <p>platform config to attach to the component.</p> <p> TYPE: <code>Message</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>the same component itself.</p> Source code in <code>tfx/dsl/components/base/base_component.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef with_platform_config(\n    self, config: message.Message\n) -&gt; typing_extensions.Self:\n  \"\"\"Attaches a proto-form platform config to a component.\n\n  The config will be a per-node platform-specific config.\n\n  Args:\n    config: platform config to attach to the component.\n\n  Returns:\n    the same component itself.\n  \"\"\"\n  self.platform_config = config\n  return self\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.Pusher","title":"Pusher","text":"<pre><code>Pusher(model: Optional[BaseChannel] = None, model_blessing: Optional[BaseChannel] = None, infra_blessing: Optional[BaseChannel] = None, push_destination: Optional[Union[PushDestination, RuntimeParameter]] = None, custom_config: Optional[Dict[str, Any]] = None, custom_executor_spec: Optional[ExecutorSpec] = None)\n</code></pre> <p>               Bases: <code>BaseComponent</code></p> <p>A TFX component to push validated TensorFlow models to a model serving platform.</p> <p>The <code>Pusher</code> component can be used to push an validated SavedModel from output of the Trainer component to TensorFlow Serving.  The Pusher will check the validation results from the Evaluator component and InfraValidator component before deploying the model.  If the model has not been blessed, then the model will not be pushed.</p> <p>Note</p> <p>The executor for this component can be overriden to enable the model to be pushed to other serving platforms than tf.serving.  The Cloud AI Platform custom executor provides an example how to implement this.</p> <p>Example</p> <pre><code># Checks whether the model passed the validation steps and pushes the model\n# to a file destination if check passed.\npusher = Pusher(\n    model=trainer.outputs['model'],\n    model_blessing=evaluator.outputs['blessing'],\n    push_destination=proto.PushDestination(\n        filesystem=proto.PushDestination.Filesystem(\n            base_directory=serving_model_dir,\n        )\n    ),\n)\n</code></pre> <p>Component <code>outputs</code> contains:</p> <ul> <li><code>pushed_model</code>: Channel of type <code>standard_artifacts.PushedModel</code> with                    result of push.</li> </ul> <p>See the Pusher guide for more details.</p> <p>Construct a Pusher component.</p> PARAMETER DESCRIPTION <code>model</code> <p>An optional BaseChannel of type <code>standard_artifacts.Model</code>, usually produced by a Trainer component.</p> <p> TYPE: <code>Optional[BaseChannel]</code> DEFAULT: <code>None</code> </p> <code>model_blessing</code> <p>An optional BaseChannel of type <code>standard_artifacts.ModelBlessing</code>, usually produced from an Evaluator component.</p> <p> TYPE: <code>Optional[BaseChannel]</code> DEFAULT: <code>None</code> </p> <code>infra_blessing</code> <p>An optional BaseChannel of type <code>standard_artifacts.InfraBlessing</code>, usually produced from an InfraValidator component.</p> <p> TYPE: <code>Optional[BaseChannel]</code> DEFAULT: <code>None</code> </p> <code>push_destination</code> <p>A pusher_pb2.PushDestination instance, providing info for tensorflow serving to load models. Optional if executor_class doesn't require push_destination.</p> <p> TYPE: <code>Optional[Union[PushDestination, RuntimeParameter]]</code> DEFAULT: <code>None</code> </p> <code>custom_config</code> <p>A dict which contains the deployment job parameters to be passed to Cloud platforms.</p> <p> TYPE: <code>Optional[Dict[str, Any]]</code> DEFAULT: <code>None</code> </p> <code>custom_executor_spec</code> <p>Optional custom executor spec. Deprecated (no compatibility guarantee), please customize component directly.</p> <p> TYPE: <code>Optional[ExecutorSpec]</code> DEFAULT: <code>None</code> </p> METHOD DESCRIPTION <code>add_downstream_node</code> <p>Experimental: Add another component that must run after this one.</p> <code>add_downstream_nodes</code> <p>Experimental: Add another component that must run after this one.</p> <code>add_upstream_node</code> <p>Experimental: Add another component that must run before this one.</p> <code>add_upstream_nodes</code> <p>Experimental: Add components that must run before this one.</p> <code>from_json_dict</code> <p>Convert from dictionary data to an object.</p> <code>to_json_dict</code> <p>Convert from an object to a JSON serializable dictionary.</p> <code>with_platform_config</code> <p>Attaches a proto-form platform config to a component.</p> ATTRIBUTE DESCRIPTION <code>id</code> <p>Node id, unique across all TFX nodes in a pipeline.</p> <p> TYPE: <code>str</code> </p> <code>outputs</code> <p>Component's output channel dict.</p> <p> TYPE: <code>Dict[str, OutputChannel]</code> </p> Source code in <code>tfx/components/pusher/component.py</code> <pre><code>def __init__(\n    self,\n    model: Optional[types.BaseChannel] = None,\n    model_blessing: Optional[types.BaseChannel] = None,\n    infra_blessing: Optional[types.BaseChannel] = None,\n    push_destination: Optional[Union[pusher_pb2.PushDestination,\n                                     data_types.RuntimeParameter]] = None,\n    custom_config: Optional[Dict[str, Any]] = None,\n    custom_executor_spec: Optional[executor_spec.ExecutorSpec] = None):\n  \"\"\"Construct a Pusher component.\n\n  Args:\n    model: An optional [BaseChannel][tfx.v1.types.BaseChannel] of type `standard_artifacts.Model`, usually\n      produced by a [Trainer][tfx.v1.components.Trainer] component.\n    model_blessing: An optional [BaseChannel][tfx.v1.types.BaseChannel] of type\n      [`standard_artifacts.ModelBlessing`][tfx.v1.types.standard_artifacts.ModelBlessing],\n      usually produced from an [Evaluator][tfx.v1.components.Evaluator] component.\n    infra_blessing: An optional [BaseChannel][tfx.v1.types.BaseChannel] of type\n      [`standard_artifacts.InfraBlessing`][tfx.v1.types.standard_artifacts.InfraBlessing],\n      usually produced from an [InfraValidator][tfx.v1.components.InfraValidator] component.\n    push_destination: A pusher_pb2.PushDestination instance, providing info\n      for tensorflow serving to load models. Optional if executor_class\n      doesn't require push_destination.\n    custom_config: A dict which contains the deployment job parameters to be\n      passed to Cloud platforms.\n    custom_executor_spec: Optional custom executor spec. Deprecated (no\n      compatibility guarantee), please customize component directly.\n  \"\"\"\n  pushed_model = types.Channel(type=standard_artifacts.PushedModel)\n  if (push_destination is None and not custom_executor_spec and\n      self.EXECUTOR_SPEC.executor_class == executor.Executor):\n    raise ValueError('push_destination is required unless a '\n                     'custom_executor_spec is supplied that does not require '\n                     'it.')\n  if custom_executor_spec:\n    logging.warning(\n        '`custom_executor_spec` is deprecated. Please customize component directly.'\n    )\n  if model is None and infra_blessing is None:\n    raise ValueError(\n        'Either one of model or infra_blessing channel should be given. '\n        'If infra_blessing is used in place of model, it must have been '\n        'created with InfraValidator with RequestSpec.make_warmup = True. '\n        'This cannot be checked during pipeline construction time but will '\n        'raise runtime error if infra_blessing does not contain a model.')\n  spec = standard_component_specs.PusherSpec(\n      model=model,\n      model_blessing=model_blessing,\n      infra_blessing=infra_blessing,\n      push_destination=push_destination,\n      custom_config=json_utils.dumps(custom_config),\n      pushed_model=pushed_model)\n  super().__init__(spec=spec, custom_executor_spec=custom_executor_spec)\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.Pusher-attributes","title":"Attributes","text":""},{"location":"api/v1/components/#tfx.v1.components.Pusher.id","title":"id  <code>property</code> <code>writable</code>","text":"<pre><code>id: str\n</code></pre> <p>Node id, unique across all TFX nodes in a pipeline.</p> <p>If <code>id</code> is set by the user, return it directly. Otherwise, return .</p> RETURNS DESCRIPTION <code>str</code> <p>node id.</p>"},{"location":"api/v1/components/#tfx.v1.components.Pusher.outputs","title":"outputs  <code>property</code>","text":"<pre><code>outputs: Dict[str, OutputChannel]\n</code></pre> <p>Component's output channel dict.</p>"},{"location":"api/v1/components/#tfx.v1.components.Pusher-functions","title":"Functions","text":""},{"location":"api/v1/components/#tfx.v1.components.Pusher.add_downstream_node","title":"add_downstream_node","text":"<pre><code>add_downstream_node(downstream_node)\n</code></pre> <p>Experimental: Add another component that must run after this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> <p>It is symmetric with <code>add_upstream_node</code>.</p> PARAMETER DESCRIPTION <code>downstream_node</code> <p>a component that must run after this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_downstream_node(self, downstream_node):\n  \"\"\"Experimental: Add another component that must run after this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n  It is symmetric with `add_upstream_node`.\n\n  Args:\n    downstream_node: a component that must run after this node.\n  \"\"\"\n  self._downstream_nodes.add(downstream_node)\n  if self not in downstream_node.upstream_nodes:\n    downstream_node.add_upstream_node(self)\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.Pusher.add_downstream_nodes","title":"add_downstream_nodes","text":"<pre><code>add_downstream_nodes(downstream_nodes)\n</code></pre> <p>Experimental: Add another component that must run after this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> <p>It is symmetric with <code>add_upstream_nodes</code>.</p> PARAMETER DESCRIPTION <code>downstream_nodes</code> <p>a list of components that must run after this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_downstream_nodes(self, downstream_nodes):\n  \"\"\"Experimental: Add another component that must run after this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n  It is symmetric with `add_upstream_nodes`.\n\n  Args:\n    downstream_nodes: a list of components that must run after this node.\n  \"\"\"\n  self._downstream_nodes.update(downstream_nodes)\n  for downstream_node in downstream_nodes:\n    if self not in downstream_node.upstream_nodes:\n      downstream_node.add_upstream_node(self)\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.Pusher.add_upstream_node","title":"add_upstream_node","text":"<pre><code>add_upstream_node(upstream_node)\n</code></pre> <p>Experimental: Add another component that must run before this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> <p>It is symmetric with <code>add_downstream_node</code>.</p> PARAMETER DESCRIPTION <code>upstream_node</code> <p>a component that must run before this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_upstream_node(self, upstream_node):\n  \"\"\"Experimental: Add another component that must run before this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n  It is symmetric with `add_downstream_node`.\n\n  Args:\n    upstream_node: a component that must run before this node.\n  \"\"\"\n  self._upstream_nodes.add(upstream_node)\n  if self not in upstream_node.downstream_nodes:\n    upstream_node.add_downstream_node(self)\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.Pusher.add_upstream_nodes","title":"add_upstream_nodes","text":"<pre><code>add_upstream_nodes(upstream_nodes)\n</code></pre> <p>Experimental: Add components that must run before this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> PARAMETER DESCRIPTION <code>upstream_nodes</code> <p>a list of components that must run before this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_upstream_nodes(self, upstream_nodes):\n  \"\"\"Experimental: Add components that must run before this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n\n  Args:\n    upstream_nodes: a list of components that must run before this node.\n  \"\"\"\n  self._upstream_nodes.update(upstream_nodes)\n  for upstream_node in upstream_nodes:\n    if self not in upstream_node.downstream_nodes:\n      upstream_node.add_downstream_node(self)\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.Pusher.from_json_dict","title":"from_json_dict  <code>classmethod</code>","text":"<pre><code>from_json_dict(dict_data: Dict[str, Any]) -&gt; Any\n</code></pre> <p>Convert from dictionary data to an object.</p> Source code in <code>tfx/utils/json_utils.py</code> <pre><code>@classmethod\n@doc_controls.do_not_doc_in_subclasses\ndef from_json_dict(cls, dict_data: Dict[str, Any]) -&gt; Any:\n  \"\"\"Convert from dictionary data to an object.\"\"\"\n  instance = cls.__new__(cls)\n  instance.__dict__ = dict_data\n  return instance\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.Pusher.to_json_dict","title":"to_json_dict","text":"<pre><code>to_json_dict() -&gt; Dict[str, Any]\n</code></pre> <p>Convert from an object to a JSON serializable dictionary.</p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef to_json_dict(self) -&gt; Dict[str, Any]:\n  \"\"\"Convert from an object to a JSON serializable dictionary.\"\"\"\n  return dict((k, v)\n              for k, v in self.__dict__.items()\n              if k not in ['_upstream_nodes', '_downstream_nodes'])\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.Pusher.with_platform_config","title":"with_platform_config","text":"<pre><code>with_platform_config(config: Message) -&gt; Self\n</code></pre> <p>Attaches a proto-form platform config to a component.</p> <p>The config will be a per-node platform-specific config.</p> PARAMETER DESCRIPTION <code>config</code> <p>platform config to attach to the component.</p> <p> TYPE: <code>Message</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>the same component itself.</p> Source code in <code>tfx/dsl/components/base/base_component.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef with_platform_config(\n    self, config: message.Message\n) -&gt; typing_extensions.Self:\n  \"\"\"Attaches a proto-form platform config to a component.\n\n  The config will be a per-node platform-specific config.\n\n  Args:\n    config: platform config to attach to the component.\n\n  Returns:\n    the same component itself.\n  \"\"\"\n  self.platform_config = config\n  return self\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.SchemaGen","title":"SchemaGen","text":"<pre><code>SchemaGen(statistics: BaseChannel, infer_feature_shape: Optional[Union[bool, RuntimeParameter]] = True, exclude_splits: Optional[List[str]] = None)\n</code></pre> <p>               Bases: <code>BaseComponent</code></p> <p>A TFX SchemaGen component to generate a schema from the training data.</p> <p>The SchemaGen component uses TensorFlow Data Validation to generate a schema from input statistics. The following TFX libraries use the schema:   - TensorFlow Data Validation   - TensorFlow Transform   - TensorFlow Model Analysis</p> <p>In a typical TFX pipeline, the SchemaGen component generates a schema which is consumed by the other pipeline components.</p> <p>Example</p> <pre><code># Generates schema based on statistics files.\ninfer_schema = SchemaGen(statistics=statistics_gen.outputs['statistics'])\n</code></pre> <p>Component <code>outputs</code> contains:</p> <ul> <li><code>schema</code>: Channel of type <code>standard_artifacts.Schema</code> for schema  result.</li> </ul> <p>See the SchemaGen guide for more details.</p> <p>Constructs a SchemaGen component.</p> PARAMETER DESCRIPTION <code>statistics</code> <p>A BaseChannel of <code>ExampleStatistics</code> type (required if spec is not passed). This should contain at least a <code>train</code> split. Other splits are currently ignored. required</p> <p> TYPE: <code>BaseChannel</code> </p> <code>infer_feature_shape</code> <p>Boolean (or RuntimeParameter) value indicating whether or not to infer the shape of features. If the feature shape is not inferred, downstream Tensorflow Transform component using the schema will parse input as tf.SparseTensor. Default to True if not set.</p> <p> TYPE: <code>Optional[Union[bool, RuntimeParameter]]</code> DEFAULT: <code>True</code> </p> <code>exclude_splits</code> <p>Names of splits that will not be taken into consideration when auto-generating a schema. Default behavior (when exclude_splits is set to None) is excluding no splits.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> METHOD DESCRIPTION <code>add_downstream_node</code> <p>Experimental: Add another component that must run after this one.</p> <code>add_downstream_nodes</code> <p>Experimental: Add another component that must run after this one.</p> <code>add_upstream_node</code> <p>Experimental: Add another component that must run before this one.</p> <code>add_upstream_nodes</code> <p>Experimental: Add components that must run before this one.</p> <code>from_json_dict</code> <p>Convert from dictionary data to an object.</p> <code>to_json_dict</code> <p>Convert from an object to a JSON serializable dictionary.</p> <code>with_platform_config</code> <p>Attaches a proto-form platform config to a component.</p> ATTRIBUTE DESCRIPTION <code>id</code> <p>Node id, unique across all TFX nodes in a pipeline.</p> <p> TYPE: <code>str</code> </p> <code>outputs</code> <p>Component's output channel dict.</p> <p> TYPE: <code>Dict[str, OutputChannel]</code> </p> Source code in <code>tfx/components/schema_gen/component.py</code> <pre><code>def __init__(\n    self,\n    statistics: types.BaseChannel,\n    infer_feature_shape: Optional[Union[bool,\n                                        data_types.RuntimeParameter]] = True,\n    exclude_splits: Optional[List[str]] = None):\n  \"\"\"Constructs a SchemaGen component.\n\n  Args:\n    statistics: A [BaseChannel][tfx.v1.types.BaseChannel]\n      of `ExampleStatistics` type (required if spec is not passed).\n      This should contain at least a `train` split. Other splits\n      are currently ignored. _required_\n    infer_feature_shape: Boolean (or [RuntimeParameter][tfx.v1.dsl.experimental.RuntimeParameter]) value indicating\n      whether or not to infer the shape of features. If the feature shape is\n      not inferred, downstream Tensorflow Transform component using the schema\n      will parse input as tf.SparseTensor. Default to True if not set.\n    exclude_splits: Names of splits that will not be taken into consideration\n      when auto-generating a schema. Default behavior (when exclude_splits is\n      set to None) is excluding no splits.\n  \"\"\"\n  if exclude_splits is None:\n    exclude_splits = []\n    logging.info('Excluding no splits because exclude_splits is not set.')\n  schema = types.Channel(type=standard_artifacts.Schema)\n  if isinstance(infer_feature_shape, bool):\n    infer_feature_shape = int(infer_feature_shape)\n  spec = standard_component_specs.SchemaGenSpec(\n      statistics=statistics,\n      infer_feature_shape=infer_feature_shape,\n      exclude_splits=json_utils.dumps(exclude_splits),\n      schema=schema)\n  super().__init__(spec=spec)\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.SchemaGen-attributes","title":"Attributes","text":""},{"location":"api/v1/components/#tfx.v1.components.SchemaGen.id","title":"id  <code>property</code> <code>writable</code>","text":"<pre><code>id: str\n</code></pre> <p>Node id, unique across all TFX nodes in a pipeline.</p> <p>If <code>id</code> is set by the user, return it directly. Otherwise, return .</p> RETURNS DESCRIPTION <code>str</code> <p>node id.</p>"},{"location":"api/v1/components/#tfx.v1.components.SchemaGen.outputs","title":"outputs  <code>property</code>","text":"<pre><code>outputs: Dict[str, OutputChannel]\n</code></pre> <p>Component's output channel dict.</p>"},{"location":"api/v1/components/#tfx.v1.components.SchemaGen-functions","title":"Functions","text":""},{"location":"api/v1/components/#tfx.v1.components.SchemaGen.add_downstream_node","title":"add_downstream_node","text":"<pre><code>add_downstream_node(downstream_node)\n</code></pre> <p>Experimental: Add another component that must run after this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> <p>It is symmetric with <code>add_upstream_node</code>.</p> PARAMETER DESCRIPTION <code>downstream_node</code> <p>a component that must run after this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_downstream_node(self, downstream_node):\n  \"\"\"Experimental: Add another component that must run after this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n  It is symmetric with `add_upstream_node`.\n\n  Args:\n    downstream_node: a component that must run after this node.\n  \"\"\"\n  self._downstream_nodes.add(downstream_node)\n  if self not in downstream_node.upstream_nodes:\n    downstream_node.add_upstream_node(self)\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.SchemaGen.add_downstream_nodes","title":"add_downstream_nodes","text":"<pre><code>add_downstream_nodes(downstream_nodes)\n</code></pre> <p>Experimental: Add another component that must run after this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> <p>It is symmetric with <code>add_upstream_nodes</code>.</p> PARAMETER DESCRIPTION <code>downstream_nodes</code> <p>a list of components that must run after this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_downstream_nodes(self, downstream_nodes):\n  \"\"\"Experimental: Add another component that must run after this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n  It is symmetric with `add_upstream_nodes`.\n\n  Args:\n    downstream_nodes: a list of components that must run after this node.\n  \"\"\"\n  self._downstream_nodes.update(downstream_nodes)\n  for downstream_node in downstream_nodes:\n    if self not in downstream_node.upstream_nodes:\n      downstream_node.add_upstream_node(self)\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.SchemaGen.add_upstream_node","title":"add_upstream_node","text":"<pre><code>add_upstream_node(upstream_node)\n</code></pre> <p>Experimental: Add another component that must run before this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> <p>It is symmetric with <code>add_downstream_node</code>.</p> PARAMETER DESCRIPTION <code>upstream_node</code> <p>a component that must run before this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_upstream_node(self, upstream_node):\n  \"\"\"Experimental: Add another component that must run before this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n  It is symmetric with `add_downstream_node`.\n\n  Args:\n    upstream_node: a component that must run before this node.\n  \"\"\"\n  self._upstream_nodes.add(upstream_node)\n  if self not in upstream_node.downstream_nodes:\n    upstream_node.add_downstream_node(self)\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.SchemaGen.add_upstream_nodes","title":"add_upstream_nodes","text":"<pre><code>add_upstream_nodes(upstream_nodes)\n</code></pre> <p>Experimental: Add components that must run before this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> PARAMETER DESCRIPTION <code>upstream_nodes</code> <p>a list of components that must run before this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_upstream_nodes(self, upstream_nodes):\n  \"\"\"Experimental: Add components that must run before this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n\n  Args:\n    upstream_nodes: a list of components that must run before this node.\n  \"\"\"\n  self._upstream_nodes.update(upstream_nodes)\n  for upstream_node in upstream_nodes:\n    if self not in upstream_node.downstream_nodes:\n      upstream_node.add_downstream_node(self)\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.SchemaGen.from_json_dict","title":"from_json_dict  <code>classmethod</code>","text":"<pre><code>from_json_dict(dict_data: Dict[str, Any]) -&gt; Any\n</code></pre> <p>Convert from dictionary data to an object.</p> Source code in <code>tfx/utils/json_utils.py</code> <pre><code>@classmethod\n@doc_controls.do_not_doc_in_subclasses\ndef from_json_dict(cls, dict_data: Dict[str, Any]) -&gt; Any:\n  \"\"\"Convert from dictionary data to an object.\"\"\"\n  instance = cls.__new__(cls)\n  instance.__dict__ = dict_data\n  return instance\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.SchemaGen.to_json_dict","title":"to_json_dict","text":"<pre><code>to_json_dict() -&gt; Dict[str, Any]\n</code></pre> <p>Convert from an object to a JSON serializable dictionary.</p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef to_json_dict(self) -&gt; Dict[str, Any]:\n  \"\"\"Convert from an object to a JSON serializable dictionary.\"\"\"\n  return dict((k, v)\n              for k, v in self.__dict__.items()\n              if k not in ['_upstream_nodes', '_downstream_nodes'])\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.SchemaGen.with_platform_config","title":"with_platform_config","text":"<pre><code>with_platform_config(config: Message) -&gt; Self\n</code></pre> <p>Attaches a proto-form platform config to a component.</p> <p>The config will be a per-node platform-specific config.</p> PARAMETER DESCRIPTION <code>config</code> <p>platform config to attach to the component.</p> <p> TYPE: <code>Message</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>the same component itself.</p> Source code in <code>tfx/dsl/components/base/base_component.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef with_platform_config(\n    self, config: message.Message\n) -&gt; typing_extensions.Self:\n  \"\"\"Attaches a proto-form platform config to a component.\n\n  The config will be a per-node platform-specific config.\n\n  Args:\n    config: platform config to attach to the component.\n\n  Returns:\n    the same component itself.\n  \"\"\"\n  self.platform_config = config\n  return self\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.StatisticsGen","title":"StatisticsGen","text":"<pre><code>StatisticsGen(examples: BaseChannel, schema: Optional[BaseChannel] = None, stats_options: Optional[StatsOptions] = None, exclude_splits: Optional[List[str]] = None)\n</code></pre> <p>               Bases: <code>BaseBeamComponent</code></p> <p>Official TFX StatisticsGen component.</p> <p>The StatisticsGen component generates features statistics and random samples over training data, which can be used for visualization and validation. StatisticsGen uses Apache Beam and approximate algorithms to scale to large datasets.</p>"},{"location":"api/v1/components/#tfx.v1.components.StatisticsGen--example","title":"Example","text":"<pre><code>  # Computes statistics over data for visualization and example validation.\n  statistics_gen = StatisticsGen(examples=example_gen.outputs['examples'])\n</code></pre> <p>Component <code>outputs</code> contains:  - <code>statistics</code>: Channel of type <code>standard_artifacts.ExampleStatistics</code> for                  statistics of each split provided in the input examples.</p> <p>Please see the StatisticsGen guide for more details.</p> <p>Construct a StatisticsGen component.</p> PARAMETER DESCRIPTION <code>examples</code> <p>A BaseChannel of <code>ExamplesPath</code> type, likely generated by the ExampleGen component.   This needs to contain two splits labeled <code>train</code> and <code>eval</code>.   required</p> <p> TYPE: <code>BaseChannel</code> </p> <code>schema</code> <p>A <code>Schema</code> channel to use for automatically configuring the value of stats options passed to TFDV.</p> <p> TYPE: <code>Optional[BaseChannel]</code> DEFAULT: <code>None</code> </p> <code>stats_options</code> <p>The StatsOptions instance to configure optional TFDV behavior. When stats_options.schema is set, it will be used instead of the <code>schema</code> channel input. Due to the requirement that stats_options be serialized, the slicer functions and custom stats generators are not usable, and an error will be raised if either is specified.</p> <p> TYPE: <code>Optional[StatsOptions]</code> DEFAULT: <code>None</code> </p> <code>exclude_splits</code> <p>Names of splits where statistics and sample should not be generated. Default behavior (when exclude_splits is set to None) is excluding no splits.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> METHOD DESCRIPTION <code>add_downstream_node</code> <p>Experimental: Add another component that must run after this one.</p> <code>add_downstream_nodes</code> <p>Experimental: Add another component that must run after this one.</p> <code>add_upstream_node</code> <p>Experimental: Add another component that must run before this one.</p> <code>add_upstream_nodes</code> <p>Experimental: Add components that must run before this one.</p> <code>from_json_dict</code> <p>Convert from dictionary data to an object.</p> <code>to_json_dict</code> <p>Convert from an object to a JSON serializable dictionary.</p> <code>with_beam_pipeline_args</code> <p>Add per component Beam pipeline args.</p> <code>with_platform_config</code> <p>Attaches a proto-form platform config to a component.</p> ATTRIBUTE DESCRIPTION <code>id</code> <p>Node id, unique across all TFX nodes in a pipeline.</p> <p> TYPE: <code>str</code> </p> <code>outputs</code> <p>Component's output channel dict.</p> <p> TYPE: <code>Dict[str, OutputChannel]</code> </p> Source code in <code>tfx/components/statistics_gen/component.py</code> <pre><code>def __init__(self,\n             examples: types.BaseChannel,\n             schema: Optional[types.BaseChannel] = None,\n             stats_options: Optional[tfdv.StatsOptions] = None,\n             exclude_splits: Optional[List[str]] = None):\n  \"\"\"Construct a StatisticsGen component.\n\n  Args:\n    examples: A BaseChannel of `ExamplesPath` type, likely generated by the\n      [ExampleGen component](../../../guide/examplegen).\n        This needs to contain two splits labeled `train` and `eval`.\n        _required_\n    schema: A `Schema` channel to use for automatically configuring the value\n      of stats options passed to TFDV.\n    stats_options: The StatsOptions instance to configure optional TFDV\n      behavior. When stats_options.schema is set, it will be used instead of\n      the `schema` channel input. Due to the requirement that stats_options be\n      serialized, the slicer functions and custom stats generators are not\n      usable, and an error will be raised if either is specified.\n    exclude_splits: Names of splits where statistics and sample should not be\n      generated. Default behavior (when exclude_splits is set to None) is\n      excluding no splits.\n  \"\"\"\n  if exclude_splits is None:\n    exclude_splits = []\n    logging.info('Excluding no splits because exclude_splits is not set.')\n  statistics = types.Channel(type=standard_artifacts.ExampleStatistics)\n  # TODO(b/150802589): Move jsonable interface to tfx_bsl and use json_utils.\n  stats_options_json = stats_options.to_json() if stats_options else None\n  spec = standard_component_specs.StatisticsGenSpec(\n      examples=examples,\n      schema=schema,\n      stats_options_json=stats_options_json,\n      exclude_splits=json_utils.dumps(exclude_splits),\n      statistics=statistics)\n  super().__init__(spec=spec)\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.StatisticsGen-attributes","title":"Attributes","text":""},{"location":"api/v1/components/#tfx.v1.components.StatisticsGen.id","title":"id  <code>property</code> <code>writable</code>","text":"<pre><code>id: str\n</code></pre> <p>Node id, unique across all TFX nodes in a pipeline.</p> <p>If <code>id</code> is set by the user, return it directly. Otherwise, return .</p> RETURNS DESCRIPTION <code>str</code> <p>node id.</p>"},{"location":"api/v1/components/#tfx.v1.components.StatisticsGen.outputs","title":"outputs  <code>property</code>","text":"<pre><code>outputs: Dict[str, OutputChannel]\n</code></pre> <p>Component's output channel dict.</p>"},{"location":"api/v1/components/#tfx.v1.components.StatisticsGen-functions","title":"Functions","text":""},{"location":"api/v1/components/#tfx.v1.components.StatisticsGen.add_downstream_node","title":"add_downstream_node","text":"<pre><code>add_downstream_node(downstream_node)\n</code></pre> <p>Experimental: Add another component that must run after this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> <p>It is symmetric with <code>add_upstream_node</code>.</p> PARAMETER DESCRIPTION <code>downstream_node</code> <p>a component that must run after this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_downstream_node(self, downstream_node):\n  \"\"\"Experimental: Add another component that must run after this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n  It is symmetric with `add_upstream_node`.\n\n  Args:\n    downstream_node: a component that must run after this node.\n  \"\"\"\n  self._downstream_nodes.add(downstream_node)\n  if self not in downstream_node.upstream_nodes:\n    downstream_node.add_upstream_node(self)\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.StatisticsGen.add_downstream_nodes","title":"add_downstream_nodes","text":"<pre><code>add_downstream_nodes(downstream_nodes)\n</code></pre> <p>Experimental: Add another component that must run after this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> <p>It is symmetric with <code>add_upstream_nodes</code>.</p> PARAMETER DESCRIPTION <code>downstream_nodes</code> <p>a list of components that must run after this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_downstream_nodes(self, downstream_nodes):\n  \"\"\"Experimental: Add another component that must run after this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n  It is symmetric with `add_upstream_nodes`.\n\n  Args:\n    downstream_nodes: a list of components that must run after this node.\n  \"\"\"\n  self._downstream_nodes.update(downstream_nodes)\n  for downstream_node in downstream_nodes:\n    if self not in downstream_node.upstream_nodes:\n      downstream_node.add_upstream_node(self)\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.StatisticsGen.add_upstream_node","title":"add_upstream_node","text":"<pre><code>add_upstream_node(upstream_node)\n</code></pre> <p>Experimental: Add another component that must run before this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> <p>It is symmetric with <code>add_downstream_node</code>.</p> PARAMETER DESCRIPTION <code>upstream_node</code> <p>a component that must run before this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_upstream_node(self, upstream_node):\n  \"\"\"Experimental: Add another component that must run before this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n  It is symmetric with `add_downstream_node`.\n\n  Args:\n    upstream_node: a component that must run before this node.\n  \"\"\"\n  self._upstream_nodes.add(upstream_node)\n  if self not in upstream_node.downstream_nodes:\n    upstream_node.add_downstream_node(self)\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.StatisticsGen.add_upstream_nodes","title":"add_upstream_nodes","text":"<pre><code>add_upstream_nodes(upstream_nodes)\n</code></pre> <p>Experimental: Add components that must run before this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> PARAMETER DESCRIPTION <code>upstream_nodes</code> <p>a list of components that must run before this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_upstream_nodes(self, upstream_nodes):\n  \"\"\"Experimental: Add components that must run before this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n\n  Args:\n    upstream_nodes: a list of components that must run before this node.\n  \"\"\"\n  self._upstream_nodes.update(upstream_nodes)\n  for upstream_node in upstream_nodes:\n    if self not in upstream_node.downstream_nodes:\n      upstream_node.add_downstream_node(self)\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.StatisticsGen.from_json_dict","title":"from_json_dict  <code>classmethod</code>","text":"<pre><code>from_json_dict(dict_data: Dict[str, Any]) -&gt; Any\n</code></pre> <p>Convert from dictionary data to an object.</p> Source code in <code>tfx/utils/json_utils.py</code> <pre><code>@classmethod\n@doc_controls.do_not_doc_in_subclasses\ndef from_json_dict(cls, dict_data: Dict[str, Any]) -&gt; Any:\n  \"\"\"Convert from dictionary data to an object.\"\"\"\n  instance = cls.__new__(cls)\n  instance.__dict__ = dict_data\n  return instance\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.StatisticsGen.to_json_dict","title":"to_json_dict","text":"<pre><code>to_json_dict() -&gt; Dict[str, Any]\n</code></pre> <p>Convert from an object to a JSON serializable dictionary.</p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef to_json_dict(self) -&gt; Dict[str, Any]:\n  \"\"\"Convert from an object to a JSON serializable dictionary.\"\"\"\n  return dict((k, v)\n              for k, v in self.__dict__.items()\n              if k not in ['_upstream_nodes', '_downstream_nodes'])\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.StatisticsGen.with_beam_pipeline_args","title":"with_beam_pipeline_args","text":"<pre><code>with_beam_pipeline_args(beam_pipeline_args: Iterable[Union[str, Placeholder]]) -&gt; BaseBeamComponent\n</code></pre> <p>Add per component Beam pipeline args.</p> PARAMETER DESCRIPTION <code>beam_pipeline_args</code> <p>List of Beam pipeline args to be added to the Beam executor spec.</p> <p> TYPE: <code>Iterable[Union[str, Placeholder]]</code> </p> RETURNS DESCRIPTION <code>BaseBeamComponent</code> <p>the same component itself.</p> Source code in <code>tfx/dsl/components/base/base_beam_component.py</code> <pre><code>def with_beam_pipeline_args(\n    self, beam_pipeline_args: Iterable[Union[str, placeholder.Placeholder]]\n) -&gt; 'BaseBeamComponent':\n  \"\"\"Add per component Beam pipeline args.\n\n  Args:\n    beam_pipeline_args: List of Beam pipeline args to be added to the Beam\n      executor spec.\n\n  Returns:\n    the same component itself.\n  \"\"\"\n  cast(executor_spec.BeamExecutorSpec,\n       self.executor_spec).add_beam_pipeline_args(beam_pipeline_args)\n  return self\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.StatisticsGen.with_platform_config","title":"with_platform_config","text":"<pre><code>with_platform_config(config: Message) -&gt; Self\n</code></pre> <p>Attaches a proto-form platform config to a component.</p> <p>The config will be a per-node platform-specific config.</p> PARAMETER DESCRIPTION <code>config</code> <p>platform config to attach to the component.</p> <p> TYPE: <code>Message</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>the same component itself.</p> Source code in <code>tfx/dsl/components/base/base_component.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef with_platform_config(\n    self, config: message.Message\n) -&gt; typing_extensions.Self:\n  \"\"\"Attaches a proto-form platform config to a component.\n\n  The config will be a per-node platform-specific config.\n\n  Args:\n    config: platform config to attach to the component.\n\n  Returns:\n    the same component itself.\n  \"\"\"\n  self.platform_config = config\n  return self\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.Trainer","title":"Trainer","text":"<pre><code>Trainer(statistics: Optional[BaseChannel] = None, examples: Optional[BaseChannel] = None, transformed_examples: Optional[BaseChannel] = None, transform_graph: Optional[BaseChannel] = None, schema: Optional[BaseChannel] = None, base_model: Optional[BaseChannel] = None, hyperparameters: Optional[BaseChannel] = None, module_file: Optional[Union[str, RuntimeParameter]] = None, run_fn: Optional[Union[str, RuntimeParameter]] = None, train_args: Optional[Union[TrainArgs, RuntimeParameter]] = None, eval_args: Optional[Union[EvalArgs, RuntimeParameter]] = None, custom_config: Optional[Union[Dict[str, Any], RuntimeParameter]] = None, custom_executor_spec: Optional[ExecutorSpec] = None)\n</code></pre> <p>               Bases: <code>BaseComponent</code></p> <p>A TFX component to train a TensorFlow model.</p> <p>The Trainer component is used to train and eval a model using given inputs and a user-supplied <code>run_fn</code> function.</p> <p>An example of <code>run_fn()</code> can be found in the user-supplied code of the TFX penguin pipeline example.</p> <p>Note</p> <p>This component trains locally. For cloud distributed training, please refer to Cloud AI Platform Trainer.</p> <p>Example</p> <pre><code># Uses user-provided Python function that trains a model using TF.\ntrainer = Trainer(\n    module_file=module_file,\n    examples=transform.outputs[\"transformed_examples\"],\n    schema=infer_schema.outputs[\"schema\"],\n    transform_graph=transform.outputs[\"transform_graph\"],\n    train_args=proto.TrainArgs(splits=[\"train\"], num_steps=10000),\n    eval_args=proto.EvalArgs(splits=[\"eval\"], num_steps=5000),\n)\n</code></pre> <p>Component <code>outputs</code> contains:</p> <ul> <li><code>model</code>: Channel of type <code>standard_artifacts.Model</code> for trained model.</li> <li><code>model_run</code>: Channel of type <code>standard_artifacts.ModelRun</code>, as the working                 dir of models, can be used to output non-model related output                 (e.g., TensorBoard logs).</li> </ul> <p>Please see the Trainer guide for more details.</p> <p>Construct a Trainer component.</p> PARAMETER DESCRIPTION <code>examples</code> <p>A BaseChannel of type <code>standard_artifacts.Examples</code>, serving as the source of examples used in training (required). May be raw or transformed.</p> <p> TYPE: <code>Optional[BaseChannel]</code> DEFAULT: <code>None</code> </p> <code>transformed_examples</code> <p>Deprecated (no compatibility guarantee). Please set 'examples' instead.</p> <p> TYPE: <code>Optional[BaseChannel]</code> DEFAULT: <code>None</code> </p> <code>transform_graph</code> <p>An optional BaseChannel of type <code>standard_artifacts.TransformGraph</code>, serving as the input transform graph if present.</p> <p> TYPE: <code>Optional[BaseChannel]</code> DEFAULT: <code>None</code> </p> <code>schema</code> <p>An optional BaseChannel of type <code>standard_artifacts.Schema</code>, serving as the schema of training and eval data. Schema is optional when</p> <ol> <li>transform_graph is provided which contains schema.</li> <li>user module bypasses the usage of schema, e.g., hardcoded.</li> </ol> <p> TYPE: <code>Optional[BaseChannel]</code> DEFAULT: <code>None</code> </p> <code>base_model</code> <p>A BaseChannel of type <code>Model</code>, containing model that will be used for training. This can be used for warmstart, transfer learning or model ensembling.</p> <p> TYPE: <code>Optional[BaseChannel]</code> DEFAULT: <code>None</code> </p> <code>hyperparameters</code> <p>A [BaseChannel] of type <code>standard_artifacts.HyperParameters</code>, serving as the hyperparameters for training module. Tuner's output best hyperparameters can be feed into this.</p> <p> TYPE: <code>Optional[BaseChannel]</code> DEFAULT: <code>None</code> </p> <code>module_file</code> <p>A path to python module file containing UDF model definition. The <code>module_file</code> must implement a function named <code>run_fn</code> at its top level with function signature: </p><pre><code>def run_fn(trainer.fn_args_utils.FnArgs)\n</code></pre> and the trained model must be saved to <code>FnArgs.serving_model_dir</code> when this function is executed. <p>Exactly one of <code>module_file</code> or <code>run_fn</code> must be supplied if Trainer uses GenericExecutor (default). Use of a RuntimeParameter for this argument is experimental.</p> <p> TYPE: <code>Optional[Union[str, RuntimeParameter]]</code> DEFAULT: <code>None</code> </p> <code>run_fn</code> <p>A python path to UDF model definition function for generic trainer. See 'module_file' for details. Exactly one of 'module_file' or 'run_fn' must be supplied if Trainer uses GenericExecutor (default). Use of a RuntimeParameter for this argument is experimental.</p> <p> TYPE: <code>Optional[Union[str, RuntimeParameter]]</code> DEFAULT: <code>None</code> </p> <code>train_args</code> <p>A proto.TrainArgs instance, containing args used for training Currently only splits and num_steps are available. Default behavior (when splits is empty) is train on <code>train</code> split.</p> <p> TYPE: <code>Optional[Union[TrainArgs, RuntimeParameter]]</code> DEFAULT: <code>None</code> </p> <code>eval_args</code> <p>A proto.EvalArgs instance, containing args used for evaluation. Currently only splits and num_steps are available. Default behavior (when splits is empty) is evaluate on <code>eval</code> split.</p> <p> TYPE: <code>Optional[Union[EvalArgs, RuntimeParameter]]</code> DEFAULT: <code>None</code> </p> <code>custom_config</code> <p>A dict which contains addtional training job parameters that will be passed into user module.</p> <p> TYPE: <code>Optional[Union[Dict[str, Any], RuntimeParameter]]</code> DEFAULT: <code>None</code> </p> <code>custom_executor_spec</code> <p>Optional custom executor spec. Deprecated (no compatibility guarantee), please customize component directly.</p> <p> TYPE: <code>Optional[ExecutorSpec]</code> DEFAULT: <code>None</code> </p> RAISES DESCRIPTION <code>ValueError</code> <ul> <li>When both or neither of <code>module_file</code> and <code>run_fn</code> is supplied.</li> <li>When both or neither of <code>examples</code> and <code>transformed_examples</code>     is supplied.</li> <li>When <code>transformed_examples</code> is supplied but <code>transform_graph</code>     is not supplied.</li> </ul> METHOD DESCRIPTION <code>add_downstream_node</code> <p>Experimental: Add another component that must run after this one.</p> <code>add_downstream_nodes</code> <p>Experimental: Add another component that must run after this one.</p> <code>add_upstream_node</code> <p>Experimental: Add another component that must run before this one.</p> <code>add_upstream_nodes</code> <p>Experimental: Add components that must run before this one.</p> <code>from_json_dict</code> <p>Convert from dictionary data to an object.</p> <code>to_json_dict</code> <p>Convert from an object to a JSON serializable dictionary.</p> <code>with_platform_config</code> <p>Attaches a proto-form platform config to a component.</p> ATTRIBUTE DESCRIPTION <code>id</code> <p>Node id, unique across all TFX nodes in a pipeline.</p> <p> TYPE: <code>str</code> </p> <code>outputs</code> <p>Component's output channel dict.</p> <p> TYPE: <code>Dict[str, OutputChannel]</code> </p> Source code in <code>tfx/components/trainer/component.py</code> <pre><code>def __init__(\n    self,\n    statistics: Optional[types.BaseChannel] = None,\n    examples: Optional[types.BaseChannel] = None,\n    transformed_examples: Optional[types.BaseChannel] = None,\n    transform_graph: Optional[types.BaseChannel] = None,\n    schema: Optional[types.BaseChannel] = None,\n    base_model: Optional[types.BaseChannel] = None,\n    hyperparameters: Optional[types.BaseChannel] = None,\n    module_file: Optional[Union[str, data_types.RuntimeParameter]] = None,\n    run_fn: Optional[Union[str, data_types.RuntimeParameter]] = None,\n    train_args: Optional[Union[trainer_pb2.TrainArgs,\n                               data_types.RuntimeParameter]] = None,\n    eval_args: Optional[Union[trainer_pb2.EvalArgs,\n                              data_types.RuntimeParameter]] = None,\n    custom_config: Optional[Union[Dict[str, Any],\n                                  data_types.RuntimeParameter]] = None,\n    custom_executor_spec: Optional[executor_spec.ExecutorSpec] = None):\n  \"\"\"Construct a Trainer component.\n\n  Args:\n    examples: A [BaseChannel][tfx.v1.types.BaseChannel] of type [`standard_artifacts.Examples`][tfx.v1.types.standard_artifacts.Examples],\n      serving as the source of examples used in training (required). May be raw or\n      transformed.\n    transformed_examples: Deprecated (no compatibility guarantee). Please set\n      'examples' instead.\n    transform_graph: An optional [BaseChannel][tfx.v1.types.BaseChannel] of type\n      [`standard_artifacts.TransformGraph`][tfx.v1.types.standard_artifacts.TransformGraph],\n      serving as the input transform graph if present.\n    schema:  An optional [BaseChannel][tfx.v1.types.BaseChannel] of type\n      [`standard_artifacts.Schema`][tfx.v1.types.standard_artifacts.Schema],\n      serving as the schema of training and eval data. Schema is optional when\n\n        1. transform_graph is provided which contains schema.\n        2. user module bypasses the usage of schema, e.g., hardcoded.\n    base_model: A [BaseChannel][tfx.v1.types.BaseChannel] of type `Model`, containing model that will be\n      used for training. This can be used for warmstart, transfer learning or\n      model ensembling.\n    hyperparameters: A [BaseChannel] of type\n      [`standard_artifacts.HyperParameters`][tfx.v1.types.standard_artifacts.HyperParameters],\n      serving as the hyperparameters for training module. Tuner's output best\n      hyperparameters can be feed into this.\n    module_file: A path to python module file containing UDF model definition.\n      The `module_file` must implement a function named `run_fn` at its top\n      level with function signature:\n      ```python\n      def run_fn(trainer.fn_args_utils.FnArgs)\n      ```\n      and the trained model must be saved to `FnArgs.serving_model_dir` when\n      this function is executed.\n\n      Exactly one of `module_file` or `run_fn` must be supplied if Trainer\n      uses GenericExecutor (default). Use of a [RuntimeParameter][tfx.v1.dsl.experimental.RuntimeParameter] for this\n      argument is experimental.\n    run_fn:  A python path to UDF model definition function for generic\n      trainer. See 'module_file' for details. Exactly one of 'module_file' or\n      'run_fn' must be supplied if Trainer uses GenericExecutor (default). Use\n      of a [RuntimeParameter][tfx.v1.dsl.experimental.RuntimeParameter] for this argument is experimental.\n    train_args: A proto.TrainArgs instance, containing args used for training\n      Currently only splits and num_steps are available. Default behavior\n      (when splits is empty) is train on `train` split.\n    eval_args: A proto.EvalArgs instance, containing args used for evaluation.\n      Currently only splits and num_steps are available. Default behavior\n      (when splits is empty) is evaluate on `eval` split.\n    custom_config: A dict which contains addtional training job parameters\n      that will be passed into user module.\n    custom_executor_spec: Optional custom executor spec. Deprecated (no\n      compatibility guarantee), please customize component directly.\n\n  Raises:\n    ValueError:\n      - When both or neither of `module_file` and `run_fn` is supplied.\n      - When both or neither of `examples` and `transformed_examples`\n          is supplied.\n      - When `transformed_examples` is supplied but `transform_graph`\n          is not supplied.\n  \"\"\"\n  if [bool(module_file), bool(run_fn)].count(True) != 1:\n    raise ValueError(\n        \"Exactly one of 'module_file', or 'run_fn' must be supplied.\")\n\n  if bool(examples) == bool(transformed_examples):\n    raise ValueError(\n        \"Exactly one of 'example' or 'transformed_example' must be supplied.\")\n\n  if transformed_examples and not transform_graph:\n    raise ValueError(\"If 'transformed_examples' is supplied, \"\n                     \"'transform_graph' must be supplied too.\")\n\n  if custom_executor_spec:\n    logging.warning(\n        \"`custom_executor_spec` is deprecated. Please customize component directly.\"\n    )\n  if transformed_examples:\n    logging.warning(\n        \"`transformed_examples` is deprecated. Please use `examples` instead.\"\n    )\n  examples = examples or transformed_examples\n  model = types.Channel(type=standard_artifacts.Model)\n  model_run = types.Channel(type=standard_artifacts.ModelRun)\n  spec = standard_component_specs.TrainerSpec(\n      statistics=statistics,\n      examples=examples,\n      transform_graph=transform_graph,\n      schema=schema,\n      base_model=base_model,\n      hyperparameters=hyperparameters,\n      train_args=train_args or trainer_pb2.TrainArgs(),\n      eval_args=eval_args or trainer_pb2.EvalArgs(),\n      module_file=module_file,\n      run_fn=run_fn,\n      custom_config=(custom_config\n                     if isinstance(custom_config, data_types.RuntimeParameter)\n                     else json_utils.dumps(custom_config)),\n      model=model,\n      model_run=model_run)\n  super().__init__(spec=spec, custom_executor_spec=custom_executor_spec)\n\n  if udf_utils.should_package_user_modules():\n    # In this case, the `MODULE_PATH_KEY` execution property will be injected\n    # as a reference to the given user module file after packaging, at which\n    # point the `MODULE_FILE_KEY` execution property will be removed.\n    udf_utils.add_user_module_dependency(\n        self, standard_component_specs.MODULE_FILE_KEY,\n        standard_component_specs.MODULE_PATH_KEY)\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.Trainer-attributes","title":"Attributes","text":""},{"location":"api/v1/components/#tfx.v1.components.Trainer.id","title":"id  <code>property</code> <code>writable</code>","text":"<pre><code>id: str\n</code></pre> <p>Node id, unique across all TFX nodes in a pipeline.</p> <p>If <code>id</code> is set by the user, return it directly. Otherwise, return .</p> RETURNS DESCRIPTION <code>str</code> <p>node id.</p>"},{"location":"api/v1/components/#tfx.v1.components.Trainer.outputs","title":"outputs  <code>property</code>","text":"<pre><code>outputs: Dict[str, OutputChannel]\n</code></pre> <p>Component's output channel dict.</p>"},{"location":"api/v1/components/#tfx.v1.components.Trainer-functions","title":"Functions","text":""},{"location":"api/v1/components/#tfx.v1.components.Trainer.add_downstream_node","title":"add_downstream_node","text":"<pre><code>add_downstream_node(downstream_node)\n</code></pre> <p>Experimental: Add another component that must run after this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> <p>It is symmetric with <code>add_upstream_node</code>.</p> PARAMETER DESCRIPTION <code>downstream_node</code> <p>a component that must run after this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_downstream_node(self, downstream_node):\n  \"\"\"Experimental: Add another component that must run after this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n  It is symmetric with `add_upstream_node`.\n\n  Args:\n    downstream_node: a component that must run after this node.\n  \"\"\"\n  self._downstream_nodes.add(downstream_node)\n  if self not in downstream_node.upstream_nodes:\n    downstream_node.add_upstream_node(self)\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.Trainer.add_downstream_nodes","title":"add_downstream_nodes","text":"<pre><code>add_downstream_nodes(downstream_nodes)\n</code></pre> <p>Experimental: Add another component that must run after this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> <p>It is symmetric with <code>add_upstream_nodes</code>.</p> PARAMETER DESCRIPTION <code>downstream_nodes</code> <p>a list of components that must run after this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_downstream_nodes(self, downstream_nodes):\n  \"\"\"Experimental: Add another component that must run after this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n  It is symmetric with `add_upstream_nodes`.\n\n  Args:\n    downstream_nodes: a list of components that must run after this node.\n  \"\"\"\n  self._downstream_nodes.update(downstream_nodes)\n  for downstream_node in downstream_nodes:\n    if self not in downstream_node.upstream_nodes:\n      downstream_node.add_upstream_node(self)\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.Trainer.add_upstream_node","title":"add_upstream_node","text":"<pre><code>add_upstream_node(upstream_node)\n</code></pre> <p>Experimental: Add another component that must run before this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> <p>It is symmetric with <code>add_downstream_node</code>.</p> PARAMETER DESCRIPTION <code>upstream_node</code> <p>a component that must run before this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_upstream_node(self, upstream_node):\n  \"\"\"Experimental: Add another component that must run before this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n  It is symmetric with `add_downstream_node`.\n\n  Args:\n    upstream_node: a component that must run before this node.\n  \"\"\"\n  self._upstream_nodes.add(upstream_node)\n  if self not in upstream_node.downstream_nodes:\n    upstream_node.add_downstream_node(self)\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.Trainer.add_upstream_nodes","title":"add_upstream_nodes","text":"<pre><code>add_upstream_nodes(upstream_nodes)\n</code></pre> <p>Experimental: Add components that must run before this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> PARAMETER DESCRIPTION <code>upstream_nodes</code> <p>a list of components that must run before this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_upstream_nodes(self, upstream_nodes):\n  \"\"\"Experimental: Add components that must run before this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n\n  Args:\n    upstream_nodes: a list of components that must run before this node.\n  \"\"\"\n  self._upstream_nodes.update(upstream_nodes)\n  for upstream_node in upstream_nodes:\n    if self not in upstream_node.downstream_nodes:\n      upstream_node.add_downstream_node(self)\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.Trainer.from_json_dict","title":"from_json_dict  <code>classmethod</code>","text":"<pre><code>from_json_dict(dict_data: Dict[str, Any]) -&gt; Any\n</code></pre> <p>Convert from dictionary data to an object.</p> Source code in <code>tfx/utils/json_utils.py</code> <pre><code>@classmethod\n@doc_controls.do_not_doc_in_subclasses\ndef from_json_dict(cls, dict_data: Dict[str, Any]) -&gt; Any:\n  \"\"\"Convert from dictionary data to an object.\"\"\"\n  instance = cls.__new__(cls)\n  instance.__dict__ = dict_data\n  return instance\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.Trainer.to_json_dict","title":"to_json_dict","text":"<pre><code>to_json_dict() -&gt; Dict[str, Any]\n</code></pre> <p>Convert from an object to a JSON serializable dictionary.</p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef to_json_dict(self) -&gt; Dict[str, Any]:\n  \"\"\"Convert from an object to a JSON serializable dictionary.\"\"\"\n  return dict((k, v)\n              for k, v in self.__dict__.items()\n              if k not in ['_upstream_nodes', '_downstream_nodes'])\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.Trainer.with_platform_config","title":"with_platform_config","text":"<pre><code>with_platform_config(config: Message) -&gt; Self\n</code></pre> <p>Attaches a proto-form platform config to a component.</p> <p>The config will be a per-node platform-specific config.</p> PARAMETER DESCRIPTION <code>config</code> <p>platform config to attach to the component.</p> <p> TYPE: <code>Message</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>the same component itself.</p> Source code in <code>tfx/dsl/components/base/base_component.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef with_platform_config(\n    self, config: message.Message\n) -&gt; typing_extensions.Self:\n  \"\"\"Attaches a proto-form platform config to a component.\n\n  The config will be a per-node platform-specific config.\n\n  Args:\n    config: platform config to attach to the component.\n\n  Returns:\n    the same component itself.\n  \"\"\"\n  self.platform_config = config\n  return self\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.Transform","title":"Transform","text":"<pre><code>Transform(examples: BaseChannel, schema: BaseChannel, module_file: Optional[Union[str, RuntimeParameter]] = None, preprocessing_fn: Optional[Union[str, RuntimeParameter]] = None, splits_config: Optional[SplitsConfig] = None, analyzer_cache: Optional[BaseChannel] = None, materialize: bool = True, disable_analyzer_cache: bool = False, force_tf_compat_v1: bool = False, custom_config: Optional[Dict[str, Any]] = None, disable_statistics: bool = False, stats_options_updater_fn: Optional[str] = None)\n</code></pre> <p>               Bases: <code>BaseBeamComponent</code></p> <p>A TFX component to transform the input examples.</p> <p>The Transform component wraps TensorFlow Transform (tf.Transform) to preprocess data in a TFX pipeline. This component will load the preprocessing_fn from input module file, preprocess both 'train' and 'eval' splits of input examples, generate the <code>tf.Transform</code> output, and save both transform function and transformed examples to orchestrator desired locations.</p> <p>The Transform component can also invoke TFDV to compute statistics on the pre-transform and post-transform data. Invocations of TFDV take an optional StatsOptions object. To configure the StatsOptions object that is passed to TFDV for both pre-transform and post-transform statistics, users can define the optional <code>stats_options_updater_fn</code> within the module file.</p>"},{"location":"api/v1/components/#tfx.v1.components.Transform--providing-a-preprocessing-function","title":"Providing a preprocessing function","text":"<p>The Transform executor will look specifically for the <code>preprocessing_fn()</code> function within that file.</p> <p>An example of <code>preprocessing_fn()</code> can be found in the user-supplied code of the TFX Chicago Taxi pipeline example.</p>"},{"location":"api/v1/components/#tfx.v1.components.Transform--updating-statsoptions","title":"Updating StatsOptions","text":"<p>The Transform executor will look specifically for the <code>stats_options_updater_fn()</code> within the module file specified above.</p> <p>An example of <code>stats_options_updater_fn()</code> can be found in the user-supplied code of the TFX BERT MRPC pipeline example.</p> <p>Example</p> <pre><code># Performs transformations and feature engineering in training and serving.\ntransform = Transform(\n    examples=example_gen.outputs['examples'],\n    schema=infer_schema.outputs['schema'],\n    module_file=module_file,\n)\n</code></pre> <p>Component <code>outputs</code> contains:</p> <ul> <li><code>transform_graph</code>: Channel of type <code>standard_artifacts.TransformGraph</code>,                       which includes an exported Tensorflow graph suitable                       for both training and serving.</li> <li><code>transformed_examples</code>: Channel of type <code>standard_artifacts.Examples</code> for                            materialized transformed examples, which includes                            transform splits as specified in splits_config.                            This is optional controlled by <code>materialize</code>.</li> </ul> <p>Please see the Transform guide for more details.</p> <p>Construct a Transform component.</p> PARAMETER DESCRIPTION <code>examples</code> <p>A BaseChannel of type <code>standard_artifacts.Examples</code> required. This should contain custom splits specified in splits_config. If custom split is not provided, this should contain two splits 'train' and 'eval'.</p> <p> TYPE: <code>BaseChannel</code> </p> <code>schema</code> <p>A BaseChannel of type <code>standard_artifacts.Schema</code>. This should contain a single schema artifact.</p> <p> TYPE: <code>BaseChannel</code> </p> <code>module_file</code> <p>The file path to a python module file, from which the 'preprocessing_fn' function will be loaded. Exactly one of 'module_file' or 'preprocessing_fn' must be supplied.</p> <p>The function needs to have the following signature: </p><pre><code>def preprocessing_fn(inputs: Dict[Text, Any]) -&gt; Dict[Text, Any]:\n    ...\n</code></pre> where the values of input and returned Dict are either tf.Tensor or tf.SparseTensor. <p>If additional inputs are needed for preprocessing_fn, they can be passed in custom_config:</p> <p></p><pre><code>def preprocessing_fn(\n    inputs: Dict[Text, Any],\n    custom_config: Dict[Text, Any],\n) -&gt; Dict[Text, Any]:\n    ...\n</code></pre> To update the stats options used to compute the pre-transform or post-transform statistics, optionally define the 'stats-options_updater_fn' within the same module. If implemented, this function needs to have the following signature: <pre><code>def stats_options_updater_fn(\n    stats_type: tfx.components.transform.stats_options_util.StatsType,\n    stats_options: tfdv.StatsOptions,\n) -&gt; tfdv.StatsOptions:\n    ...\n</code></pre> Use of a RuntimeParameter for this argument is experimental.                <p> TYPE: <code>Optional[Union[str, RuntimeParameter]]</code> DEFAULT: <code>None</code> </p> <code>preprocessing_fn</code> <p>The path to python function that implements a 'preprocessing_fn'. See 'module_file' for expected signature of the function. Exactly one of 'module_file' or 'preprocessing_fn' must be supplied. Use of a RuntimeParameter for this argument is experimental.</p> <p> TYPE: <code>Optional[Union[str, RuntimeParameter]]</code> DEFAULT: <code>None</code> </p> <code>splits_config</code> <p>A transform_pb2.SplitsConfig instance, providing splits that should be analyzed and splits that should be transformed. Note analyze and transform splits can have overlap. Default behavior (when splits_config is not set) is analyze the 'train' split and transform all splits. If splits_config is set, analyze cannot be empty.</p> <p> TYPE: <code>Optional[SplitsConfig]</code> DEFAULT: <code>None</code> </p> <code>analyzer_cache</code> <p>Optional input 'TransformCache' channel containing cached information from previous Transform runs. When provided, Transform will try use the cached calculation if possible.</p> <p> TYPE: <code>Optional[BaseChannel]</code> DEFAULT: <code>None</code> </p> <code>materialize</code> <p>If True, write transformed examples as an output.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>disable_analyzer_cache</code> <p>If False, Transform will use input cache if provided and write cache output. If True, <code>analyzer_cache</code> must not be provided.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>force_tf_compat_v1</code> <p>(Optional) If True and/or TF2 behaviors are disabled Transform will use Tensorflow in compat.v1 mode irrespective of installed version of Tensorflow. Defaults to <code>False</code>.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>custom_config</code> <p>A dict which contains additional parameters that will be passed to preprocessing_fn.</p> <p> TYPE: <code>Optional[Dict[str, Any]]</code> DEFAULT: <code>None</code> </p> <code>disable_statistics</code> <p>If True, do not invoke TFDV to compute pre-transform and post-transform statistics. When statistics are computed, they will will be stored in the <code>pre_transform_feature_stats/</code> and <code>post_transform_feature_stats/</code> subfolders of the <code>transform_graph</code> export.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>stats_options_updater_fn</code> <p>The path to a python function that implements a 'stats_options_updater_fn'. See 'module_file' for expected signature of the function. 'stats_options_updater_fn' cannot be defined if 'module_file' is specified.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>When both or neither of 'module_file' and 'preprocessing_fn' is supplied.</p> METHOD DESCRIPTION <code>add_downstream_node</code> <p>Experimental: Add another component that must run after this one.</p> <code>add_downstream_nodes</code> <p>Experimental: Add another component that must run after this one.</p> <code>add_upstream_node</code> <p>Experimental: Add another component that must run before this one.</p> <code>add_upstream_nodes</code> <p>Experimental: Add components that must run before this one.</p> <code>from_json_dict</code> <p>Convert from dictionary data to an object.</p> <code>to_json_dict</code> <p>Convert from an object to a JSON serializable dictionary.</p> <code>with_beam_pipeline_args</code> <p>Add per component Beam pipeline args.</p> <code>with_platform_config</code> <p>Attaches a proto-form platform config to a component.</p> ATTRIBUTE DESCRIPTION <code>id</code> <p>Node id, unique across all TFX nodes in a pipeline.</p> <p> TYPE: <code>str</code> </p> <code>outputs</code> <p>Component's output channel dict.</p> <p> TYPE: <code>Dict[str, OutputChannel]</code> </p> Source code in <code>tfx/components/transform/component.py</code> <pre><code>def __init__(\n    self,\n    examples: types.BaseChannel,\n    schema: types.BaseChannel,\n    module_file: Optional[Union[str, data_types.RuntimeParameter]] = None,\n    preprocessing_fn: Optional[Union[str,\n                                     data_types.RuntimeParameter]] = None,\n    splits_config: Optional[transform_pb2.SplitsConfig] = None,\n    analyzer_cache: Optional[types.BaseChannel] = None,\n    materialize: bool = True,\n    disable_analyzer_cache: bool = False,\n    force_tf_compat_v1: bool = False,\n    custom_config: Optional[Dict[str, Any]] = None,\n    disable_statistics: bool = False,\n    stats_options_updater_fn: Optional[str] = None):\n  \"\"\"Construct a Transform component.\n\n  Args:\n    examples: A [BaseChannel][tfx.v1.types.BaseChannel] of type [`standard_artifacts.Examples`][tfx.v1.types.standard_artifacts.Examples] _required_.\n      This should contain custom splits specified in splits_config. If custom\n      split is not provided, this should contain two splits 'train' and\n      'eval'.\n    schema: A [BaseChannel][tfx.v1.types.BaseChannel] of type [`standard_artifacts.Schema`][tfx.v1.types.standard_artifacts.Schema]. This should\n      contain a single schema artifact.\n    module_file: The file path to a python module file, from which the\n      'preprocessing_fn' function will be loaded.\n      Exactly one of 'module_file' or 'preprocessing_fn' must be supplied.\n\n      The function needs to have the following signature:\n      ``` {.python .no-copy}\n      def preprocessing_fn(inputs: Dict[Text, Any]) -&gt; Dict[Text, Any]:\n          ...\n      ```\n      where the values of input and returned Dict are either tf.Tensor or\n      tf.SparseTensor.\n\n      If additional inputs are needed for preprocessing_fn, they can be passed\n      in custom_config:\n\n      ``` {.python .no-copy}\n      def preprocessing_fn(\n          inputs: Dict[Text, Any],\n          custom_config: Dict[Text, Any],\n      ) -&gt; Dict[Text, Any]:\n          ...\n      ```\n      To update the stats options used to compute the pre-transform or\n      post-transform statistics, optionally define the\n      'stats-options_updater_fn' within the same module. If implemented,\n      this function needs to have the following signature:\n      ``` {.python .no-copy}\n      def stats_options_updater_fn(\n          stats_type: tfx.components.transform.stats_options_util.StatsType,\n          stats_options: tfdv.StatsOptions,\n      ) -&gt; tfdv.StatsOptions:\n          ...\n      ```\n      Use of a [RuntimeParameter][tfx.v1.dsl.experimental.RuntimeParameter] for this argument is experimental.\n    preprocessing_fn: The path to python function that implements a\n      'preprocessing_fn'. See 'module_file' for expected signature of the\n      function. Exactly one of 'module_file' or 'preprocessing_fn' must be\n      supplied. Use of a [RuntimeParameter][tfx.v1.dsl.experimental.RuntimeParameter] for this argument is experimental.\n    splits_config: A transform_pb2.SplitsConfig instance, providing splits\n      that should be analyzed and splits that should be transformed. Note\n      analyze and transform splits can have overlap. Default behavior (when\n      splits_config is not set) is analyze the 'train' split and transform all\n      splits. If splits_config is set, analyze cannot be empty.\n    analyzer_cache: Optional input 'TransformCache' channel containing cached\n      information from previous Transform runs. When provided, Transform will\n      try use the cached calculation if possible.\n    materialize: If True, write transformed examples as an output.\n    disable_analyzer_cache: If False, Transform will use input cache if\n      provided and write cache output. If True, `analyzer_cache` must not be\n      provided.\n    force_tf_compat_v1: (Optional) If True and/or TF2 behaviors are disabled\n      Transform will use Tensorflow in compat.v1 mode irrespective of\n      installed version of Tensorflow. Defaults to `False`.\n    custom_config: A dict which contains additional parameters that will be\n      passed to preprocessing_fn.\n    disable_statistics: If True, do not invoke TFDV to compute pre-transform\n      and post-transform statistics. When statistics are computed, they will\n      will be stored in the `pre_transform_feature_stats/` and\n      `post_transform_feature_stats/` subfolders of the `transform_graph`\n      export.\n    stats_options_updater_fn: The path to a python function that implements a\n      'stats_options_updater_fn'. See 'module_file' for expected signature of\n      the function. 'stats_options_updater_fn' cannot be defined if\n      'module_file' is specified.\n\n  Raises:\n    ValueError: When both or neither of 'module_file' and 'preprocessing_fn'\n      is supplied.\n  \"\"\"\n  if bool(module_file) == bool(preprocessing_fn):\n    raise ValueError(\n        \"Exactly one of 'module_file' or 'preprocessing_fn' must be supplied.\"\n    )\n\n  if bool(module_file) and bool(stats_options_updater_fn):\n    raise ValueError(\n        \"'stats_options_updater_fn' cannot be specified together with \"\n        \"'module_file'\")\n\n  transform_graph = types.Channel(type=standard_artifacts.TransformGraph)\n  transformed_examples = None\n  if materialize:\n    transformed_examples = types.Channel(type=standard_artifacts.Examples)\n    transformed_examples.matching_channel_name = \"examples\"\n\n  (pre_transform_schema, pre_transform_stats, post_transform_schema,\n   post_transform_stats, post_transform_anomalies) = (None,) * 5\n  if not disable_statistics:\n    pre_transform_schema = types.Channel(type=standard_artifacts.Schema)\n    post_transform_schema = types.Channel(type=standard_artifacts.Schema)\n    pre_transform_stats = types.Channel(\n        type=standard_artifacts.ExampleStatistics)\n    post_transform_stats = types.Channel(\n        type=standard_artifacts.ExampleStatistics)\n    post_transform_anomalies = types.Channel(\n        type=standard_artifacts.ExampleAnomalies)\n\n  if disable_analyzer_cache:\n    updated_analyzer_cache = None\n    if analyzer_cache:\n      raise ValueError(\n          \"`analyzer_cache` is set when disable_analyzer_cache is True.\")\n  else:\n    updated_analyzer_cache = types.Channel(\n        type=standard_artifacts.TransformCache)\n\n  spec = standard_component_specs.TransformSpec(\n      examples=examples,\n      schema=schema,\n      module_file=module_file,\n      preprocessing_fn=preprocessing_fn,\n      stats_options_updater_fn=stats_options_updater_fn,\n      force_tf_compat_v1=int(force_tf_compat_v1),\n      splits_config=splits_config,\n      transform_graph=transform_graph,\n      transformed_examples=transformed_examples,\n      analyzer_cache=analyzer_cache,\n      updated_analyzer_cache=updated_analyzer_cache,\n      custom_config=json_utils.dumps(custom_config),\n      disable_statistics=int(disable_statistics),\n      pre_transform_schema=pre_transform_schema,\n      pre_transform_stats=pre_transform_stats,\n      post_transform_schema=post_transform_schema,\n      post_transform_stats=post_transform_stats,\n      post_transform_anomalies=post_transform_anomalies)\n  super().__init__(spec=spec)\n\n  if udf_utils.should_package_user_modules():\n    # In this case, the `MODULE_PATH_KEY` execution property will be injected\n    # as a reference to the given user module file after packaging, at which\n    # point the `MODULE_FILE_KEY` execution property will be removed.\n    udf_utils.add_user_module_dependency(\n        self, standard_component_specs.MODULE_FILE_KEY,\n        standard_component_specs.MODULE_PATH_KEY)\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.Transform-attributes","title":"Attributes","text":""},{"location":"api/v1/components/#tfx.v1.components.Transform.id","title":"id  <code>property</code> <code>writable</code>","text":"<pre><code>id: str\n</code></pre> <p>Node id, unique across all TFX nodes in a pipeline.</p> <p>If <code>id</code> is set by the user, return it directly. Otherwise, return .</p> RETURNS DESCRIPTION <code>str</code> <p>node id.</p>"},{"location":"api/v1/components/#tfx.v1.components.Transform.outputs","title":"outputs  <code>property</code>","text":"<pre><code>outputs: Dict[str, OutputChannel]\n</code></pre> <p>Component's output channel dict.</p>"},{"location":"api/v1/components/#tfx.v1.components.Transform-functions","title":"Functions","text":""},{"location":"api/v1/components/#tfx.v1.components.Transform.add_downstream_node","title":"add_downstream_node","text":"<pre><code>add_downstream_node(downstream_node)\n</code></pre> <p>Experimental: Add another component that must run after this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> <p>It is symmetric with <code>add_upstream_node</code>.</p> PARAMETER DESCRIPTION <code>downstream_node</code> <p>a component that must run after this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_downstream_node(self, downstream_node):\n  \"\"\"Experimental: Add another component that must run after this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n  It is symmetric with `add_upstream_node`.\n\n  Args:\n    downstream_node: a component that must run after this node.\n  \"\"\"\n  self._downstream_nodes.add(downstream_node)\n  if self not in downstream_node.upstream_nodes:\n    downstream_node.add_upstream_node(self)\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.Transform.add_downstream_nodes","title":"add_downstream_nodes","text":"<pre><code>add_downstream_nodes(downstream_nodes)\n</code></pre> <p>Experimental: Add another component that must run after this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> <p>It is symmetric with <code>add_upstream_nodes</code>.</p> PARAMETER DESCRIPTION <code>downstream_nodes</code> <p>a list of components that must run after this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_downstream_nodes(self, downstream_nodes):\n  \"\"\"Experimental: Add another component that must run after this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n  It is symmetric with `add_upstream_nodes`.\n\n  Args:\n    downstream_nodes: a list of components that must run after this node.\n  \"\"\"\n  self._downstream_nodes.update(downstream_nodes)\n  for downstream_node in downstream_nodes:\n    if self not in downstream_node.upstream_nodes:\n      downstream_node.add_upstream_node(self)\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.Transform.add_upstream_node","title":"add_upstream_node","text":"<pre><code>add_upstream_node(upstream_node)\n</code></pre> <p>Experimental: Add another component that must run before this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> <p>It is symmetric with <code>add_downstream_node</code>.</p> PARAMETER DESCRIPTION <code>upstream_node</code> <p>a component that must run before this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_upstream_node(self, upstream_node):\n  \"\"\"Experimental: Add another component that must run before this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n  It is symmetric with `add_downstream_node`.\n\n  Args:\n    upstream_node: a component that must run before this node.\n  \"\"\"\n  self._upstream_nodes.add(upstream_node)\n  if self not in upstream_node.downstream_nodes:\n    upstream_node.add_downstream_node(self)\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.Transform.add_upstream_nodes","title":"add_upstream_nodes","text":"<pre><code>add_upstream_nodes(upstream_nodes)\n</code></pre> <p>Experimental: Add components that must run before this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> PARAMETER DESCRIPTION <code>upstream_nodes</code> <p>a list of components that must run before this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_upstream_nodes(self, upstream_nodes):\n  \"\"\"Experimental: Add components that must run before this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n\n  Args:\n    upstream_nodes: a list of components that must run before this node.\n  \"\"\"\n  self._upstream_nodes.update(upstream_nodes)\n  for upstream_node in upstream_nodes:\n    if self not in upstream_node.downstream_nodes:\n      upstream_node.add_downstream_node(self)\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.Transform.from_json_dict","title":"from_json_dict  <code>classmethod</code>","text":"<pre><code>from_json_dict(dict_data: Dict[str, Any]) -&gt; Any\n</code></pre> <p>Convert from dictionary data to an object.</p> Source code in <code>tfx/utils/json_utils.py</code> <pre><code>@classmethod\n@doc_controls.do_not_doc_in_subclasses\ndef from_json_dict(cls, dict_data: Dict[str, Any]) -&gt; Any:\n  \"\"\"Convert from dictionary data to an object.\"\"\"\n  instance = cls.__new__(cls)\n  instance.__dict__ = dict_data\n  return instance\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.Transform.to_json_dict","title":"to_json_dict","text":"<pre><code>to_json_dict() -&gt; Dict[str, Any]\n</code></pre> <p>Convert from an object to a JSON serializable dictionary.</p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef to_json_dict(self) -&gt; Dict[str, Any]:\n  \"\"\"Convert from an object to a JSON serializable dictionary.\"\"\"\n  return dict((k, v)\n              for k, v in self.__dict__.items()\n              if k not in ['_upstream_nodes', '_downstream_nodes'])\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.Transform.with_beam_pipeline_args","title":"with_beam_pipeline_args","text":"<pre><code>with_beam_pipeline_args(beam_pipeline_args: Iterable[Union[str, Placeholder]]) -&gt; BaseBeamComponent\n</code></pre> <p>Add per component Beam pipeline args.</p> PARAMETER DESCRIPTION <code>beam_pipeline_args</code> <p>List of Beam pipeline args to be added to the Beam executor spec.</p> <p> TYPE: <code>Iterable[Union[str, Placeholder]]</code> </p> RETURNS DESCRIPTION <code>BaseBeamComponent</code> <p>the same component itself.</p> Source code in <code>tfx/dsl/components/base/base_beam_component.py</code> <pre><code>def with_beam_pipeline_args(\n    self, beam_pipeline_args: Iterable[Union[str, placeholder.Placeholder]]\n) -&gt; 'BaseBeamComponent':\n  \"\"\"Add per component Beam pipeline args.\n\n  Args:\n    beam_pipeline_args: List of Beam pipeline args to be added to the Beam\n      executor spec.\n\n  Returns:\n    the same component itself.\n  \"\"\"\n  cast(executor_spec.BeamExecutorSpec,\n       self.executor_spec).add_beam_pipeline_args(beam_pipeline_args)\n  return self\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.Transform.with_platform_config","title":"with_platform_config","text":"<pre><code>with_platform_config(config: Message) -&gt; Self\n</code></pre> <p>Attaches a proto-form platform config to a component.</p> <p>The config will be a per-node platform-specific config.</p> PARAMETER DESCRIPTION <code>config</code> <p>platform config to attach to the component.</p> <p> TYPE: <code>Message</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>the same component itself.</p> Source code in <code>tfx/dsl/components/base/base_component.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef with_platform_config(\n    self, config: message.Message\n) -&gt; typing_extensions.Self:\n  \"\"\"Attaches a proto-form platform config to a component.\n\n  The config will be a per-node platform-specific config.\n\n  Args:\n    config: platform config to attach to the component.\n\n  Returns:\n    the same component itself.\n  \"\"\"\n  self.platform_config = config\n  return self\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.Tuner","title":"Tuner","text":"<pre><code>Tuner(examples: BaseChannel, schema: Optional[BaseChannel] = None, transform_graph: Optional[BaseChannel] = None, base_model: Optional[BaseChannel] = None, module_file: Optional[str] = None, tuner_fn: Optional[str] = None, train_args: Optional[TrainArgs] = None, eval_args: Optional[EvalArgs] = None, tune_args: Optional[TuneArgs] = None, custom_config: Optional[Dict[str, Any]] = None)\n</code></pre> <p>               Bases: <code>BaseComponent</code></p> <p>A TFX component for model hyperparameter tuning.</p> <p>Component <code>outputs</code> contains:</p> <ul> <li><code>best_hyperparameters</code>: Channel of type                            <code>standard_artifacts.HyperParameters</code> for result of                            the best hparams.</li> <li><code>tuner_results</code>: Channel of type <code>standard_artifacts.TunerResults</code> for  results of all trials. Experimental: subject to change and no backwards  compatibility guarantees.</li> </ul> <p>See the Tuner guide for more details.</p> <p>Construct a Tuner component.</p> PARAMETER DESCRIPTION <code>examples</code> <p>A BaseChannel of type <code>standard_artifacts.Examples</code>, serving as the source of examples that are used in tuning (required).</p> <p> TYPE: <code>BaseChannel</code> </p> <code>schema</code> <p>An optional BaseChannel of type <code>standard_artifacts.Schema</code>, serving as the schema of training and eval data. This is used when raw examples are provided.</p> <p> TYPE: <code>Optional[BaseChannel]</code> DEFAULT: <code>None</code> </p> <code>transform_graph</code> <p>An optional BaseChannel of type <code>standard_artifacts.TransformGraph</code>, serving as the input transform graph if present. This is used when transformed examples are provided.</p> <p> TYPE: <code>Optional[BaseChannel]</code> DEFAULT: <code>None</code> </p> <code>base_model</code> <p>A BaseChannel of type <code>Model</code>, containing model that will be used for training. This can be used for warmstart, transfer learning or model ensembling.</p> <p> TYPE: <code>Optional[BaseChannel]</code> DEFAULT: <code>None</code> </p> <code>module_file</code> <p>A path to python module file containing UDF tuner definition. The module_file must implement a function named <code>tuner_fn</code> at its top level. The function must have the following signature.     </p><pre><code>def tuner_fn(fn_args: FnArgs) -&gt; TunerFnResult:\n    ...\n</code></pre>     Exactly one of 'module_file' or 'tuner_fn' must be supplied.                <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>tuner_fn</code> <p>A python path to UDF model definition function. See 'module_file' for the required signature of the UDF. Exactly one of 'module_file' or 'tuner_fn' must be supplied.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>train_args</code> <p>A trainer_pb2.TrainArgs instance, containing args used for training. Currently only splits and num_steps are available. Default behavior (when splits is empty) is train on <code>train</code> split.</p> <p> TYPE: <code>Optional[TrainArgs]</code> DEFAULT: <code>None</code> </p> <code>eval_args</code> <p>A trainer_pb2.EvalArgs instance, containing args used for eval. Currently only splits and num_steps are available. Default behavior (when splits is empty) is evaluate on <code>eval</code> split.</p> <p> TYPE: <code>Optional[EvalArgs]</code> DEFAULT: <code>None</code> </p> <code>tune_args</code> <p>A tuner_pb2.TuneArgs instance, containing args used for tuning. Currently only num_parallel_trials is available.</p> <p> TYPE: <code>Optional[TuneArgs]</code> DEFAULT: <code>None</code> </p> <code>custom_config</code> <p>A dict which contains addtional training job parameters that will be passed into user module.</p> <p> TYPE: <code>Optional[Dict[str, Any]]</code> DEFAULT: <code>None</code> </p> METHOD DESCRIPTION <code>add_downstream_node</code> <p>Experimental: Add another component that must run after this one.</p> <code>add_downstream_nodes</code> <p>Experimental: Add another component that must run after this one.</p> <code>add_upstream_node</code> <p>Experimental: Add another component that must run before this one.</p> <code>add_upstream_nodes</code> <p>Experimental: Add components that must run before this one.</p> <code>from_json_dict</code> <p>Convert from dictionary data to an object.</p> <code>to_json_dict</code> <p>Convert from an object to a JSON serializable dictionary.</p> <code>with_platform_config</code> <p>Attaches a proto-form platform config to a component.</p> ATTRIBUTE DESCRIPTION <code>id</code> <p>Node id, unique across all TFX nodes in a pipeline.</p> <p> TYPE: <code>str</code> </p> <code>outputs</code> <p>Component's output channel dict.</p> <p> TYPE: <code>Dict[str, OutputChannel]</code> </p> Source code in <code>tfx/components/tuner/component.py</code> <pre><code>def __init__(self,\n             examples: types.BaseChannel,\n             schema: Optional[types.BaseChannel] = None,\n             transform_graph: Optional[types.BaseChannel] = None,\n             base_model: Optional[types.BaseChannel] = None,\n             module_file: Optional[str] = None,\n             tuner_fn: Optional[str] = None,\n             train_args: Optional[trainer_pb2.TrainArgs] = None,\n             eval_args: Optional[trainer_pb2.EvalArgs] = None,\n             tune_args: Optional[tuner_pb2.TuneArgs] = None,\n             custom_config: Optional[Dict[str, Any]] = None):\n  \"\"\"Construct a Tuner component.\n\n  Args:\n    examples: A [BaseChannel][tfx.v1.types.BaseChannel] of type [`standard_artifacts.Examples`][tfx.v1.types.standard_artifacts.Examples], serving as\n      the source of examples that are used in tuning (required).\n    schema:  An optional [BaseChannel][tfx.v1.types.BaseChannel] of type [`standard_artifacts.Schema`][tfx.v1.types.standard_artifacts.Schema],\n      serving as the schema of training and eval data. This is used when raw\n      examples are provided.\n    transform_graph: An optional [BaseChannel][tfx.v1.types.BaseChannel] of type\n      [`standard_artifacts.TransformGraph`][tfx.v1.types.standard_artifacts.TransformGraph], serving as the input transform\n      graph if present. This is used when transformed examples are provided.\n    base_model: A [BaseChannel][tfx.v1.types.BaseChannel] of type [`Model`][tfx.v1.types.standard_artifacts.Model], containing model that will be\n      used for training. This can be used for warmstart, transfer learning or\n      model ensembling.\n    module_file: A path to python module file containing UDF tuner definition.\n      The module_file must implement a function named `tuner_fn` at its top\n      level. The function must have the following signature.\n          ``` {.python .no-copy}\n          def tuner_fn(fn_args: FnArgs) -&gt; TunerFnResult:\n              ...\n          ```\n          Exactly one of 'module_file' or 'tuner_fn' must be supplied.\n    tuner_fn:  A python path to UDF model definition function. See\n      'module_file' for the required signature of the UDF. Exactly one of\n      'module_file' or 'tuner_fn' must be supplied.\n    train_args: A trainer_pb2.TrainArgs instance, containing args used for\n      training. Currently only splits and num_steps are available. Default\n      behavior (when splits is empty) is train on `train` split.\n    eval_args: A trainer_pb2.EvalArgs instance, containing args used for eval.\n      Currently only splits and num_steps are available. Default behavior\n      (when splits is empty) is evaluate on `eval` split.\n    tune_args: A tuner_pb2.TuneArgs instance, containing args used for tuning.\n      Currently only num_parallel_trials is available.\n    custom_config: A dict which contains addtional training job parameters\n      that will be passed into user module.\n  \"\"\"\n  if bool(module_file) == bool(tuner_fn):\n    raise ValueError(\n        \"Exactly one of 'module_file' or 'tuner_fn' must be supplied\")\n\n  best_hyperparameters = types.Channel(\n      type=standard_artifacts.HyperParameters)\n  tuner_results = types.Channel(type=standard_artifacts.TunerResults)\n  spec = standard_component_specs.TunerSpec(\n      examples=examples,\n      schema=schema,\n      transform_graph=transform_graph,\n      base_model=base_model,\n      module_file=module_file,\n      tuner_fn=tuner_fn,\n      train_args=train_args or trainer_pb2.TrainArgs(),\n      eval_args=eval_args or trainer_pb2.EvalArgs(),\n      tune_args=tune_args,\n      best_hyperparameters=best_hyperparameters,\n      tuner_results=tuner_results,\n      custom_config=json_utils.dumps(custom_config),\n  )\n  super().__init__(spec=spec)\n\n  if udf_utils.should_package_user_modules():\n    # In this case, the `MODULE_PATH_KEY` execution property will be injected\n    # as a reference to the given user module file after packaging, at which\n    # point the `MODULE_FILE_KEY` execution property will be removed.\n    udf_utils.add_user_module_dependency(\n        self, standard_component_specs.MODULE_FILE_KEY,\n        standard_component_specs.MODULE_PATH_KEY)\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.Tuner-attributes","title":"Attributes","text":""},{"location":"api/v1/components/#tfx.v1.components.Tuner.id","title":"id  <code>property</code> <code>writable</code>","text":"<pre><code>id: str\n</code></pre> <p>Node id, unique across all TFX nodes in a pipeline.</p> <p>If <code>id</code> is set by the user, return it directly. Otherwise, return .</p> RETURNS DESCRIPTION <code>str</code> <p>node id.</p>"},{"location":"api/v1/components/#tfx.v1.components.Tuner.outputs","title":"outputs  <code>property</code>","text":"<pre><code>outputs: Dict[str, OutputChannel]\n</code></pre> <p>Component's output channel dict.</p>"},{"location":"api/v1/components/#tfx.v1.components.Tuner-functions","title":"Functions","text":""},{"location":"api/v1/components/#tfx.v1.components.Tuner.add_downstream_node","title":"add_downstream_node","text":"<pre><code>add_downstream_node(downstream_node)\n</code></pre> <p>Experimental: Add another component that must run after this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> <p>It is symmetric with <code>add_upstream_node</code>.</p> PARAMETER DESCRIPTION <code>downstream_node</code> <p>a component that must run after this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_downstream_node(self, downstream_node):\n  \"\"\"Experimental: Add another component that must run after this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n  It is symmetric with `add_upstream_node`.\n\n  Args:\n    downstream_node: a component that must run after this node.\n  \"\"\"\n  self._downstream_nodes.add(downstream_node)\n  if self not in downstream_node.upstream_nodes:\n    downstream_node.add_upstream_node(self)\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.Tuner.add_downstream_nodes","title":"add_downstream_nodes","text":"<pre><code>add_downstream_nodes(downstream_nodes)\n</code></pre> <p>Experimental: Add another component that must run after this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> <p>It is symmetric with <code>add_upstream_nodes</code>.</p> PARAMETER DESCRIPTION <code>downstream_nodes</code> <p>a list of components that must run after this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_downstream_nodes(self, downstream_nodes):\n  \"\"\"Experimental: Add another component that must run after this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n  It is symmetric with `add_upstream_nodes`.\n\n  Args:\n    downstream_nodes: a list of components that must run after this node.\n  \"\"\"\n  self._downstream_nodes.update(downstream_nodes)\n  for downstream_node in downstream_nodes:\n    if self not in downstream_node.upstream_nodes:\n      downstream_node.add_upstream_node(self)\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.Tuner.add_upstream_node","title":"add_upstream_node","text":"<pre><code>add_upstream_node(upstream_node)\n</code></pre> <p>Experimental: Add another component that must run before this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> <p>It is symmetric with <code>add_downstream_node</code>.</p> PARAMETER DESCRIPTION <code>upstream_node</code> <p>a component that must run before this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_upstream_node(self, upstream_node):\n  \"\"\"Experimental: Add another component that must run before this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n  It is symmetric with `add_downstream_node`.\n\n  Args:\n    upstream_node: a component that must run before this node.\n  \"\"\"\n  self._upstream_nodes.add(upstream_node)\n  if self not in upstream_node.downstream_nodes:\n    upstream_node.add_downstream_node(self)\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.Tuner.add_upstream_nodes","title":"add_upstream_nodes","text":"<pre><code>add_upstream_nodes(upstream_nodes)\n</code></pre> <p>Experimental: Add components that must run before this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> PARAMETER DESCRIPTION <code>upstream_nodes</code> <p>a list of components that must run before this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_upstream_nodes(self, upstream_nodes):\n  \"\"\"Experimental: Add components that must run before this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n\n  Args:\n    upstream_nodes: a list of components that must run before this node.\n  \"\"\"\n  self._upstream_nodes.update(upstream_nodes)\n  for upstream_node in upstream_nodes:\n    if self not in upstream_node.downstream_nodes:\n      upstream_node.add_downstream_node(self)\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.Tuner.from_json_dict","title":"from_json_dict  <code>classmethod</code>","text":"<pre><code>from_json_dict(dict_data: Dict[str, Any]) -&gt; Any\n</code></pre> <p>Convert from dictionary data to an object.</p> Source code in <code>tfx/utils/json_utils.py</code> <pre><code>@classmethod\n@doc_controls.do_not_doc_in_subclasses\ndef from_json_dict(cls, dict_data: Dict[str, Any]) -&gt; Any:\n  \"\"\"Convert from dictionary data to an object.\"\"\"\n  instance = cls.__new__(cls)\n  instance.__dict__ = dict_data\n  return instance\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.Tuner.to_json_dict","title":"to_json_dict","text":"<pre><code>to_json_dict() -&gt; Dict[str, Any]\n</code></pre> <p>Convert from an object to a JSON serializable dictionary.</p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef to_json_dict(self) -&gt; Dict[str, Any]:\n  \"\"\"Convert from an object to a JSON serializable dictionary.\"\"\"\n  return dict((k, v)\n              for k, v in self.__dict__.items()\n              if k not in ['_upstream_nodes', '_downstream_nodes'])\n</code></pre>"},{"location":"api/v1/components/#tfx.v1.components.Tuner.with_platform_config","title":"with_platform_config","text":"<pre><code>with_platform_config(config: Message) -&gt; Self\n</code></pre> <p>Attaches a proto-form platform config to a component.</p> <p>The config will be a per-node platform-specific config.</p> PARAMETER DESCRIPTION <code>config</code> <p>platform config to attach to the component.</p> <p> TYPE: <code>Message</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>the same component itself.</p> Source code in <code>tfx/dsl/components/base/base_component.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef with_platform_config(\n    self, config: message.Message\n) -&gt; typing_extensions.Self:\n  \"\"\"Attaches a proto-form platform config to a component.\n\n  The config will be a per-node platform-specific config.\n\n  Args:\n    config: platform config to attach to the component.\n\n  Returns:\n    the same component itself.\n  \"\"\"\n  self.platform_config = config\n  return self\n</code></pre>"},{"location":"api/v1/dsl/","title":"DSL","text":""},{"location":"api/v1/dsl/#tfx.v1.dsl","title":"tfx.v1.dsl","text":"<p>TFX DSL module.</p> MODULE DESCRIPTION <code>components</code> <p>TFX DSL components module.</p> <code>experimental</code> <p>TFX dsl.experimental module.</p> <code>io</code> <p>TFX DSL I/O module.</p> <code>placeholders</code> <p>TFX placeholders module.</p> <code>standard_annotations</code> <p>Public API for base type annotations.</p> CLASS DESCRIPTION <code>Artifact</code> <p>TFX artifact used for orchestration.</p> <code>Channel</code> <p>Legacy channel interface.</p> <code>Cond</code> <p>Cond context manager that disable containing nodes if predicate is False.</p> <code>ExecutionMode</code> <p>Execution mode of a pipeline.</p> <code>Importer</code> <p>Definition for TFX Importer.</p> <code>Pipeline</code> <p>Logical TFX pipeline object.</p> <code>Resolver</code> <p>Definition for TFX Resolver.</p>"},{"location":"api/v1/dsl/#tfx.v1.dsl-classes","title":"Classes","text":""},{"location":"api/v1/dsl/#tfx.v1.dsl.Artifact","title":"Artifact","text":"<pre><code>Artifact(mlmd_artifact_type: Optional[ArtifactType] = None)\n</code></pre> <p>               Bases: <code>Jsonable</code></p> <p>TFX artifact used for orchestration.</p> <p>This is used for type-checking and inter-component communication. Currently, it wraps a tuple of (<code>ml_metadata.proto.Artifact</code>, <code>ml_metadata.proto.ArtifactType</code>) with additional property accessors for internal state.</p> <p>A user may create a subclass of Artifact and override the TYPE_NAME property with the type for this artifact subclass. Users of the subclass may then omit the \"type_name\" field when construction the object.</p> <p>A user may specify artifact type-specific properties for an Artifact subclass by overriding the PROPERTIES dictionary, as detailed below.</p> <p>Note</p> <p>The behavior of this class is experimental, without backwards compatibility guarantees, and may change in upcoming releases.</p> <p>Construct an instance of Artifact.</p> <p>Used by TFX internal implementation: create an empty Artifact with type_name and optional split info specified. The remaining info will be filled in during compiling and running time. The Artifact should be transparent to end users and should not be initiated directly by pipeline users.</p> PARAMETER DESCRIPTION <code>mlmd_artifact_type</code> <p>Proto message defining the underlying ArtifactType. Optional and intended for internal use.</p> <p> TYPE: <code>Optional[ArtifactType]</code> DEFAULT: <code>None</code> </p> METHOD DESCRIPTION <code>copy_from</code> <p>Set uri, properties and custom properties from a given Artifact.</p> <code>from_json_dict</code> <p>Convert from dictionary data to an object.</p> <code>get_bool_custom_property</code> <p>Get a custom property of bool type.</p> <code>get_custom_property</code> <p>Gets a custom property with key. Return None if not found.</p> <code>get_float_custom_property</code> <p>Gets a custom property of float type.</p> <code>get_int_custom_property</code> <p>Get a custom property of int type.</p> <code>get_json_value_custom_property</code> <p>Get a custom property of JSON type.</p> <code>get_proto_custom_property</code> <p>Get a custom property of proto type.</p> <code>get_string_custom_property</code> <p>Get a custom property of string type.</p> <code>set_bool_custom_property</code> <p>Sets a custom property of bool type.</p> <code>set_float_custom_property</code> <p>Sets a custom property of float type.</p> <code>set_int_custom_property</code> <p>Set a custom property of int type.</p> <code>set_json_value_custom_property</code> <p>Sets a custom property of JSON type.</p> <code>set_mlmd_artifact</code> <p>Replace the MLMD artifact object on this artifact.</p> <code>set_mlmd_artifact_type</code> <p>Set entire ArtifactType in this object.</p> <code>set_proto_custom_property</code> <p>Sets a custom property of proto type.</p> <code>set_string_custom_property</code> <p>Set a custom property of string type.</p> <code>to_json_dict</code> <p>Convert from an object to a JSON serializable dictionary.</p> ATTRIBUTE DESCRIPTION <code>artifact_type</code> <p>Type of the underlying mlmd artifact.</p> <p> </p> <code>external_id</code> <p>external id of the underlying artifact.</p> <p> TYPE: <code>str</code> </p> <code>id</code> <p>Id of the underlying mlmd artifact.</p> <p> TYPE: <code>int</code> </p> <code>is_external</code> <p>Returns true if the artifact is external.</p> <p> TYPE: <code>bool</code> </p> <code>mlmd_artifact</code> <p>Underlying mlmd artifact.</p> <p> </p> <code>name</code> <p>Name of the underlying mlmd artifact.</p> <p> TYPE: <code>str</code> </p> <code>pipeline_name</code> <p>Name of the pipeline that produce the artifact.</p> <p> TYPE: <code>str</code> </p> <code>producer_component</code> <p>Producer component of the artifact.</p> <p> TYPE: <code>str</code> </p> <code>state</code> <p>State of the underlying mlmd artifact.</p> <p> TYPE: <code>str</code> </p> <code>type</code> <p>Type of the artifact.</p> <p> </p> <code>type_id</code> <p>Type id of the underlying mlmd artifact.</p> <p> TYPE: <code>int</code> </p> <code>type_name</code> <p>Type name of the underlying mlmd artifact.</p> <p> </p> <code>uri</code> <p>Artifact URI.</p> <p> TYPE: <code>str</code> </p> Source code in <code>tfx/types/artifact.py</code> <pre><code>def __init__(\n    self,\n    mlmd_artifact_type: Optional[metadata_store_pb2.ArtifactType] = None):\n  \"\"\"Construct an instance of Artifact.\n\n  Used by TFX internal implementation: create an empty Artifact with\n  type_name and optional split info specified. The remaining info will be\n  filled in during compiling and running time. The Artifact should be\n  transparent to end users and should not be initiated directly by pipeline\n  users.\n\n  Args:\n    mlmd_artifact_type: Proto message defining the underlying ArtifactType.\n      Optional and intended for internal use.\n  \"\"\"\n  if self.__class__ == Artifact:\n    if not mlmd_artifact_type:\n      raise ValueError(\n          'The \"mlmd_artifact_type\" argument must be passed to specify a '\n          'type for this Artifact.')\n    if not isinstance(mlmd_artifact_type, metadata_store_pb2.ArtifactType):\n      raise ValueError(\n          'The \"mlmd_artifact_type\" argument must be an instance of the '\n          'proto message ml_metadata.proto.metadata_store_pb2.ArtifactType.')\n  else:\n    if mlmd_artifact_type:\n      raise ValueError(\n          'The \"mlmd_artifact_type\" argument must not be passed for '\n          'Artifact subclass %s.' % self.__class__)\n    mlmd_artifact_type = self._get_artifact_type()\n\n  # MLMD artifact type proto object.\n  self._artifact_type = mlmd_artifact_type\n  # Underlying MLMD artifact proto object.\n  self._artifact = metadata_store_pb2.Artifact()\n  # When list/dict JSON or proto value properties are read, it is possible\n  # they will be modified without knowledge of this class. Therefore,\n  # deserialized values need to be cached here and reserialized into the\n  # metadata proto when requested.\n  self._cached_modifiable_properties = {}\n  self._cached_modifiable_custom_properties = {}\n  # Initialization flag to prevent recursive getattr / setattr errors.\n  self._initialized = True\n</code></pre>"},{"location":"api/v1/dsl/#tfx.v1.dsl.Artifact-attributes","title":"Attributes","text":""},{"location":"api/v1/dsl/#tfx.v1.dsl.Artifact.artifact_type","title":"artifact_type  <code>property</code>","text":"<pre><code>artifact_type\n</code></pre> <p>Type of the underlying mlmd artifact.</p>"},{"location":"api/v1/dsl/#tfx.v1.dsl.Artifact.external_id","title":"external_id  <code>property</code>","text":"<pre><code>external_id: str\n</code></pre> <p>external id of the underlying artifact.</p>"},{"location":"api/v1/dsl/#tfx.v1.dsl.Artifact.id","title":"id  <code>property</code> <code>writable</code>","text":"<pre><code>id: int\n</code></pre> <p>Id of the underlying mlmd artifact.</p>"},{"location":"api/v1/dsl/#tfx.v1.dsl.Artifact.is_external","title":"is_external  <code>property</code> <code>writable</code>","text":"<pre><code>is_external: bool\n</code></pre> <p>Returns true if the artifact is external.</p>"},{"location":"api/v1/dsl/#tfx.v1.dsl.Artifact.mlmd_artifact","title":"mlmd_artifact  <code>property</code>","text":"<pre><code>mlmd_artifact\n</code></pre> <p>Underlying mlmd artifact.</p>"},{"location":"api/v1/dsl/#tfx.v1.dsl.Artifact.name","title":"name  <code>property</code> <code>writable</code>","text":"<pre><code>name: str\n</code></pre> <p>Name of the underlying mlmd artifact.</p>"},{"location":"api/v1/dsl/#tfx.v1.dsl.Artifact.pipeline_name","title":"pipeline_name  <code>property</code> <code>writable</code>","text":"<pre><code>pipeline_name: str\n</code></pre> <p>Name of the pipeline that produce the artifact.</p>"},{"location":"api/v1/dsl/#tfx.v1.dsl.Artifact.producer_component","title":"producer_component  <code>property</code> <code>writable</code>","text":"<pre><code>producer_component: str\n</code></pre> <p>Producer component of the artifact.</p>"},{"location":"api/v1/dsl/#tfx.v1.dsl.Artifact.state","title":"state  <code>property</code> <code>writable</code>","text":"<pre><code>state: str\n</code></pre> <p>State of the underlying mlmd artifact.</p>"},{"location":"api/v1/dsl/#tfx.v1.dsl.Artifact.type","title":"type  <code>property</code>","text":"<pre><code>type\n</code></pre> <p>Type of the artifact.</p>"},{"location":"api/v1/dsl/#tfx.v1.dsl.Artifact.type_id","title":"type_id  <code>property</code> <code>writable</code>","text":"<pre><code>type_id: int\n</code></pre> <p>Type id of the underlying mlmd artifact.</p>"},{"location":"api/v1/dsl/#tfx.v1.dsl.Artifact.type_name","title":"type_name  <code>property</code>","text":"<pre><code>type_name\n</code></pre> <p>Type name of the underlying mlmd artifact.</p>"},{"location":"api/v1/dsl/#tfx.v1.dsl.Artifact.uri","title":"uri  <code>property</code> <code>writable</code>","text":"<pre><code>uri: str\n</code></pre> <p>Artifact URI.</p>"},{"location":"api/v1/dsl/#tfx.v1.dsl.Artifact-functions","title":"Functions","text":""},{"location":"api/v1/dsl/#tfx.v1.dsl.Artifact.copy_from","title":"copy_from","text":"<pre><code>copy_from(other: Artifact)\n</code></pre> <p>Set uri, properties and custom properties from a given Artifact.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef copy_from(self, other: 'Artifact'):\n  \"\"\"Set uri, properties and custom properties from a given Artifact.\"\"\"\n  assert self.type is other.type, (\n      'Unable to set properties from an artifact of different type: {} vs {}'\n      .format(self.type_name, other.type_name))\n  self.uri = other.uri\n  if other.artifact_type.HasField('id'):\n    self.type_id = other.artifact_type.id\n\n  self._artifact.properties.clear()\n  self._artifact.properties.MergeFrom(other._artifact.properties)  # pylint: disable=protected-access\n  self._artifact.custom_properties.clear()\n  self._artifact.custom_properties.MergeFrom(\n      other._artifact.custom_properties)  # pylint: disable=protected-access\n  self._cached_modifiable_properties = copy.deepcopy(\n      other._cached_modifiable_properties)  # pylint: disable=protected-access\n  self._cached_modifiable_custom_properties = copy.deepcopy(\n      other._cached_modifiable_custom_properties)  # pylint: disable=protected-access\n</code></pre>"},{"location":"api/v1/dsl/#tfx.v1.dsl.Artifact.from_json_dict","title":"from_json_dict  <code>classmethod</code>","text":"<pre><code>from_json_dict(dict_data: Dict[str, Any]) -&gt; Any\n</code></pre> <p>Convert from dictionary data to an object.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@classmethod\n@doc_controls.do_not_doc_inheritable\ndef from_json_dict(cls, dict_data: Dict[str, Any]) -&gt; Any:\n  module_name = dict_data['__artifact_class_module__']\n  class_name = dict_data['__artifact_class_name__']\n  artifact = metadata_store_pb2.Artifact()\n  artifact_type = metadata_store_pb2.ArtifactType()\n  json_format.Parse(json.dumps(dict_data['artifact']), artifact)\n  json_format.Parse(json.dumps(dict_data['artifact_type']), artifact_type)\n\n  # First, try to resolve the specific class used for the artifact; if this\n  # is not possible, use a generic artifact.Artifact object.\n  result = None\n  try:\n    artifact_cls = getattr(importlib.import_module(module_name), class_name)\n    # If the artifact type is the base Artifact class, do not construct the\n    # object here since that constructor requires the mlmd_artifact_type\n    # argument.\n    if artifact_cls != Artifact:\n      result = artifact_cls()\n  except (AttributeError, ImportError, ValueError):\n    logging.warning((\n        'Could not load artifact class %s.%s; using fallback deserialization '\n        'for the relevant artifact. Please make sure that any artifact '\n        'classes can be imported within your container or environment.'),\n                    module_name, class_name)\n  if not result:\n    result = Artifact(mlmd_artifact_type=artifact_type)\n  result.set_mlmd_artifact_type(artifact_type)\n  result.set_mlmd_artifact(artifact)\n  return result\n</code></pre>"},{"location":"api/v1/dsl/#tfx.v1.dsl.Artifact.get_bool_custom_property","title":"get_bool_custom_property","text":"<pre><code>get_bool_custom_property(key: str) -&gt; bool\n</code></pre> <p>Get a custom property of bool type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_bool_custom_property(self, key: str) -&gt; bool:\n  \"\"\"Get a custom property of bool type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return False\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, bool):\n    return json_value\n  return self._artifact.custom_properties[key].bool_value\n</code></pre>"},{"location":"api/v1/dsl/#tfx.v1.dsl.Artifact.get_custom_property","title":"get_custom_property","text":"<pre><code>get_custom_property(key: str) -&gt; Optional[Union[int, float, str, bool, JsonValueType]]\n</code></pre> <p>Gets a custom property with key. Return None if not found.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_custom_property(\n    self, key: str\n) -&gt; Optional[Union[int, float, str, bool, JsonValueType]]:\n  \"\"\"Gets a custom property with key. Return None if not found.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return None\n\n  json_value = self.get_json_value_custom_property(key)\n  if json_value:\n    return json_value\n\n  mlmd_value = self._artifact.custom_properties[key]\n  if mlmd_value.HasField('int_value'):\n    return mlmd_value.int_value\n  elif mlmd_value.HasField('double_value'):\n    return mlmd_value.double_value\n  elif mlmd_value.HasField('string_value'):\n    return mlmd_value.string_value\n  elif mlmd_value.HasField('bool_value'):\n    return mlmd_value.bool_value\n  return None\n</code></pre>"},{"location":"api/v1/dsl/#tfx.v1.dsl.Artifact.get_float_custom_property","title":"get_float_custom_property","text":"<pre><code>get_float_custom_property(key: str) -&gt; float\n</code></pre> <p>Gets a custom property of float type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_float_custom_property(self, key: str) -&gt; float:\n  \"\"\"Gets a custom property of float type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return 0.0\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, float):\n    return json_value\n  return self._artifact.custom_properties[key].double_value\n</code></pre>"},{"location":"api/v1/dsl/#tfx.v1.dsl.Artifact.get_int_custom_property","title":"get_int_custom_property","text":"<pre><code>get_int_custom_property(key: str) -&gt; int\n</code></pre> <p>Get a custom property of int type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_int_custom_property(self, key: str) -&gt; int:\n  \"\"\"Get a custom property of int type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return 0\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, float):\n    return int(json_value)\n  return self._artifact.custom_properties[key].int_value\n</code></pre>"},{"location":"api/v1/dsl/#tfx.v1.dsl.Artifact.get_json_value_custom_property","title":"get_json_value_custom_property","text":"<pre><code>get_json_value_custom_property(key: str) -&gt; JsonValueType\n</code></pre> <p>Get a custom property of JSON type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef get_json_value_custom_property(self, key: str) -&gt; JsonValueType:\n  \"\"\"Get a custom property of JSON type.\"\"\"\n  if key in self._cached_modifiable_custom_properties:\n    return self._cached_modifiable_custom_properties[key]\n  if (key not in self._artifact.custom_properties or\n      not self._artifact.custom_properties[key].HasField('struct_value')):\n    return None\n  value = _decode_struct_value(\n      self._artifact.custom_properties[key].struct_value)\n  # We must cache the decoded lists or dictionaries returned here so that\n  # if their recursive contents are modified, the Metadata proto message\n  # can be updated to reflect this.\n  if isinstance(value, (dict, list)):\n    self._cached_modifiable_custom_properties[key] = value\n  return value\n</code></pre>"},{"location":"api/v1/dsl/#tfx.v1.dsl.Artifact.get_proto_custom_property","title":"get_proto_custom_property","text":"<pre><code>get_proto_custom_property(key: str) -&gt; Optional[Message]\n</code></pre> <p>Get a custom property of proto type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef get_proto_custom_property(self, key: str) -&gt; Optional[message.Message]:\n  \"\"\"Get a custom property of proto type.\"\"\"\n  if key in self._cached_modifiable_custom_properties:\n    return self._cached_modifiable_custom_properties[key]\n  if (key not in self._artifact.custom_properties or\n      not self._artifact.custom_properties[key].HasField('proto_value')):\n    return None\n  value = proto_utils.unpack_proto_any(\n      self._artifact.custom_properties[key].proto_value)\n  # We must cache the protobuf message here so that if its contents are\n  # modified, the Metadata proto message can be updated to reflect this.\n  if isinstance(value, message.Message):\n    self._cached_modifiable_custom_properties[key] = value\n  return value\n</code></pre>"},{"location":"api/v1/dsl/#tfx.v1.dsl.Artifact.get_string_custom_property","title":"get_string_custom_property","text":"<pre><code>get_string_custom_property(key: str) -&gt; str\n</code></pre> <p>Get a custom property of string type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_string_custom_property(self, key: str) -&gt; str:\n  \"\"\"Get a custom property of string type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return ''\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, str):\n    return json_value\n  return self._artifact.custom_properties[key].string_value\n</code></pre>"},{"location":"api/v1/dsl/#tfx.v1.dsl.Artifact.set_bool_custom_property","title":"set_bool_custom_property","text":"<pre><code>set_bool_custom_property(key: str, value: bool)\n</code></pre> <p>Sets a custom property of bool type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_bool_custom_property(self, key: str, value: bool):\n  \"\"\"Sets a custom property of bool type.\"\"\"\n  self._artifact.custom_properties[key].bool_value = value\n</code></pre>"},{"location":"api/v1/dsl/#tfx.v1.dsl.Artifact.set_float_custom_property","title":"set_float_custom_property","text":"<pre><code>set_float_custom_property(key: str, value: float)\n</code></pre> <p>Sets a custom property of float type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_float_custom_property(self, key: str, value: float):\n  \"\"\"Sets a custom property of float type.\"\"\"\n  self._artifact.custom_properties[key].double_value = builtins.float(value)\n</code></pre>"},{"location":"api/v1/dsl/#tfx.v1.dsl.Artifact.set_int_custom_property","title":"set_int_custom_property","text":"<pre><code>set_int_custom_property(key: str, value: int)\n</code></pre> <p>Set a custom property of int type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_int_custom_property(self, key: str, value: int):\n  \"\"\"Set a custom property of int type.\"\"\"\n  self._artifact.custom_properties[key].int_value = builtins.int(value)\n</code></pre>"},{"location":"api/v1/dsl/#tfx.v1.dsl.Artifact.set_json_value_custom_property","title":"set_json_value_custom_property","text":"<pre><code>set_json_value_custom_property(key: str, value: JsonValueType)\n</code></pre> <p>Sets a custom property of JSON type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_json_value_custom_property(self, key: str, value: JsonValueType):\n  \"\"\"Sets a custom property of JSON type.\"\"\"\n  self._cached_modifiable_custom_properties[key] = value\n</code></pre>"},{"location":"api/v1/dsl/#tfx.v1.dsl.Artifact.set_mlmd_artifact","title":"set_mlmd_artifact","text":"<pre><code>set_mlmd_artifact(artifact: Artifact)\n</code></pre> <p>Replace the MLMD artifact object on this artifact.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_mlmd_artifact(self, artifact: metadata_store_pb2.Artifact):\n  \"\"\"Replace the MLMD artifact object on this artifact.\"\"\"\n  if not isinstance(artifact, metadata_store_pb2.Artifact):\n    raise ValueError(\n        ('Expected instance of metadata_store_pb2.Artifact, got %s '\n         'instead.') % (artifact,))\n  self._artifact = artifact\n  self._cached_modifiable_properties = {}\n  self._cached_modifiable_custom_properties = {}\n</code></pre>"},{"location":"api/v1/dsl/#tfx.v1.dsl.Artifact.set_mlmd_artifact_type","title":"set_mlmd_artifact_type","text":"<pre><code>set_mlmd_artifact_type(artifact_type: ArtifactType)\n</code></pre> <p>Set entire ArtifactType in this object.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_mlmd_artifact_type(self,\n                           artifact_type: metadata_store_pb2.ArtifactType):\n  \"\"\"Set entire ArtifactType in this object.\"\"\"\n  if not isinstance(artifact_type, metadata_store_pb2.ArtifactType):\n    raise ValueError(\n        ('Expected instance of metadata_store_pb2.ArtifactType, got %s '\n         'instead.') % (artifact_type,))\n  self._artifact_type = artifact_type\n  self._artifact.type_id = artifact_type.id\n</code></pre>"},{"location":"api/v1/dsl/#tfx.v1.dsl.Artifact.set_proto_custom_property","title":"set_proto_custom_property","text":"<pre><code>set_proto_custom_property(key: str, value: Message)\n</code></pre> <p>Sets a custom property of proto type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_proto_custom_property(self, key: str, value: message.Message):\n  \"\"\"Sets a custom property of proto type.\"\"\"\n  self._cached_modifiable_custom_properties[key] = value\n</code></pre>"},{"location":"api/v1/dsl/#tfx.v1.dsl.Artifact.set_string_custom_property","title":"set_string_custom_property","text":"<pre><code>set_string_custom_property(key: str, value: str)\n</code></pre> <p>Set a custom property of string type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_string_custom_property(self, key: str, value: str):\n  \"\"\"Set a custom property of string type.\"\"\"\n  self._artifact.custom_properties[key].string_value = value\n</code></pre>"},{"location":"api/v1/dsl/#tfx.v1.dsl.Artifact.to_json_dict","title":"to_json_dict","text":"<pre><code>to_json_dict() -&gt; Dict[str, Any]\n</code></pre> <p>Convert from an object to a JSON serializable dictionary.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef to_json_dict(self) -&gt; Dict[str, Any]:\n  return {\n      'artifact':\n          json.loads(\n              json_format.MessageToJson(\n                  message=self.mlmd_artifact,\n                  preserving_proto_field_name=True)),\n      'artifact_type':\n          json.loads(\n              json_format.MessageToJson(\n                  message=self._artifact_type,\n                  preserving_proto_field_name=True)),\n      '__artifact_class_module__':\n          self.__class__.__module__,\n      '__artifact_class_name__':\n          self.__class__.__name__,\n  }\n</code></pre>"},{"location":"api/v1/dsl/#tfx.v1.dsl.Channel","title":"Channel","text":"<pre><code>Channel(type: Type[Artifact], additional_properties: Optional[Dict[str, Property]] = None, additional_custom_properties: Optional[Dict[str, Property]] = None, artifacts: Optional[Iterable[Artifact]] = None, producer_component_id: Optional[str] = None, output_key: Optional[str] = None)\n</code></pre> <p>               Bases: <code>Jsonable</code>, <code>BaseChannel</code></p> <p>Legacy channel interface.</p> <p><code>Channel</code> used to represent the <code>BaseChannel</code> concept in the early TFX code, but due to having too much features in the same class, we refactored it to multiple classes:</p> <ul> <li>BaseChannel for the general input abstraction</li> <li>OutputChannel for <code>component.outputs['key']</code>.</li> <li>MLMDQueryChannel for simple filter-based input resolution.</li> </ul> <p>Please do not use this class directly, but instead use the alternatives. This class won't be removed in TFX 1.x due to backward compatibility guarantee though.</p> <p>Initialization of Channel.</p> PARAMETER DESCRIPTION <code>type</code> <p>Subclass of Artifact that represents the type of this Channel.</p> <p> TYPE: <code>Type[Artifact]</code> </p> <code>additional_properties</code> <p>(Optional) A mapping of properties which will be added to artifacts when this channel is used as an output of components. This is experimental and is subject to change in the future.</p> <p> TYPE: <code>Optional[Dict[str, Property]]</code> DEFAULT: <code>None</code> </p> <code>additional_custom_properties</code> <p>(Optional) A mapping of custom_properties which will be added to artifacts when this channel is used as an output of components. This is experimental and is subject to change in the future.</p> <p> TYPE: <code>Optional[Dict[str, Property]]</code> DEFAULT: <code>None</code> </p> <code>artifacts</code> <p>Deprecated and ignored, kept only for backward compatibility.</p> <p> TYPE: <code>Optional[Iterable[Artifact]]</code> DEFAULT: <code>None</code> </p> <code>producer_component_id</code> <p>(Optional) Producer component id of the Channel. This argument is internal/experimental and is subject to change in the future.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>output_key</code> <p>(Optional) The output key when producer component produces the artifacts in this Channel. This argument is internal/experimental and is subject to change in the future.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> METHOD DESCRIPTION <code>as_optional</code> <p>Creates an optional version of self.</p> <code>as_output_channel</code> <p>Internal method to derive OutputChannel from the Channel instance.</p> <code>from_json_dict</code> <p>Convert from dictionary data to an object.</p> <code>get</code> <p>Returns all artifacts that can be get from this Channel.</p> <code>get_data_dependent_node_ids</code> <p>Get data dependent nodes of this channel.</p> <code>set_artifacts</code> <p>Sets artifacts for a static Channel. Will be deprecated.</p> <code>to_json_dict</code> <p>Convert from an object to a JSON serializable dictionary.</p> ATTRIBUTE DESCRIPTION <code>is_optional</code> <p>If this is an \"optional\" channel. Changes Pipeline runtime behavior.</p> <p> TYPE: <code>Optional[bool]</code> </p> <code>type_name</code> <p>Name of the artifact type class that Channel takes.</p> <p> </p> Source code in <code>tfx/types/channel.py</code> <pre><code>def __init__(\n    self,\n    type: Type[Artifact],  # pylint: disable=redefined-builtin\n    additional_properties: Optional[Dict[str, Property]] = None,\n    additional_custom_properties: Optional[Dict[str, Property]] = None,\n    # TODO(b/161490287): deprecate static artifact.\n    artifacts: Optional[Iterable[Artifact]] = None,\n    producer_component_id: Optional[str] = None,\n    output_key: Optional[str] = None):\n  \"\"\"Initialization of Channel.\n\n  Args:\n    type: Subclass of Artifact that represents the type of this Channel.\n    additional_properties: (Optional) A mapping of properties which will be\n      added to artifacts when this channel is used as an output of components.\n      This is experimental and is subject to change in the future.\n    additional_custom_properties: (Optional) A mapping of custom_properties\n      which will be added to artifacts when this channel is used as an output\n      of components. This is experimental and is subject to change in the\n      future.\n    artifacts: Deprecated and ignored, kept only for backward compatibility.\n    producer_component_id: (Optional) Producer component id of the Channel.\n      This argument is internal/experimental and is subject to change in the\n      future.\n    output_key: (Optional) The output key when producer component produces the\n      artifacts in this Channel. This argument is internal/experimental and is\n      subject to change in the future.\n  \"\"\"\n  super().__init__(type=type)\n\n  if additional_properties is not None:\n    self._validate_additional_properties(additional_properties)\n  self.additional_properties = additional_properties or {}\n\n  if additional_custom_properties is not None:\n    self._validate_additional_custom_properties(additional_custom_properties)\n  self.additional_custom_properties = additional_custom_properties or {}\n\n  if producer_component_id is not None:\n    self._validate_producer_component_id(producer_component_id)\n  # Use a protected attribute &amp; getter/setter property as OutputChannel is\n  # overriding it.\n  self._producer_component_id = producer_component_id\n\n  if output_key is not None:\n    self._validate_output_key(output_key)\n  self.output_key = output_key\n\n  if artifacts:\n    logging.warning(\n        'Artifacts param is ignored by Channel constructor, please remove!')\n  self._artifacts = []\n  self._matching_channel_name = None\n</code></pre>"},{"location":"api/v1/dsl/#tfx.v1.dsl.Channel-attributes","title":"Attributes","text":""},{"location":"api/v1/dsl/#tfx.v1.dsl.Channel.is_optional","title":"is_optional  <code>property</code>","text":"<pre><code>is_optional: Optional[bool]\n</code></pre> <p>If this is an \"optional\" channel. Changes Pipeline runtime behavior.</p>"},{"location":"api/v1/dsl/#tfx.v1.dsl.Channel.type_name","title":"type_name  <code>property</code>","text":"<pre><code>type_name\n</code></pre> <p>Name of the artifact type class that Channel takes.</p>"},{"location":"api/v1/dsl/#tfx.v1.dsl.Channel-functions","title":"Functions","text":""},{"location":"api/v1/dsl/#tfx.v1.dsl.Channel.as_optional","title":"as_optional","text":"<pre><code>as_optional() -&gt; Self\n</code></pre> <p>Creates an optional version of self.</p> <p>By default component input channels are considered required, meaning if the channel does not contain at least 1 artifact, the component will be skipped. Making channel optional disables this requirement and allows componenst to be executed with no artifacts from this channel.</p> RETURNS DESCRIPTION <code>Self</code> <p>A copy of self which is optional.</p> Source code in <code>tfx/types/channel.py</code> <pre><code>def as_optional(self) -&gt; typing_extensions.Self:\n  \"\"\"Creates an optional version of self.\n\n  By default component input channels are considered required, meaning\n  if the channel does not contain at least 1 artifact, the component\n  will be skipped. Making channel optional disables this requirement and\n  allows componenst to be executed with no artifacts from this channel.\n\n  Returns:\n    A copy of self which is optional.\n  \"\"\"\n  new_channel = copy.copy(self)\n  new_channel._is_optional = True  # pylint: disable=protected-access\n  return new_channel\n</code></pre>"},{"location":"api/v1/dsl/#tfx.v1.dsl.Channel.as_output_channel","title":"as_output_channel","text":"<pre><code>as_output_channel(producer_component: Any, output_key: str) -&gt; 'OutputChannel'\n</code></pre> <p>Internal method to derive OutputChannel from the Channel instance.</p> <p>Return value (OutputChannel instance) is based on the shallow copy of self, so that any attribute change in one is reflected on the others.</p> PARAMETER DESCRIPTION <code>producer_component</code> <p>A BaseNode instance that is producing this channel.</p> <p> TYPE: <code>Any</code> </p> <code>output_key</code> <p>Corresponding node.outputs key for this channel.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>'OutputChannel'</code> <p>An OutputChannel instance that shares attributes with self.</p> Source code in <code>tfx/types/channel.py</code> <pre><code>@doc_controls.do_not_generate_docs\ndef as_output_channel(\n    self, producer_component: Any, output_key: str) -&gt; 'OutputChannel':\n  \"\"\"Internal method to derive OutputChannel from the Channel instance.\n\n  Return value (OutputChannel instance) is based on the shallow copy of self,\n  so that any attribute change in one is reflected on the others.\n\n  Args:\n    producer_component: A BaseNode instance that is producing this channel.\n    output_key: Corresponding node.outputs key for this channel.\n\n  Returns:\n    An OutputChannel instance that shares attributes with self.\n  \"\"\"\n  # Disable pylint false alarm for safe access of protected attributes.\n  # pylint: disable=protected-access\n  result = OutputChannel(self.type, producer_component, output_key)\n  result.additional_properties = self.additional_properties\n  result.additional_custom_properties = self.additional_custom_properties\n  result.set_artifacts(self._artifacts)\n  return result\n</code></pre>"},{"location":"api/v1/dsl/#tfx.v1.dsl.Channel.from_json_dict","title":"from_json_dict  <code>classmethod</code>","text":"<pre><code>from_json_dict(dict_data: Dict[str, Any]) -&gt; Any\n</code></pre> <p>Convert from dictionary data to an object.</p> Source code in <code>tfx/types/channel.py</code> <pre><code>@classmethod\n@doc_controls.do_not_doc_inheritable\ndef from_json_dict(cls, dict_data: Dict[str, Any]) -&gt; Any:\n  artifact_type = metadata_store_pb2.ArtifactType()\n  json_format.Parse(json.dumps(dict_data['type']), artifact_type)\n  type_cls = artifact_utils.get_artifact_type_class(artifact_type)\n  artifacts = list(Artifact.from_json_dict(a) for a in dict_data['artifacts'])\n  additional_properties = dict_data['additional_properties']\n  additional_custom_properties = dict_data['additional_custom_properties']\n  producer_component_id = dict_data.get('producer_component_id', None)\n  output_key = dict_data.get('output_key', None)\n  return Channel(\n      type=type_cls,\n      additional_properties=additional_properties,\n      additional_custom_properties=additional_custom_properties,\n      producer_component_id=producer_component_id,\n      output_key=output_key).set_artifacts(artifacts)\n</code></pre>"},{"location":"api/v1/dsl/#tfx.v1.dsl.Channel.get","title":"get","text":"<pre><code>get() -&gt; Iterable[Artifact]\n</code></pre> <p>Returns all artifacts that can be get from this Channel.</p> RETURNS DESCRIPTION <code>Iterable[Artifact]</code> <p>An artifact collection.</p> Source code in <code>tfx/types/channel.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef get(self) -&gt; Iterable[Artifact]:\n  \"\"\"Returns all artifacts that can be get from this Channel.\n\n  Returns:\n    An artifact collection.\n  \"\"\"\n  # TODO(b/125037186): We should support dynamic query against a Channel\n  # instead of a static Artifact collection.\n  return self._artifacts\n</code></pre>"},{"location":"api/v1/dsl/#tfx.v1.dsl.Channel.get_data_dependent_node_ids","title":"get_data_dependent_node_ids","text":"<pre><code>get_data_dependent_node_ids() -&gt; Set[str]\n</code></pre> <p>Get data dependent nodes of this channel.</p> <p>Currently only the <code>OutputChannel</code> directly imposes the data dependency, but other channels can also indirectly have a data dependency if they depend on the OutputChannel. Use this abstract method to define transitive data dependency.</p> RETURNS DESCRIPTION <code>Set[str]</code> <p>A set of data-dependent node IDs.</p> Source code in <code>tfx/types/channel.py</code> <pre><code>def get_data_dependent_node_ids(self) -&gt; Set[str]:\n  return set()\n</code></pre>"},{"location":"api/v1/dsl/#tfx.v1.dsl.Channel.set_artifacts","title":"set_artifacts","text":"<pre><code>set_artifacts(artifacts: Iterable[Artifact]) -&gt; 'Channel'\n</code></pre> <p>Sets artifacts for a static Channel. Will be deprecated.</p> Source code in <code>tfx/types/channel.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_artifacts(self, artifacts: Iterable[Artifact]) -&gt; 'Channel':\n  \"\"\"Sets artifacts for a static Channel. Will be deprecated.\"\"\"\n  if self._matching_channel_name:\n    raise ValueError(\n        'Only one of `artifacts` and `matching_channel_name` should be set.')\n  self._validate_static_artifacts(artifacts)\n  self._artifacts = artifacts\n  return self\n</code></pre>"},{"location":"api/v1/dsl/#tfx.v1.dsl.Channel.to_json_dict","title":"to_json_dict","text":"<pre><code>to_json_dict() -&gt; Dict[str, Any]\n</code></pre> <p>Convert from an object to a JSON serializable dictionary.</p> Source code in <code>tfx/types/channel.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef to_json_dict(self) -&gt; Dict[str, Any]:\n  return {\n      'type':\n          json.loads(\n              json_format.MessageToJson(\n                  message=self.type._get_artifact_type(),  # pylint: disable=protected-access\n                  preserving_proto_field_name=True)),\n      'artifacts':\n          list(a.to_json_dict() for a in self._artifacts),\n      'additional_properties':\n          self.additional_properties,\n      'additional_custom_properties':\n          self.additional_custom_properties,\n      'producer_component_id':\n          (self.producer_component_id if self.producer_component_id else None\n          ),\n      'output_key': (self.output_key if self.output_key else None),\n  }\n</code></pre>"},{"location":"api/v1/dsl/#tfx.v1.dsl.Cond","title":"Cond","text":"<pre><code>Cond(predicate: Predicate)\n</code></pre> <p>               Bases: <code>DslContextManager[None]</code></p> <p>Cond context manager that disable containing nodes if predicate is False.</p> <p>Cond blocks can be nested to express the nested conditions.</p> <p>Usage:</p> <pre><code>evaluator = Evaluator(\n      examples=example_gen.outputs[\"examples\"],\n      model=trainer.outputs[\"model\"],\n      eval_config=EvalConfig(...),\n)\n\nwith Cond(evaluator.outputs[\"blessing\"].future().custom_property(\"blessed\") == 1):\n    pusher = Pusher(\n        model=trainer.outputs[\"model\"], push_destination=PushDestination(...)\n    )\n</code></pre> Source code in <code>tfx/dsl/experimental/conditionals/conditional.py</code> <pre><code>def __init__(self, predicate: placeholder.Predicate):\n  super().__init__()\n  self._predicate = predicate\n</code></pre>"},{"location":"api/v1/dsl/#tfx.v1.dsl.ExecutionMode","title":"ExecutionMode","text":"<p>               Bases: <code>Enum</code></p> <p>Execution mode of a pipeline.</p> <p>Please see this RFC for more details.</p>"},{"location":"api/v1/dsl/#tfx.v1.dsl.Importer","title":"Importer","text":"<pre><code>Importer(source_uri: str, artifact_type: Type[Artifact], reimport: Optional[bool] = False, properties: Optional[Dict[str, Union[str, int]]] = None, custom_properties: Optional[Dict[str, Union[str, int]]] = None, output_key: Optional[str] = None)\n</code></pre> <p>               Bases: <code>BaseNode</code></p> <p>Definition for TFX Importer.</p> <p>The Importer is a special TFX node which registers an external resource into MLMD so that downstream nodes can use the registered artifact as an input.</p> <p>Here is an example to use the Importer:</p> <pre><code>importer = Importer(\n    source_uri=\"uri/to/schema\",\n    artifact_type=standard_artifacts.Schema,\n    reimport=False,\n).with_id(\"import_schema\")\nschema_gen = SchemaGen(\n    fixed_schema=importer.outputs[\"result\"],\n    examples=...,\n)\n</code></pre> <p>Init function for the Importer.</p> PARAMETER DESCRIPTION <code>source_uri</code> <p>the URI of the resource that needs to be registered.</p> <p> TYPE: <code>str</code> </p> <code>artifact_type</code> <p>the type of the artifact to import.</p> <p> TYPE: <code>Type[Artifact]</code> </p> <code>reimport</code> <p>whether or not to re-import as a new artifact if the URI has been imported in before.</p> <p> TYPE: <code>Optional[bool]</code> DEFAULT: <code>False</code> </p> <code>properties</code> <p>Dictionary of properties for the imported Artifact. These properties should be ones declared for the given artifact_type (see the PROPERTIES attribute of the definition of the type for details).</p> <p> TYPE: <code>Optional[Dict[str, Union[str, int]]]</code> DEFAULT: <code>None</code> </p> <code>custom_properties</code> <p>Dictionary of custom properties for the imported Artifact. These properties should be of type Text or int.</p> <p> TYPE: <code>Optional[Dict[str, Union[str, int]]]</code> DEFAULT: <code>None</code> </p> <code>output_key</code> <p>The key to use for the imported artifact in the Importer's output dictionary. Defaults to 'result'.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> METHOD DESCRIPTION <code>add_downstream_node</code> <p>Experimental: Add another component that must run after this one.</p> <code>add_downstream_nodes</code> <p>Experimental: Add another component that must run after this one.</p> <code>add_upstream_node</code> <p>Experimental: Add another component that must run before this one.</p> <code>add_upstream_nodes</code> <p>Experimental: Add components that must run before this one.</p> <code>from_json_dict</code> <p>Convert from dictionary data to an object.</p> <code>to_json_dict</code> <p>Convert from an object to a JSON serializable dictionary.</p> ATTRIBUTE DESCRIPTION <code>id</code> <p>Node id, unique across all TFX nodes in a pipeline.</p> <p> TYPE: <code>str</code> </p> <code>outputs</code> <p>Output Channel dict that contains imported artifacts.</p> <p> TYPE: <code>Dict[str, Any]</code> </p> Source code in <code>tfx/dsl/components/common/importer.py</code> <pre><code>def __init__(self,\n             source_uri: str,\n             artifact_type: Type[types.Artifact],\n             reimport: Optional[bool] = False,\n             properties: Optional[Dict[str, Union[str, int]]] = None,\n             custom_properties: Optional[Dict[str, Union[str, int]]] = None,\n             output_key: Optional[str] = None):\n  \"\"\"Init function for the Importer.\n\n  Args:\n    source_uri: the URI of the resource that needs to be registered.\n    artifact_type: the type of the artifact to import.\n    reimport: whether or not to re-import as a new artifact if the URI has\n      been imported in before.\n    properties: Dictionary of properties for the imported Artifact. These\n      properties should be ones declared for the given artifact_type (see the\n      PROPERTIES attribute of the definition of the type for details).\n    custom_properties: Dictionary of custom properties for the imported\n      Artifact. These properties should be of type Text or int.\n    output_key: The key to use for the imported artifact in the Importer's\n      output dictionary. Defaults to 'result'.\n  \"\"\"\n  self._source_uri = source_uri\n  self._reimport = reimport\n  self._output_key = output_key or IMPORT_RESULT_KEY\n\n  artifact = artifact_type()\n  artifact.is_external = True\n  _set_artifact_properties(artifact, properties, custom_properties)\n\n  output_channel = types.OutputChannel(\n      artifact_type,\n      producer_component=self,\n      output_key=self._output_key,\n      additional_properties=properties,\n      additional_custom_properties=custom_properties)\n\n  # TODO(b/161490287): remove static artifacts.\n  output_channel.set_artifacts([artifact])\n  self._output_dict = {self._output_key: output_channel}\n\n  super().__init__(driver_class=ImporterDriver)\n</code></pre>"},{"location":"api/v1/dsl/#tfx.v1.dsl.Importer-attributes","title":"Attributes","text":""},{"location":"api/v1/dsl/#tfx.v1.dsl.Importer.id","title":"id  <code>property</code> <code>writable</code>","text":"<pre><code>id: str\n</code></pre> <p>Node id, unique across all TFX nodes in a pipeline.</p> <p>If <code>id</code> is set by the user, return it directly. Otherwise, return .</p> RETURNS DESCRIPTION <code>str</code> <p>node id.</p>"},{"location":"api/v1/dsl/#tfx.v1.dsl.Importer.outputs","title":"outputs  <code>property</code>","text":"<pre><code>outputs: Dict[str, Any]\n</code></pre> <p>Output Channel dict that contains imported artifacts.</p>"},{"location":"api/v1/dsl/#tfx.v1.dsl.Importer-functions","title":"Functions","text":""},{"location":"api/v1/dsl/#tfx.v1.dsl.Importer.add_downstream_node","title":"add_downstream_node","text":"<pre><code>add_downstream_node(downstream_node)\n</code></pre> <p>Experimental: Add another component that must run after this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> <p>It is symmetric with <code>add_upstream_node</code>.</p> PARAMETER DESCRIPTION <code>downstream_node</code> <p>a component that must run after this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_downstream_node(self, downstream_node):\n  \"\"\"Experimental: Add another component that must run after this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n  It is symmetric with `add_upstream_node`.\n\n  Args:\n    downstream_node: a component that must run after this node.\n  \"\"\"\n  self._downstream_nodes.add(downstream_node)\n  if self not in downstream_node.upstream_nodes:\n    downstream_node.add_upstream_node(self)\n</code></pre>"},{"location":"api/v1/dsl/#tfx.v1.dsl.Importer.add_downstream_nodes","title":"add_downstream_nodes","text":"<pre><code>add_downstream_nodes(downstream_nodes)\n</code></pre> <p>Experimental: Add another component that must run after this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> <p>It is symmetric with <code>add_upstream_nodes</code>.</p> PARAMETER DESCRIPTION <code>downstream_nodes</code> <p>a list of components that must run after this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_downstream_nodes(self, downstream_nodes):\n  \"\"\"Experimental: Add another component that must run after this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n  It is symmetric with `add_upstream_nodes`.\n\n  Args:\n    downstream_nodes: a list of components that must run after this node.\n  \"\"\"\n  self._downstream_nodes.update(downstream_nodes)\n  for downstream_node in downstream_nodes:\n    if self not in downstream_node.upstream_nodes:\n      downstream_node.add_upstream_node(self)\n</code></pre>"},{"location":"api/v1/dsl/#tfx.v1.dsl.Importer.add_upstream_node","title":"add_upstream_node","text":"<pre><code>add_upstream_node(upstream_node)\n</code></pre> <p>Experimental: Add another component that must run before this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> <p>It is symmetric with <code>add_downstream_node</code>.</p> PARAMETER DESCRIPTION <code>upstream_node</code> <p>a component that must run before this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_upstream_node(self, upstream_node):\n  \"\"\"Experimental: Add another component that must run before this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n  It is symmetric with `add_downstream_node`.\n\n  Args:\n    upstream_node: a component that must run before this node.\n  \"\"\"\n  self._upstream_nodes.add(upstream_node)\n  if self not in upstream_node.downstream_nodes:\n    upstream_node.add_downstream_node(self)\n</code></pre>"},{"location":"api/v1/dsl/#tfx.v1.dsl.Importer.add_upstream_nodes","title":"add_upstream_nodes","text":"<pre><code>add_upstream_nodes(upstream_nodes)\n</code></pre> <p>Experimental: Add components that must run before this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> PARAMETER DESCRIPTION <code>upstream_nodes</code> <p>a list of components that must run before this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_upstream_nodes(self, upstream_nodes):\n  \"\"\"Experimental: Add components that must run before this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n\n  Args:\n    upstream_nodes: a list of components that must run before this node.\n  \"\"\"\n  self._upstream_nodes.update(upstream_nodes)\n  for upstream_node in upstream_nodes:\n    if self not in upstream_node.downstream_nodes:\n      upstream_node.add_downstream_node(self)\n</code></pre>"},{"location":"api/v1/dsl/#tfx.v1.dsl.Importer.from_json_dict","title":"from_json_dict  <code>classmethod</code>","text":"<pre><code>from_json_dict(dict_data: Dict[str, Any]) -&gt; Any\n</code></pre> <p>Convert from dictionary data to an object.</p> Source code in <code>tfx/utils/json_utils.py</code> <pre><code>@classmethod\n@doc_controls.do_not_doc_in_subclasses\ndef from_json_dict(cls, dict_data: Dict[str, Any]) -&gt; Any:\n  \"\"\"Convert from dictionary data to an object.\"\"\"\n  instance = cls.__new__(cls)\n  instance.__dict__ = dict_data\n  return instance\n</code></pre>"},{"location":"api/v1/dsl/#tfx.v1.dsl.Importer.to_json_dict","title":"to_json_dict","text":"<pre><code>to_json_dict() -&gt; Dict[str, Any]\n</code></pre> <p>Convert from an object to a JSON serializable dictionary.</p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef to_json_dict(self) -&gt; Dict[str, Any]:\n  \"\"\"Convert from an object to a JSON serializable dictionary.\"\"\"\n  return dict((k, v)\n              for k, v in self.__dict__.items()\n              if k not in ['_upstream_nodes', '_downstream_nodes'])\n</code></pre>"},{"location":"api/v1/dsl/#tfx.v1.dsl.Pipeline","title":"Pipeline","text":"<pre><code>Pipeline(pipeline_name: str, pipeline_root: Optional[Union[str, Placeholder]] = '', metadata_connection_config: Optional[ConnectionConfigType] = None, components: Iterable[BaseNode] = (), enable_cache: bool = False, beam_pipeline_args: Optional[List[Union[str, Placeholder]]] = None, platform_config: Optional[Message] = None, execution_mode: ExecutionMode = SYNC, inputs: Optional[PipelineInputs] = None, outputs: Optional[Dict[str, OutputChannel]] = None, dsl_context_registry: Optional[DslContextRegistry] = None)\n</code></pre> <p>               Bases: <code>BaseNode</code></p> <p>Logical TFX pipeline object.</p> <p>Pipeline object represents the DAG of TFX components, which can be run using one of the pipeline orchestration systems that TFX supports. For details, please refer to the guide.</p> ATTRIBUTE DESCRIPTION <code>components</code> <p>A deterministic list of logical components of this pipeline, which are deduped and topologically sorted.</p> <p> </p> <code>enable_cache</code> <p>Whether or not cache is enabled for this run.</p> <p> </p> <code>metadata_connection_config</code> <p>The config to connect to ML metadata.</p> <p> </p> <code>execution_mode</code> <p>Execution mode of the pipeline. Currently only support synchronous execution mode.</p> <p> </p> <code>beam_pipeline_args</code> <p>Pipeline arguments for Beam powered Components. Use <code>with_beam_pipeline_args</code> to set component level Beam args.</p> <p> </p> <code>platform_config</code> <p>Pipeline level platform config, in proto form.</p> <p> </p> <p>Initialize pipeline.</p> PARAMETER DESCRIPTION <code>pipeline_name</code> <p>Name of the pipeline;</p> <p> TYPE: <code>str</code> </p> <code>pipeline_root</code> <p>Path to root directory of the pipeline. This will most often be just a string. Some orchestrators may have limited support for constructing this from a Placeholder, e.g. a RuntimeInfoPlaceholder that refers to fields from the platform config. pipeline_root is optional only if the pipeline is composed within another parent pipeline, in which case it will inherit its parent pipeline's root.</p> <p> TYPE: <code>Optional[Union[str, Placeholder]]</code> DEFAULT: <code>''</code> </p> <code>metadata_connection_config</code> <p>The config to connect to ML metadata.</p> <p> TYPE: <code>Optional[ConnectionConfigType]</code> DEFAULT: <code>None</code> </p> <code>components</code> <p>Optional list of components to construct the pipeline.</p> <p> TYPE: <code>Iterable[BaseNode]</code> DEFAULT: <code>()</code> </p> <code>enable_cache</code> <p>Whether or not cache is enabled for this run.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>beam_pipeline_args</code> <p>Pipeline arguments for Beam powered Components.</p> <p> TYPE: <code>Optional[List[Union[str, Placeholder]]]</code> DEFAULT: <code>None</code> </p> <code>platform_config</code> <p>Pipeline level platform config, in proto form.</p> <p> TYPE: <code>Optional[Message]</code> DEFAULT: <code>None</code> </p> <code>execution_mode</code> <p>The execution mode of the pipeline, can be SYNC or ASYNC.</p> <p> TYPE: <code>ExecutionMode</code> DEFAULT: <code>SYNC</code> </p> <code>inputs</code> <p>Optional inputs of a pipeline.</p> <p> TYPE: <code>Optional[PipelineInputs]</code> DEFAULT: <code>None</code> </p> <code>outputs</code> <p>Optional outputs of a pipeline.</p> <p> TYPE: <code>Optional[Dict[str, OutputChannel]]</code> DEFAULT: <code>None</code> </p> <code>dsl_context_registry</code> <p>DslContextRegistry to use for this pipeline, if not provided then the current context (potentially a new DslContext) will be used.</p> <p> TYPE: <code>Optional[DslContextRegistry]</code> DEFAULT: <code>None</code> </p> METHOD DESCRIPTION <code>add_downstream_node</code> <p>Experimental: Add another component that must run after this one.</p> <code>add_downstream_nodes</code> <p>Experimental: Add another component that must run after this one.</p> <code>add_upstream_node</code> <p>Experimental: Add another component that must run before this one.</p> <code>add_upstream_nodes</code> <p>Experimental: Add components that must run before this one.</p> <code>from_json_dict</code> <p>Convert from dictionary data to an object.</p> <code>to_json_dict</code> <p>Convert from an object to a JSON serializable dictionary.</p> Source code in <code>tfx/orchestration/pipeline.py</code> <pre><code>def __init__(\n    self,\n    pipeline_name: str,\n    pipeline_root: Optional[Union[str, ph.Placeholder]] = '',\n    metadata_connection_config: Optional[\n        metadata.ConnectionConfigType\n    ] = None,\n    components: Iterable[base_node.BaseNode] = (),\n    enable_cache: bool = False,\n    beam_pipeline_args: Optional[List[Union[str, ph.Placeholder]]] = None,\n    platform_config: Optional[message.Message] = None,\n    execution_mode: ExecutionMode = ExecutionMode.SYNC,\n    inputs: Optional[PipelineInputs] = None,\n    outputs: Optional[Dict[str, channel.OutputChannel]] = None,\n    dsl_context_registry: Optional[\n        dsl_context_registry_lib.DslContextRegistry\n    ] = None,\n):\n  \"\"\"Initialize pipeline.\n\n  Args:\n    pipeline_name: Name of the pipeline;\n    pipeline_root: Path to root directory of the pipeline. This will most\n      often be just a string. Some orchestrators may have limited support for\n      constructing this from a Placeholder, e.g. a RuntimeInfoPlaceholder that\n      refers to fields from the platform config. pipeline_root is optional\n      only if the pipeline is composed within another parent pipeline, in\n      which case it will inherit its parent pipeline's root.\n    metadata_connection_config: The config to connect to ML metadata.\n    components: Optional list of components to construct the pipeline.\n    enable_cache: Whether or not cache is enabled for this run.\n    beam_pipeline_args: Pipeline arguments for Beam powered Components.\n    platform_config: Pipeline level platform config, in proto form.\n    execution_mode: The execution mode of the pipeline, can be SYNC or ASYNC.\n    inputs: Optional inputs of a pipeline.\n    outputs: Optional outputs of a pipeline.\n    dsl_context_registry: DslContextRegistry to use for this pipeline, if not\n      provided then the current context (potentially a new DslContext) will be\n      used.\n  \"\"\"\n  if len(pipeline_name) &gt; _MAX_PIPELINE_NAME_LENGTH:\n    raise ValueError(\n        f'pipeline {pipeline_name} exceeds maximum allowed length: {_MAX_PIPELINE_NAME_LENGTH}.'\n    )\n  self.pipeline_name = pipeline_name\n\n  # Registry extraction should come before super().__init__() which put self\n  # to the active DslContextRegistry.\n  self._dsl_context_registry = dsl_context_registry\n  if self._dsl_context_registry is None:\n    parent_reg = dsl_context_registry_lib.get()\n    self._dsl_context_registry = parent_reg.extract_for_pipeline(components)\n\n  # Initialize pipeline as a node.\n  super().__init__()\n\n  if inputs:\n    inputs.pipeline = self\n  self._inputs = inputs\n  if outputs:\n    self._outputs = {\n        k: channel.PipelineOutputChannel(v, pipeline=self, output_key=k)\n        for k, v in outputs.items()\n    }\n  else:\n    self._outputs = {}\n  self._id = pipeline_name\n\n  # Once pipeline is finalized, this instance is regarded as immutable and\n  # any detectable mutation will raise an error.\n  self._finalized = False\n\n  # TODO(b/183621450): deprecate PipelineInfo.\n  self.pipeline_info = data_types.PipelineInfo(  # pylint: disable=g-missing-from-attributes\n      pipeline_name=pipeline_name,\n      pipeline_root=pipeline_root)\n  self.enable_cache = enable_cache\n  self.metadata_connection_config = metadata_connection_config\n  self.execution_mode = execution_mode\n\n  self._beam_pipeline_args = beam_pipeline_args or []\n\n  self.platform_config = platform_config\n\n  # TODO: b/324635891 - Remove all references and clean this up.\n  self.additional_pipeline_args = {}\n\n  # TODO(b/216581002): Use self._dsl_context_registry to obtain components.\n  self._components = []\n  if components:\n    self._set_components(components)\n</code></pre>"},{"location":"api/v1/dsl/#tfx.v1.dsl.Pipeline-attributes","title":"Attributes","text":""},{"location":"api/v1/dsl/#tfx.v1.dsl.Pipeline.beam_pipeline_args","title":"beam_pipeline_args  <code>property</code>","text":"<pre><code>beam_pipeline_args\n</code></pre> <p>Beam pipeline args used for all components in the pipeline.</p>"},{"location":"api/v1/dsl/#tfx.v1.dsl.Pipeline.components","title":"components  <code>property</code> <code>writable</code>","text":"<pre><code>components\n</code></pre> <p>A deterministic list of logical components that are deduped and topologically sorted.</p>"},{"location":"api/v1/dsl/#tfx.v1.dsl.Pipeline.id","title":"id  <code>property</code>","text":"<pre><code>id\n</code></pre> <p>Node id, unique across all TFX nodes in a pipeline.</p> <p>If <code>id</code> is set by the user, return it directly. Otherwise, return .</p> RETURNS DESCRIPTION <code>str</code> <p>node id.</p>"},{"location":"api/v1/dsl/#tfx.v1.dsl.Pipeline-functions","title":"Functions","text":""},{"location":"api/v1/dsl/#tfx.v1.dsl.Pipeline.add_downstream_node","title":"add_downstream_node","text":"<pre><code>add_downstream_node(downstream_node)\n</code></pre> <p>Experimental: Add another component that must run after this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> <p>It is symmetric with <code>add_upstream_node</code>.</p> PARAMETER DESCRIPTION <code>downstream_node</code> <p>a component that must run after this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_downstream_node(self, downstream_node):\n  \"\"\"Experimental: Add another component that must run after this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n  It is symmetric with `add_upstream_node`.\n\n  Args:\n    downstream_node: a component that must run after this node.\n  \"\"\"\n  self._downstream_nodes.add(downstream_node)\n  if self not in downstream_node.upstream_nodes:\n    downstream_node.add_upstream_node(self)\n</code></pre>"},{"location":"api/v1/dsl/#tfx.v1.dsl.Pipeline.add_downstream_nodes","title":"add_downstream_nodes","text":"<pre><code>add_downstream_nodes(downstream_nodes)\n</code></pre> <p>Experimental: Add another component that must run after this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> <p>It is symmetric with <code>add_upstream_nodes</code>.</p> PARAMETER DESCRIPTION <code>downstream_nodes</code> <p>a list of components that must run after this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_downstream_nodes(self, downstream_nodes):\n  \"\"\"Experimental: Add another component that must run after this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n  It is symmetric with `add_upstream_nodes`.\n\n  Args:\n    downstream_nodes: a list of components that must run after this node.\n  \"\"\"\n  self._downstream_nodes.update(downstream_nodes)\n  for downstream_node in downstream_nodes:\n    if self not in downstream_node.upstream_nodes:\n      downstream_node.add_upstream_node(self)\n</code></pre>"},{"location":"api/v1/dsl/#tfx.v1.dsl.Pipeline.add_upstream_node","title":"add_upstream_node","text":"<pre><code>add_upstream_node(upstream_node)\n</code></pre> <p>Experimental: Add another component that must run before this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> <p>It is symmetric with <code>add_downstream_node</code>.</p> PARAMETER DESCRIPTION <code>upstream_node</code> <p>a component that must run before this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_upstream_node(self, upstream_node):\n  \"\"\"Experimental: Add another component that must run before this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n  It is symmetric with `add_downstream_node`.\n\n  Args:\n    upstream_node: a component that must run before this node.\n  \"\"\"\n  self._upstream_nodes.add(upstream_node)\n  if self not in upstream_node.downstream_nodes:\n    upstream_node.add_downstream_node(self)\n</code></pre>"},{"location":"api/v1/dsl/#tfx.v1.dsl.Pipeline.add_upstream_nodes","title":"add_upstream_nodes","text":"<pre><code>add_upstream_nodes(upstream_nodes)\n</code></pre> <p>Experimental: Add components that must run before this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> PARAMETER DESCRIPTION <code>upstream_nodes</code> <p>a list of components that must run before this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_upstream_nodes(self, upstream_nodes):\n  \"\"\"Experimental: Add components that must run before this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n\n  Args:\n    upstream_nodes: a list of components that must run before this node.\n  \"\"\"\n  self._upstream_nodes.update(upstream_nodes)\n  for upstream_node in upstream_nodes:\n    if self not in upstream_node.downstream_nodes:\n      upstream_node.add_downstream_node(self)\n</code></pre>"},{"location":"api/v1/dsl/#tfx.v1.dsl.Pipeline.from_json_dict","title":"from_json_dict  <code>classmethod</code>","text":"<pre><code>from_json_dict(dict_data: Dict[str, Any]) -&gt; Any\n</code></pre> <p>Convert from dictionary data to an object.</p> Source code in <code>tfx/utils/json_utils.py</code> <pre><code>@classmethod\n@doc_controls.do_not_doc_in_subclasses\ndef from_json_dict(cls, dict_data: Dict[str, Any]) -&gt; Any:\n  \"\"\"Convert from dictionary data to an object.\"\"\"\n  instance = cls.__new__(cls)\n  instance.__dict__ = dict_data\n  return instance\n</code></pre>"},{"location":"api/v1/dsl/#tfx.v1.dsl.Pipeline.to_json_dict","title":"to_json_dict","text":"<pre><code>to_json_dict() -&gt; Dict[str, Any]\n</code></pre> <p>Convert from an object to a JSON serializable dictionary.</p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef to_json_dict(self) -&gt; Dict[str, Any]:\n  \"\"\"Convert from an object to a JSON serializable dictionary.\"\"\"\n  return dict((k, v)\n              for k, v in self.__dict__.items()\n              if k not in ['_upstream_nodes', '_downstream_nodes'])\n</code></pre>"},{"location":"api/v1/dsl/#tfx.v1.dsl.Resolver","title":"Resolver","text":"<pre><code>Resolver(strategy_class: Optional[Type[ResolverStrategy]] = None, config: Optional[Dict[str, JsonableType]] = None, **channels: BaseChannel)\n</code></pre> <p>               Bases: <code>BaseNode</code></p> <p>Definition for TFX Resolver.</p> <p>Resolver is a special TFX node which handles special artifact resolution logics that will be used as inputs for downstream nodes.</p> <p>To use Resolver, pass the followings to the Resolver constructor:</p> <ul> <li>Name of the Resolver instance</li> <li>A subclass of ResolverStrategy</li> <li>Configs that will be used to construct an instance of ResolverStrategy</li> <li>Channels to resolve with their tag, in the form of kwargs</li> </ul> <p>Here is an example:</p> <pre><code>example_gen = ImportExampleGen(...)\nexamples_resolver = Resolver(\n    strategy_class=tfx.dsl.experimental.SpanRangeStrategy,\n    config={\"range_config\": range_config},\n    examples=Channel(\n        type=Examples,\n        producer_component_id=example_gen.id,\n    ),\n).with_id(\"Resolver.span_resolver\")\ntrainer = Trainer(\n    examples=examples_resolver.outputs[\"examples\"],\n    ...,\n)\n</code></pre> <p>You can find experimental <code>ResolverStrategy</code> classes under <code>tfx.v1.dsl.experimental</code> module, including <code>LatestArtifactStrategy</code>, <code>LatestBlessedModelStrategy</code>, <code>SpanRangeStrategy</code>, etc.</p> <p>Init function for Resolver.</p> PARAMETER DESCRIPTION <code>strategy_class</code> <p>Optional <code>ResolverStrategy</code> which contains the artifact   resolution logic.</p> <p> TYPE: <code>Optional[Type[ResolverStrategy]]</code> DEFAULT: <code>None</code> </p> <code>config</code> <p>Optional dict of key to Jsonable type for constructing   resolver_strategy.</p> <p> TYPE: <code>Optional[Dict[str, JsonableType]]</code> DEFAULT: <code>None</code> </p> <code>**channels</code> <p>Input channels to the Resolver node as keyword arguments.</p> <p> TYPE: <code>BaseChannel</code> DEFAULT: <code>{}</code> </p> METHOD DESCRIPTION <code>add_downstream_node</code> <p>Experimental: Add another component that must run after this one.</p> <code>add_downstream_nodes</code> <p>Experimental: Add another component that must run after this one.</p> <code>add_upstream_node</code> <p>Experimental: Add another component that must run before this one.</p> <code>add_upstream_nodes</code> <p>Experimental: Add components that must run before this one.</p> <code>from_json_dict</code> <p>Convert from dictionary data to an object.</p> <code>to_json_dict</code> <p>Convert from an object to a JSON serializable dictionary.</p> ATTRIBUTE DESCRIPTION <code>id</code> <p>Node id, unique across all TFX nodes in a pipeline.</p> <p> TYPE: <code>str</code> </p> <code>outputs</code> <p>Output Channel dict that contains resolved artifacts.</p> <p> TYPE: <code>Dict[str, OutputChannel]</code> </p> Source code in <code>tfx/dsl/components/common/resolver.py</code> <pre><code>def __init__(self,\n             strategy_class: Optional[Type[ResolverStrategy]] = None,\n             config: Optional[Dict[str, json_utils.JsonableType]] = None,\n             **channels: types.BaseChannel):\n  \"\"\"Init function for Resolver.\n\n  Args:\n    strategy_class: Optional `ResolverStrategy` which contains the artifact\n        resolution logic.\n    config: Optional dict of key to Jsonable type for constructing\n        resolver_strategy.\n    **channels: Input channels to the Resolver node as keyword arguments.\n  \"\"\"\n  if (strategy_class is not None and\n      not issubclass(strategy_class, ResolverStrategy)):\n    raise TypeError('strategy_class should be ResolverStrategy, but got '\n                    f'{strategy_class} instead.')\n  if strategy_class is None and config is not None:\n    raise ValueError('Cannot use config parameter without strategy_class.')\n  for input_key, channel in channels.items():\n    if not isinstance(channel, channel_types.BaseChannel):\n      raise ValueError(f'Resolver got non-BaseChannel argument {input_key}.')\n  self._strategy_class = strategy_class\n  self._config = config or {}\n  # An observed inputs from DSL as if Resolver node takes an inputs.\n  # TODO(b/246907396): Remove raw_inputs usage.\n  self._raw_inputs = dict(channels)\n  if strategy_class is not None:\n    output_node = resolver_op.OpNode(\n        op_type=strategy_class,\n        output_data_type=resolver_op.DataType.ARTIFACT_MULTIMAP,\n        args=[\n            resolver_op.DictNode({\n                input_key: resolver_op.InputNode(channel)\n                for input_key, channel in channels.items()\n            })\n        ],\n        kwargs=self._config)\n    self._input_dict = {\n        k: resolved_channel.ResolvedChannel(c.type, output_node, k)\n        for k, c in channels.items()\n    }\n  else:\n    self._input_dict = channels\n  self._output_dict = {\n      input_key: types.OutputChannel(channel.type, self, input_key)\n      for input_key, channel in channels.items()\n  }\n  super().__init__(driver_class=_ResolverDriver)\n</code></pre>"},{"location":"api/v1/dsl/#tfx.v1.dsl.Resolver-attributes","title":"Attributes","text":""},{"location":"api/v1/dsl/#tfx.v1.dsl.Resolver.id","title":"id  <code>property</code> <code>writable</code>","text":"<pre><code>id: str\n</code></pre> <p>Node id, unique across all TFX nodes in a pipeline.</p> <p>If <code>id</code> is set by the user, return it directly. Otherwise, return .</p> RETURNS DESCRIPTION <code>str</code> <p>node id.</p>"},{"location":"api/v1/dsl/#tfx.v1.dsl.Resolver.outputs","title":"outputs  <code>property</code>","text":"<pre><code>outputs: Dict[str, OutputChannel]\n</code></pre> <p>Output Channel dict that contains resolved artifacts.</p>"},{"location":"api/v1/dsl/#tfx.v1.dsl.Resolver-functions","title":"Functions","text":""},{"location":"api/v1/dsl/#tfx.v1.dsl.Resolver.add_downstream_node","title":"add_downstream_node","text":"<pre><code>add_downstream_node(downstream_node)\n</code></pre> <p>Experimental: Add another component that must run after this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> <p>It is symmetric with <code>add_upstream_node</code>.</p> PARAMETER DESCRIPTION <code>downstream_node</code> <p>a component that must run after this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_downstream_node(self, downstream_node):\n  \"\"\"Experimental: Add another component that must run after this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n  It is symmetric with `add_upstream_node`.\n\n  Args:\n    downstream_node: a component that must run after this node.\n  \"\"\"\n  self._downstream_nodes.add(downstream_node)\n  if self not in downstream_node.upstream_nodes:\n    downstream_node.add_upstream_node(self)\n</code></pre>"},{"location":"api/v1/dsl/#tfx.v1.dsl.Resolver.add_downstream_nodes","title":"add_downstream_nodes","text":"<pre><code>add_downstream_nodes(downstream_nodes)\n</code></pre> <p>Experimental: Add another component that must run after this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> <p>It is symmetric with <code>add_upstream_nodes</code>.</p> PARAMETER DESCRIPTION <code>downstream_nodes</code> <p>a list of components that must run after this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_downstream_nodes(self, downstream_nodes):\n  \"\"\"Experimental: Add another component that must run after this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n  It is symmetric with `add_upstream_nodes`.\n\n  Args:\n    downstream_nodes: a list of components that must run after this node.\n  \"\"\"\n  self._downstream_nodes.update(downstream_nodes)\n  for downstream_node in downstream_nodes:\n    if self not in downstream_node.upstream_nodes:\n      downstream_node.add_upstream_node(self)\n</code></pre>"},{"location":"api/v1/dsl/#tfx.v1.dsl.Resolver.add_upstream_node","title":"add_upstream_node","text":"<pre><code>add_upstream_node(upstream_node)\n</code></pre> <p>Experimental: Add another component that must run before this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> <p>It is symmetric with <code>add_downstream_node</code>.</p> PARAMETER DESCRIPTION <code>upstream_node</code> <p>a component that must run before this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_upstream_node(self, upstream_node):\n  \"\"\"Experimental: Add another component that must run before this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n  It is symmetric with `add_downstream_node`.\n\n  Args:\n    upstream_node: a component that must run before this node.\n  \"\"\"\n  self._upstream_nodes.add(upstream_node)\n  if self not in upstream_node.downstream_nodes:\n    upstream_node.add_downstream_node(self)\n</code></pre>"},{"location":"api/v1/dsl/#tfx.v1.dsl.Resolver.add_upstream_nodes","title":"add_upstream_nodes","text":"<pre><code>add_upstream_nodes(upstream_nodes)\n</code></pre> <p>Experimental: Add components that must run before this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> PARAMETER DESCRIPTION <code>upstream_nodes</code> <p>a list of components that must run before this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_upstream_nodes(self, upstream_nodes):\n  \"\"\"Experimental: Add components that must run before this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n\n  Args:\n    upstream_nodes: a list of components that must run before this node.\n  \"\"\"\n  self._upstream_nodes.update(upstream_nodes)\n  for upstream_node in upstream_nodes:\n    if self not in upstream_node.downstream_nodes:\n      upstream_node.add_downstream_node(self)\n</code></pre>"},{"location":"api/v1/dsl/#tfx.v1.dsl.Resolver.from_json_dict","title":"from_json_dict  <code>classmethod</code>","text":"<pre><code>from_json_dict(dict_data: Dict[str, Any]) -&gt; Any\n</code></pre> <p>Convert from dictionary data to an object.</p> Source code in <code>tfx/utils/json_utils.py</code> <pre><code>@classmethod\n@doc_controls.do_not_doc_in_subclasses\ndef from_json_dict(cls, dict_data: Dict[str, Any]) -&gt; Any:\n  \"\"\"Convert from dictionary data to an object.\"\"\"\n  instance = cls.__new__(cls)\n  instance.__dict__ = dict_data\n  return instance\n</code></pre>"},{"location":"api/v1/dsl/#tfx.v1.dsl.Resolver.to_json_dict","title":"to_json_dict","text":"<pre><code>to_json_dict() -&gt; Dict[str, Any]\n</code></pre> <p>Convert from an object to a JSON serializable dictionary.</p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef to_json_dict(self) -&gt; Dict[str, Any]:\n  \"\"\"Convert from an object to a JSON serializable dictionary.\"\"\"\n  return dict((k, v)\n              for k, v in self.__dict__.items()\n              if k not in ['_upstream_nodes', '_downstream_nodes'])\n</code></pre>"},{"location":"api/v1/dsl/#tfx.v1.dsl-modules","title":"Modules","text":""},{"location":"api/v1/dsl/#tfx.v1.dsl.components","title":"components","text":"<p>TFX DSL components module.</p> CLASS DESCRIPTION <code>AsyncOutputArtifact</code> <p>Intermediate artifact object type annotation.</p> <code>BeamComponentParameter</code> <p>Component parameter type annotation.</p> <code>InputArtifact</code> <p>Input artifact object type annotation.</p> <code>OutputArtifact</code> <p>Output artifact object type annotation.</p> <code>OutputDict</code> <p>Decorator declaring component executor function outputs.</p> <code>Parameter</code> <p>Component parameter type annotation.</p> FUNCTION DESCRIPTION <code>component</code> <p>Decorator: creates a component from a typehint-annotated Python function.</p>"},{"location":"api/v1/dsl/#tfx.v1.dsl.components-classes","title":"Classes","text":""},{"location":"api/v1/dsl/#tfx.v1.dsl.components.AsyncOutputArtifact","title":"AsyncOutputArtifact","text":"<p>               Bases: <code>Generic[T]</code></p> <p>Intermediate artifact object type annotation.</p>"},{"location":"api/v1/dsl/#tfx.v1.dsl.components.BeamComponentParameter","title":"BeamComponentParameter","text":"<pre><code>BeamComponentParameter(artifact_type: Type[_BeamPipeline], _init_via_getitem=False)\n</code></pre> <p>               Bases: <code>_PipelineTypeGeneric</code></p> <p>Component parameter type annotation.</p> Source code in <code>tfx/dsl/component/experimental/annotations.py</code> <pre><code>def __init__(  # pylint: disable=invalid-name\n    self, artifact_type: Type[_BeamPipeline], _init_via_getitem=False\n):\n  if not _init_via_getitem:\n    class_name = self.__class__.__name__\n    raise ValueError(\n        (\n            '%s should be instantiated via the syntax `%s[T]`, where T is '\n            '`beam.Pipeline`.'\n        )\n        % (class_name, class_name)\n    )\n  self._type = artifact_type\n</code></pre>"},{"location":"api/v1/dsl/#tfx.v1.dsl.components.InputArtifact","title":"InputArtifact","text":"<pre><code>InputArtifact(artifact_type: Union[Type[Artifact], Type[List[Artifact]]], _init_via_getitem=False)\n</code></pre> <p>               Bases: <code>_ArtifactGeneric</code></p> <p>Input artifact object type annotation.</p> Source code in <code>tfx/dsl/component/experimental/annotations.py</code> <pre><code>def __init__(  # pylint: disable=invalid-name\n    self,\n    artifact_type: Union[\n        Type[artifact.Artifact], Type[List[artifact.Artifact]]\n    ],\n    _init_via_getitem=False,\n):\n  if not _init_via_getitem:\n    class_name = self.__class__.__name__\n    raise ValueError(\n        '%s should be instantiated via the syntax `%s[T]`, where T is a '\n        'subclass of tfx.types.Artifact.' % (class_name, class_name)\n    )\n  self.type = artifact_type\n</code></pre>"},{"location":"api/v1/dsl/#tfx.v1.dsl.components.OutputArtifact","title":"OutputArtifact","text":"<pre><code>OutputArtifact(artifact_type: Union[Type[Artifact], Type[List[Artifact]]], _init_via_getitem=False)\n</code></pre> <p>               Bases: <code>_ArtifactGeneric</code></p> <p>Output artifact object type annotation.</p> Source code in <code>tfx/dsl/component/experimental/annotations.py</code> <pre><code>def __init__(  # pylint: disable=invalid-name\n    self,\n    artifact_type: Union[\n        Type[artifact.Artifact], Type[List[artifact.Artifact]]\n    ],\n    _init_via_getitem=False,\n):\n  if not _init_via_getitem:\n    class_name = self.__class__.__name__\n    raise ValueError(\n        '%s should be instantiated via the syntax `%s[T]`, where T is a '\n        'subclass of tfx.types.Artifact.' % (class_name, class_name)\n    )\n  self.type = artifact_type\n</code></pre>"},{"location":"api/v1/dsl/#tfx.v1.dsl.components.OutputDict","title":"OutputDict","text":"<pre><code>OutputDict(**kwargs)\n</code></pre> <p>Decorator declaring component executor function outputs.</p> <p>Now @component can understand TypedDict return type annotation as well, so please use a TypedDict instead of using an OutputDict.</p> Source code in <code>tfx/dsl/component/experimental/annotations.py</code> <pre><code>@deprecation_utils.deprecated('2023-08-25', 'Please use TypedDict instead.')\ndef __init__(self, **kwargs):\n  self.kwargs = kwargs\n</code></pre>"},{"location":"api/v1/dsl/#tfx.v1.dsl.components.Parameter","title":"Parameter","text":"<pre><code>Parameter(artifact_type: Type[Union[int, float, str, bool, Message]], _init_via_getitem=False)\n</code></pre> <p>               Bases: <code>_PrimitiveAndProtoTypeGeneric</code></p> <p>Component parameter type annotation.</p> Source code in <code>tfx/dsl/component/experimental/annotations.py</code> <pre><code>def __init__(  # pylint: disable=invalid-name\n    self,\n    artifact_type: Type[Union[int, float, str, bool, message.Message]],\n    _init_via_getitem=False,\n):\n  if not _init_via_getitem:\n    class_name = self.__class__.__name__\n    raise ValueError(\n        (\n            '%s should be instantiated via the syntax `%s[T]`, where T is '\n            '`int`, `float`, `str`, `bool` or proto type.'\n        )\n        % (class_name, class_name)\n    )\n  self._type = artifact_type\n</code></pre>"},{"location":"api/v1/dsl/#tfx.v1.dsl.components-functions","title":"Functions","text":""},{"location":"api/v1/dsl/#tfx.v1.dsl.components.component","title":"component","text":"<pre><code>component(func: FunctionType) -&gt; BaseFunctionalComponentFactory\n</code></pre><pre><code>component(*, component_annotation: Optional[type[SystemExecution]] = None, use_beam: bool = False) -&gt; Callable[[FunctionType], BaseFunctionalComponentFactory]\n</code></pre> <pre><code>component(func: Optional[FunctionType] = None, /, *, component_annotation: Optional[Type[SystemExecution]] = None, use_beam: bool = False) -&gt; Union[BaseFunctionalComponentFactory, Callable[[FunctionType], BaseFunctionalComponentFactory]]\n</code></pre> <p>Decorator: creates a component from a typehint-annotated Python function.</p> <p>This decorator creates a component based on typehint annotations specified for the arguments and return value for a Python function. The decorator can be supplied with a parameter <code>component_annotation</code> to specify the annotation for this component decorator. This annotation hints which system execution type this python function-based component belongs to. Specifically, function arguments can be annotated with the following types and associated semantics:</p> <ul> <li><code>Parameter[T]</code> where <code>T</code> is <code>int</code>, <code>float</code>, <code>str</code>, or <code>bool</code>:   indicates that a primitive type execution parameter, whose value is known at   pipeline construction time, will be passed for this argument. These   parameters will be recorded in ML Metadata as part of the component's   execution record. Can be an optional argument.</li> <li><code>int</code>, <code>float</code>, <code>str</code>, <code>bytes</code>, <code>bool</code>, <code>Dict</code>, <code>List</code>: indicates that a   primitive type value will be passed for this argument. This value is tracked   as an <code>Integer</code>, <code>Float</code>, <code>String</code>, <code>Bytes</code>, <code>Boolean</code> or <code>JsonValue</code>   artifact (see <code>tfx.types.standard_artifacts</code>) whose value is read and passed   into the given Python component function. Can be an optional argument.</li> <li><code>InputArtifact[ArtifactType]</code>: indicates that an input artifact object of   type <code>ArtifactType</code> (deriving from <code>tfx.types.Artifact</code>) will be passed for   this argument. This artifact is intended to be consumed as an input by this   component (possibly reading from the path specified by its <code>.uri</code>). Can be   an optional argument by specifying a default value of <code>None</code>.</li> <li><code>OutputArtifact[ArtifactType]</code>: indicates that an output artifact object of   type <code>ArtifactType</code> (deriving from <code>tfx.types.Artifact</code>) will be passed for   this argument. This artifact is intended to be emitted as an output by this   component (and written to the path specified by its <code>.uri</code>). Cannot be an   optional argument.</li> </ul> <p>The return value typehint should be either empty or <code>None</code>, in the case of a component function that has no return values, or a <code>TypedDict</code> of primitive value types (<code>int</code>, <code>float</code>, <code>str</code>, <code>bytes</code>, <code>bool</code>, <code>dict</code> or <code>list</code>; or <code>Optional[T]</code>, where T is a primitive type value, in which case <code>None</code> can be returned), to indicate that the return value is a dictionary with specified keys and value types.</p> <p>Note that output artifacts should not be included in the return value typehint; they should be included as <code>OutputArtifact</code> annotations in the function inputs, as described above.</p> <p>The function to which this decorator is applied must be at the top level of its Python module (it may not be defined within nested classes or function closures).</p> <p>This is example usage of component definition using this decorator:</p> <pre><code>from tfx import v1 as tfx\n\nInputArtifact = tfx.dsl.components.InputArtifact\nOutputArtifact = tfx.dsl.components.OutputArtifact\nParameter = tfx.dsl.components.Parameter\nExamples = tfx.types.standard_artifacts.Examples\nModel = tfx.types.standard_artifacts.Model\n\n\nclass MyOutput(TypedDict):\n    loss: float\n    accuracy: float\n\n\n@component(component_annotation=tfx.dsl.standard_annotations.Train)\ndef MyTrainerComponent(\n    training_data: InputArtifact[Examples],\n    model: OutputArtifact[Model],\n    dropout_hyperparameter: float,\n    num_iterations: Parameter[int] = 10,\n) -&gt; MyOutput:\n    \"\"\"My simple trainer component.\"\"\"\n\n    records = read_examples(training_data.uri)\n    model_obj = train_model(records, num_iterations, dropout_hyperparameter)\n    model_obj.write_to(model.uri)\n\n    return {\"loss\": model_obj.loss, \"accuracy\": model_obj.accuracy}\n\n\n# Example usage in a pipeline graph definition:\n# ...\ntrainer = MyTrainerComponent(\n    training_data=example_gen.outputs[\"examples\"],\n    dropout_hyperparameter=other_component.outputs[\"dropout\"],\n    num_iterations=1000,\n)\npusher = Pusher(model=trainer.outputs[\"model\"])\n# ...\n</code></pre> <p>When the parameter <code>component_annotation</code> is not supplied, the default value is None. This is another example usage with <code>component_annotation</code> = None:</p> <pre><code>@component\ndef MyTrainerComponent(\n    training_data: InputArtifact[standard_artifacts.Examples],\n    model: OutputArtifact[standard_artifacts.Model],\n    dropout_hyperparameter: float,\n    num_iterations: Parameter[int] = 10,\n) -&gt; Output:\n    \"\"\"My simple trainer component.\"\"\"\n\n    records = read_examples(training_data.uri)\n    model_obj = train_model(records, num_iterations, dropout_hyperparameter)\n    model_obj.write_to(model.uri)\n\n    return {\"loss\": model_obj.loss, \"accuracy\": model_obj.accuracy}\n</code></pre> <p>When the parameter <code>use_beam</code> is True, one of the parameters of the decorated function type-annotated by BeamComponentParameter[beam.Pipeline] and the default value can only be None. It will be replaced by a beam Pipeline made with the tfx pipeline's beam_pipeline_args that's shared with other beam-based components:</p> <pre><code>@component(use_beam=True)\ndef DataProcessingComponent(\n    input_examples: InputArtifact[standard_artifacts.Examples],\n    output_examples: OutputArtifact[standard_artifacts.Examples],\n    beam_pipeline: BeamComponentParameter[beam.Pipeline] = None,\n) -&gt; None:\n    \"\"\"My simple trainer component.\"\"\"\n\n    records = read_examples(training_data.uri)\n    with beam_pipeline as p:\n        ...\n</code></pre> <p>Experimental: no backwards compatibility guarantees.</p> PARAMETER DESCRIPTION <code>func</code> <p>Typehint-annotated component executor function.</p> <p> TYPE: <code>Optional[FunctionType]</code> DEFAULT: <code>None</code> </p> <code>component_annotation</code> <p>used to annotate the python function-based component. It is a subclass of SystemExecution from third_party/py/tfx/types/system_executions.py; it can be None.</p> <p> TYPE: <code>Optional[Type[SystemExecution]]</code> DEFAULT: <code>None</code> </p> <code>use_beam</code> <p>Whether to create a component that is a subclass of BaseBeamComponent. This allows a beam.Pipeline to be made with tfx-pipeline-wise beam_pipeline_args.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Union[BaseFunctionalComponentFactory, Callable[[FunctionType], BaseFunctionalComponentFactory]]</code> <p>An object that:</p> <ol> <li>you can call like the initializer of a subclass of <code>base_component.BaseComponent</code> (or <code>base_component.BaseBeamComponent</code>).</li> <li>has a test_call() member function for unit testing the inner implementation of the component.</li> </ol> <p>Today, the returned object is literally a subclass of BaseComponent, so it can be used as a <code>Type</code> e.g. in isinstance() checks. But you must not rely on this, as we reserve the right to reserve a different kind of object in the future, which only satisfies the two criteria (1.) and (2.) above without being a <code>Type</code> itself.</p> RAISES DESCRIPTION <code>EnvironmentError</code> <p>if the current Python interpreter is not Python 3.</p> Source code in <code>tfx/dsl/component/experimental/decorators.py</code> <pre><code>def component(\n    func: Optional[types.FunctionType] = None,\n    /,\n    *,\n    component_annotation: Optional[\n        Type[system_executions.SystemExecution]\n    ] = None,\n    use_beam: bool = False,\n) -&gt; Union[\n    BaseFunctionalComponentFactory,\n    Callable[[types.FunctionType], BaseFunctionalComponentFactory],\n]:\n  '''Decorator: creates a component from a typehint-annotated Python function.\n\n  This decorator creates a component based on typehint annotations specified for\n  the arguments and return value for a Python function. The decorator can be\n  supplied with a parameter `component_annotation` to specify the annotation for\n  this component decorator. This annotation hints which system execution type\n  this python function-based component belongs to.\n  Specifically, function arguments can be annotated with the following types and\n  associated semantics:\n\n  * `Parameter[T]` where `T` is `int`, `float`, `str`, or `bool`:\n    indicates that a primitive type execution parameter, whose value is known at\n    pipeline construction time, will be passed for this argument. These\n    parameters will be recorded in ML Metadata as part of the component's\n    execution record. Can be an optional argument.\n  * `int`, `float`, `str`, `bytes`, `bool`, `Dict`, `List`: indicates that a\n    primitive type value will be passed for this argument. This value is tracked\n    as an `Integer`, `Float`, `String`, `Bytes`, `Boolean` or `JsonValue`\n    artifact (see `tfx.types.standard_artifacts`) whose value is read and passed\n    into the given Python component function. Can be an optional argument.\n  * `InputArtifact[ArtifactType]`: indicates that an input artifact object of\n    type `ArtifactType` (deriving from `tfx.types.Artifact`) will be passed for\n    this argument. This artifact is intended to be consumed as an input by this\n    component (possibly reading from the path specified by its `.uri`). Can be\n    an optional argument by specifying a default value of `None`.\n  * `OutputArtifact[ArtifactType]`: indicates that an output artifact object of\n    type `ArtifactType` (deriving from `tfx.types.Artifact`) will be passed for\n    this argument. This artifact is intended to be emitted as an output by this\n    component (and written to the path specified by its `.uri`). Cannot be an\n    optional argument.\n\n  The return value typehint should be either empty or `None`, in the case of a\n  component function that has no return values, or a `TypedDict` of primitive\n  value types (`int`, `float`, `str`, `bytes`, `bool`, `dict` or `list`; or\n  `Optional[T]`, where T is a primitive type value, in which case `None` can be\n  returned), to indicate that the return value is a dictionary with specified\n  keys and value types.\n\n  Note that output artifacts should not be included in the return value\n  typehint; they should be included as `OutputArtifact` annotations in the\n  function inputs, as described above.\n\n  The function to which this decorator is applied must be at the top level of\n  its Python module (it may not be defined within nested classes or function\n  closures).\n\n  This is example usage of component definition using this decorator:\n\n  ``` python\n  from tfx import v1 as tfx\n\n  InputArtifact = tfx.dsl.components.InputArtifact\n  OutputArtifact = tfx.dsl.components.OutputArtifact\n  Parameter = tfx.dsl.components.Parameter\n  Examples = tfx.types.standard_artifacts.Examples\n  Model = tfx.types.standard_artifacts.Model\n\n\n  class MyOutput(TypedDict):\n      loss: float\n      accuracy: float\n\n\n  @component(component_annotation=tfx.dsl.standard_annotations.Train)\n  def MyTrainerComponent(\n      training_data: InputArtifact[Examples],\n      model: OutputArtifact[Model],\n      dropout_hyperparameter: float,\n      num_iterations: Parameter[int] = 10,\n  ) -&gt; MyOutput:\n      \"\"\"My simple trainer component.\"\"\"\n\n      records = read_examples(training_data.uri)\n      model_obj = train_model(records, num_iterations, dropout_hyperparameter)\n      model_obj.write_to(model.uri)\n\n      return {\"loss\": model_obj.loss, \"accuracy\": model_obj.accuracy}\n\n\n  # Example usage in a pipeline graph definition:\n  # ...\n  trainer = MyTrainerComponent(\n      training_data=example_gen.outputs[\"examples\"],\n      dropout_hyperparameter=other_component.outputs[\"dropout\"],\n      num_iterations=1000,\n  )\n  pusher = Pusher(model=trainer.outputs[\"model\"])\n  # ...\n  ```\n\n  When the parameter `component_annotation` is not supplied, the default value\n  is None. This is another example usage with `component_annotation` = None:\n\n  ``` python\n  @component\n  def MyTrainerComponent(\n      training_data: InputArtifact[standard_artifacts.Examples],\n      model: OutputArtifact[standard_artifacts.Model],\n      dropout_hyperparameter: float,\n      num_iterations: Parameter[int] = 10,\n  ) -&gt; Output:\n      \"\"\"My simple trainer component.\"\"\"\n\n      records = read_examples(training_data.uri)\n      model_obj = train_model(records, num_iterations, dropout_hyperparameter)\n      model_obj.write_to(model.uri)\n\n      return {\"loss\": model_obj.loss, \"accuracy\": model_obj.accuracy}\n  ```\n\n  When the parameter `use_beam` is True, one of the parameters of the decorated\n  function type-annotated by BeamComponentParameter[beam.Pipeline] and the\n  default value can only be None. It will be replaced by a beam Pipeline made\n  with the tfx pipeline's beam_pipeline_args that's shared with other beam-based\n  components:\n\n  ``` python\n  @component(use_beam=True)\n  def DataProcessingComponent(\n      input_examples: InputArtifact[standard_artifacts.Examples],\n      output_examples: OutputArtifact[standard_artifacts.Examples],\n      beam_pipeline: BeamComponentParameter[beam.Pipeline] = None,\n  ) -&gt; None:\n      \"\"\"My simple trainer component.\"\"\"\n\n      records = read_examples(training_data.uri)\n      with beam_pipeline as p:\n          ...\n  ```\n\n  Experimental: no backwards compatibility guarantees.\n\n  Args:\n    func: Typehint-annotated component executor function.\n    component_annotation: used to annotate the python function-based component.\n      It is a subclass of SystemExecution from\n      third_party/py/tfx/types/system_executions.py; it can be None.\n    use_beam: Whether to create a component that is a subclass of\n      BaseBeamComponent. This allows a beam.Pipeline to be made with\n      tfx-pipeline-wise beam_pipeline_args.\n\n  Returns:\n    An object that:\n\n      1. you can call like the initializer of a subclass of [`base_component.BaseComponent`][tfx.v1.types.BaseChannel] (or [`base_component.BaseBeamComponent`][tfx.v1.types.BaseBeamComponent]).\n      2. has a test_call() member function for unit testing the inner implementation of the component.\n\n      Today, the returned object is literally a subclass of [BaseComponent][tfx.v1.types.BaseChannel], so it can be used as a `Type` e.g. in isinstance() checks. But you must not rely on this, as we reserve the right to reserve a different kind of object in the future, which _only_ satisfies the two criteria (1.) and (2.) above without being a `Type` itself.\n\n  Raises:\n    EnvironmentError: if the current Python interpreter is not Python 3.\n  '''\n  if func is None:\n    # Python decorators with arguments in parentheses result in two function\n    # calls. The first function call supplies the kwargs and the second supplies\n    # the decorated function. Here we forward the kwargs to the second call.\n    return functools.partial(\n        component,\n        component_annotation=component_annotation,\n        use_beam=use_beam,\n    )\n\n  utils.assert_is_top_level_func(func)\n\n  (inputs, outputs, parameters, arg_formats, arg_defaults, returned_values,\n   json_typehints, return_json_typehints) = (\n       function_parser.parse_typehint_component_function(func))\n  if use_beam and list(parameters.values()).count(_BeamPipeline) != 1:\n    raise ValueError('The decorated function must have one and only one '\n                     'optional parameter of type '\n                     'BeamComponentParameter[beam.Pipeline] with '\n                     'default value None when use_beam=True.')\n\n  component_class = utils.create_component_class(\n      func=func,\n      arg_defaults=arg_defaults,\n      arg_formats=arg_formats,\n      base_executor_class=(\n          _FunctionBeamExecutor if use_beam else _FunctionExecutor\n      ),\n      executor_spec_class=(\n          executor_spec.BeamExecutorSpec\n          if use_beam\n          else executor_spec.ExecutorClassSpec\n      ),\n      base_component_class=(\n          _SimpleBeamComponent if use_beam else _SimpleComponent\n      ),\n      inputs=inputs,\n      outputs=outputs,\n      parameters=parameters,\n      type_annotation=component_annotation,\n      json_compatible_inputs=json_typehints,\n      json_compatible_outputs=return_json_typehints,\n      return_values_optionality=returned_values,\n  )\n  return typing.cast(BaseFunctionalComponentFactory, component_class)\n</code></pre>"},{"location":"api/v1/dsl/#tfx.v1.dsl.experimental","title":"experimental","text":"<p>TFX dsl.experimental module.</p> CLASS DESCRIPTION <code>LatestArtifactStrategy</code> <p>Strategy that resolves the latest n(=1) artifacts per each channel.</p> <code>LatestBlessedModelStrategy</code> <p>LatestBlessedModelStrategy resolves the latest blessed Model artifact.</p> <code>ResolverStrategy</code> <p>Base class for ResolverStrategy.</p> <code>RuntimeParameter</code> <p>Runtime parameter.</p> <code>SpanRangeStrategy</code> <p>SpanRangeStrategy resolves artifacts based on \"span\" property.</p> FUNCTION DESCRIPTION <code>create_container_component</code> <p>Creates a container-based component.</p>"},{"location":"api/v1/dsl/#tfx.v1.dsl.experimental-classes","title":"Classes","text":""},{"location":"api/v1/dsl/#tfx.v1.dsl.experimental.LatestArtifactStrategy","title":"LatestArtifactStrategy","text":"<pre><code>LatestArtifactStrategy(desired_num_of_artifacts: Optional[int] = 1)\n</code></pre> <p>               Bases: <code>ResolverStrategy</code></p> <p>Strategy that resolves the latest n(=1) artifacts per each channel.</p> <p>Note that this ResolverStrategy is experimental and is subject to change in terms of both interface and implementation.</p> <p>Don't construct LatestArtifactStrategy directly, example usage: </p><pre><code>model_resolver.outputs['model']\nmodel_resolver = Resolver(\n    strategy_class=LatestArtifactStrategy,\n    model=Channel(type=Model),\n).with_id(\"latest_model_resolver\")\nmodel_resolver.outputs[\"model\"]\n</code></pre> METHOD DESCRIPTION <code>resolve_artifacts</code> <p>Resolves artifacts from channels by querying MLMD.</p> Source code in <code>tfx/dsl/input_resolution/strategies/latest_artifact_strategy.py</code> <pre><code>def __init__(self, desired_num_of_artifacts: Optional[int] = 1):\n  self._desired_num_of_artifact = desired_num_of_artifacts\n</code></pre> Functions\u00b6 <code></code> resolve_artifacts \u00b6 <pre><code>resolve_artifacts(store: MetadataStore, input_dict: Dict[str, List[Artifact]]) -&gt; Optional[Dict[str, List[Artifact]]]\n</code></pre> <p>Resolves artifacts from channels by querying MLMD.</p> PARAMETER DESCRIPTION <code>store</code> <p>An MLMD MetadataStore object.</p> <p> TYPE: <code>MetadataStore</code> </p> <code>input_dict</code> <p>The input_dict to resolve from.</p> <p> TYPE: <code>Dict[str, List[Artifact]]</code> </p> RETURNS DESCRIPTION <code>Optional[Dict[str, List[Artifact]]]</code> <p>If <code>min_count</code> for every input is met, returns a Dict[str, List[Artifact]]. Otherwise, return None.</p> Source code in <code>tfx/dsl/input_resolution/strategies/latest_artifact_strategy.py</code> <pre><code>@doc_controls.do_not_generate_docs\ndef resolve_artifacts(\n    self, store: mlmd.MetadataStore,\n    input_dict: Dict[str, List[types.Artifact]]\n) -&gt; Optional[Dict[str, List[types.Artifact]]]:\n  \"\"\"Resolves artifacts from channels by querying MLMD.\n\n  Args:\n    store: An MLMD MetadataStore object.\n    input_dict: The input_dict to resolve from.\n\n  Returns:\n    If `min_count` for every input is met, returns a\n      Dict[str, List[Artifact]]. Otherwise, return None.\n  \"\"\"\n  resolved_dict = self._resolve(input_dict)\n  all_min_count_met = all(\n      len(artifact_list) &gt;= self._desired_num_of_artifact\n      for artifact_list in resolved_dict.values())\n  return resolved_dict if all_min_count_met else None\n</code></pre>"},{"location":"api/v1/dsl/#tfx.v1.dsl.experimental.LatestBlessedModelStrategy","title":"LatestBlessedModelStrategy","text":"<p>               Bases: <code>ResolverStrategy</code></p> <p>LatestBlessedModelStrategy resolves the latest blessed Model artifact.</p> <p>Note that this ResolverStrategy is experimental and is subject to change in terms of both interface and implementation.</p> <p>Don't construct LatestBlessedModelStrategy directly, example usage: </p><pre><code>model_resolver.outputs['model']\nmodel_resolver = Resolver(\n    strategy_class=LatestBlessedModelStrategy,\n    model=Channel(type=Model),\n    model_blessing=Channel(type=ModelBlessing),\n).with_id(\"latest_blessed_model_resolver\")\nmodel_resolver.outputs[\"model\"]\n</code></pre> METHOD DESCRIPTION <code>resolve_artifacts</code> <p>Resolves artifacts from channels by querying MLMD.</p> Functions\u00b6 <code></code> resolve_artifacts \u00b6 <pre><code>resolve_artifacts(store: MetadataStore, input_dict: Dict[str, List[Artifact]]) -&gt; Optional[Dict[str, List[Artifact]]]\n</code></pre> <p>Resolves artifacts from channels by querying MLMD.</p> PARAMETER DESCRIPTION <code>store</code> <p>An MLMD MetadataStore object.</p> <p> TYPE: <code>MetadataStore</code> </p> <code>input_dict</code> <p>The input_dict to resolve from.</p> <p> TYPE: <code>Dict[str, List[Artifact]]</code> </p> RETURNS DESCRIPTION <code>Optional[Dict[str, List[Artifact]]]</code> <p>The latest blessed Model and its corresponding ModelBlessing, respectively in the same input channel they were contained to.</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>if input_dict contains unsupported artifact types.</p> Source code in <code>tfx/dsl/input_resolution/strategies/latest_blessed_model_strategy.py</code> <pre><code>@doc_controls.do_not_generate_docs\ndef resolve_artifacts(\n    self, store: mlmd.MetadataStore,\n    input_dict: Dict[str, List[types.Artifact]]\n) -&gt; Optional[Dict[str, List[types.Artifact]]]:\n  \"\"\"Resolves artifacts from channels by querying MLMD.\n\n  Args:\n    store: An MLMD MetadataStore object.\n    input_dict: The input_dict to resolve from.\n\n  Returns:\n    The latest blessed Model and its corresponding [ModelBlessing][tfx.v1.types.standard_artifacts.ModelBlessing], respectively\n      in the same input channel they were contained to.\n\n  Raises:\n    RuntimeError: if input_dict contains unsupported artifact types.\n  \"\"\"\n  model_channel_key = None\n  model_blessing_channel_key = None\n  assert len(input_dict) == 2, 'Expecting 2 input Channels'\n  for k, artifact_list in input_dict.items():\n    if not artifact_list:\n      # If model or model blessing channel has no artifacts, the min_count\n      # can not be met, short cut to return empty dict here.\n      return {key: [] for key in input_dict}\n    artifact = artifact_list[0]\n    if issubclass(type(artifact), standard_artifacts.Model):\n      model_channel_key = k\n    elif issubclass(type(artifact), standard_artifacts.ModelBlessing):\n      model_blessing_channel_key = k\n    else:\n      raise RuntimeError('Only expecting Model or ModelBlessing, got %s' %\n                         artifact.TYPE_NAME)\n  assert model_channel_key is not None, 'Expecting Model as input'\n  assert model_blessing_channel_key is not None, ('Expecting ModelBlessing as'\n                                                  ' input')\n\n  result = self._resolve(input_dict, model_channel_key,\n                         model_blessing_channel_key)\n  return result\n</code></pre>"},{"location":"api/v1/dsl/#tfx.v1.dsl.experimental.ResolverStrategy","title":"ResolverStrategy","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for ResolverStrategy.</p> <p>ResolverStrategy is used with <code>tfx.dsl.Resolver</code> to express the input resolution logic. Currently TFX supports the following builtin ResolverStrategy:</p> <ul> <li>LatestArtifactStrategy</li> <li>LatestBlessedModelStrategy</li> <li>SpanRangeStrategy</li> </ul> <p>A resolver strategy defines a type behavior used for input selection. A resolver strategy subclass must override the <code>resolve_artifacts()</code> function which takes a Dict[str, List[Artifact]] as parameters and returns the resolved dict of the same type.</p> METHOD DESCRIPTION <code>resolve_artifacts</code> <p>Resolves artifacts from channels, optionally querying MLMD if needed.</p> Functions\u00b6 <code></code> resolve_artifacts <code>abstractmethod</code> \u00b6 <pre><code>resolve_artifacts(store: MetadataStore, input_dict: Dict[str, List[Artifact]]) -&gt; Optional[Dict[str, List[Artifact]]]\n</code></pre> <p>Resolves artifacts from channels, optionally querying MLMD if needed.</p> <p>In asynchronous execution mode, resolver classes may composed in sequence where the resolve_artifacts() result from the previous resolver instance would be passed to the next resolver instance's resolve_artifacts() inputs.</p> <p>If resolve_artifacts() returns None, it is considered as \"no inputs available\", and the remaining resolvers will not be executed.</p> <p>Also if resolve_artifacts() omits any key from the input_dict it will not be available from the downstream resolver instances. General recommendation is to preserve all keys in the input_dict unless you have specific reason.</p> PARAMETER DESCRIPTION <code>store</code> <p>An MLMD MetadataStore.</p> <p> TYPE: <code>MetadataStore</code> </p> <code>input_dict</code> <p>The input_dict to resolve from.</p> <p> TYPE: <code>Dict[str, List[Artifact]]</code> </p> RETURNS DESCRIPTION <code>Optional[Dict[str, List[Artifact]]]</code> <p>If all entries has enough data after the resolving, returns the resolved input_dict. Otherise, return None.</p> Source code in <code>tfx/dsl/components/common/resolver.py</code> <pre><code>@abc.abstractmethod\ndef resolve_artifacts(\n    self, store: mlmd.MetadataStore,\n    input_dict: Dict[str, List[types.Artifact]]\n) -&gt; Optional[Dict[str, List[types.Artifact]]]:\n  \"\"\"Resolves artifacts from channels, optionally querying MLMD if needed.\n\n  In asynchronous execution mode, resolver classes may composed in sequence\n  where the resolve_artifacts() result from the previous resolver instance\n  would be passed to the next resolver instance's resolve_artifacts() inputs.\n\n  If resolve_artifacts() returns None, it is considered as \"no inputs\n  available\", and the remaining resolvers will not be executed.\n\n  Also if resolve_artifacts() omits any key from the input_dict it will not\n  be available from the downstream resolver instances. General recommendation\n  is to preserve all keys in the input_dict unless you have specific reason.\n\n  Args:\n    store: An MLMD MetadataStore.\n    input_dict: The input_dict to resolve from.\n\n  Returns:\n    If all entries has enough data after the resolving, returns the resolved\n      input_dict. Otherise, return None.\n  \"\"\"\n</code></pre>"},{"location":"api/v1/dsl/#tfx.v1.dsl.experimental.RuntimeParameter","title":"RuntimeParameter","text":"<pre><code>RuntimeParameter(name: str, ptype: Optional[Type] = None, default: Optional[Union[int, float, str]] = None, description: Optional[str] = None)\n</code></pre> <p>               Bases: <code>Jsonable</code></p> <p>Runtime parameter.</p> <p>Currently only supported on KubeflowV2DagRunner.</p> <p>For protos, use text type RuntimeParameter, which holds the proto json string, e.g., <code>'{\"num_steps\": 5}'</code> for TrainArgs proto.</p> ATTRIBUTE DESCRIPTION <code>name</code> <p>The name of the runtime parameter.</p> <p> </p> <code>default</code> <p>Default value for runtime params when it's not explicitly specified.</p> <p> </p> <code>ptype</code> <p>The type of the runtime parameter.</p> <p> </p> <code>description</code> <p>Description of the usage of the parameter.</p> <p> </p> METHOD DESCRIPTION <code>from_json_dict</code> <p>Convert from dictionary data to an object.</p> <code>to_json_dict</code> <p>Convert from an object to a JSON serializable dictionary.</p> Source code in <code>tfx/orchestration/data_types.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    ptype: Optional[Type] = None,  # pylint: disable=g-bare-generic\n    default: Optional[Union[int, float, str]] = None,\n    description: Optional[str] = None):\n  if ptype and ptype not in [int, float, str]:\n    raise RuntimeError('Only str and scalar runtime parameters are supported')\n  if (default and ptype) and not isinstance(default, ptype):\n    raise TypeError('Default value must be consistent with specified ptype')\n  self.name = name\n  self.default = default\n  self.ptype = ptype\n  self.description = description\n</code></pre> Functions\u00b6 <code></code> from_json_dict <code>classmethod</code> \u00b6 <pre><code>from_json_dict(dict_data: Dict[str, Any]) -&gt; Any\n</code></pre> <p>Convert from dictionary data to an object.</p> Source code in <code>tfx/utils/json_utils.py</code> <pre><code>@classmethod\n@doc_controls.do_not_doc_in_subclasses\ndef from_json_dict(cls, dict_data: Dict[str, Any]) -&gt; Any:\n  \"\"\"Convert from dictionary data to an object.\"\"\"\n  instance = cls.__new__(cls)\n  instance.__dict__ = dict_data\n  return instance\n</code></pre> <code></code> to_json_dict \u00b6 <pre><code>to_json_dict() -&gt; Dict[str, Any]\n</code></pre> <p>Convert from an object to a JSON serializable dictionary.</p> Source code in <code>tfx/utils/json_utils.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef to_json_dict(self) -&gt; Dict[str, Any]:\n  \"\"\"Convert from an object to a JSON serializable dictionary.\"\"\"\n  return self.__dict__\n</code></pre>"},{"location":"api/v1/dsl/#tfx.v1.dsl.experimental.SpanRangeStrategy","title":"SpanRangeStrategy","text":"<pre><code>SpanRangeStrategy(range_config: Any)\n</code></pre> <p>               Bases: <code>ResolverStrategy</code></p> <p>SpanRangeStrategy resolves artifacts based on \"span\" property.</p> <p>Note that this ResolverStrategy is experimental and is subject to change in terms of both interface and implementation.</p> <p>Don't construct SpanRangeStrategy directly, example usage: </p><pre><code>examples_resolver = Resolver(\n    strategy_class=SpanRangeStrategy,\n    config={\"range_config\": range_config},\n    examples=Channel(type=Examples, producer_component_id=example_gen.id),\n).with_id(\"span_resolver\")\nexamples_resolver.outputs[\"examples\"]\n</code></pre> METHOD DESCRIPTION <code>resolve_artifacts</code> <p>Resolves artifacts from channels by querying MLMD.</p> Source code in <code>tfx/dsl/input_resolution/strategies/span_range_strategy.py</code> <pre><code>def __init__(self, range_config: Any):\n  # Import range_config locally, as SpanRangeStrategy is included in\n  # ml-pipelines-sdk package while tfx.proto is not.\n  from tfx.proto import range_config_pb2  # pylint: disable=g-import-not-at-top\n  self._range_config: range_config_pb2.RangeConfig = range_config\n</code></pre> Functions\u00b6 <code></code> resolve_artifacts \u00b6 <pre><code>resolve_artifacts(store: MetadataStore, input_dict: Dict[str, List[Artifact]]) -&gt; Optional[Dict[str, List[Artifact]]]\n</code></pre> <p>Resolves artifacts from channels by querying MLMD.</p> PARAMETER DESCRIPTION <code>store</code> <p>An MLMD MetadataStore object.</p> <p> TYPE: <code>MetadataStore</code> </p> <code>input_dict</code> <p>The input_dict to resolve from.</p> <p> TYPE: <code>Dict[str, List[Artifact]]</code> </p> RETURNS DESCRIPTION <code>Optional[Dict[str, List[Artifact]]]</code> <p>If <code>min_count</code> for every input is met, returns a</p> <code>Optional[Dict[str, List[Artifact]]]</code> <p>Dict[Text, List[Artifact]]. Otherwise, return None.</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>if input_dict contains artifact without span property.</p> Source code in <code>tfx/dsl/input_resolution/strategies/span_range_strategy.py</code> <pre><code>@doc_controls.do_not_generate_docs\ndef resolve_artifacts(\n    self, store: mlmd.MetadataStore,\n    input_dict: Dict[str, List[types.Artifact]]\n) -&gt; Optional[Dict[str, List[types.Artifact]]]:\n  \"\"\"Resolves artifacts from channels by querying MLMD.\n\n  Args:\n    store: An MLMD MetadataStore object.\n    input_dict: The input_dict to resolve from.\n\n  Returns:\n    If `min_count` for every input is met, returns a\n    Dict[Text, List[Artifact]]. Otherwise, return None.\n\n  Raises:\n    RuntimeError: if input_dict contains artifact without span property.\n  \"\"\"\n  resolved_dict = self._resolve(input_dict)\n  all_min_count_met = all(\n      bool(artifact_list) for artifact_list in resolved_dict.values())\n  return resolved_dict if all_min_count_met else None\n</code></pre>"},{"location":"api/v1/dsl/#tfx.v1.dsl.experimental-functions","title":"Functions","text":""},{"location":"api/v1/dsl/#tfx.v1.dsl.experimental.create_container_component","title":"create_container_component","text":"<pre><code>create_container_component(name: str, image: str, command: List[CommandlineArgumentType], inputs: Optional[Dict[str, Any]] = None, outputs: Optional[Dict[str, Any]] = None, parameters: Optional[Dict[str, Any]] = None) -&gt; Callable[..., BaseComponent]\n</code></pre> <p>Creates a container-based component.</p> PARAMETER DESCRIPTION <code>name</code> <p>The name of the component</p> <p> TYPE: <code>str</code> </p> <code>image</code> <p>Container image name.</p> <p> TYPE: <code>str</code> </p> <code>command</code> <p>Container entrypoint command-line. Not executed within a shell. The command-line can use placeholder objects that will be replaced at the compilation time. The placeholder objects can be imported from tfx.dsl.component.experimental.placeholders. Note that Jinja templates are not supported.</p> <p> TYPE: <code>List[CommandlineArgumentType]</code> </p> <code>inputs</code> <p>The list of component inputs</p> <p> TYPE: <code>Optional[Dict[str, Any]]</code> DEFAULT: <code>None</code> </p> <code>outputs</code> <p>The list of component outputs</p> <p> TYPE: <code>Optional[Dict[str, Any]]</code> DEFAULT: <code>None</code> </p> <code>parameters</code> <p>The list of component parameters</p> <p> TYPE: <code>Optional[Dict[str, Any]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Callable[..., BaseComponent]</code> <p>Component that can be instantiated and user inside pipeline.</p> <p>Example</p> <pre><code>component = create_container_component(\n    name=\"TrainModel\",\n    inputs={\n        \"training_data\": Dataset,\n    },\n    outputs={\n        \"model\": Model,\n    },\n    parameters={\n        \"num_training_steps\": int,\n    },\n    image=\"gcr.io/my-project/my-trainer\",\n    command=[\n        \"python3\", \"my_trainer\",\n        \"--training_data_uri\", InputUriPlaceholder(\"training_data\"),\n        \"--model_uri\", OutputUriPlaceholder(\"model\"),\n        \"--num_training-steps\", InputValuePlaceholder(\"num_training_steps\"),\n    ],\n)\n</code></pre> Source code in <code>tfx/dsl/component/experimental/container_component.py</code> <pre><code>def create_container_component(\n    name: str,\n    image: str,\n    command: List[placeholders.CommandlineArgumentType],\n    inputs: Optional[Dict[str, Any]] = None,\n    outputs: Optional[Dict[str, Any]] = None,\n    parameters: Optional[Dict[str, Any]] = None,\n) -&gt; Callable[..., base_component.BaseComponent]:\n  \"\"\"Creates a container-based component.\n\n  Args:\n    name: The name of the component\n    image: Container image name.\n    command: Container entrypoint command-line. Not executed within a shell. The\n      command-line can use placeholder objects that will be replaced at the\n      compilation time. The placeholder objects can be imported from\n      tfx.dsl.component.experimental.placeholders. Note that Jinja templates are\n      not supported.\n    inputs: The list of component inputs\n    outputs: The list of component outputs\n    parameters: The list of component parameters\n\n  Returns:\n    Component that can be instantiated and user inside pipeline.\n\n  !!! Example\n      ``` python\n      component = create_container_component(\n          name=\"TrainModel\",\n          inputs={\n              \"training_data\": Dataset,\n          },\n          outputs={\n              \"model\": Model,\n          },\n          parameters={\n              \"num_training_steps\": int,\n          },\n          image=\"gcr.io/my-project/my-trainer\",\n          command=[\n              \"python3\", \"my_trainer\",\n              \"--training_data_uri\", InputUriPlaceholder(\"training_data\"),\n              \"--model_uri\", OutputUriPlaceholder(\"model\"),\n              \"--num_training-steps\", InputValuePlaceholder(\"num_training_steps\"),\n          ],\n      )\n      ```\n  \"\"\"\n  if not name:\n    raise ValueError('Component name cannot be empty.')\n\n  if inputs is None:\n    inputs = {}\n  if outputs is None:\n    outputs = {}\n  if parameters is None:\n    parameters = {}\n\n  input_channel_parameters = {}\n  output_channel_parameters = {}\n  output_channels = {}\n  execution_parameters = {}\n\n  for input_name, channel_type in inputs.items():\n    # TODO(b/155804245) Sanitize the names so that they're valid python names\n    input_channel_parameters[input_name] = (\n        component_spec.ChannelParameter(type=channel_type,))\n\n  for output_name, channel_type in outputs.items():\n    # TODO(b/155804245) Sanitize the names so that they're valid python names\n    output_channel_parameters[output_name] = (\n        component_spec.ChannelParameter(type=channel_type))\n    artifact = channel_type()\n    channel = channel_utils.as_channel([artifact])\n    output_channels[output_name] = channel\n\n  for param_name, parameter_type in parameters.items():\n    # TODO(b/155804245) Sanitize the names so that they're valid python names\n\n    execution_parameters[param_name] = (\n        component_spec.ExecutionParameter(type=parameter_type))\n\n  default_init_args = {**output_channels}\n\n  return component_utils.create_tfx_component_class(\n      name=name,\n      tfx_executor_spec=executor_specs.TemplatedExecutorContainerSpec(\n          image=image,\n          command=command,\n      ),\n      input_channel_parameters=input_channel_parameters,\n      output_channel_parameters=output_channel_parameters,\n      execution_parameters=execution_parameters,\n      default_init_args=default_init_args)\n</code></pre>"},{"location":"api/v1/dsl/#tfx.v1.dsl.io","title":"io","text":"<p>TFX DSL I/O module.</p> MODULE DESCRIPTION <code>fileio</code> <p>TFX DSL file I/O module.</p>"},{"location":"api/v1/dsl/#tfx.v1.dsl.io-modules","title":"Modules","text":""},{"location":"api/v1/dsl/#tfx.v1.dsl.io.fileio","title":"fileio","text":"<p>TFX DSL file I/O module.</p> FUNCTION DESCRIPTION <code>copy</code> <p>Copy a file from the source to the destination.</p> <code>exists</code> <p>Return whether a path exists.</p> <code>glob</code> <p>Return the paths that match a glob pattern.</p> <code>isdir</code> <p>Return whether a path is a directory.</p> <code>listdir</code> <p>Return the list of files in a directory.</p> <code>makedirs</code> <p>Make a directory at the given path, recursively creating parents.</p> <code>mkdir</code> <p>Make a directory at the given path; parent directory must exist.</p> <code>open</code> <p>Open a file at the given path.</p> <code>remove</code> <p>Remove the file at the given path.</p> <code>rename</code> <p>Rename a source file to a destination path.</p> <code>rmtree</code> <p>Remove the given directory and its recursive contents.</p> <code>stat</code> <p>Return the stat descriptor for a given file path.</p> <code>walk</code> <p>Return an iterator walking a directory tree.</p> Functions\u00b6 <code></code> copy \u00b6 <pre><code>copy(src: PathType, dst: PathType, overwrite: bool = False) -&gt; None\n</code></pre> <p>Copy a file from the source to the destination.</p> Source code in <code>tfx/dsl/io/fileio.py</code> <pre><code>def copy(src: PathType, dst: PathType, overwrite: bool = False) -&gt; None:\n  \"\"\"Copy a file from the source to the destination.\"\"\"\n  src_fs = _get_filesystem(src)\n  dst_fs = _get_filesystem(dst)\n  if src_fs is dst_fs:\n    src_fs.copy(src, dst, overwrite=overwrite)\n  else:\n    if not overwrite and exists(dst):\n      raise OSError(\n          ('Destination file %r already exists and argument `overwrite` is '\n           'false.') % dst)\n    with open(src, mode='rb') as f_src:\n      contents = f_src.read()\n    with open(dst, mode='wb') as f_dst:\n      f_dst.write(contents)\n</code></pre> <code></code> exists \u00b6 <pre><code>exists(path: PathType) -&gt; bool\n</code></pre> <p>Return whether a path exists.</p> Source code in <code>tfx/dsl/io/fileio.py</code> <pre><code>def exists(path: PathType) -&gt; bool:\n  \"\"\"Return whether a path exists.\"\"\"\n  return _get_filesystem(path).exists(path)\n</code></pre> <code></code> glob \u00b6 <pre><code>glob(pattern: PathType) -&gt; List[PathType]\n</code></pre> <p>Return the paths that match a glob pattern.</p> Source code in <code>tfx/dsl/io/fileio.py</code> <pre><code>def glob(pattern: PathType) -&gt; List[PathType]:\n  \"\"\"Return the paths that match a glob pattern.\"\"\"\n  return _get_filesystem(pattern).glob(pattern)\n</code></pre> <code></code> isdir \u00b6 <pre><code>isdir(path: PathType) -&gt; bool\n</code></pre> <p>Return whether a path is a directory.</p> Source code in <code>tfx/dsl/io/fileio.py</code> <pre><code>def isdir(path: PathType) -&gt; bool:\n  \"\"\"Return whether a path is a directory.\"\"\"\n  return _get_filesystem(path).isdir(path)\n</code></pre> <code></code> listdir \u00b6 <pre><code>listdir(path: PathType) -&gt; List[PathType]\n</code></pre> <p>Return the list of files in a directory.</p> Source code in <code>tfx/dsl/io/fileio.py</code> <pre><code>def listdir(path: PathType) -&gt; List[PathType]:\n  \"\"\"Return the list of files in a directory.\"\"\"\n  return _get_filesystem(path).listdir(path)\n</code></pre> <code></code> makedirs \u00b6 <pre><code>makedirs(path: PathType) -&gt; None\n</code></pre> <p>Make a directory at the given path, recursively creating parents.</p> Source code in <code>tfx/dsl/io/fileio.py</code> <pre><code>def makedirs(path: PathType) -&gt; None:\n  \"\"\"Make a directory at the given path, recursively creating parents.\"\"\"\n  _get_filesystem(path).makedirs(path)\n</code></pre> <code></code> mkdir \u00b6 <pre><code>mkdir(path: PathType) -&gt; None\n</code></pre> <p>Make a directory at the given path; parent directory must exist.</p> Source code in <code>tfx/dsl/io/fileio.py</code> <pre><code>def mkdir(path: PathType) -&gt; None:\n  \"\"\"Make a directory at the given path; parent directory must exist.\"\"\"\n  _get_filesystem(path).mkdir(path)\n</code></pre> <code></code> open \u00b6 <pre><code>open(path: PathType, mode: str = 'r')\n</code></pre> <p>Open a file at the given path.</p> Source code in <code>tfx/dsl/io/fileio.py</code> <pre><code>def open(path: PathType, mode: str = 'r'):  # pylint: disable=redefined-builtin\n  \"\"\"Open a file at the given path.\"\"\"\n  return _get_filesystem(path).open(path, mode=mode)\n</code></pre> <code></code> remove \u00b6 <pre><code>remove(path: PathType) -&gt; None\n</code></pre> <p>Remove the file at the given path.</p> Source code in <code>tfx/dsl/io/fileio.py</code> <pre><code>def remove(path: PathType) -&gt; None:\n  \"\"\"Remove the file at the given path.\"\"\"\n  _get_filesystem(path).remove(path)\n</code></pre> <code></code> rename \u00b6 <pre><code>rename(src: PathType, dst: PathType, overwrite: bool = False) -&gt; None\n</code></pre> <p>Rename a source file to a destination path.</p> Source code in <code>tfx/dsl/io/fileio.py</code> <pre><code>def rename(src: PathType, dst: PathType, overwrite: bool = False) -&gt; None:\n  \"\"\"Rename a source file to a destination path.\"\"\"\n  src_fs = _get_filesystem(src)\n  dst_fs = _get_filesystem(dst)\n  if src_fs is dst_fs:\n    src_fs.rename(src, dst, overwrite=overwrite)\n  else:\n    raise NotImplementedError(\n        ('Rename from %r to %r using different filesystems plugins is '\n         'currently not supported.') % (src, dst))\n</code></pre> <code></code> rmtree \u00b6 <pre><code>rmtree(path: PathType) -&gt; None\n</code></pre> <p>Remove the given directory and its recursive contents.</p> Source code in <code>tfx/dsl/io/fileio.py</code> <pre><code>def rmtree(path: PathType) -&gt; None:\n  \"\"\"Remove the given directory and its recursive contents.\"\"\"\n  _get_filesystem(path).rmtree(path)\n</code></pre> <code></code> stat \u00b6 <pre><code>stat(path: PathType) -&gt; Any\n</code></pre> <p>Return the stat descriptor for a given file path.</p> Source code in <code>tfx/dsl/io/fileio.py</code> <pre><code>def stat(path: PathType) -&gt; Any:\n  \"\"\"Return the stat descriptor for a given file path.\"\"\"\n  return _get_filesystem(path).stat(path)\n</code></pre> <code></code> walk \u00b6 <pre><code>walk(top: PathType, topdown: bool = True, onerror: Optional[Callable[..., None]] = None) -&gt; Iterable[Tuple[PathType, List[PathType], List[PathType]]]\n</code></pre> <p>Return an iterator walking a directory tree.</p> Source code in <code>tfx/dsl/io/fileio.py</code> <pre><code>def walk(\n    top: PathType,\n    topdown: bool = True,\n    onerror: Optional[Callable[..., None]] = None\n) -&gt; Iterable[Tuple[PathType, List[PathType], List[PathType]]]:\n  \"\"\"Return an iterator walking a directory tree.\"\"\"\n  return _get_filesystem(top).walk(top, topdown=topdown, onerror=onerror)\n</code></pre>"},{"location":"api/v1/dsl/#tfx.v1.dsl.placeholders","title":"placeholders","text":"<p>TFX placeholders module.</p> FUNCTION DESCRIPTION <code>exec_property</code> <p>Returns a Placeholder that represents an execution property.</p> <code>execution_invocation</code> <p>Returns a Placeholder representing ExecutionInvocation proto.</p> <code>input</code> <p>Returns a Placeholder that represents an input artifact.</p> <code>output</code> <p>Returns a Placeholder that represents an output artifact.</p>"},{"location":"api/v1/dsl/#tfx.v1.dsl.placeholders-functions","title":"Functions","text":""},{"location":"api/v1/dsl/#tfx.v1.dsl.placeholders.exec_property","title":"exec_property","text":"<pre><code>exec_property(key: str) -&gt; ExecPropertyPlaceholder\n</code></pre> <p>Returns a Placeholder that represents an execution property.</p> PARAMETER DESCRIPTION <code>key</code> <p>The key of the output artifact.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>ExecPropertyPlaceholder</code> <p>A Placeholder that supports</p> <ol> <li>Rendering the value of an execution property at a given key.    Example: <code>exec_property('version')</code></li> <li>Rendering the whole proto or a proto field of an execution property,    if the value is a proto type.    The (possibly nested) proto field in a placeholder can be accessed as    if accessing a proto field in Python.    Example: <code>exec_property('model_config').num_layers</code></li> <li>Concatenating with other placeholders or strings.    Example: <code>output('model').uri + '/model/' + exec_property('version')</code></li> </ol> Source code in <code>tfx/dsl/placeholder/runtime_placeholders.py</code> <pre><code>def exec_property(key: str) -&gt; ExecPropertyPlaceholder:\n  \"\"\"Returns a Placeholder that represents an execution property.\n\n  Args:\n    key: The key of the output artifact.\n\n  Returns:\n    A Placeholder that supports\n\n      1. Rendering the value of an execution property at a given key.\n         Example: `#!python exec_property('version')`\n      2. Rendering the whole proto or a proto field of an execution property,\n         if the value is a proto type.\n         The (possibly nested) proto field in a placeholder can be accessed as\n         if accessing a proto field in Python.\n         Example: `#!python exec_property('model_config').num_layers`\n      3. Concatenating with other placeholders or strings.\n         Example: `#!python output('model').uri + '/model/' + exec_property('version')`\n  \"\"\"\n  return ExecPropertyPlaceholder(key)\n</code></pre>"},{"location":"api/v1/dsl/#tfx.v1.dsl.placeholders.execution_invocation","title":"execution_invocation","text":"<pre><code>execution_invocation() -&gt; ExecInvocationPlaceholder\n</code></pre> <p>Returns a Placeholder representing ExecutionInvocation proto.</p> RETURNS DESCRIPTION <code>ExecInvocationPlaceholder</code> <p>A Placeholder that will render to the ExecutionInvocation proto. Accessing a proto field is the same as if accessing a proto field in Python.</p> <p>Prefer to use input(key)/output(key)/exec_property(key) functions instead of input_dict/output_dict/execution_properties field from ExecutionInvocation proto.</p> Source code in <code>tfx/dsl/placeholder/runtime_placeholders.py</code> <pre><code>def execution_invocation() -&gt; ExecInvocationPlaceholder:\n  \"\"\"Returns a Placeholder representing ExecutionInvocation proto.\n\n  Returns:\n    A Placeholder that will render to the ExecutionInvocation proto.\n      Accessing a proto field is the same as if accessing a proto field in Python.\n\n      Prefer to use input(key)/output(key)/exec_property(key) functions instead of\n      input_dict/output_dict/execution_properties field from ExecutionInvocation\n      proto.\n  \"\"\"\n  return ExecInvocationPlaceholder()\n</code></pre>"},{"location":"api/v1/dsl/#tfx.v1.dsl.placeholders.input","title":"input","text":"<pre><code>input(key: str) -&gt; ArtifactPlaceholder\n</code></pre> <p>Returns a Placeholder that represents an input artifact.</p> PARAMETER DESCRIPTION <code>key</code> <p>The key of the input artifact.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>ArtifactPlaceholder</code> <p>A Placeholder that supports</p> <ol> <li>Rendering the whole MLMD artifact proto as text_format.    Example: <code>input('model')</code></li> <li>Accessing a specific index using <code>[index]</code>, if multiple artifacts are    associated with the given key. If not specified, default to the first    artifact.    Example: <code>input('model')[0]</code></li> <li>Getting the URI of an artifact through .uri property.    Example: <code>input('model').uri or input('model')[0].uri</code></li> <li>Getting the URI of a specific split of an artifact using    <code>.split_uri(split_name)</code> method.    Example: <code>input('examples')[0].split_uri('train')</code></li> <li>Getting the value of a primitive artifact through .value property.    Example: <code>input('primitive').value</code></li> <li>Concatenating with other placeholders or strings.    Example: <code>input('model').uri + '/model/' + exec_property('version')</code></li> </ol> Source code in <code>tfx/dsl/placeholder/artifact_placeholder.py</code> <pre><code>def input(key: str) -&gt; ArtifactPlaceholder:  # pylint: disable=redefined-builtin\n  \"\"\"Returns a Placeholder that represents an input artifact.\n\n  Args:\n    key: The key of the input artifact.\n\n  Returns:\n    A Placeholder that supports\n\n      1. Rendering the whole MLMD artifact proto as text_format.\n         Example: `#!python input('model')`\n      2. Accessing a specific index using `#!python [index]`, if multiple artifacts are\n         associated with the given key. If not specified, default to the first\n         artifact.\n         Example: `#!python input('model')[0]`\n      3. Getting the URI of an artifact through .uri property.\n         Example: `#!python input('model').uri or input('model')[0].uri`\n      4. Getting the URI of a specific split of an artifact using\n         `#!python .split_uri(split_name)` method.\n         Example: `#!python input('examples')[0].split_uri('train')`\n      5. Getting the value of a primitive artifact through .value property.\n         Example: `#!python input('primitive').value`\n      6. Concatenating with other placeholders or strings.\n         Example: `#!python input('model').uri + '/model/' + exec_property('version')`\n  \"\"\"\n  return ArtifactPlaceholder(key, is_input=True)\n</code></pre>"},{"location":"api/v1/dsl/#tfx.v1.dsl.placeholders.output","title":"output","text":"<pre><code>output(key: str) -&gt; ArtifactPlaceholder\n</code></pre> <p>Returns a Placeholder that represents an output artifact.</p> <p>It is the same as input(...) function, except it is for output artifacts.</p> PARAMETER DESCRIPTION <code>key</code> <p>The key of the output artifact.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>ArtifactPlaceholder</code> <p>A Placeholder that supports</p> <ol> <li>Rendering the whole artifact as text_format.    Example: <code>output('model')</code></li> <li>Accessing a specific index using [index], if multiple artifacts are    associated with the given key. If not specified, default to the first    artifact.    Example: <code>output('model')[0]</code></li> <li>Getting the URI of an artifact through .uri property.    Example: <code>output('model').uri or output('model')[0].uri</code></li> <li>Getting the URI of a specific split of an artifact using    <code>.split_uri(split_name)</code> method.    Example: <code>output('examples')[0].split_uri('train')</code></li> <li>Getting the value of a primitive artifact through .value property.    Example: <code>output('primitive').value</code></li> <li>Concatenating with other placeholders or strings.    Example: <code>output('model').uri + '/model/' + exec_property('version')</code></li> </ol> Source code in <code>tfx/dsl/placeholder/artifact_placeholder.py</code> <pre><code>def output(key: str) -&gt; ArtifactPlaceholder:\n  \"\"\"Returns a Placeholder that represents an output artifact.\n\n  It is the same as input(...) function, except it is for output artifacts.\n\n  Args:\n    key: The key of the output artifact.\n\n  Returns:\n    A Placeholder that supports\n\n      1. Rendering the whole artifact as text_format.\n         Example: `#!python output('model')`\n      2. Accessing a specific index using [index], if multiple artifacts are\n         associated with the given key. If not specified, default to the first\n         artifact.\n         Example: `#!python output('model')[0]`\n      3. Getting the URI of an artifact through .uri property.\n         Example: `#!python output('model').uri or output('model')[0].uri`\n      4. Getting the URI of a specific split of an artifact using\n         `#!python .split_uri(split_name)` method.\n         Example: `#!python output('examples')[0].split_uri('train')`\n      5. Getting the value of a primitive artifact through .value property.\n         Example: `#!python output('primitive').value`\n      6. Concatenating with other placeholders or strings.\n         Example: `#!python output('model').uri + '/model/' + exec_property('version')`\n  \"\"\"\n  return ArtifactPlaceholder(key, is_input=False)\n</code></pre>"},{"location":"api/v1/dsl/#tfx.v1.dsl.standard_annotations","title":"standard_annotations","text":"<p>Public API for base type annotations.</p> CLASS DESCRIPTION <code>Dataset</code> <p>Dataset is a TFX pre-defined system artifact.</p> <code>Deploy</code> <p>Deploy is a TFX pre-defined system execution.</p> <code>Evaluate</code> <p>Evaluate is a TFX pre-defined system execution.</p> <code>Metrics</code> <p>Metrics is a TFX pre-defined system artifact.</p> <code>Model</code> <p>Model is a TFX pre-defined system artifact.</p> <code>Process</code> <p>Process is a TFX pre-defined system execution.</p> <code>Statistics</code> <p>Statistics is a TFX pre-defined system artifact.</p> <code>Train</code> <p>Train is a TFX pre-defined system execution.</p> <code>Transform</code> <p>Transform is a TFX pre-defined system execution.</p>"},{"location":"api/v1/dsl/#tfx.v1.dsl.standard_annotations-classes","title":"Classes","text":""},{"location":"api/v1/dsl/#tfx.v1.dsl.standard_annotations.Dataset","title":"Dataset","text":"<p>               Bases: <code>SystemArtifact</code></p> <p>Dataset is a TFX pre-defined system artifact.</p>"},{"location":"api/v1/dsl/#tfx.v1.dsl.standard_annotations.Deploy","title":"Deploy","text":"<p>               Bases: <code>SystemExecution</code></p> <p>Deploy is a TFX pre-defined system execution.</p> <p>This execution performs model deployment. For example, Pusher component can be annotated as Deploy execution, which checks whether the model passed the validation steps and pushes fully validated models to Servomatic, CNS/Placer, TF-Hub, and other destinations.</p>"},{"location":"api/v1/dsl/#tfx.v1.dsl.standard_annotations.Evaluate","title":"Evaluate","text":"<p>               Bases: <code>SystemExecution</code></p> <p>Evaluate is a TFX pre-defined system execution.</p> <p>It computes a model\u2019s evaluation statistics over (slices of) features.</p>"},{"location":"api/v1/dsl/#tfx.v1.dsl.standard_annotations.Metrics","title":"Metrics","text":"<p>               Bases: <code>SystemArtifact</code></p> <p>Metrics is a TFX pre-defined system artifact.</p>"},{"location":"api/v1/dsl/#tfx.v1.dsl.standard_annotations.Model","title":"Model","text":"<p>               Bases: <code>SystemArtifact</code></p> <p>Model is a TFX pre-defined system artifact.</p>"},{"location":"api/v1/dsl/#tfx.v1.dsl.standard_annotations.Process","title":"Process","text":"<p>               Bases: <code>SystemExecution</code></p> <p>Process is a TFX pre-defined system execution.</p> <p>It includes various executions such as ExampleGen, SchemaGen, SkewDetection, e.t.c., which performs data/model/statistics processing.</p>"},{"location":"api/v1/dsl/#tfx.v1.dsl.standard_annotations.Statistics","title":"Statistics","text":"<p>               Bases: <code>SystemArtifact</code></p> <p>Statistics is a TFX pre-defined system artifact.</p>"},{"location":"api/v1/dsl/#tfx.v1.dsl.standard_annotations.Train","title":"Train","text":"<p>               Bases: <code>SystemExecution</code></p> <p>Train is a TFX pre-defined system execution.</p> <p>Train is one of the key executions that performs the actual model training.</p>"},{"location":"api/v1/dsl/#tfx.v1.dsl.standard_annotations.Transform","title":"Transform","text":"<p>               Bases: <code>SystemExecution</code></p> <p>Transform is a TFX pre-defined system execution.</p> <p>It performs transformations and feature engineering in training and serving.</p>"},{"location":"api/v1/extensions/","title":"Extension","text":""},{"location":"api/v1/extensions/#tfx.v1.extensions","title":"tfx.v1.extensions","text":"<p>TFX extensions module.</p> MODULE DESCRIPTION <code>google_cloud_ai_platform</code> <p>Google cloud AI platform module.</p> <code>google_cloud_big_query</code> <p>Google Cloud Big Query module.</p>"},{"location":"api/v1/extensions/#tfx.v1.extensions-modules","title":"Modules","text":""},{"location":"api/v1/extensions/#tfx.v1.extensions.google_cloud_ai_platform","title":"google_cloud_ai_platform","text":"<p>Google cloud AI platform module.</p> MODULE DESCRIPTION <code>experimental</code> <p>Types used in Google Cloud AI Platform under experimental stage.</p> CLASS DESCRIPTION <code>BulkInferrer</code> <p>A Cloud AI component to do batch inference on a remote hosted model.</p> <code>Pusher</code> <p>Component for pushing model to Cloud AI Platform serving.</p> <code>Trainer</code> <p>Cloud AI Platform Trainer component.</p> <code>Tuner</code> <p>TFX component for model hyperparameter tuning on AI Platform Training.</p> ATTRIBUTE DESCRIPTION <code>ENABLE_UCAIP_KEY</code> <p> </p> <code>ENABLE_VERTEX_KEY</code> <p> </p> <code>JOB_ID_KEY</code> <p> </p> <code>LABELS_KEY</code> <p> </p> <code>SERVING_ARGS_KEY</code> <p> </p> <code>TRAINING_ARGS_KEY</code> <p> </p> <code>UCAIP_REGION_KEY</code> <p> </p> <code>VERTEX_CONTAINER_IMAGE_URI_KEY</code> <p> </p> <code>VERTEX_REGION_KEY</code> <p> </p>"},{"location":"api/v1/extensions/#tfx.v1.extensions.google_cloud_ai_platform-attributes","title":"Attributes","text":""},{"location":"api/v1/extensions/#tfx.v1.extensions.google_cloud_ai_platform.ENABLE_UCAIP_KEY","title":"ENABLE_UCAIP_KEY  <code>module-attribute</code>","text":"<pre><code>ENABLE_UCAIP_KEY = documented(obj='ai_platform_training_enable_ucaip', doc='Deprecated. Please use ENABLE_VERTEX_KEY instead. Keys to the items in custom_config of Trainer for enabling uCAIP Training. ')\n</code></pre>"},{"location":"api/v1/extensions/#tfx.v1.extensions.google_cloud_ai_platform.ENABLE_VERTEX_KEY","title":"ENABLE_VERTEX_KEY  <code>module-attribute</code>","text":"<pre><code>ENABLE_VERTEX_KEY = documented(obj='ai_platform_enable_vertex', doc='Keys to the items in custom_config of Trainer and Pusher for enabling Vertex AI.')\n</code></pre>"},{"location":"api/v1/extensions/#tfx.v1.extensions.google_cloud_ai_platform.JOB_ID_KEY","title":"JOB_ID_KEY  <code>module-attribute</code>","text":"<pre><code>JOB_ID_KEY = documented(obj='ai_platform_training_job_id', doc='Keys to the items in custom_config of Trainer for specifying job id.')\n</code></pre>"},{"location":"api/v1/extensions/#tfx.v1.extensions.google_cloud_ai_platform.LABELS_KEY","title":"LABELS_KEY  <code>module-attribute</code>","text":"<pre><code>LABELS_KEY = documented(obj='ai_platform_training_labels', doc='Keys to the items in custom_config of Trainer for specifying labels for training jobs on the AI Platform only. Not applicable for Vertex AI, where labels are specified in the CustomJob as defined in: https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.customJobs#CustomJob.')\n</code></pre>"},{"location":"api/v1/extensions/#tfx.v1.extensions.google_cloud_ai_platform.SERVING_ARGS_KEY","title":"SERVING_ARGS_KEY  <code>module-attribute</code>","text":"<pre><code>SERVING_ARGS_KEY = documented(obj='ai_platform_serving_args', doc='Keys to the items in custom_config of Pusher/BulkInferrer for passing serving args to AI Platform.')\n</code></pre>"},{"location":"api/v1/extensions/#tfx.v1.extensions.google_cloud_ai_platform.TRAINING_ARGS_KEY","title":"TRAINING_ARGS_KEY  <code>module-attribute</code>","text":"<pre><code>TRAINING_ARGS_KEY = documented(obj='ai_platform_training_args', doc='Keys to the items in custom_config of Trainer for passing training_job to AI Platform, and the GCP project under which the training job will be executed. In Vertex AI, this corresponds to a CustomJob as defined in:https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.customJobs#CustomJob.In CAIP, this corresponds to TrainingInputs as defined in:https://cloud.google.com/ml-engine/reference/rest/v1/projects.jobs#TrainingInput')\n</code></pre>"},{"location":"api/v1/extensions/#tfx.v1.extensions.google_cloud_ai_platform.UCAIP_REGION_KEY","title":"UCAIP_REGION_KEY  <code>module-attribute</code>","text":"<pre><code>UCAIP_REGION_KEY = documented(obj='ai_platform_training_ucaip_region', doc='Deprecated. Please use VERTEX_REGION_KEY instead. Keys to the items in custom_config of Trainer for specifying the region of uCAIP.')\n</code></pre>"},{"location":"api/v1/extensions/#tfx.v1.extensions.google_cloud_ai_platform.VERTEX_CONTAINER_IMAGE_URI_KEY","title":"VERTEX_CONTAINER_IMAGE_URI_KEY  <code>module-attribute</code>","text":"<pre><code>VERTEX_CONTAINER_IMAGE_URI_KEY = documented(obj='ai_platform_vertex_container_image_uri', doc='Keys to the items in custom_config of Pusher/BulkInferrer for the serving container image URI in Vertex AI.')\n</code></pre>"},{"location":"api/v1/extensions/#tfx.v1.extensions.google_cloud_ai_platform.VERTEX_REGION_KEY","title":"VERTEX_REGION_KEY  <code>module-attribute</code>","text":"<pre><code>VERTEX_REGION_KEY = documented(obj='ai_platform_vertex_region', doc='Keys to the items in custom_config of Trainer and Pusher for specifying the region of Vertex AI.')\n</code></pre>"},{"location":"api/v1/extensions/#tfx.v1.extensions.google_cloud_ai_platform-classes","title":"Classes","text":""},{"location":"api/v1/extensions/#tfx.v1.extensions.google_cloud_ai_platform.BulkInferrer","title":"BulkInferrer","text":"<pre><code>BulkInferrer(examples: Channel, model: Optional[Channel] = None, model_blessing: Optional[Channel] = None, data_spec: Optional[Union[DataSpec, RuntimeParameter]] = None, output_example_spec: Optional[Union[OutputExampleSpec, RuntimeParameter]] = None, custom_config: Optional[Dict[str, Any]] = None)\n</code></pre> <p>               Bases: <code>BaseComponent</code></p> <p>A Cloud AI component to do batch inference on a remote hosted model.</p> <p>BulkInferrer component will push a model to Google Cloud AI Platform, consume examples data, send request to the remote hosted model, and produces the inference results to an external location as PredictionLog proto. After inference, it will delete the model from Google Cloud AI Platform.</p> <p>TODO(b/155325467): Creates a end-to-end test for this component.</p> <p>Component <code>outputs</code> contains:</p> <ul> <li><code>inference_result</code>: Channel of type <code>standard_artifacts.InferenceResult</code>                        to store the inference results.</li> <li><code>output_examples</code>: Channel of type <code>standard_artifacts.Examples</code>                       to store the output examples.</li> </ul> <p>Construct an BulkInferrer component.</p> PARAMETER DESCRIPTION <code>examples</code> <p>A Channel of type <code>standard_artifacts.Examples</code>, usually produced by an ExampleGen component. required</p> <p> TYPE: <code>Channel</code> </p> <code>model</code> <p>A Channel of type <code>standard_artifacts.Model</code>, usually produced by a Trainer component.</p> <p> TYPE: <code>Optional[Channel]</code> DEFAULT: <code>None</code> </p> <code>model_blessing</code> <p>A Channel of type <code>standard_artifacts.ModelBlessing</code>, usually produced by a ModelValidator component.</p> <p> TYPE: <code>Optional[Channel]</code> DEFAULT: <code>None</code> </p> <code>data_spec</code> <p>bulk_inferrer_pb2.DataSpec instance that describes data selection.</p> <p> TYPE: <code>Optional[Union[DataSpec, RuntimeParameter]]</code> DEFAULT: <code>None</code> </p> <code>output_example_spec</code> <p>bulk_inferrer_pb2.OutputExampleSpec instance, specify if you want BulkInferrer to output examples instead of inference result.</p> <p> TYPE: <code>Optional[Union[OutputExampleSpec, RuntimeParameter]]</code> DEFAULT: <code>None</code> </p> <code>custom_config</code> <p>A dict which contains the deployment job parameters to be passed to Google Cloud AI Platform. custom_config.ai_platform_serving_args need to contain the serving job parameters. For the full set of parameters, refer to https://cloud.google.com/ml-engine/reference/rest/v1/projects.models</p> <p> TYPE: <code>Optional[Dict[str, Any]]</code> DEFAULT: <code>None</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>Must not specify inference_result or output_examples depends on whether output_example_spec is set or not.</p> ATTRIBUTE DESCRIPTION <code>EXECUTOR_SPEC</code> <p> </p> <code>SPEC_CLASS</code> <p> </p> Source code in <code>tfx/extensions/google_cloud_ai_platform/bulk_inferrer/component.py</code> <pre><code>def __init__(\n    self,\n    examples: types.Channel,\n    model: Optional[types.Channel] = None,\n    model_blessing: Optional[types.Channel] = None,\n    data_spec: Optional[Union[bulk_inferrer_pb2.DataSpec,\n                              data_types.RuntimeParameter]] = None,\n    output_example_spec: Optional[Union[bulk_inferrer_pb2.OutputExampleSpec,\n                                        data_types.RuntimeParameter]] = None,\n    custom_config: Optional[Dict[str, Any]] = None):\n  \"\"\"Construct an BulkInferrer component.\n\n  Args:\n    examples: A Channel of type [`standard_artifacts.Examples`][tfx.v1.types.standard_artifacts.Examples], usually\n      produced by an ExampleGen component. _required_\n    model: A Channel of type [`standard_artifacts.Model`][tfx.v1.types.standard_artifacts.Model], usually produced by\n      a Trainer component.\n    model_blessing: A Channel of type [`standard_artifacts.ModelBlessing`][tfx.v1.types.standard_artifacts.ModelBlessing],\n      usually produced by a ModelValidator component.\n    data_spec: bulk_inferrer_pb2.DataSpec instance that describes data\n      selection.\n    output_example_spec: bulk_inferrer_pb2.OutputExampleSpec instance, specify\n      if you want BulkInferrer to output examples instead of inference result.\n    custom_config: A dict which contains the deployment job parameters to be\n      passed to Google Cloud AI Platform.\n      custom_config.ai_platform_serving_args need to contain the serving job\n      parameters. For the full set of parameters, refer to\n      [https://cloud.google.com/ml-engine/reference/rest/v1/projects.models](https://cloud.google.com/ml-engine/reference/rest/v1/projects.models)\n\n  Raises:\n    ValueError: Must not specify inference_result or output_examples depends\n      on whether output_example_spec is set or not.\n  \"\"\"\n  if output_example_spec:\n    output_examples = types.Channel(type=standard_artifacts.Examples)\n    inference_result = None\n  else:\n    inference_result = types.Channel(type=standard_artifacts.InferenceResult)\n    output_examples = None\n\n  spec = CloudAIBulkInferrerComponentSpec(\n      examples=examples,\n      model=model,\n      model_blessing=model_blessing,\n      data_spec=data_spec or bulk_inferrer_pb2.DataSpec(),\n      output_example_spec=output_example_spec,\n      custom_config=json_utils.dumps(custom_config),\n      inference_result=inference_result,\n      output_examples=output_examples)\n  super().__init__(spec=spec)\n</code></pre> Attributes\u00b6 <code></code> EXECUTOR_SPEC <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <pre><code>EXECUTOR_SPEC = ExecutorClassSpec(Executor)\n</code></pre> <code></code> SPEC_CLASS <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <pre><code>SPEC_CLASS = CloudAIBulkInferrerComponentSpec\n</code></pre> Functions\u00b6"},{"location":"api/v1/extensions/#tfx.v1.extensions.google_cloud_ai_platform.Pusher","title":"Pusher","text":"<pre><code>Pusher(model: Optional[Channel] = None, model_blessing: Optional[Channel] = None, infra_blessing: Optional[Channel] = None, custom_config: Optional[Dict[str, Any]] = None)\n</code></pre> <p>               Bases: <code>Pusher</code></p> <p>Component for pushing model to Cloud AI Platform serving.</p> <p>Construct a Pusher component.</p> PARAMETER DESCRIPTION <code>model</code> <p>An optional Channel of type <code>standard_artifacts.Model</code>, usually produced by a Trainer component, representing the model used for training.</p> <p> TYPE: <code>Optional[Channel]</code> DEFAULT: <code>None</code> </p> <code>model_blessing</code> <p>An optional Channel of type <code>standard_artifacts.ModelBlessing</code>, usually produced from an Evaluator component, containing the blessing model.</p> <p> TYPE: <code>Optional[Channel]</code> DEFAULT: <code>None</code> </p> <code>infra_blessing</code> <p>An optional Channel of type <code>standard_artifacts.InfraBlessing</code>, usually produced from an InfraValidator component, containing the validation result.</p> <p> TYPE: <code>Optional[Channel]</code> DEFAULT: <code>None</code> </p> <code>custom_config</code> <p>A dict which contains the deployment job parameters to be passed to Cloud platforms.</p> <p> TYPE: <code>Optional[Dict[str, Any]]</code> DEFAULT: <code>None</code> </p> METHOD DESCRIPTION <code>add_downstream_node</code> <p>Experimental: Add another component that must run after this one.</p> <code>add_downstream_nodes</code> <p>Experimental: Add another component that must run after this one.</p> <code>add_upstream_node</code> <p>Experimental: Add another component that must run before this one.</p> <code>add_upstream_nodes</code> <p>Experimental: Add components that must run before this one.</p> <code>from_json_dict</code> <p>Convert from dictionary data to an object.</p> <code>get_class_type</code> <code>remove_downstream_node</code> <code>remove_upstream_node</code> <code>to_json_dict</code> <p>Convert from an object to a JSON serializable dictionary.</p> <code>with_id</code> <code>with_node_execution_options</code> <code>with_platform_config</code> <p>Attaches a proto-form platform config to a component.</p> ATTRIBUTE DESCRIPTION <code>DRIVER_CLASS</code> <p> </p> <code>EXECUTOR_SPEC</code> <p> </p> <code>POST_EXECUTABLE_SPEC</code> <p> </p> <code>PRE_EXECUTABLE_SPEC</code> <p> </p> <code>SPEC_CLASS</code> <p> </p> <code>component_id</code> <p> TYPE: <code>str</code> </p> <code>component_type</code> <p> TYPE: <code>str</code> </p> <code>downstream_nodes</code> <p> </p> <code>driver_class</code> <p> </p> <code>exec_properties</code> <p> TYPE: <code>Dict[str, Any]</code> </p> <code>executor_spec</code> <p> </p> <code>id</code> <p>Node id, unique across all TFX nodes in a pipeline.</p> <p> TYPE: <code>str</code> </p> <code>inputs</code> <p> TYPE: <code>Dict[str, Channel]</code> </p> <code>node_execution_options</code> <p> TYPE: <code>Optional[NodeExecutionOptions]</code> </p> <code>outputs</code> <p>Component's output channel dict.</p> <p> TYPE: <code>Dict[str, OutputChannel]</code> </p> <code>platform_config</code> <p> </p> <code>spec</code> <p> </p> <code>type</code> <p> TYPE: <code>str</code> </p> <code>type_annotation</code> <p> TYPE: <code>Optional[Type[SystemExecution]]</code> </p> <code>upstream_nodes</code> <p> </p> Source code in <code>tfx/extensions/google_cloud_ai_platform/pusher/component.py</code> <pre><code>def __init__(self,\n             model: Optional[types.Channel] = None,\n             model_blessing: Optional[types.Channel] = None,\n             infra_blessing: Optional[types.Channel] = None,\n             custom_config: Optional[Dict[str, Any]] = None):\n  \"\"\"Construct a Pusher component.\n\n  Args:\n    model: An optional Channel of type [`standard_artifacts.Model`][tfx.v1.types.standard_artifacts.Model], usually\n      produced by a [Trainer][tfx.v1.components.Trainer] component, representing the model used for\n      training.\n    model_blessing: An optional Channel of type\n      [`standard_artifacts.ModelBlessing`][tfx.v1.types.standard_artifacts.ModelBlessing], usually produced from an [Evaluator][tfx.v1.components.Evaluator]\n      component, containing the blessing model.\n    infra_blessing: An optional Channel of type\n      [`standard_artifacts.InfraBlessing`][tfx.v1.types.standard_artifacts.InfraBlessing], usually produced from an\n      [InfraValidator][tfx.v1.components.InfraValidator] component, containing the validation result.\n    custom_config: A dict which contains the deployment job parameters to be\n      passed to Cloud platforms.\n  \"\"\"\n  super().__init__(\n      model=model,\n      model_blessing=model_blessing,\n      infra_blessing=infra_blessing,\n      custom_config=custom_config)\n</code></pre> Attributes\u00b6 <code></code> DRIVER_CLASS <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <pre><code>DRIVER_CLASS = BaseDriver\n</code></pre> <code></code> EXECUTOR_SPEC <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <pre><code>EXECUTOR_SPEC = ExecutorClassSpec(Executor)\n</code></pre> <code></code> POST_EXECUTABLE_SPEC <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <pre><code>POST_EXECUTABLE_SPEC = None\n</code></pre> <code></code> PRE_EXECUTABLE_SPEC <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <pre><code>PRE_EXECUTABLE_SPEC = None\n</code></pre> <code></code> SPEC_CLASS <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <pre><code>SPEC_CLASS = PusherSpec\n</code></pre> <code></code> component_id <code>property</code> \u00b6 <pre><code>component_id: str\n</code></pre> <code></code> component_type <code>property</code> \u00b6 <pre><code>component_type: str\n</code></pre> <code></code> downstream_nodes <code>property</code> \u00b6 <pre><code>downstream_nodes\n</code></pre> <code></code> driver_class <code>instance-attribute</code> \u00b6 <pre><code>driver_class = driver_class\n</code></pre> <code></code> exec_properties <code>property</code> \u00b6 <pre><code>exec_properties: Dict[str, Any]\n</code></pre> <code></code> executor_spec <code>instance-attribute</code> \u00b6 <pre><code>executor_spec = executor_spec\n</code></pre> <code></code> id <code>property</code> <code>writable</code> \u00b6 <pre><code>id: str\n</code></pre> <p>Node id, unique across all TFX nodes in a pipeline.</p> <p>If <code>id</code> is set by the user, return it directly. Otherwise, return .</p> RETURNS DESCRIPTION <code>str</code> <p>node id.</p> <code></code> inputs <code>property</code> \u00b6 <pre><code>inputs: Dict[str, Channel]\n</code></pre> <code></code> node_execution_options <code>property</code> <code>writable</code> \u00b6 <pre><code>node_execution_options: Optional[NodeExecutionOptions]\n</code></pre> <code></code> outputs <code>property</code> \u00b6 <pre><code>outputs: Dict[str, OutputChannel]\n</code></pre> <p>Component's output channel dict.</p> <code></code> platform_config <code>instance-attribute</code> \u00b6 <pre><code>platform_config = None\n</code></pre> <code></code> spec <code>instance-attribute</code> \u00b6 <pre><code>spec = spec\n</code></pre> <code></code> type <code>property</code> \u00b6 <pre><code>type: str\n</code></pre> <code></code> type_annotation <code>property</code> \u00b6 <pre><code>type_annotation: Optional[Type[SystemExecution]]\n</code></pre> <code></code> upstream_nodes <code>property</code> \u00b6 <pre><code>upstream_nodes\n</code></pre> Functions\u00b6 <code></code> add_downstream_node \u00b6 <pre><code>add_downstream_node(downstream_node)\n</code></pre> <p>Experimental: Add another component that must run after this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> <p>It is symmetric with <code>add_upstream_node</code>.</p> PARAMETER DESCRIPTION <code>downstream_node</code> <p>a component that must run after this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_downstream_node(self, downstream_node):\n  \"\"\"Experimental: Add another component that must run after this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n  It is symmetric with `add_upstream_node`.\n\n  Args:\n    downstream_node: a component that must run after this node.\n  \"\"\"\n  self._downstream_nodes.add(downstream_node)\n  if self not in downstream_node.upstream_nodes:\n    downstream_node.add_upstream_node(self)\n</code></pre> <code></code> add_downstream_nodes \u00b6 <pre><code>add_downstream_nodes(downstream_nodes)\n</code></pre> <p>Experimental: Add another component that must run after this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> <p>It is symmetric with <code>add_upstream_nodes</code>.</p> PARAMETER DESCRIPTION <code>downstream_nodes</code> <p>a list of components that must run after this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_downstream_nodes(self, downstream_nodes):\n  \"\"\"Experimental: Add another component that must run after this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n  It is symmetric with `add_upstream_nodes`.\n\n  Args:\n    downstream_nodes: a list of components that must run after this node.\n  \"\"\"\n  self._downstream_nodes.update(downstream_nodes)\n  for downstream_node in downstream_nodes:\n    if self not in downstream_node.upstream_nodes:\n      downstream_node.add_upstream_node(self)\n</code></pre> <code></code> add_upstream_node \u00b6 <pre><code>add_upstream_node(upstream_node)\n</code></pre> <p>Experimental: Add another component that must run before this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> <p>It is symmetric with <code>add_downstream_node</code>.</p> PARAMETER DESCRIPTION <code>upstream_node</code> <p>a component that must run before this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_upstream_node(self, upstream_node):\n  \"\"\"Experimental: Add another component that must run before this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n  It is symmetric with `add_downstream_node`.\n\n  Args:\n    upstream_node: a component that must run before this node.\n  \"\"\"\n  self._upstream_nodes.add(upstream_node)\n  if self not in upstream_node.downstream_nodes:\n    upstream_node.add_downstream_node(self)\n</code></pre> <code></code> add_upstream_nodes \u00b6 <pre><code>add_upstream_nodes(upstream_nodes)\n</code></pre> <p>Experimental: Add components that must run before this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> PARAMETER DESCRIPTION <code>upstream_nodes</code> <p>a list of components that must run before this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_upstream_nodes(self, upstream_nodes):\n  \"\"\"Experimental: Add components that must run before this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n\n  Args:\n    upstream_nodes: a list of components that must run before this node.\n  \"\"\"\n  self._upstream_nodes.update(upstream_nodes)\n  for upstream_node in upstream_nodes:\n    if self not in upstream_node.downstream_nodes:\n      upstream_node.add_downstream_node(self)\n</code></pre> <code></code> from_json_dict <code>classmethod</code> \u00b6 <pre><code>from_json_dict(dict_data: Dict[str, Any]) -&gt; Any\n</code></pre> <p>Convert from dictionary data to an object.</p> Source code in <code>tfx/utils/json_utils.py</code> <pre><code>@classmethod\n@doc_controls.do_not_doc_in_subclasses\ndef from_json_dict(cls, dict_data: Dict[str, Any]) -&gt; Any:\n  \"\"\"Convert from dictionary data to an object.\"\"\"\n  instance = cls.__new__(cls)\n  instance.__dict__ = dict_data\n  return instance\n</code></pre> <code></code> get_class_type <code>classmethod</code> \u00b6 <pre><code>get_class_type() -&gt; str\n</code></pre> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@classmethod\n@doc_controls.do_not_doc_in_subclasses\ndef get_class_type(cls) -&gt; str:\n  nondeprecated_class = deprecation_utils.get_first_nondeprecated_class(cls)\n  # TODO(b/221166027): Turn strict_check=True once failing tests are fixed.\n  return name_utils.get_full_name(nondeprecated_class, strict_check=False)\n</code></pre> <code></code> remove_downstream_node \u00b6 <pre><code>remove_downstream_node(downstream_node)\n</code></pre> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef remove_downstream_node(self, downstream_node):\n  self._downstream_nodes.remove(downstream_node)\n  if self in downstream_node.upstream_nodes:\n    downstream_node.remove_upstream_node(self)\n</code></pre> <code></code> remove_upstream_node \u00b6 <pre><code>remove_upstream_node(upstream_node)\n</code></pre> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef remove_upstream_node(self, upstream_node):\n  self._upstream_nodes.remove(upstream_node)\n  if self in upstream_node.downstream_nodes:\n    upstream_node.remove_downstream_node(self)\n</code></pre> <code></code> to_json_dict \u00b6 <pre><code>to_json_dict() -&gt; Dict[str, Any]\n</code></pre> <p>Convert from an object to a JSON serializable dictionary.</p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef to_json_dict(self) -&gt; Dict[str, Any]:\n  \"\"\"Convert from an object to a JSON serializable dictionary.\"\"\"\n  return dict((k, v)\n              for k, v in self.__dict__.items()\n              if k not in ['_upstream_nodes', '_downstream_nodes'])\n</code></pre> <code></code> with_id \u00b6 <pre><code>with_id(id: str) -&gt; Self\n</code></pre> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef with_id(self, id: str) -&gt; typing_extensions.Self:  # pylint: disable=redefined-builtin\n  self._id = id\n  return self\n</code></pre> <code></code> with_node_execution_options \u00b6 <pre><code>with_node_execution_options(node_execution_options: NodeExecutionOptions) -&gt; Self\n</code></pre> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>def with_node_execution_options(\n    self, node_execution_options: utils.NodeExecutionOptions\n) -&gt; typing_extensions.Self:\n  self.node_execution_options = node_execution_options\n  return self\n</code></pre> <code></code> with_platform_config \u00b6 <pre><code>with_platform_config(config: Message) -&gt; Self\n</code></pre> <p>Attaches a proto-form platform config to a component.</p> <p>The config will be a per-node platform-specific config.</p> PARAMETER DESCRIPTION <code>config</code> <p>platform config to attach to the component.</p> <p> TYPE: <code>Message</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>the same component itself.</p> Source code in <code>tfx/dsl/components/base/base_component.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef with_platform_config(\n    self, config: message.Message\n) -&gt; typing_extensions.Self:\n  \"\"\"Attaches a proto-form platform config to a component.\n\n  The config will be a per-node platform-specific config.\n\n  Args:\n    config: platform config to attach to the component.\n\n  Returns:\n    the same component itself.\n  \"\"\"\n  self.platform_config = config\n  return self\n</code></pre>"},{"location":"api/v1/extensions/#tfx.v1.extensions.google_cloud_ai_platform.Trainer","title":"Trainer","text":"<pre><code>Trainer(examples: Optional[Channel] = None, transformed_examples: Optional[Channel] = None, transform_graph: Optional[Channel] = None, schema: Optional[Channel] = None, base_model: Optional[Channel] = None, hyperparameters: Optional[Channel] = None, module_file: Optional[Union[str, RuntimeParameter]] = None, run_fn: Optional[Union[str, RuntimeParameter]] = None, train_args: Optional[Union[TrainArgs, RuntimeParameter]] = None, eval_args: Optional[Union[EvalArgs, RuntimeParameter]] = None, custom_config: Optional[Dict[str, Any]] = None)\n</code></pre> <p>               Bases: <code>Trainer</code></p> <p>Cloud AI Platform Trainer component.</p> <p>Construct a Trainer component.</p> PARAMETER DESCRIPTION <code>examples</code> <p>A Channel of type <code>standard_artifacts.Examples</code>, serving as the source of examples used in training (required). May be raw or transformed.</p> <p> TYPE: <code>Optional[Channel]</code> DEFAULT: <code>None</code> </p> <code>transformed_examples</code> <p>Deprecated field. Please set <code>examples</code> instead.</p> <p> TYPE: <code>Optional[Channel]</code> DEFAULT: <code>None</code> </p> <code>transform_graph</code> <p>An optional Channel of type <code>standard_artifacts.TransformGraph</code>, serving as the input transform graph if present.</p> <p> TYPE: <code>Optional[Channel]</code> DEFAULT: <code>None</code> </p> <code>schema</code> <p>An optional Channel of type <code>standard_artifacts.Schema</code>, serving as the schema of training and eval data. Schema is optional when 1) transform_graph is provided which contains schema. 2) user module bypasses the usage of schema, e.g., hardcoded.</p> <p> TYPE: <code>Optional[Channel]</code> DEFAULT: <code>None</code> </p> <code>base_model</code> <p>A Channel of type <code>Model</code>, containing model that will be used for training. This can be used for warmstart, transfer learning or model ensembling.</p> <p> TYPE: <code>Optional[Channel]</code> DEFAULT: <code>None</code> </p> <code>hyperparameters</code> <p>A Channel of type <code>standard_artifacts.HyperParameters</code>, serving as the hyperparameters for training module. Tuner's output best hyperparameters can be feed into this.</p> <p> TYPE: <code>Optional[Channel]</code> DEFAULT: <code>None</code> </p> <code>module_file</code> <p>A path to python module file containing UDF model definition. The module_file must implement a function named <code>run_fn</code> at its top level with function signature: </p><pre><code>def run_fn(trainer.fn_args_utils.FnArgs): ...\n</code></pre> <p> TYPE: <code>Optional[Union[str, RuntimeParameter]]</code> DEFAULT: <code>None</code> </p> <code>run_fn</code> <p>A python path to UDF model definition function for generic trainer. See 'module_file' for details. Exactly one of 'module_file' or 'run_fn' must be supplied if Trainer uses GenericExecutor (default).</p> <p> TYPE: <code>Optional[Union[str, RuntimeParameter]]</code> DEFAULT: <code>None</code> </p> <code>train_args</code> <p>A proto.TrainArgs instance, containing args used for training Currently only splits and num_steps are available. Default behavior (when splits is empty) is train on <code>train</code> split.</p> <p> TYPE: <code>Optional[Union[TrainArgs, RuntimeParameter]]</code> DEFAULT: <code>None</code> </p> <code>eval_args</code> <p>A proto.EvalArgs instance, containing args used for evaluation. Currently only splits and num_steps are available. Default behavior (when splits is empty) is evaluate on <code>eval</code> split.</p> <p> TYPE: <code>Optional[Union[EvalArgs, RuntimeParameter]]</code> DEFAULT: <code>None</code> </p> <code>custom_config</code> <p>A dict which contains addtional training job parameters that will be passed into user module.</p> <p> TYPE: <code>Optional[Dict[str, Any]]</code> DEFAULT: <code>None</code> </p> METHOD DESCRIPTION <code>add_downstream_node</code> <p>Experimental: Add another component that must run after this one.</p> <code>add_downstream_nodes</code> <p>Experimental: Add another component that must run after this one.</p> <code>add_upstream_node</code> <p>Experimental: Add another component that must run before this one.</p> <code>add_upstream_nodes</code> <p>Experimental: Add components that must run before this one.</p> <code>from_json_dict</code> <p>Convert from dictionary data to an object.</p> <code>get_class_type</code> <code>remove_downstream_node</code> <code>remove_upstream_node</code> <code>to_json_dict</code> <p>Convert from an object to a JSON serializable dictionary.</p> <code>with_id</code> <code>with_node_execution_options</code> <code>with_platform_config</code> <p>Attaches a proto-form platform config to a component.</p> ATTRIBUTE DESCRIPTION <code>DRIVER_CLASS</code> <p> </p> <code>EXECUTOR_SPEC</code> <p> </p> <code>POST_EXECUTABLE_SPEC</code> <p> </p> <code>PRE_EXECUTABLE_SPEC</code> <p> </p> <code>SPEC_CLASS</code> <p> </p> <code>component_id</code> <p> TYPE: <code>str</code> </p> <code>component_type</code> <p> TYPE: <code>str</code> </p> <code>downstream_nodes</code> <p> </p> <code>driver_class</code> <p> </p> <code>exec_properties</code> <p> TYPE: <code>Dict[str, Any]</code> </p> <code>executor_spec</code> <p> </p> <code>id</code> <p>Node id, unique across all TFX nodes in a pipeline.</p> <p> TYPE: <code>str</code> </p> <code>inputs</code> <p> TYPE: <code>Dict[str, Channel]</code> </p> <code>node_execution_options</code> <p> TYPE: <code>Optional[NodeExecutionOptions]</code> </p> <code>outputs</code> <p>Component's output channel dict.</p> <p> TYPE: <code>Dict[str, OutputChannel]</code> </p> <code>platform_config</code> <p> </p> <code>spec</code> <p> </p> <code>type</code> <p> TYPE: <code>str</code> </p> <code>type_annotation</code> <p> TYPE: <code>Optional[Type[SystemExecution]]</code> </p> <code>upstream_nodes</code> <p> </p> Source code in <code>tfx/extensions/google_cloud_ai_platform/trainer/component.py</code> <pre><code>def __init__(self,\n             examples: Optional[types.Channel] = None,\n             transformed_examples: Optional[types.Channel] = None,\n             transform_graph: Optional[types.Channel] = None,\n             schema: Optional[types.Channel] = None,\n             base_model: Optional[types.Channel] = None,\n             hyperparameters: Optional[types.Channel] = None,\n             module_file: Optional[Union[str,\n                                         data_types.RuntimeParameter]] = None,\n             run_fn: Optional[Union[str, data_types.RuntimeParameter]] = None,\n             train_args: Optional[Union[trainer_pb2.TrainArgs,\n                                        data_types.RuntimeParameter]] = None,\n             eval_args: Optional[Union[trainer_pb2.EvalArgs,\n                                       data_types.RuntimeParameter]] = None,\n             custom_config: Optional[Dict[str, Any]] = None):\n  \"\"\"Construct a Trainer component.\n\n  Args:\n    examples: A Channel of type [`standard_artifacts.Examples`][tfx.v1.types.standard_artifacts.Examples], serving as the\n      source of examples used in training (required). May be raw or\n      transformed.\n    transformed_examples: Deprecated field. Please set `examples` instead.\n    transform_graph: An optional Channel of type\n      [`standard_artifacts.TransformGraph`][tfx.v1.types.standard_artifacts.TransformGraph], serving as the input transform\n      graph if present.\n    schema:  An optional Channel of type [`standard_artifacts.Schema`][tfx.v1.types.standard_artifacts.Schema], serving\n      as the schema of training and eval data. Schema is optional when 1)\n      transform_graph is provided which contains schema. 2) user module\n      bypasses the usage of schema, e.g., hardcoded.\n    base_model: A Channel of type [`Model`][tfx.v1.types.standard_artifacts.Model], containing model that will be used\n      for training. This can be used for warmstart, transfer learning or model\n      ensembling.\n    hyperparameters: A Channel of type [`standard_artifacts.HyperParameters`][tfx.v1.types.standard_artifacts.HyperParameters],\n      serving as the hyperparameters for training module. Tuner's output best\n      hyperparameters can be feed into this.\n    module_file: A path to python module file containing UDF model definition.\n      The module_file must implement a function named `run_fn` at its top\n      level with function signature:\n      ```python\n      def run_fn(trainer.fn_args_utils.FnArgs): ...\n      ```\n    run_fn:  A python path to UDF model definition function for generic\n      trainer. See 'module_file' for details. Exactly one of 'module_file' or\n      'run_fn' must be supplied if Trainer uses GenericExecutor (default).\n    train_args: A proto.TrainArgs instance, containing args used for training\n      Currently only splits and num_steps are available. Default behavior\n      (when splits is empty) is train on `train` split.\n    eval_args: A proto.EvalArgs instance, containing args used for evaluation.\n      Currently only splits and num_steps are available. Default behavior\n      (when splits is empty) is evaluate on `eval` split.\n    custom_config: A dict which contains addtional training job parameters\n      that will be passed into user module.\n  \"\"\"\n  super().__init__(\n      examples=examples,\n      transformed_examples=transformed_examples,\n      transform_graph=transform_graph,\n      schema=schema,\n      base_model=base_model,\n      hyperparameters=hyperparameters,\n      train_args=train_args,\n      eval_args=eval_args,\n      module_file=module_file,\n      run_fn=run_fn,\n      custom_config=custom_config)\n</code></pre> Attributes\u00b6 <code></code> DRIVER_CLASS <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <pre><code>DRIVER_CLASS = BaseDriver\n</code></pre> <code></code> EXECUTOR_SPEC <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <pre><code>EXECUTOR_SPEC = ExecutorClassSpec(GenericExecutor)\n</code></pre> <code></code> POST_EXECUTABLE_SPEC <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <pre><code>POST_EXECUTABLE_SPEC = None\n</code></pre> <code></code> PRE_EXECUTABLE_SPEC <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <pre><code>PRE_EXECUTABLE_SPEC = None\n</code></pre> <code></code> SPEC_CLASS <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <pre><code>SPEC_CLASS = TrainerSpec\n</code></pre> <code></code> component_id <code>property</code> \u00b6 <pre><code>component_id: str\n</code></pre> <code></code> component_type <code>property</code> \u00b6 <pre><code>component_type: str\n</code></pre> <code></code> downstream_nodes <code>property</code> \u00b6 <pre><code>downstream_nodes\n</code></pre> <code></code> driver_class <code>instance-attribute</code> \u00b6 <pre><code>driver_class = driver_class\n</code></pre> <code></code> exec_properties <code>property</code> \u00b6 <pre><code>exec_properties: Dict[str, Any]\n</code></pre> <code></code> executor_spec <code>instance-attribute</code> \u00b6 <pre><code>executor_spec = executor_spec\n</code></pre> <code></code> id <code>property</code> <code>writable</code> \u00b6 <pre><code>id: str\n</code></pre> <p>Node id, unique across all TFX nodes in a pipeline.</p> <p>If <code>id</code> is set by the user, return it directly. Otherwise, return .</p> RETURNS DESCRIPTION <code>str</code> <p>node id.</p> <code></code> inputs <code>property</code> \u00b6 <pre><code>inputs: Dict[str, Channel]\n</code></pre> <code></code> node_execution_options <code>property</code> <code>writable</code> \u00b6 <pre><code>node_execution_options: Optional[NodeExecutionOptions]\n</code></pre> <code></code> outputs <code>property</code> \u00b6 <pre><code>outputs: Dict[str, OutputChannel]\n</code></pre> <p>Component's output channel dict.</p> <code></code> platform_config <code>instance-attribute</code> \u00b6 <pre><code>platform_config = None\n</code></pre> <code></code> spec <code>instance-attribute</code> \u00b6 <pre><code>spec = spec\n</code></pre> <code></code> type <code>property</code> \u00b6 <pre><code>type: str\n</code></pre> <code></code> type_annotation <code>property</code> \u00b6 <pre><code>type_annotation: Optional[Type[SystemExecution]]\n</code></pre> <code></code> upstream_nodes <code>property</code> \u00b6 <pre><code>upstream_nodes\n</code></pre> Functions\u00b6 <code></code> add_downstream_node \u00b6 <pre><code>add_downstream_node(downstream_node)\n</code></pre> <p>Experimental: Add another component that must run after this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> <p>It is symmetric with <code>add_upstream_node</code>.</p> PARAMETER DESCRIPTION <code>downstream_node</code> <p>a component that must run after this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_downstream_node(self, downstream_node):\n  \"\"\"Experimental: Add another component that must run after this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n  It is symmetric with `add_upstream_node`.\n\n  Args:\n    downstream_node: a component that must run after this node.\n  \"\"\"\n  self._downstream_nodes.add(downstream_node)\n  if self not in downstream_node.upstream_nodes:\n    downstream_node.add_upstream_node(self)\n</code></pre> <code></code> add_downstream_nodes \u00b6 <pre><code>add_downstream_nodes(downstream_nodes)\n</code></pre> <p>Experimental: Add another component that must run after this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> <p>It is symmetric with <code>add_upstream_nodes</code>.</p> PARAMETER DESCRIPTION <code>downstream_nodes</code> <p>a list of components that must run after this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_downstream_nodes(self, downstream_nodes):\n  \"\"\"Experimental: Add another component that must run after this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n  It is symmetric with `add_upstream_nodes`.\n\n  Args:\n    downstream_nodes: a list of components that must run after this node.\n  \"\"\"\n  self._downstream_nodes.update(downstream_nodes)\n  for downstream_node in downstream_nodes:\n    if self not in downstream_node.upstream_nodes:\n      downstream_node.add_upstream_node(self)\n</code></pre> <code></code> add_upstream_node \u00b6 <pre><code>add_upstream_node(upstream_node)\n</code></pre> <p>Experimental: Add another component that must run before this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> <p>It is symmetric with <code>add_downstream_node</code>.</p> PARAMETER DESCRIPTION <code>upstream_node</code> <p>a component that must run before this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_upstream_node(self, upstream_node):\n  \"\"\"Experimental: Add another component that must run before this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n  It is symmetric with `add_downstream_node`.\n\n  Args:\n    upstream_node: a component that must run before this node.\n  \"\"\"\n  self._upstream_nodes.add(upstream_node)\n  if self not in upstream_node.downstream_nodes:\n    upstream_node.add_downstream_node(self)\n</code></pre> <code></code> add_upstream_nodes \u00b6 <pre><code>add_upstream_nodes(upstream_nodes)\n</code></pre> <p>Experimental: Add components that must run before this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> PARAMETER DESCRIPTION <code>upstream_nodes</code> <p>a list of components that must run before this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_upstream_nodes(self, upstream_nodes):\n  \"\"\"Experimental: Add components that must run before this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n\n  Args:\n    upstream_nodes: a list of components that must run before this node.\n  \"\"\"\n  self._upstream_nodes.update(upstream_nodes)\n  for upstream_node in upstream_nodes:\n    if self not in upstream_node.downstream_nodes:\n      upstream_node.add_downstream_node(self)\n</code></pre> <code></code> from_json_dict <code>classmethod</code> \u00b6 <pre><code>from_json_dict(dict_data: Dict[str, Any]) -&gt; Any\n</code></pre> <p>Convert from dictionary data to an object.</p> Source code in <code>tfx/utils/json_utils.py</code> <pre><code>@classmethod\n@doc_controls.do_not_doc_in_subclasses\ndef from_json_dict(cls, dict_data: Dict[str, Any]) -&gt; Any:\n  \"\"\"Convert from dictionary data to an object.\"\"\"\n  instance = cls.__new__(cls)\n  instance.__dict__ = dict_data\n  return instance\n</code></pre> <code></code> get_class_type <code>classmethod</code> \u00b6 <pre><code>get_class_type() -&gt; str\n</code></pre> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@classmethod\n@doc_controls.do_not_doc_in_subclasses\ndef get_class_type(cls) -&gt; str:\n  nondeprecated_class = deprecation_utils.get_first_nondeprecated_class(cls)\n  # TODO(b/221166027): Turn strict_check=True once failing tests are fixed.\n  return name_utils.get_full_name(nondeprecated_class, strict_check=False)\n</code></pre> <code></code> remove_downstream_node \u00b6 <pre><code>remove_downstream_node(downstream_node)\n</code></pre> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef remove_downstream_node(self, downstream_node):\n  self._downstream_nodes.remove(downstream_node)\n  if self in downstream_node.upstream_nodes:\n    downstream_node.remove_upstream_node(self)\n</code></pre> <code></code> remove_upstream_node \u00b6 <pre><code>remove_upstream_node(upstream_node)\n</code></pre> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef remove_upstream_node(self, upstream_node):\n  self._upstream_nodes.remove(upstream_node)\n  if self in upstream_node.downstream_nodes:\n    upstream_node.remove_downstream_node(self)\n</code></pre> <code></code> to_json_dict \u00b6 <pre><code>to_json_dict() -&gt; Dict[str, Any]\n</code></pre> <p>Convert from an object to a JSON serializable dictionary.</p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef to_json_dict(self) -&gt; Dict[str, Any]:\n  \"\"\"Convert from an object to a JSON serializable dictionary.\"\"\"\n  return dict((k, v)\n              for k, v in self.__dict__.items()\n              if k not in ['_upstream_nodes', '_downstream_nodes'])\n</code></pre> <code></code> with_id \u00b6 <pre><code>with_id(id: str) -&gt; Self\n</code></pre> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef with_id(self, id: str) -&gt; typing_extensions.Self:  # pylint: disable=redefined-builtin\n  self._id = id\n  return self\n</code></pre> <code></code> with_node_execution_options \u00b6 <pre><code>with_node_execution_options(node_execution_options: NodeExecutionOptions) -&gt; Self\n</code></pre> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>def with_node_execution_options(\n    self, node_execution_options: utils.NodeExecutionOptions\n) -&gt; typing_extensions.Self:\n  self.node_execution_options = node_execution_options\n  return self\n</code></pre> <code></code> with_platform_config \u00b6 <pre><code>with_platform_config(config: Message) -&gt; Self\n</code></pre> <p>Attaches a proto-form platform config to a component.</p> <p>The config will be a per-node platform-specific config.</p> PARAMETER DESCRIPTION <code>config</code> <p>platform config to attach to the component.</p> <p> TYPE: <code>Message</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>the same component itself.</p> Source code in <code>tfx/dsl/components/base/base_component.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef with_platform_config(\n    self, config: message.Message\n) -&gt; typing_extensions.Self:\n  \"\"\"Attaches a proto-form platform config to a component.\n\n  The config will be a per-node platform-specific config.\n\n  Args:\n    config: platform config to attach to the component.\n\n  Returns:\n    the same component itself.\n  \"\"\"\n  self.platform_config = config\n  return self\n</code></pre>"},{"location":"api/v1/extensions/#tfx.v1.extensions.google_cloud_ai_platform.Tuner","title":"Tuner","text":"<pre><code>Tuner(examples: BaseChannel, schema: Optional[BaseChannel] = None, transform_graph: Optional[BaseChannel] = None, base_model: Optional[BaseChannel] = None, module_file: Optional[str] = None, tuner_fn: Optional[str] = None, train_args: Optional[TrainArgs] = None, eval_args: Optional[EvalArgs] = None, tune_args: Optional[TuneArgs] = None, custom_config: Optional[Dict[str, Any]] = None)\n</code></pre> <p>               Bases: <code>Tuner</code></p> <p>TFX component for model hyperparameter tuning on AI Platform Training.</p> <p>Construct a Tuner component.</p> PARAMETER DESCRIPTION <code>examples</code> <p>A BaseChannel of type <code>standard_artifacts.Examples</code>, serving as the source of examples that are used in tuning (required).</p> <p> TYPE: <code>BaseChannel</code> </p> <code>schema</code> <p>An optional BaseChannel of type <code>standard_artifacts.Schema</code>, serving as the schema of training and eval data. This is used when raw examples are provided.</p> <p> TYPE: <code>Optional[BaseChannel]</code> DEFAULT: <code>None</code> </p> <code>transform_graph</code> <p>An optional BaseChannel of type <code>standard_artifacts.TransformGraph</code>, serving as the input transform graph if present. This is used when transformed examples are provided.</p> <p> TYPE: <code>Optional[BaseChannel]</code> DEFAULT: <code>None</code> </p> <code>base_model</code> <p>A BaseChannel of type <code>Model</code>, containing model that will be used for training. This can be used for warmstart, transfer learning or model ensembling.</p> <p> TYPE: <code>Optional[BaseChannel]</code> DEFAULT: <code>None</code> </p> <code>module_file</code> <p>A path to python module file containing UDF tuner definition. The module_file must implement a function named <code>tuner_fn</code> at its top level. The function must have the following signature.     </p><pre><code>def tuner_fn(fn_args: FnArgs) -&gt; TunerFnResult:\n    ...\n</code></pre>     Exactly one of 'module_file' or 'tuner_fn' must be supplied.                <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>tuner_fn</code> <p>A python path to UDF model definition function. See 'module_file' for the required signature of the UDF. Exactly one of 'module_file' or 'tuner_fn' must be supplied.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>train_args</code> <p>A trainer_pb2.TrainArgs instance, containing args used for training. Currently only splits and num_steps are available. Default behavior (when splits is empty) is train on <code>train</code> split.</p> <p> TYPE: <code>Optional[TrainArgs]</code> DEFAULT: <code>None</code> </p> <code>eval_args</code> <p>A trainer_pb2.EvalArgs instance, containing args used for eval. Currently only splits and num_steps are available. Default behavior (when splits is empty) is evaluate on <code>eval</code> split.</p> <p> TYPE: <code>Optional[EvalArgs]</code> DEFAULT: <code>None</code> </p> <code>tune_args</code> <p>A tuner_pb2.TuneArgs instance, containing args used for tuning. Currently only num_parallel_trials is available.</p> <p> TYPE: <code>Optional[TuneArgs]</code> DEFAULT: <code>None</code> </p> <code>custom_config</code> <p>A dict which contains addtional training job parameters that will be passed into user module.</p> <p> TYPE: <code>Optional[Dict[str, Any]]</code> DEFAULT: <code>None</code> </p> METHOD DESCRIPTION <code>add_downstream_node</code> <p>Experimental: Add another component that must run after this one.</p> <code>add_downstream_nodes</code> <p>Experimental: Add another component that must run after this one.</p> <code>add_upstream_node</code> <p>Experimental: Add another component that must run before this one.</p> <code>add_upstream_nodes</code> <p>Experimental: Add components that must run before this one.</p> <code>from_json_dict</code> <p>Convert from dictionary data to an object.</p> <code>get_class_type</code> <code>remove_downstream_node</code> <code>remove_upstream_node</code> <code>to_json_dict</code> <p>Convert from an object to a JSON serializable dictionary.</p> <code>with_id</code> <code>with_node_execution_options</code> <code>with_platform_config</code> <p>Attaches a proto-form platform config to a component.</p> ATTRIBUTE DESCRIPTION <code>DRIVER_CLASS</code> <p> </p> <code>EXECUTOR_SPEC</code> <p> </p> <code>POST_EXECUTABLE_SPEC</code> <p> </p> <code>PRE_EXECUTABLE_SPEC</code> <p> </p> <code>SPEC_CLASS</code> <p> </p> <code>component_id</code> <p> TYPE: <code>str</code> </p> <code>component_type</code> <p> TYPE: <code>str</code> </p> <code>downstream_nodes</code> <p> </p> <code>driver_class</code> <p> </p> <code>exec_properties</code> <p> TYPE: <code>Dict[str, Any]</code> </p> <code>executor_spec</code> <p> </p> <code>id</code> <p>Node id, unique across all TFX nodes in a pipeline.</p> <p> TYPE: <code>str</code> </p> <code>inputs</code> <p> TYPE: <code>Dict[str, Channel]</code> </p> <code>node_execution_options</code> <p> TYPE: <code>Optional[NodeExecutionOptions]</code> </p> <code>outputs</code> <p>Component's output channel dict.</p> <p> TYPE: <code>Dict[str, OutputChannel]</code> </p> <code>platform_config</code> <p> </p> <code>spec</code> <p> </p> <code>type</code> <p> TYPE: <code>str</code> </p> <code>type_annotation</code> <p> TYPE: <code>Optional[Type[SystemExecution]]</code> </p> <code>upstream_nodes</code> <p> </p> Source code in <code>tfx/components/tuner/component.py</code> <pre><code>def __init__(self,\n             examples: types.BaseChannel,\n             schema: Optional[types.BaseChannel] = None,\n             transform_graph: Optional[types.BaseChannel] = None,\n             base_model: Optional[types.BaseChannel] = None,\n             module_file: Optional[str] = None,\n             tuner_fn: Optional[str] = None,\n             train_args: Optional[trainer_pb2.TrainArgs] = None,\n             eval_args: Optional[trainer_pb2.EvalArgs] = None,\n             tune_args: Optional[tuner_pb2.TuneArgs] = None,\n             custom_config: Optional[Dict[str, Any]] = None):\n  \"\"\"Construct a Tuner component.\n\n  Args:\n    examples: A [BaseChannel][tfx.v1.types.BaseChannel] of type [`standard_artifacts.Examples`][tfx.v1.types.standard_artifacts.Examples], serving as\n      the source of examples that are used in tuning (required).\n    schema:  An optional [BaseChannel][tfx.v1.types.BaseChannel] of type [`standard_artifacts.Schema`][tfx.v1.types.standard_artifacts.Schema],\n      serving as the schema of training and eval data. This is used when raw\n      examples are provided.\n    transform_graph: An optional [BaseChannel][tfx.v1.types.BaseChannel] of type\n      [`standard_artifacts.TransformGraph`][tfx.v1.types.standard_artifacts.TransformGraph], serving as the input transform\n      graph if present. This is used when transformed examples are provided.\n    base_model: A [BaseChannel][tfx.v1.types.BaseChannel] of type [`Model`][tfx.v1.types.standard_artifacts.Model], containing model that will be\n      used for training. This can be used for warmstart, transfer learning or\n      model ensembling.\n    module_file: A path to python module file containing UDF tuner definition.\n      The module_file must implement a function named `tuner_fn` at its top\n      level. The function must have the following signature.\n          ``` {.python .no-copy}\n          def tuner_fn(fn_args: FnArgs) -&gt; TunerFnResult:\n              ...\n          ```\n          Exactly one of 'module_file' or 'tuner_fn' must be supplied.\n    tuner_fn:  A python path to UDF model definition function. See\n      'module_file' for the required signature of the UDF. Exactly one of\n      'module_file' or 'tuner_fn' must be supplied.\n    train_args: A trainer_pb2.TrainArgs instance, containing args used for\n      training. Currently only splits and num_steps are available. Default\n      behavior (when splits is empty) is train on `train` split.\n    eval_args: A trainer_pb2.EvalArgs instance, containing args used for eval.\n      Currently only splits and num_steps are available. Default behavior\n      (when splits is empty) is evaluate on `eval` split.\n    tune_args: A tuner_pb2.TuneArgs instance, containing args used for tuning.\n      Currently only num_parallel_trials is available.\n    custom_config: A dict which contains addtional training job parameters\n      that will be passed into user module.\n  \"\"\"\n  if bool(module_file) == bool(tuner_fn):\n    raise ValueError(\n        \"Exactly one of 'module_file' or 'tuner_fn' must be supplied\")\n\n  best_hyperparameters = types.Channel(\n      type=standard_artifacts.HyperParameters)\n  tuner_results = types.Channel(type=standard_artifacts.TunerResults)\n  spec = standard_component_specs.TunerSpec(\n      examples=examples,\n      schema=schema,\n      transform_graph=transform_graph,\n      base_model=base_model,\n      module_file=module_file,\n      tuner_fn=tuner_fn,\n      train_args=train_args or trainer_pb2.TrainArgs(),\n      eval_args=eval_args or trainer_pb2.EvalArgs(),\n      tune_args=tune_args,\n      best_hyperparameters=best_hyperparameters,\n      tuner_results=tuner_results,\n      custom_config=json_utils.dumps(custom_config),\n  )\n  super().__init__(spec=spec)\n\n  if udf_utils.should_package_user_modules():\n    # In this case, the `MODULE_PATH_KEY` execution property will be injected\n    # as a reference to the given user module file after packaging, at which\n    # point the `MODULE_FILE_KEY` execution property will be removed.\n    udf_utils.add_user_module_dependency(\n        self, standard_component_specs.MODULE_FILE_KEY,\n        standard_component_specs.MODULE_PATH_KEY)\n</code></pre> Attributes\u00b6 <code></code> DRIVER_CLASS <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <pre><code>DRIVER_CLASS = BaseDriver\n</code></pre> <code></code> EXECUTOR_SPEC <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <pre><code>EXECUTOR_SPEC = ExecutorClassSpec(Executor)\n</code></pre> <code></code> POST_EXECUTABLE_SPEC <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <pre><code>POST_EXECUTABLE_SPEC = None\n</code></pre> <code></code> PRE_EXECUTABLE_SPEC <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <pre><code>PRE_EXECUTABLE_SPEC = None\n</code></pre> <code></code> SPEC_CLASS <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <pre><code>SPEC_CLASS = TunerSpec\n</code></pre> <code></code> component_id <code>property</code> \u00b6 <pre><code>component_id: str\n</code></pre> <code></code> component_type <code>property</code> \u00b6 <pre><code>component_type: str\n</code></pre> <code></code> downstream_nodes <code>property</code> \u00b6 <pre><code>downstream_nodes\n</code></pre> <code></code> driver_class <code>instance-attribute</code> \u00b6 <pre><code>driver_class = driver_class\n</code></pre> <code></code> exec_properties <code>property</code> \u00b6 <pre><code>exec_properties: Dict[str, Any]\n</code></pre> <code></code> executor_spec <code>instance-attribute</code> \u00b6 <pre><code>executor_spec = executor_spec\n</code></pre> <code></code> id <code>property</code> <code>writable</code> \u00b6 <pre><code>id: str\n</code></pre> <p>Node id, unique across all TFX nodes in a pipeline.</p> <p>If <code>id</code> is set by the user, return it directly. Otherwise, return .</p> RETURNS DESCRIPTION <code>str</code> <p>node id.</p> <code></code> inputs <code>property</code> \u00b6 <pre><code>inputs: Dict[str, Channel]\n</code></pre> <code></code> node_execution_options <code>property</code> <code>writable</code> \u00b6 <pre><code>node_execution_options: Optional[NodeExecutionOptions]\n</code></pre> <code></code> outputs <code>property</code> \u00b6 <pre><code>outputs: Dict[str, OutputChannel]\n</code></pre> <p>Component's output channel dict.</p> <code></code> platform_config <code>instance-attribute</code> \u00b6 <pre><code>platform_config = None\n</code></pre> <code></code> spec <code>instance-attribute</code> \u00b6 <pre><code>spec = spec\n</code></pre> <code></code> type <code>property</code> \u00b6 <pre><code>type: str\n</code></pre> <code></code> type_annotation <code>property</code> \u00b6 <pre><code>type_annotation: Optional[Type[SystemExecution]]\n</code></pre> <code></code> upstream_nodes <code>property</code> \u00b6 <pre><code>upstream_nodes\n</code></pre> Functions\u00b6 <code></code> add_downstream_node \u00b6 <pre><code>add_downstream_node(downstream_node)\n</code></pre> <p>Experimental: Add another component that must run after this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> <p>It is symmetric with <code>add_upstream_node</code>.</p> PARAMETER DESCRIPTION <code>downstream_node</code> <p>a component that must run after this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_downstream_node(self, downstream_node):\n  \"\"\"Experimental: Add another component that must run after this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n  It is symmetric with `add_upstream_node`.\n\n  Args:\n    downstream_node: a component that must run after this node.\n  \"\"\"\n  self._downstream_nodes.add(downstream_node)\n  if self not in downstream_node.upstream_nodes:\n    downstream_node.add_upstream_node(self)\n</code></pre> <code></code> add_downstream_nodes \u00b6 <pre><code>add_downstream_nodes(downstream_nodes)\n</code></pre> <p>Experimental: Add another component that must run after this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> <p>It is symmetric with <code>add_upstream_nodes</code>.</p> PARAMETER DESCRIPTION <code>downstream_nodes</code> <p>a list of components that must run after this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_downstream_nodes(self, downstream_nodes):\n  \"\"\"Experimental: Add another component that must run after this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n  It is symmetric with `add_upstream_nodes`.\n\n  Args:\n    downstream_nodes: a list of components that must run after this node.\n  \"\"\"\n  self._downstream_nodes.update(downstream_nodes)\n  for downstream_node in downstream_nodes:\n    if self not in downstream_node.upstream_nodes:\n      downstream_node.add_upstream_node(self)\n</code></pre> <code></code> add_upstream_node \u00b6 <pre><code>add_upstream_node(upstream_node)\n</code></pre> <p>Experimental: Add another component that must run before this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> <p>It is symmetric with <code>add_downstream_node</code>.</p> PARAMETER DESCRIPTION <code>upstream_node</code> <p>a component that must run before this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_upstream_node(self, upstream_node):\n  \"\"\"Experimental: Add another component that must run before this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n  It is symmetric with `add_downstream_node`.\n\n  Args:\n    upstream_node: a component that must run before this node.\n  \"\"\"\n  self._upstream_nodes.add(upstream_node)\n  if self not in upstream_node.downstream_nodes:\n    upstream_node.add_downstream_node(self)\n</code></pre> <code></code> add_upstream_nodes \u00b6 <pre><code>add_upstream_nodes(upstream_nodes)\n</code></pre> <p>Experimental: Add components that must run before this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> PARAMETER DESCRIPTION <code>upstream_nodes</code> <p>a list of components that must run before this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_upstream_nodes(self, upstream_nodes):\n  \"\"\"Experimental: Add components that must run before this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n\n  Args:\n    upstream_nodes: a list of components that must run before this node.\n  \"\"\"\n  self._upstream_nodes.update(upstream_nodes)\n  for upstream_node in upstream_nodes:\n    if self not in upstream_node.downstream_nodes:\n      upstream_node.add_downstream_node(self)\n</code></pre> <code></code> from_json_dict <code>classmethod</code> \u00b6 <pre><code>from_json_dict(dict_data: Dict[str, Any]) -&gt; Any\n</code></pre> <p>Convert from dictionary data to an object.</p> Source code in <code>tfx/utils/json_utils.py</code> <pre><code>@classmethod\n@doc_controls.do_not_doc_in_subclasses\ndef from_json_dict(cls, dict_data: Dict[str, Any]) -&gt; Any:\n  \"\"\"Convert from dictionary data to an object.\"\"\"\n  instance = cls.__new__(cls)\n  instance.__dict__ = dict_data\n  return instance\n</code></pre> <code></code> get_class_type <code>classmethod</code> \u00b6 <pre><code>get_class_type() -&gt; str\n</code></pre> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@classmethod\n@doc_controls.do_not_doc_in_subclasses\ndef get_class_type(cls) -&gt; str:\n  nondeprecated_class = deprecation_utils.get_first_nondeprecated_class(cls)\n  # TODO(b/221166027): Turn strict_check=True once failing tests are fixed.\n  return name_utils.get_full_name(nondeprecated_class, strict_check=False)\n</code></pre> <code></code> remove_downstream_node \u00b6 <pre><code>remove_downstream_node(downstream_node)\n</code></pre> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef remove_downstream_node(self, downstream_node):\n  self._downstream_nodes.remove(downstream_node)\n  if self in downstream_node.upstream_nodes:\n    downstream_node.remove_upstream_node(self)\n</code></pre> <code></code> remove_upstream_node \u00b6 <pre><code>remove_upstream_node(upstream_node)\n</code></pre> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef remove_upstream_node(self, upstream_node):\n  self._upstream_nodes.remove(upstream_node)\n  if self in upstream_node.downstream_nodes:\n    upstream_node.remove_downstream_node(self)\n</code></pre> <code></code> to_json_dict \u00b6 <pre><code>to_json_dict() -&gt; Dict[str, Any]\n</code></pre> <p>Convert from an object to a JSON serializable dictionary.</p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef to_json_dict(self) -&gt; Dict[str, Any]:\n  \"\"\"Convert from an object to a JSON serializable dictionary.\"\"\"\n  return dict((k, v)\n              for k, v in self.__dict__.items()\n              if k not in ['_upstream_nodes', '_downstream_nodes'])\n</code></pre> <code></code> with_id \u00b6 <pre><code>with_id(id: str) -&gt; Self\n</code></pre> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef with_id(self, id: str) -&gt; typing_extensions.Self:  # pylint: disable=redefined-builtin\n  self._id = id\n  return self\n</code></pre> <code></code> with_node_execution_options \u00b6 <pre><code>with_node_execution_options(node_execution_options: NodeExecutionOptions) -&gt; Self\n</code></pre> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>def with_node_execution_options(\n    self, node_execution_options: utils.NodeExecutionOptions\n) -&gt; typing_extensions.Self:\n  self.node_execution_options = node_execution_options\n  return self\n</code></pre> <code></code> with_platform_config \u00b6 <pre><code>with_platform_config(config: Message) -&gt; Self\n</code></pre> <p>Attaches a proto-form platform config to a component.</p> <p>The config will be a per-node platform-specific config.</p> PARAMETER DESCRIPTION <code>config</code> <p>platform config to attach to the component.</p> <p> TYPE: <code>Message</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>the same component itself.</p> Source code in <code>tfx/dsl/components/base/base_component.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef with_platform_config(\n    self, config: message.Message\n) -&gt; typing_extensions.Self:\n  \"\"\"Attaches a proto-form platform config to a component.\n\n  The config will be a per-node platform-specific config.\n\n  Args:\n    config: platform config to attach to the component.\n\n  Returns:\n    the same component itself.\n  \"\"\"\n  self.platform_config = config\n  return self\n</code></pre>"},{"location":"api/v1/extensions/#tfx.v1.extensions.google_cloud_ai_platform-modules","title":"Modules","text":""},{"location":"api/v1/extensions/#tfx.v1.extensions.google_cloud_ai_platform.experimental","title":"experimental","text":"<p>Types used in Google Cloud AI Platform under experimental stage.</p> ATTRIBUTE DESCRIPTION <code>BULK_INFERRER_SERVING_ARGS_KEY</code> <p> </p> <code>ENDPOINT_ARGS_KEY</code> <p> </p> <code>PUSHER_SERVING_ARGS_KEY</code> <p> </p> <code>REMOTE_TRIALS_WORKING_DIR_KEY</code> <p> </p> <code>TUNING_ARGS_KEY</code> <p> </p> Attributes\u00b6 <code></code> BULK_INFERRER_SERVING_ARGS_KEY <code>module-attribute</code> \u00b6 <pre><code>BULK_INFERRER_SERVING_ARGS_KEY = documented(obj='ai_platform_serving_args', doc='Keys to the items in custom_config of Bulk Inferrer for passing bulkinferrer args to AI Platform.')\n</code></pre> <code></code> ENDPOINT_ARGS_KEY <code>module-attribute</code> \u00b6 <pre><code>ENDPOINT_ARGS_KEY = documented(obj='endpoint', doc='Keys to the items in custom_config of Pusher/BulkInferrer for optional endpoint override (CAIP).')\n</code></pre> <code></code> PUSHER_SERVING_ARGS_KEY <code>module-attribute</code> \u00b6 <pre><code>PUSHER_SERVING_ARGS_KEY = documented(obj='ai_platform_serving_args', doc='Keys to the items in custom_config of Pusher/BulkInferrer for passing serving args to AI Platform.')\n</code></pre> <code></code> REMOTE_TRIALS_WORKING_DIR_KEY <code>module-attribute</code> \u00b6 <pre><code>REMOTE_TRIALS_WORKING_DIR_KEY = documented(obj='remote_trials_working_dir', doc='Keys to the items in custom_config of Tuner for specifying a working dir for remote trial.')\n</code></pre> <code></code> TUNING_ARGS_KEY <code>module-attribute</code> \u00b6 <pre><code>TUNING_ARGS_KEY = documented(obj='ai_platform_tuning_args', doc='Keys to the items in custom_config of Tuner for passing training_job to AI Platform, and the GCP project under which the training job will be executed. In Vertex AI, this corresponds to a CustomJob as defined in:https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.customJobs#CustomJob.In CAIP, this corresponds to TrainingInputs as defined in:https://cloud.google.com/ml-engine/reference/rest/v1/projects.jobs#TrainingInput')\n</code></pre>"},{"location":"api/v1/extensions/#tfx.v1.extensions.google_cloud_big_query","title":"google_cloud_big_query","text":"<p>Google Cloud Big Query module.</p> CLASS DESCRIPTION <code>BigQueryExampleGen</code> <p>Cloud BigQueryExampleGen component.</p> <code>Pusher</code> <p>Cloud Big Query Pusher component.</p> ATTRIBUTE DESCRIPTION <code>PUSHER_SERVING_ARGS_KEY</code> <p> </p>"},{"location":"api/v1/extensions/#tfx.v1.extensions.google_cloud_big_query-attributes","title":"Attributes","text":""},{"location":"api/v1/extensions/#tfx.v1.extensions.google_cloud_big_query.PUSHER_SERVING_ARGS_KEY","title":"PUSHER_SERVING_ARGS_KEY  <code>module-attribute</code>","text":"<pre><code>PUSHER_SERVING_ARGS_KEY = documented(obj='bigquery_serving_args', doc='Keys to the items in custom_config of Pusher for passing serving args to Big Query.')\n</code></pre>"},{"location":"api/v1/extensions/#tfx.v1.extensions.google_cloud_big_query-classes","title":"Classes","text":""},{"location":"api/v1/extensions/#tfx.v1.extensions.google_cloud_big_query.BigQueryExampleGen","title":"BigQueryExampleGen","text":"<pre><code>BigQueryExampleGen(query: Optional[str] = None, input_config: Optional[Union[Input, RuntimeParameter]] = None, output_config: Optional[Union[Output, RuntimeParameter]] = None, range_config: Optional[Union[RangeConfig, RuntimeParameter, Placeholder]] = None, custom_executor_spec: Optional[ExecutorSpec] = None, custom_config: Optional[Union[CustomConfig, RuntimeParameter]] = None)\n</code></pre> <p>               Bases: <code>QueryBasedExampleGen</code></p> <p>Cloud BigQueryExampleGen component.</p> <p>The BigQuery examplegen component takes a query, and generates train and eval examples for downstream components.</p> <p>Component <code>outputs</code> contains:</p> <ul> <li><code>examples</code>: Channel of type <code>standard_artifacts.Examples</code> for output train                and eval examples.</li> </ul> <p>Constructs a BigQueryExampleGen component.</p> PARAMETER DESCRIPTION <code>query</code> <p>BigQuery sql string, query result will be treated as a single split, can be overwritten by input_config.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>input_config</code> <p>An example_gen_pb2.Input instance with Split.pattern as BigQuery sql string. If set, it overwrites the 'query' arg, and allows different queries per split. If any field is provided as a RuntimeParameter, input_config should be constructed as a dict with the same field names as Input proto message.</p> <p> TYPE: <code>Optional[Union[Input, RuntimeParameter]]</code> DEFAULT: <code>None</code> </p> <code>output_config</code> <p>An example_gen_pb2.Output instance, providing output configuration. If unset, default splits will be 'train' and 'eval' with size 2:1. If any field is provided as a RuntimeParameter, input_config should be constructed as a dict with the same field names as Output proto message.</p> <p> TYPE: <code>Optional[Union[Output, RuntimeParameter]]</code> DEFAULT: <code>None</code> </p> <code>range_config</code> <p>An optional range_config_pb2.RangeConfig instance, specifying the range of span values to consider.</p> <p> TYPE: <code>Optional[Union[RangeConfig, RuntimeParameter, Placeholder]]</code> DEFAULT: <code>None</code> </p> <code>custom_executor_spec</code> <p>Optional custom executor spec overriding the default executor spec specified in the component attribute.</p> <p> TYPE: <code>Optional[ExecutorSpec]</code> DEFAULT: <code>None</code> </p> <code>custom_config</code> <p>An example_gen_pb2.CustomConfig instance, providing custom configuration for ExampleGen.</p> <p> TYPE: <code>Optional[Union[CustomConfig, RuntimeParameter]]</code> DEFAULT: <code>None</code> </p> RAISES DESCRIPTION <code>RuntimeError</code> <p>Only one of query and input_config should be set.</p> METHOD DESCRIPTION <code>add_downstream_node</code> <p>Experimental: Add another component that must run after this one.</p> <code>add_downstream_nodes</code> <p>Experimental: Add another component that must run after this one.</p> <code>add_upstream_node</code> <p>Experimental: Add another component that must run before this one.</p> <code>add_upstream_nodes</code> <p>Experimental: Add components that must run before this one.</p> <code>from_json_dict</code> <p>Convert from dictionary data to an object.</p> <code>get_class_type</code> <code>remove_downstream_node</code> <code>remove_upstream_node</code> <code>to_json_dict</code> <p>Convert from an object to a JSON serializable dictionary.</p> <code>with_beam_pipeline_args</code> <p>Add per component Beam pipeline args.</p> <code>with_id</code> <code>with_node_execution_options</code> <code>with_platform_config</code> <p>Attaches a proto-form platform config to a component.</p> ATTRIBUTE DESCRIPTION <code>DRIVER_CLASS</code> <p> </p> <code>EXECUTOR_SPEC</code> <p> </p> <code>POST_EXECUTABLE_SPEC</code> <p> </p> <code>PRE_EXECUTABLE_SPEC</code> <p> </p> <code>SPEC_CLASS</code> <p> </p> <code>component_id</code> <p> TYPE: <code>str</code> </p> <code>component_type</code> <p> TYPE: <code>str</code> </p> <code>downstream_nodes</code> <p> </p> <code>driver_class</code> <p> </p> <code>exec_properties</code> <p> TYPE: <code>Dict[str, Any]</code> </p> <code>executor_spec</code> <p> </p> <code>id</code> <p>Node id, unique across all TFX nodes in a pipeline.</p> <p> TYPE: <code>str</code> </p> <code>inputs</code> <p> TYPE: <code>Dict[str, Channel]</code> </p> <code>node_execution_options</code> <p> TYPE: <code>Optional[NodeExecutionOptions]</code> </p> <code>outputs</code> <p>Component's output channel dict.</p> <p> TYPE: <code>Dict[str, OutputChannel]</code> </p> <code>platform_config</code> <p> </p> <code>spec</code> <p> </p> <code>type</code> <p> TYPE: <code>str</code> </p> <code>type_annotation</code> <p> TYPE: <code>Optional[Type[SystemExecution]]</code> </p> <code>upstream_nodes</code> <p> </p> Source code in <code>tfx/extensions/google_cloud_big_query/example_gen/component.py</code> <pre><code>def __init__(\n    self,\n    query: Optional[str] = None,\n    input_config: Optional[\n        Union[example_gen_pb2.Input, data_types.RuntimeParameter]\n    ] = None,\n    output_config: Optional[\n        Union[example_gen_pb2.Output, data_types.RuntimeParameter]\n    ] = None,\n    range_config: Optional[\n        Union[\n            range_config_pb2.RangeConfig,\n            data_types.RuntimeParameter,\n            ph.Placeholder,\n        ]\n    ] = None,\n    custom_executor_spec: Optional[executor_spec.ExecutorSpec] = None,\n    custom_config: Optional[\n        Union[example_gen_pb2.CustomConfig, data_types.RuntimeParameter]\n    ] = None,\n):\n  \"\"\"Constructs a BigQueryExampleGen component.\n\n  Args:\n    query: BigQuery sql string, query result will be treated as a single\n      split, can be overwritten by input_config.\n    input_config: An example_gen_pb2.Input instance with Split.pattern as\n      BigQuery sql string. If set, it overwrites the 'query' arg, and allows\n      different queries per split. If any field is provided as a\n      RuntimeParameter, input_config should be constructed as a dict with the\n      same field names as Input proto message.\n    output_config: An example_gen_pb2.Output instance, providing output\n      configuration. If unset, default splits will be 'train' and 'eval' with\n      size 2:1. If any field is provided as a RuntimeParameter, input_config\n      should be constructed as a dict with the same field names as Output\n      proto message.\n    range_config: An optional range_config_pb2.RangeConfig instance,\n      specifying the range of span values to consider.\n    custom_executor_spec: Optional custom executor spec overriding the default\n      executor spec specified in the component attribute.\n    custom_config: An\n      [example_gen_pb2.CustomConfig](https://github.com/tensorflow/tfx/blob/master/tfx/proto/example_gen.proto)\n      instance, providing custom configuration for ExampleGen.\n\n  Raises:\n    RuntimeError: Only one of query and input_config should be set.\n  \"\"\"\n  if bool(query) == bool(input_config):\n    raise RuntimeError('Exactly one of query and input_config should be set.')\n  input_config = input_config or utils.make_default_input_config(query)\n  super().__init__(\n      input_config=input_config,\n      output_config=output_config,\n      custom_config=custom_config,\n      range_config=range_config,\n      custom_executor_spec=custom_executor_spec,\n  )\n</code></pre> Attributes\u00b6 <code></code> DRIVER_CLASS <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <pre><code>DRIVER_CLASS = QueryBasedDriver\n</code></pre> <code></code> EXECUTOR_SPEC <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <pre><code>EXECUTOR_SPEC = BeamExecutorSpec(Executor)\n</code></pre> <code></code> POST_EXECUTABLE_SPEC <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <pre><code>POST_EXECUTABLE_SPEC = None\n</code></pre> <code></code> PRE_EXECUTABLE_SPEC <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <pre><code>PRE_EXECUTABLE_SPEC = None\n</code></pre> <code></code> SPEC_CLASS <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <pre><code>SPEC_CLASS = QueryBasedExampleGenSpec\n</code></pre> <code></code> component_id <code>property</code> \u00b6 <pre><code>component_id: str\n</code></pre> <code></code> component_type <code>property</code> \u00b6 <pre><code>component_type: str\n</code></pre> <code></code> downstream_nodes <code>property</code> \u00b6 <pre><code>downstream_nodes\n</code></pre> <code></code> driver_class <code>instance-attribute</code> \u00b6 <pre><code>driver_class = driver_class\n</code></pre> <code></code> exec_properties <code>property</code> \u00b6 <pre><code>exec_properties: Dict[str, Any]\n</code></pre> <code></code> executor_spec <code>instance-attribute</code> \u00b6 <pre><code>executor_spec = executor_spec\n</code></pre> <code></code> id <code>property</code> <code>writable</code> \u00b6 <pre><code>id: str\n</code></pre> <p>Node id, unique across all TFX nodes in a pipeline.</p> <p>If <code>id</code> is set by the user, return it directly. Otherwise, return .</p> RETURNS DESCRIPTION <code>str</code> <p>node id.</p> <code></code> inputs <code>property</code> \u00b6 <pre><code>inputs: Dict[str, Channel]\n</code></pre> <code></code> node_execution_options <code>property</code> <code>writable</code> \u00b6 <pre><code>node_execution_options: Optional[NodeExecutionOptions]\n</code></pre> <code></code> outputs <code>property</code> \u00b6 <pre><code>outputs: Dict[str, OutputChannel]\n</code></pre> <p>Component's output channel dict.</p> <code></code> platform_config <code>instance-attribute</code> \u00b6 <pre><code>platform_config = None\n</code></pre> <code></code> spec <code>instance-attribute</code> \u00b6 <pre><code>spec = spec\n</code></pre> <code></code> type <code>property</code> \u00b6 <pre><code>type: str\n</code></pre> <code></code> type_annotation <code>property</code> \u00b6 <pre><code>type_annotation: Optional[Type[SystemExecution]]\n</code></pre> <code></code> upstream_nodes <code>property</code> \u00b6 <pre><code>upstream_nodes\n</code></pre> Functions\u00b6 <code></code> add_downstream_node \u00b6 <pre><code>add_downstream_node(downstream_node)\n</code></pre> <p>Experimental: Add another component that must run after this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> <p>It is symmetric with <code>add_upstream_node</code>.</p> PARAMETER DESCRIPTION <code>downstream_node</code> <p>a component that must run after this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_downstream_node(self, downstream_node):\n  \"\"\"Experimental: Add another component that must run after this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n  It is symmetric with `add_upstream_node`.\n\n  Args:\n    downstream_node: a component that must run after this node.\n  \"\"\"\n  self._downstream_nodes.add(downstream_node)\n  if self not in downstream_node.upstream_nodes:\n    downstream_node.add_upstream_node(self)\n</code></pre> <code></code> add_downstream_nodes \u00b6 <pre><code>add_downstream_nodes(downstream_nodes)\n</code></pre> <p>Experimental: Add another component that must run after this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> <p>It is symmetric with <code>add_upstream_nodes</code>.</p> PARAMETER DESCRIPTION <code>downstream_nodes</code> <p>a list of components that must run after this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_downstream_nodes(self, downstream_nodes):\n  \"\"\"Experimental: Add another component that must run after this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n  It is symmetric with `add_upstream_nodes`.\n\n  Args:\n    downstream_nodes: a list of components that must run after this node.\n  \"\"\"\n  self._downstream_nodes.update(downstream_nodes)\n  for downstream_node in downstream_nodes:\n    if self not in downstream_node.upstream_nodes:\n      downstream_node.add_upstream_node(self)\n</code></pre> <code></code> add_upstream_node \u00b6 <pre><code>add_upstream_node(upstream_node)\n</code></pre> <p>Experimental: Add another component that must run before this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> <p>It is symmetric with <code>add_downstream_node</code>.</p> PARAMETER DESCRIPTION <code>upstream_node</code> <p>a component that must run before this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_upstream_node(self, upstream_node):\n  \"\"\"Experimental: Add another component that must run before this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n  It is symmetric with `add_downstream_node`.\n\n  Args:\n    upstream_node: a component that must run before this node.\n  \"\"\"\n  self._upstream_nodes.add(upstream_node)\n  if self not in upstream_node.downstream_nodes:\n    upstream_node.add_downstream_node(self)\n</code></pre> <code></code> add_upstream_nodes \u00b6 <pre><code>add_upstream_nodes(upstream_nodes)\n</code></pre> <p>Experimental: Add components that must run before this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> PARAMETER DESCRIPTION <code>upstream_nodes</code> <p>a list of components that must run before this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_upstream_nodes(self, upstream_nodes):\n  \"\"\"Experimental: Add components that must run before this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n\n  Args:\n    upstream_nodes: a list of components that must run before this node.\n  \"\"\"\n  self._upstream_nodes.update(upstream_nodes)\n  for upstream_node in upstream_nodes:\n    if self not in upstream_node.downstream_nodes:\n      upstream_node.add_downstream_node(self)\n</code></pre> <code></code> from_json_dict <code>classmethod</code> \u00b6 <pre><code>from_json_dict(dict_data: Dict[str, Any]) -&gt; Any\n</code></pre> <p>Convert from dictionary data to an object.</p> Source code in <code>tfx/utils/json_utils.py</code> <pre><code>@classmethod\n@doc_controls.do_not_doc_in_subclasses\ndef from_json_dict(cls, dict_data: Dict[str, Any]) -&gt; Any:\n  \"\"\"Convert from dictionary data to an object.\"\"\"\n  instance = cls.__new__(cls)\n  instance.__dict__ = dict_data\n  return instance\n</code></pre> <code></code> get_class_type <code>classmethod</code> \u00b6 <pre><code>get_class_type() -&gt; str\n</code></pre> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@classmethod\n@doc_controls.do_not_doc_in_subclasses\ndef get_class_type(cls) -&gt; str:\n  nondeprecated_class = deprecation_utils.get_first_nondeprecated_class(cls)\n  # TODO(b/221166027): Turn strict_check=True once failing tests are fixed.\n  return name_utils.get_full_name(nondeprecated_class, strict_check=False)\n</code></pre> <code></code> remove_downstream_node \u00b6 <pre><code>remove_downstream_node(downstream_node)\n</code></pre> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef remove_downstream_node(self, downstream_node):\n  self._downstream_nodes.remove(downstream_node)\n  if self in downstream_node.upstream_nodes:\n    downstream_node.remove_upstream_node(self)\n</code></pre> <code></code> remove_upstream_node \u00b6 <pre><code>remove_upstream_node(upstream_node)\n</code></pre> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef remove_upstream_node(self, upstream_node):\n  self._upstream_nodes.remove(upstream_node)\n  if self in upstream_node.downstream_nodes:\n    upstream_node.remove_downstream_node(self)\n</code></pre> <code></code> to_json_dict \u00b6 <pre><code>to_json_dict() -&gt; Dict[str, Any]\n</code></pre> <p>Convert from an object to a JSON serializable dictionary.</p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef to_json_dict(self) -&gt; Dict[str, Any]:\n  \"\"\"Convert from an object to a JSON serializable dictionary.\"\"\"\n  return dict((k, v)\n              for k, v in self.__dict__.items()\n              if k not in ['_upstream_nodes', '_downstream_nodes'])\n</code></pre> <code></code> with_beam_pipeline_args \u00b6 <pre><code>with_beam_pipeline_args(beam_pipeline_args: Iterable[Union[str, Placeholder]]) -&gt; BaseBeamComponent\n</code></pre> <p>Add per component Beam pipeline args.</p> PARAMETER DESCRIPTION <code>beam_pipeline_args</code> <p>List of Beam pipeline args to be added to the Beam executor spec.</p> <p> TYPE: <code>Iterable[Union[str, Placeholder]]</code> </p> RETURNS DESCRIPTION <code>BaseBeamComponent</code> <p>the same component itself.</p> Source code in <code>tfx/dsl/components/base/base_beam_component.py</code> <pre><code>def with_beam_pipeline_args(\n    self, beam_pipeline_args: Iterable[Union[str, placeholder.Placeholder]]\n) -&gt; 'BaseBeamComponent':\n  \"\"\"Add per component Beam pipeline args.\n\n  Args:\n    beam_pipeline_args: List of Beam pipeline args to be added to the Beam\n      executor spec.\n\n  Returns:\n    the same component itself.\n  \"\"\"\n  cast(executor_spec.BeamExecutorSpec,\n       self.executor_spec).add_beam_pipeline_args(beam_pipeline_args)\n  return self\n</code></pre> <code></code> with_id \u00b6 <pre><code>with_id(id: str) -&gt; Self\n</code></pre> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef with_id(self, id: str) -&gt; typing_extensions.Self:  # pylint: disable=redefined-builtin\n  self._id = id\n  return self\n</code></pre> <code></code> with_node_execution_options \u00b6 <pre><code>with_node_execution_options(node_execution_options: NodeExecutionOptions) -&gt; Self\n</code></pre> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>def with_node_execution_options(\n    self, node_execution_options: utils.NodeExecutionOptions\n) -&gt; typing_extensions.Self:\n  self.node_execution_options = node_execution_options\n  return self\n</code></pre> <code></code> with_platform_config \u00b6 <pre><code>with_platform_config(config: Message) -&gt; Self\n</code></pre> <p>Attaches a proto-form platform config to a component.</p> <p>The config will be a per-node platform-specific config.</p> PARAMETER DESCRIPTION <code>config</code> <p>platform config to attach to the component.</p> <p> TYPE: <code>Message</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>the same component itself.</p> Source code in <code>tfx/dsl/components/base/base_component.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef with_platform_config(\n    self, config: message.Message\n) -&gt; typing_extensions.Self:\n  \"\"\"Attaches a proto-form platform config to a component.\n\n  The config will be a per-node platform-specific config.\n\n  Args:\n    config: platform config to attach to the component.\n\n  Returns:\n    the same component itself.\n  \"\"\"\n  self.platform_config = config\n  return self\n</code></pre>"},{"location":"api/v1/extensions/#tfx.v1.extensions.google_cloud_big_query.Pusher","title":"Pusher","text":"<pre><code>Pusher(model: Optional[Channel] = None, model_blessing: Optional[Channel] = None, infra_blessing: Optional[Channel] = None, custom_config: Optional[Dict[str, Any]] = None)\n</code></pre> <p>               Bases: <code>Pusher</code></p> <p>Cloud Big Query Pusher component.</p> <p>Component <code>outputs</code> contains:</p> <ul> <li><code>pushed_model</code>: Channel of type <code>standard_artifacts.PushedModel</code> with                    result of push.</li> </ul> <p>Construct a Pusher component.</p> PARAMETER DESCRIPTION <code>model</code> <p>An optional Channel of type <code>standard_artifacts.Model</code>, usually produced by a Trainer component.</p> <p> TYPE: <code>Optional[Channel]</code> DEFAULT: <code>None</code> </p> <code>model_blessing</code> <p>An optional Channel of type <code>standard_artifacts.ModelBlessing</code>, usually produced from an Evaluator component.</p> <p> TYPE: <code>Optional[Channel]</code> DEFAULT: <code>None</code> </p> <code>infra_blessing</code> <p>An optional Channel of type <code>standard_artifacts.InfraBlessing</code>, usually produced from an InfraValidator component.</p> <p> TYPE: <code>Optional[Channel]</code> DEFAULT: <code>None</code> </p> <code>custom_config</code> <p>A dict which contains the deployment job parameters to be passed to Cloud platforms.</p> <p> TYPE: <code>Optional[Dict[str, Any]]</code> DEFAULT: <code>None</code> </p> METHOD DESCRIPTION <code>add_downstream_node</code> <p>Experimental: Add another component that must run after this one.</p> <code>add_downstream_nodes</code> <p>Experimental: Add another component that must run after this one.</p> <code>add_upstream_node</code> <p>Experimental: Add another component that must run before this one.</p> <code>add_upstream_nodes</code> <p>Experimental: Add components that must run before this one.</p> <code>from_json_dict</code> <p>Convert from dictionary data to an object.</p> <code>get_class_type</code> <code>remove_downstream_node</code> <code>remove_upstream_node</code> <code>to_json_dict</code> <p>Convert from an object to a JSON serializable dictionary.</p> <code>with_id</code> <code>with_node_execution_options</code> <code>with_platform_config</code> <p>Attaches a proto-form platform config to a component.</p> ATTRIBUTE DESCRIPTION <code>DRIVER_CLASS</code> <p> </p> <code>EXECUTOR_SPEC</code> <p> </p> <code>POST_EXECUTABLE_SPEC</code> <p> </p> <code>PRE_EXECUTABLE_SPEC</code> <p> </p> <code>SPEC_CLASS</code> <p> </p> <code>component_id</code> <p> TYPE: <code>str</code> </p> <code>component_type</code> <p> TYPE: <code>str</code> </p> <code>downstream_nodes</code> <p> </p> <code>driver_class</code> <p> </p> <code>exec_properties</code> <p> TYPE: <code>Dict[str, Any]</code> </p> <code>executor_spec</code> <p> </p> <code>id</code> <p>Node id, unique across all TFX nodes in a pipeline.</p> <p> TYPE: <code>str</code> </p> <code>inputs</code> <p> TYPE: <code>Dict[str, Channel]</code> </p> <code>node_execution_options</code> <p> TYPE: <code>Optional[NodeExecutionOptions]</code> </p> <code>outputs</code> <p>Component's output channel dict.</p> <p> TYPE: <code>Dict[str, OutputChannel]</code> </p> <code>platform_config</code> <p> </p> <code>spec</code> <p> </p> <code>type</code> <p> TYPE: <code>str</code> </p> <code>type_annotation</code> <p> TYPE: <code>Optional[Type[SystemExecution]]</code> </p> <code>upstream_nodes</code> <p> </p> Source code in <code>tfx/extensions/google_cloud_big_query/pusher/component.py</code> <pre><code>def __init__(self,\n             model: Optional[types.Channel] = None,\n             model_blessing: Optional[types.Channel] = None,\n             infra_blessing: Optional[types.Channel] = None,\n             custom_config: Optional[Dict[str, Any]] = None):\n  \"\"\"Construct a Pusher component.\n\n  Args:\n    model: An optional Channel of type [`standard_artifacts.Model`][tfx.v1.types.standard_artifacts.Model], usually\n      produced by a [Trainer][tfx.v1.components.Trainer] component.\n    model_blessing: An optional Channel of type\n      [`standard_artifacts.ModelBlessing`][tfx.v1.types.standard_artifacts.ModelBlessing], usually produced from an Evaluator\n      component.\n    infra_blessing: An optional Channel of type\n      [`standard_artifacts.InfraBlessing`][tfx.v1.types.standard_artifacts.InfraBlessing], usually produced from an\n      [InfraValidator][tfx.v1.components.InfraValidator] component.\n    custom_config: A dict which contains the deployment job parameters to be\n      passed to Cloud platforms.\n  \"\"\"\n  super().__init__(\n      model=model,\n      model_blessing=model_blessing,\n      infra_blessing=infra_blessing,\n      custom_config=custom_config)\n</code></pre> Attributes\u00b6 <code></code> DRIVER_CLASS <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <pre><code>DRIVER_CLASS = BaseDriver\n</code></pre> <code></code> EXECUTOR_SPEC <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <pre><code>EXECUTOR_SPEC = ExecutorClassSpec(Executor)\n</code></pre> <code></code> POST_EXECUTABLE_SPEC <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <pre><code>POST_EXECUTABLE_SPEC = None\n</code></pre> <code></code> PRE_EXECUTABLE_SPEC <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <pre><code>PRE_EXECUTABLE_SPEC = None\n</code></pre> <code></code> SPEC_CLASS <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <pre><code>SPEC_CLASS = PusherSpec\n</code></pre> <code></code> component_id <code>property</code> \u00b6 <pre><code>component_id: str\n</code></pre> <code></code> component_type <code>property</code> \u00b6 <pre><code>component_type: str\n</code></pre> <code></code> downstream_nodes <code>property</code> \u00b6 <pre><code>downstream_nodes\n</code></pre> <code></code> driver_class <code>instance-attribute</code> \u00b6 <pre><code>driver_class = driver_class\n</code></pre> <code></code> exec_properties <code>property</code> \u00b6 <pre><code>exec_properties: Dict[str, Any]\n</code></pre> <code></code> executor_spec <code>instance-attribute</code> \u00b6 <pre><code>executor_spec = executor_spec\n</code></pre> <code></code> id <code>property</code> <code>writable</code> \u00b6 <pre><code>id: str\n</code></pre> <p>Node id, unique across all TFX nodes in a pipeline.</p> <p>If <code>id</code> is set by the user, return it directly. Otherwise, return .</p> RETURNS DESCRIPTION <code>str</code> <p>node id.</p> <code></code> inputs <code>property</code> \u00b6 <pre><code>inputs: Dict[str, Channel]\n</code></pre> <code></code> node_execution_options <code>property</code> <code>writable</code> \u00b6 <pre><code>node_execution_options: Optional[NodeExecutionOptions]\n</code></pre> <code></code> outputs <code>property</code> \u00b6 <pre><code>outputs: Dict[str, OutputChannel]\n</code></pre> <p>Component's output channel dict.</p> <code></code> platform_config <code>instance-attribute</code> \u00b6 <pre><code>platform_config = None\n</code></pre> <code></code> spec <code>instance-attribute</code> \u00b6 <pre><code>spec = spec\n</code></pre> <code></code> type <code>property</code> \u00b6 <pre><code>type: str\n</code></pre> <code></code> type_annotation <code>property</code> \u00b6 <pre><code>type_annotation: Optional[Type[SystemExecution]]\n</code></pre> <code></code> upstream_nodes <code>property</code> \u00b6 <pre><code>upstream_nodes\n</code></pre> Functions\u00b6 <code></code> add_downstream_node \u00b6 <pre><code>add_downstream_node(downstream_node)\n</code></pre> <p>Experimental: Add another component that must run after this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> <p>It is symmetric with <code>add_upstream_node</code>.</p> PARAMETER DESCRIPTION <code>downstream_node</code> <p>a component that must run after this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_downstream_node(self, downstream_node):\n  \"\"\"Experimental: Add another component that must run after this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n  It is symmetric with `add_upstream_node`.\n\n  Args:\n    downstream_node: a component that must run after this node.\n  \"\"\"\n  self._downstream_nodes.add(downstream_node)\n  if self not in downstream_node.upstream_nodes:\n    downstream_node.add_upstream_node(self)\n</code></pre> <code></code> add_downstream_nodes \u00b6 <pre><code>add_downstream_nodes(downstream_nodes)\n</code></pre> <p>Experimental: Add another component that must run after this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> <p>It is symmetric with <code>add_upstream_nodes</code>.</p> PARAMETER DESCRIPTION <code>downstream_nodes</code> <p>a list of components that must run after this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_downstream_nodes(self, downstream_nodes):\n  \"\"\"Experimental: Add another component that must run after this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n  It is symmetric with `add_upstream_nodes`.\n\n  Args:\n    downstream_nodes: a list of components that must run after this node.\n  \"\"\"\n  self._downstream_nodes.update(downstream_nodes)\n  for downstream_node in downstream_nodes:\n    if self not in downstream_node.upstream_nodes:\n      downstream_node.add_upstream_node(self)\n</code></pre> <code></code> add_upstream_node \u00b6 <pre><code>add_upstream_node(upstream_node)\n</code></pre> <p>Experimental: Add another component that must run before this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> <p>It is symmetric with <code>add_downstream_node</code>.</p> PARAMETER DESCRIPTION <code>upstream_node</code> <p>a component that must run before this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_upstream_node(self, upstream_node):\n  \"\"\"Experimental: Add another component that must run before this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n  It is symmetric with `add_downstream_node`.\n\n  Args:\n    upstream_node: a component that must run before this node.\n  \"\"\"\n  self._upstream_nodes.add(upstream_node)\n  if self not in upstream_node.downstream_nodes:\n    upstream_node.add_downstream_node(self)\n</code></pre> <code></code> add_upstream_nodes \u00b6 <pre><code>add_upstream_nodes(upstream_nodes)\n</code></pre> <p>Experimental: Add components that must run before this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> PARAMETER DESCRIPTION <code>upstream_nodes</code> <p>a list of components that must run before this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_upstream_nodes(self, upstream_nodes):\n  \"\"\"Experimental: Add components that must run before this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n\n  Args:\n    upstream_nodes: a list of components that must run before this node.\n  \"\"\"\n  self._upstream_nodes.update(upstream_nodes)\n  for upstream_node in upstream_nodes:\n    if self not in upstream_node.downstream_nodes:\n      upstream_node.add_downstream_node(self)\n</code></pre> <code></code> from_json_dict <code>classmethod</code> \u00b6 <pre><code>from_json_dict(dict_data: Dict[str, Any]) -&gt; Any\n</code></pre> <p>Convert from dictionary data to an object.</p> Source code in <code>tfx/utils/json_utils.py</code> <pre><code>@classmethod\n@doc_controls.do_not_doc_in_subclasses\ndef from_json_dict(cls, dict_data: Dict[str, Any]) -&gt; Any:\n  \"\"\"Convert from dictionary data to an object.\"\"\"\n  instance = cls.__new__(cls)\n  instance.__dict__ = dict_data\n  return instance\n</code></pre> <code></code> get_class_type <code>classmethod</code> \u00b6 <pre><code>get_class_type() -&gt; str\n</code></pre> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@classmethod\n@doc_controls.do_not_doc_in_subclasses\ndef get_class_type(cls) -&gt; str:\n  nondeprecated_class = deprecation_utils.get_first_nondeprecated_class(cls)\n  # TODO(b/221166027): Turn strict_check=True once failing tests are fixed.\n  return name_utils.get_full_name(nondeprecated_class, strict_check=False)\n</code></pre> <code></code> remove_downstream_node \u00b6 <pre><code>remove_downstream_node(downstream_node)\n</code></pre> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef remove_downstream_node(self, downstream_node):\n  self._downstream_nodes.remove(downstream_node)\n  if self in downstream_node.upstream_nodes:\n    downstream_node.remove_upstream_node(self)\n</code></pre> <code></code> remove_upstream_node \u00b6 <pre><code>remove_upstream_node(upstream_node)\n</code></pre> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef remove_upstream_node(self, upstream_node):\n  self._upstream_nodes.remove(upstream_node)\n  if self in upstream_node.downstream_nodes:\n    upstream_node.remove_downstream_node(self)\n</code></pre> <code></code> to_json_dict \u00b6 <pre><code>to_json_dict() -&gt; Dict[str, Any]\n</code></pre> <p>Convert from an object to a JSON serializable dictionary.</p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef to_json_dict(self) -&gt; Dict[str, Any]:\n  \"\"\"Convert from an object to a JSON serializable dictionary.\"\"\"\n  return dict((k, v)\n              for k, v in self.__dict__.items()\n              if k not in ['_upstream_nodes', '_downstream_nodes'])\n</code></pre> <code></code> with_id \u00b6 <pre><code>with_id(id: str) -&gt; Self\n</code></pre> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef with_id(self, id: str) -&gt; typing_extensions.Self:  # pylint: disable=redefined-builtin\n  self._id = id\n  return self\n</code></pre> <code></code> with_node_execution_options \u00b6 <pre><code>with_node_execution_options(node_execution_options: NodeExecutionOptions) -&gt; Self\n</code></pre> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>def with_node_execution_options(\n    self, node_execution_options: utils.NodeExecutionOptions\n) -&gt; typing_extensions.Self:\n  self.node_execution_options = node_execution_options\n  return self\n</code></pre> <code></code> with_platform_config \u00b6 <pre><code>with_platform_config(config: Message) -&gt; Self\n</code></pre> <p>Attaches a proto-form platform config to a component.</p> <p>The config will be a per-node platform-specific config.</p> PARAMETER DESCRIPTION <code>config</code> <p>platform config to attach to the component.</p> <p> TYPE: <code>Message</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>the same component itself.</p> Source code in <code>tfx/dsl/components/base/base_component.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef with_platform_config(\n    self, config: message.Message\n) -&gt; typing_extensions.Self:\n  \"\"\"Attaches a proto-form platform config to a component.\n\n  The config will be a per-node platform-specific config.\n\n  Args:\n    config: platform config to attach to the component.\n\n  Returns:\n    the same component itself.\n  \"\"\"\n  self.platform_config = config\n  return self\n</code></pre>"},{"location":"api/v1/orchestration/","title":"Orchestration","text":""},{"location":"api/v1/orchestration/#tfx.v1.orchestration","title":"tfx.v1.orchestration","text":"<p>TFX orchestration module.</p> MODULE DESCRIPTION <code>experimental</code> <p>TFX orchestration.experimental module.</p> <code>metadata</code> <p>Public API for metadata.</p> CLASS DESCRIPTION <code>LocalDagRunner</code> <p>Local TFX DAG runner.</p>"},{"location":"api/v1/orchestration/#tfx.v1.orchestration-classes","title":"Classes","text":""},{"location":"api/v1/orchestration/#tfx.v1.orchestration.LocalDagRunner","title":"LocalDagRunner","text":"<pre><code>LocalDagRunner()\n</code></pre> <p>               Bases: <code>IrBasedRunner</code></p> <p>Local TFX DAG runner.</p> <p>Initializes LocalDagRunner as a TFX orchestrator.</p> METHOD DESCRIPTION <code>run</code> <p>See TfxRunner.</p> <code>run_with_ir</code> <p>Runs given pipeline locally.</p> Source code in <code>tfx/orchestration/local/local_dag_runner.py</code> <pre><code>def __init__(self):\n  \"\"\"Initializes LocalDagRunner as a TFX orchestrator.\"\"\"\n  pass\n</code></pre>"},{"location":"api/v1/orchestration/#tfx.v1.orchestration.LocalDagRunner-functions","title":"Functions","text":""},{"location":"api/v1/orchestration/#tfx.v1.orchestration.LocalDagRunner.run","title":"run","text":"<pre><code>run(pipeline: Pipeline, run_options: Optional[RunOptions] = None, **kwargs: Any) -&gt; Optional[Any]\n</code></pre> <p>See TfxRunner.</p> Source code in <code>tfx/orchestration/portable/tfx_runner.py</code> <pre><code>def run(\n    self,\n    pipeline: pipeline_py.Pipeline,\n    run_options: Optional[pipeline_py.RunOptions] = None,\n    **kwargs: Any,\n) -&gt; Optional[Any]:\n  \"\"\"See TfxRunner.\"\"\"\n  pipeline_pb = _make_pipeline_proto(pipeline)\n  if run_options:\n    run_options_pb = _run_opts_to_proto(run_options)\n  else:\n    run_options_pb = None\n  return self.run_with_ir(pipeline_pb, run_options=run_options_pb, **kwargs)\n</code></pre>"},{"location":"api/v1/orchestration/#tfx.v1.orchestration.LocalDagRunner.run_with_ir","title":"run_with_ir","text":"<pre><code>run_with_ir(pipeline: Pipeline, run_options: Optional[RunOptions] = None) -&gt; None\n</code></pre> <p>Runs given pipeline locally.</p> PARAMETER DESCRIPTION <code>pipeline</code> <p>Pipeline IR containing pipeline args and components.</p> <p> TYPE: <code>Pipeline</code> </p> <code>run_options</code> <p>Optional args for the run.</p> <p> TYPE: <code>Optional[RunOptions]</code> DEFAULT: <code>None</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If run_options is provided, and partial_run_options.from_nodes and partial_run_options.to_nodes are both empty.</p> Source code in <code>tfx/orchestration/local/local_dag_runner.py</code> <pre><code>@doc_controls.do_not_generate_docs\ndef run_with_ir(\n    self,\n    pipeline: pipeline_pb2.Pipeline,\n    run_options: Optional[pipeline_pb2.RunOptions] = None,\n) -&gt; None:\n  \"\"\"Runs given pipeline locally.\n\n  Args:\n    pipeline: Pipeline IR containing pipeline args and components.\n    run_options: Optional args for the run.\n\n  Raises:\n    ValueError: If run_options is provided, and partial_run_options.from_nodes\n      and partial_run_options.to_nodes are both empty.\n  \"\"\"\n  # Substitute the runtime parameter to be a concrete run_id\n  runtime_parameter_utils.substitute_runtime_parameter(\n      pipeline, {\n          constants.PIPELINE_RUN_ID_PARAMETER_NAME:\n              datetime.datetime.now().isoformat(),\n      })\n\n  deployment_config = runner_utils.extract_local_deployment_config(pipeline)\n  connection_config = getattr(\n      deployment_config.metadata_connection_config,\n      deployment_config.metadata_connection_config.WhichOneof(\n          'connection_config'))\n\n  logging.info('Using deployment config:\\n %s', deployment_config)\n  logging.info('Using connection config:\\n %s', connection_config)\n\n  if run_options:\n    logging.info('Using run_options:\\n %s', run_options)\n    pr_opts = run_options.partial_run\n    partial_run_utils.mark_pipeline(\n        pipeline,\n        from_nodes=pr_opts.from_nodes or None,\n        to_nodes=pr_opts.to_nodes or None,\n        snapshot_settings=pr_opts.snapshot_settings)\n\n  with telemetry_utils.scoped_labels(\n      {telemetry_utils.LABEL_TFX_RUNNER: 'local'}):\n    # Run each component. Note that the pipeline.components list is in\n    # topological order.\n    #\n    # TODO(b/171319478): After IR-based execution is used, used multi-threaded\n    # execution so that independent components can be run in parallel.\n    for node in pipeline.nodes:\n      pipeline_node = node.pipeline_node\n      node_id = pipeline_node.node_info.id\n      if pipeline_node.execution_options.HasField('skip'):\n        logging.info('Skipping component %s.', node_id)\n        continue\n      executor_spec = runner_utils.extract_executor_spec(\n          deployment_config, node_id)\n      custom_driver_spec = runner_utils.extract_custom_driver_spec(\n          deployment_config, node_id)\n\n      component_launcher = launcher.Launcher(\n          pipeline_node=pipeline_node,\n          mlmd_connection=metadata.Metadata(connection_config),\n          pipeline_info=pipeline.pipeline_info,\n          pipeline_runtime_spec=pipeline.runtime_spec,\n          executor_spec=executor_spec,\n          custom_driver_spec=custom_driver_spec)\n      logging.info('Component %s is running.', node_id)\n      if pipeline_node.execution_options.run.perform_snapshot:\n        with metadata.Metadata(connection_config) as mlmd_handle:\n          partial_run_utils.snapshot(mlmd_handle, pipeline)\n      component_launcher.launch()\n      logging.info('Component %s is finished.', node_id)\n</code></pre>"},{"location":"api/v1/orchestration/#tfx.v1.orchestration-modules","title":"Modules","text":""},{"location":"api/v1/orchestration/#tfx.v1.orchestration.experimental","title":"experimental","text":"<p>TFX orchestration.experimental module.</p> CLASS DESCRIPTION <code>FinalStatusStr</code> <p>FinalStatusStr: is the type for parameter receiving PipelineTaskFinalStatus.</p> <code>KubeflowV2DagRunner</code> <p>Kubeflow V2 pipeline runner (currently for managed pipelines).</p> <code>KubeflowV2DagRunnerConfig</code> <p>Runtime configuration specific to execution on Kubeflow V2 pipelines.</p> FUNCTION DESCRIPTION <code>exit_handler</code> <p>Creates an exit handler from a typehint-annotated Python function.</p>"},{"location":"api/v1/orchestration/#tfx.v1.orchestration.experimental-classes","title":"Classes","text":""},{"location":"api/v1/orchestration/#tfx.v1.orchestration.experimental.FinalStatusStr","title":"FinalStatusStr","text":"<p>               Bases: <code>str</code></p> <p>FinalStatusStr: is the type for parameter receiving PipelineTaskFinalStatus.</p> <p>Vertex AI backend passes in jsonlized string of <code>kfp.pipeline_spec.pipeline_spec_pb2.PipelineTaskFinalStatus</code>.</p> <p>Example</p> <p>This is example usage of FinalStatusStr definition: </p><pre><code>exit_handler = exit_handler_component(\n    final_status=tfx.dsl.experimental.FinalStatusStr()\n)\n</code></pre>"},{"location":"api/v1/orchestration/#tfx.v1.orchestration.experimental.KubeflowV2DagRunner","title":"KubeflowV2DagRunner","text":"<pre><code>KubeflowV2DagRunner(config: KubeflowV2DagRunnerConfig, output_dir: Optional[str] = None, output_filename: Optional[str] = None)\n</code></pre> <p>               Bases: <code>TfxRunner</code></p> <p>Kubeflow V2 pipeline runner (currently for managed pipelines).</p> <p>Builds a pipeline job spec in json format based on TFX pipeline DSL object.</p> <p>Constructs an KubeflowV2DagRunner for compiling pipelines.</p> PARAMETER DESCRIPTION <code>config</code> <p>An KubeflowV2DagRunnerConfig object to specify runtime configuration when running the pipeline in Kubeflow.</p> <p> TYPE: <code>KubeflowV2DagRunnerConfig</code> </p> <code>output_dir</code> <p>An optional output directory into which to output the pipeline definition files. Defaults to the current working directory.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>output_filename</code> <p>An optional output file name for the pipeline definition file. The file output format will be a JSON-serialized or YAML-serialized PipelineJob pb message. Defaults to 'pipeline.json'.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> METHOD DESCRIPTION <code>run</code> <p>Compiles a pipeline DSL object into pipeline file.</p> <code>set_exit_handler</code> <p>Set exit handler components for the Kubeflow V2(Vertex AI) dag runner.</p> ATTRIBUTE DESCRIPTION <code>config</code> <p> TYPE: <code>PipelineConfig</code> </p> Source code in <code>tfx/orchestration/kubeflow/v2/kubeflow_v2_dag_runner.py</code> <pre><code>def __init__(\n    self,\n    config: KubeflowV2DagRunnerConfig,\n    output_dir: Optional[str] = None,\n    output_filename: Optional[str] = None,\n):\n  \"\"\"Constructs an KubeflowV2DagRunner for compiling pipelines.\n\n  Args:\n    config: An KubeflowV2DagRunnerConfig object to specify runtime\n      configuration when running the pipeline in Kubeflow.\n    output_dir: An optional output directory into which to output the pipeline\n      definition files. Defaults to the current working directory.\n    output_filename: An optional output file name for the pipeline definition\n      file. The file output format will be a JSON-serialized or\n      YAML-serialized PipelineJob pb message. Defaults to 'pipeline.json'.\n  \"\"\"\n  if not isinstance(config, KubeflowV2DagRunnerConfig):\n    raise TypeError('config must be type of KubeflowV2DagRunnerConfig.')\n  super().__init__()\n  self._config = config\n  self._output_dir = output_dir or os.getcwd()\n  self._output_filename = output_filename or 'pipeline.json'\n  self._exit_handler = None\n</code></pre> Attributes\u00b6 <code></code> config <code>property</code> \u00b6 <pre><code>config: PipelineConfig\n</code></pre> Functions\u00b6 <code></code> run \u00b6 <pre><code>run(pipeline: Pipeline, parameter_values: Optional[Dict[str, Any]] = None, write_out: Optional[bool] = True) -&gt; Dict[str, Any]\n</code></pre> <p>Compiles a pipeline DSL object into pipeline file.</p> PARAMETER DESCRIPTION <code>pipeline</code> <p>TFX pipeline object.</p> <p> TYPE: <code>Pipeline</code> </p> <code>parameter_values</code> <p>mapping from runtime parameter names to its values.</p> <p> TYPE: <code>Optional[Dict[str, Any]]</code> DEFAULT: <code>None</code> </p> <code>write_out</code> <p>set to True to actually write out the file to the place designated by output_dir and output_filename. Otherwise return the JSON-serialized pipeline job spec.</p> <p> TYPE: <code>Optional[bool]</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>Dict[str, Any]</code> <p>Returns the JSON/YAML pipeline job spec.</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>if trying to write out to a place occupied by an existing</p> Source code in <code>tfx/orchestration/kubeflow/v2/kubeflow_v2_dag_runner.py</code> <pre><code>def run(\n    self,\n    pipeline: tfx_pipeline.Pipeline,\n    parameter_values: Optional[Dict[str, Any]] = None,\n    write_out: Optional[bool] = True,\n) -&gt; Dict[str, Any]:\n  \"\"\"Compiles a pipeline DSL object into pipeline file.\n\n  Args:\n    pipeline: TFX pipeline object.\n    parameter_values: mapping from runtime parameter names to its values.\n    write_out: set to True to actually write out the file to the place\n      designated by output_dir and output_filename. Otherwise return the\n      JSON-serialized pipeline job spec.\n\n  Returns:\n    Returns the JSON/YAML pipeline job spec.\n\n  Raises:\n    RuntimeError: if trying to write out to a place occupied by an existing\n    file.\n  \"\"\"\n  for component in pipeline.components:\n    # TODO(b/187122662): Pass through pip dependencies as a first-class\n    # component flag.\n    if isinstance(component, base_component.BaseComponent):\n      component._resolve_pip_dependencies(  # pylint: disable=protected-access\n          pipeline.pipeline_info.pipeline_root\n      )\n\n  # TODO(b/166343606): Support user-provided labels.\n  # TODO(b/169095387): Deprecate .run() method in favor of the unified API\n  # client.\n  display_name = (\n      self._config.display_name or pipeline.pipeline_info.pipeline_name\n  )\n  pipeline_spec = pipeline_builder.PipelineBuilder(\n      tfx_pipeline=pipeline,\n      default_image=self._config.default_image,\n      default_commands=self._config.default_commands,\n      exit_handler=self._exit_handler,\n      use_pipeline_spec_2_1=self._config.use_pipeline_spec_2_1,\n  ).build()\n  pipeline_spec.sdk_version = 'tfx-{}'.format(version.__version__)\n  if self._config.use_pipeline_spec_2_1:\n    pipeline_spec.schema_version = _SCHEMA_VERSION_2_1\n  else:\n    pipeline_spec.schema_version = _SCHEMA_VERSION_2_0\n  runtime_config = pipeline_builder.RuntimeConfigBuilder(\n      pipeline_info=pipeline.pipeline_info,\n      parameter_values=parameter_values,\n      use_pipeline_spec_2_1=self._config.use_pipeline_spec_2_1,\n  ).build()\n  with telemetry_utils.scoped_labels(\n      {telemetry_utils.LABEL_TFX_RUNNER: 'kubeflow_v2'}\n  ):\n    result = pipeline_spec_pb2.PipelineJob(\n        display_name=display_name or pipeline.pipeline_info.pipeline_name,\n        labels=telemetry_utils.make_labels_dict(),\n        runtime_config=runtime_config,\n    )\n  result.pipeline_spec.update(json_format.MessageToDict(pipeline_spec))\n  pipeline_json_dict = json_format.MessageToDict(result)\n  if write_out:\n    if fileio.exists(self._output_dir) and not fileio.isdir(self._output_dir):\n      raise RuntimeError(\n          'Output path: %s is pointed to a file.' % self._output_dir\n      )\n    if not fileio.exists(self._output_dir):\n      fileio.makedirs(self._output_dir)\n\n    _write_pipeline_spec_to_file(\n        pipeline_json_dict,\n        'This is converted from TFX pipeline from tfx-{}.'.format(\n            version.__version__\n        ),\n        os.path.join(self._output_dir, self._output_filename),\n    )\n\n  return pipeline_json_dict\n</code></pre> <code></code> set_exit_handler \u00b6 <pre><code>set_exit_handler(exit_handler: BaseNode)\n</code></pre> <p>Set exit handler components for the Kubeflow V2(Vertex AI) dag runner.</p> <p>This feature is currently experimental without backward compatibility gaurantee.</p> PARAMETER DESCRIPTION <code>exit_handler</code> <p>exit handler component.</p> <p> TYPE: <code>BaseNode</code> </p> Source code in <code>tfx/orchestration/kubeflow/v2/kubeflow_v2_dag_runner.py</code> <pre><code>def set_exit_handler(self, exit_handler: base_node.BaseNode):\n  \"\"\"Set exit handler components for the Kubeflow V2(Vertex AI) dag runner.\n\n  This feature is currently experimental without backward compatibility\n  gaurantee.\n\n  Args:\n    exit_handler: exit handler component.\n  \"\"\"\n  if not exit_handler:\n    logging.error('Setting empty exit handler is not allowed.')\n    return\n  self._exit_handler = exit_handler\n</code></pre>"},{"location":"api/v1/orchestration/#tfx.v1.orchestration.experimental.KubeflowV2DagRunnerConfig","title":"KubeflowV2DagRunnerConfig","text":"<pre><code>KubeflowV2DagRunnerConfig(display_name: Optional[str] = None, default_image: Optional[Union[str, MutableMapping[str, str]]] = None, default_commands: Optional[List[str]] = None, use_pipeline_spec_2_1: bool = False, **kwargs)\n</code></pre> <p>               Bases: <code>PipelineConfig</code></p> <p>Runtime configuration specific to execution on Kubeflow V2 pipelines.</p> <p>Constructs a Kubeflow V2 runner config.</p> PARAMETER DESCRIPTION <code>display_name</code> <p>Optional human-readable pipeline name. Defaults to the pipeline name passed into <code>KubeflowV2DagRunner.run()</code>.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>default_image</code> <p>The default TFX image to be used if not overriden by per component specification. It can be a map whose key is a component id and value is an image path to set the image by a component level.</p> <p> TYPE: <code>Optional[Union[str, MutableMapping[str, str]]]</code> DEFAULT: <code>None</code> </p> <code>default_commands</code> <p>Optionally specifies the commands of the provided container image. When not provided, the default <code>ENTRYPOINT</code> specified in the docker image is used. Note: the commands here refers to the K8S container command, which maps to Docker entrypoint field. If one supplies command but no args are provided for the container, the container will be invoked with the provided command, ignoring the <code>ENTRYPOINT</code> and <code>CMD</code> defined in the Dockerfile. One can find more details regarding the difference between K8S and Docker conventions at https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#notes</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>use_pipeline_spec_2_1</code> <p>Use the KFP pipeline spec schema 2.1 to support Vertex ML pipeline teamplate gallary.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>**kwargs</code> <p>Additional args passed to base PipelineConfig.</p> <p> DEFAULT: <code>{}</code> </p> ATTRIBUTE DESCRIPTION <code>component_config_overrides</code> <p> </p> <code>default_commands</code> <p> </p> <code>default_component_configs</code> <p> </p> <code>default_image</code> <p> </p> <code>display_name</code> <p> </p> <code>supported_launcher_classes</code> <p> </p> <code>use_pipeline_spec_2_1</code> <p> </p> Source code in <code>tfx/orchestration/kubeflow/v2/kubeflow_v2_dag_runner.py</code> <pre><code>def __init__(\n    self,\n    display_name: Optional[str] = None,\n    default_image: Optional[Union[str, MutableMapping[str, str]]] = None,\n    default_commands: Optional[List[str]] = None,\n    use_pipeline_spec_2_1: bool = False,\n    **kwargs,\n):\n  \"\"\"Constructs a Kubeflow V2 runner config.\n\n  Args:\n    display_name: Optional human-readable pipeline name. Defaults to the\n      pipeline name passed into `KubeflowV2DagRunner.run()`.\n    default_image: The default TFX image to be used if not overriden by per\n      component specification. It can be a map whose key is a component id and\n      value is an image path to set the image by a component level.\n    default_commands: Optionally specifies the commands of the provided\n      container image. When not provided, the default `ENTRYPOINT` specified\n      in the docker image is used. Note: the commands here refers to the K8S\n      container command, which maps to Docker entrypoint field. If one\n      supplies command but no args are provided for the container, the\n      container will be invoked with the provided command, ignoring the\n      `ENTRYPOINT` and `CMD` defined in the Dockerfile. One can find more\n      details regarding the difference between K8S and Docker conventions at\n      https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#notes\n    use_pipeline_spec_2_1: Use the KFP pipeline spec schema 2.1 to support\n      Vertex ML pipeline teamplate gallary.\n    **kwargs: Additional args passed to base PipelineConfig.\n  \"\"\"\n  super().__init__(**kwargs)\n  self.display_name = display_name\n  self.default_image = default_image or _KUBEFLOW_TFX_IMAGE\n  if (\n      isinstance(self.default_image, MutableMapping)\n      and self.default_image.get(_DEFAULT_IMAGE_PATH_KEY) is None\n  ):\n    self.default_image[_DEFAULT_IMAGE_PATH_KEY] = _KUBEFLOW_TFX_IMAGE\n  if default_commands is None:\n    self.default_commands = KUBEFLOW_TFX_CMD\n  else:\n    self.default_commands = default_commands\n  self.use_pipeline_spec_2_1 = use_pipeline_spec_2_1\n</code></pre> Attributes\u00b6 <code></code> component_config_overrides <code>instance-attribute</code> \u00b6 <pre><code>component_config_overrides = component_config_overrides or {}\n</code></pre> <code></code> default_commands <code>instance-attribute</code> \u00b6 <pre><code>default_commands = KUBEFLOW_TFX_CMD\n</code></pre> <code></code> default_component_configs <code>instance-attribute</code> \u00b6 <pre><code>default_component_configs = default_component_configs or []\n</code></pre> <code></code> default_image <code>instance-attribute</code> \u00b6 <pre><code>default_image = default_image or _KUBEFLOW_TFX_IMAGE\n</code></pre> <code></code> display_name <code>instance-attribute</code> \u00b6 <pre><code>display_name = display_name\n</code></pre> <code></code> supported_launcher_classes <code>instance-attribute</code> \u00b6 <pre><code>supported_launcher_classes = supported_launcher_classes or [InProcessComponentLauncher]\n</code></pre> <code></code> use_pipeline_spec_2_1 <code>instance-attribute</code> \u00b6 <pre><code>use_pipeline_spec_2_1 = use_pipeline_spec_2_1\n</code></pre> Functions\u00b6"},{"location":"api/v1/orchestration/#tfx.v1.orchestration.experimental-functions","title":"Functions","text":""},{"location":"api/v1/orchestration/#tfx.v1.orchestration.experimental.exit_handler","title":"exit_handler","text":"<pre><code>exit_handler(func: FunctionType) -&gt; Callable[..., Any]\n</code></pre> <p>Creates an exit handler from a typehint-annotated Python function.</p> <p>This decorator creates an exit handler wrapping the component typehint annotation - typehint annotations specified for the arguments and return value for a Python function. Exit handler is to annotate the component for post actions of a pipeline, only supported in Vertex AI. Specifically, function arguments can be annotated with the following types and associated semantics supported in component. In order to get in the final status of dependent pipeline, parameter should be defined as Parameter[str], passing in FinalStatusStr type when initializing the component.</p> <p>Example</p> <p>This is example usage of component definition using this decorator: </p><pre><code>from tfx import v1 as tfx\n\n\n@tfx.orchestration.experimental.exit_handler\ndef MyExitHandlerComponent(final_status: tfx.dsl.components.Parameter[str]):\n    # parse the final status\n    pipeline_task_status = pipeline_pb2.PipelineTaskFinalStatus()\n    proto_utils.json_to_proto(final_status, pipeline_task_status)\n    print(pipeline_task_status)\n</code></pre> <p>Example</p> <p>Example usage in a Vertex AI graph definition: </p><pre><code>exit_handler = exit_handler_component(\n    final_status=tfx.dsl.experimental.FinalStatusStr()\n)\n\ndsl_pipeline = tfx.dsl.Pipeline(...)\n\nrunner = tfx.orchestration.experimental.KubeflowV2DagRunner(...)\nrunner.set_exit_handler([exit_handler])\nrunner.run(pipeline=dsl_pipeline)\n</code></pre> <p>Experimental: no backwards compatibility guarantees.</p> PARAMETER DESCRIPTION <code>func</code> <p>Typehint-annotated component executor function.</p> <p> TYPE: <code>FunctionType</code> </p> RETURNS DESCRIPTION <code>Callable[..., Any]</code> <p><code>base_component.BaseComponent</code> subclass for the given component executor</p> <code>Callable[..., Any]</code> <p>function.</p> Source code in <code>tfx/orchestration/kubeflow/decorators.py</code> <pre><code>def exit_handler(func: types.FunctionType) -&gt; Callable[..., Any]:\n  \"\"\"Creates an exit handler from a typehint-annotated Python function.\n\n  This decorator creates an exit handler wrapping the component typehint\n  annotation - typehint annotations specified for the arguments and return value\n  for a Python function.\n  Exit handler is to annotate the component for post actions of a pipeline,\n  only supported in Vertex AI. Specifically, function\n  arguments can be annotated with the following types and associated semantics\n  supported in component. In order to get in the final status of dependent\n  pipeline, parameter should be defined as Parameter[str], passing in\n  FinalStatusStr type when initializing the component.\n\n  !!! example\n      This is example usage of component definition using this decorator:\n      ``` python\n      from tfx import v1 as tfx\n\n\n      @tfx.orchestration.experimental.exit_handler\n      def MyExitHandlerComponent(final_status: tfx.dsl.components.Parameter[str]):\n          # parse the final status\n          pipeline_task_status = pipeline_pb2.PipelineTaskFinalStatus()\n          proto_utils.json_to_proto(final_status, pipeline_task_status)\n          print(pipeline_task_status)\n      ```\n\n  !!! example\n      Example usage in a Vertex AI graph definition:\n      ```python\n      exit_handler = exit_handler_component(\n          final_status=tfx.dsl.experimental.FinalStatusStr()\n      )\n\n      dsl_pipeline = tfx.dsl.Pipeline(...)\n\n      runner = tfx.orchestration.experimental.KubeflowV2DagRunner(...)\n      runner.set_exit_handler([exit_handler])\n      runner.run(pipeline=dsl_pipeline)\n      ```\n  Experimental: no backwards compatibility guarantees.\n\n  Args:\n    func: Typehint-annotated component executor function.\n\n  Returns:\n    [`base_component.BaseComponent`][tfx.v1.types.BaseComponent] subclass for the given component executor\n    function.\n  \"\"\"\n  return component(func)\n</code></pre>"},{"location":"api/v1/orchestration/#tfx.v1.orchestration.metadata","title":"metadata","text":"<p>Public API for metadata.</p> FUNCTION DESCRIPTION <code>mysql_metadata_connection_config</code> <p>Convenience function to create mysql-based metadata connection config.</p> <code>sqlite_metadata_connection_config</code> <p>Convenience function to create file based metadata connection config.</p> ATTRIBUTE DESCRIPTION <code>ConnectionConfigType</code> <p> </p>"},{"location":"api/v1/orchestration/#tfx.v1.orchestration.metadata-attributes","title":"Attributes","text":""},{"location":"api/v1/orchestration/#tfx.v1.orchestration.metadata.ConnectionConfigType","title":"ConnectionConfigType  <code>module-attribute</code>","text":"<pre><code>ConnectionConfigType = Union[ConnectionConfig, MetadataStoreClientConfig]\n</code></pre>"},{"location":"api/v1/orchestration/#tfx.v1.orchestration.metadata-functions","title":"Functions","text":""},{"location":"api/v1/orchestration/#tfx.v1.orchestration.metadata.mysql_metadata_connection_config","title":"mysql_metadata_connection_config","text":"<pre><code>mysql_metadata_connection_config(host: str, port: int, database: str, username: str, password: str) -&gt; ConnectionConfig\n</code></pre> <p>Convenience function to create mysql-based metadata connection config.</p> PARAMETER DESCRIPTION <code>host</code> <p>The name or network address of the instance of MySQL to connect to.</p> <p> TYPE: <code>str</code> </p> <code>port</code> <p>The port MySQL is using to listen for connections.</p> <p> TYPE: <code>int</code> </p> <code>database</code> <p>The name of the database to use.</p> <p> TYPE: <code>str</code> </p> <code>username</code> <p>The MySQL login account being used.</p> <p> TYPE: <code>str</code> </p> <code>password</code> <p>The password for the MySQL account being used.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>ConnectionConfig</code> <p>A metadata_store_pb2.ConnectionConfig based on given metadata db uri.</p> Source code in <code>tfx/orchestration/metadata.py</code> <pre><code>def mysql_metadata_connection_config(\n    host: str, port: int, database: str, username: str,\n    password: str) -&gt; metadata_store_pb2.ConnectionConfig:\n  \"\"\"Convenience function to create mysql-based metadata connection config.\n\n  Args:\n    host: The name or network address of the instance of MySQL to connect to.\n    port: The port MySQL is using to listen for connections.\n    database: The name of the database to use.\n    username: The MySQL login account being used.\n    password: The password for the MySQL account being used.\n\n  Returns:\n    A metadata_store_pb2.ConnectionConfig based on given metadata db uri.\n  \"\"\"\n  return metadata_store_pb2.ConnectionConfig(\n      mysql=metadata_store_pb2.MySQLDatabaseConfig(\n          host=host,\n          port=port,\n          database=database,\n          user=username,\n          password=password))\n</code></pre>"},{"location":"api/v1/orchestration/#tfx.v1.orchestration.metadata.sqlite_metadata_connection_config","title":"sqlite_metadata_connection_config","text":"<pre><code>sqlite_metadata_connection_config(metadata_db_uri: str) -&gt; ConnectionConfig\n</code></pre> <p>Convenience function to create file based metadata connection config.</p> PARAMETER DESCRIPTION <code>metadata_db_uri</code> <p>uri to metadata db.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>ConnectionConfig</code> <p>A metadata_store_pb2.ConnectionConfig based on given metadata db uri.</p> Source code in <code>tfx/orchestration/metadata.py</code> <pre><code>def sqlite_metadata_connection_config(\n    metadata_db_uri: str) -&gt; metadata_store_pb2.ConnectionConfig:\n  \"\"\"Convenience function to create file based metadata connection config.\n\n  Args:\n    metadata_db_uri: uri to metadata db.\n\n  Returns:\n    A metadata_store_pb2.ConnectionConfig based on given metadata db uri.\n  \"\"\"\n  fileio.makedirs(os.path.dirname(metadata_db_uri))\n  connection_config = metadata_store_pb2.ConnectionConfig()\n  connection_config.sqlite.filename_uri = metadata_db_uri\n  connection_config.sqlite.connection_mode = (\n      metadata_store_pb2.SqliteMetadataSourceConfig.READWRITE_OPENCREATE)\n  return connection_config\n</code></pre>"},{"location":"api/v1/proto/","title":"Proto","text":""},{"location":"api/v1/proto/#tfx.v1.proto","title":"tfx.v1.proto","text":"<p>TFX proto module.</p> MODULE DESCRIPTION <code>orchestration</code> <p>TFX orchestrator proto imports.</p>"},{"location":"api/v1/proto/#tfx.v1.proto-modules","title":"Modules","text":""},{"location":"api/v1/proto/#tfx.v1.proto.orchestration","title":"orchestration","text":"<p>TFX orchestrator proto imports.</p> ATTRIBUTE DESCRIPTION <code>RunState</code> <p> </p>"},{"location":"api/v1/proto/#tfx.v1.proto.orchestration-attributes","title":"Attributes","text":""},{"location":"api/v1/proto/#tfx.v1.proto.orchestration.RunState","title":"RunState  <code>module-attribute</code>","text":"<pre><code>RunState = RunState\n</code></pre>"},{"location":"api/v1/testing/","title":"Testing","text":""},{"location":"api/v1/testing/#tfx.v1.testing","title":"tfx.v1.testing","text":"<p>Public testing modules for TFX.</p> CLASS DESCRIPTION <code>Channel</code> <p>Dummy channel for testing.</p>"},{"location":"api/v1/testing/#tfx.v1.testing-classes","title":"Classes","text":""},{"location":"api/v1/testing/#tfx.v1.testing.Channel","title":"Channel","text":"<pre><code>Channel(artifact_type: Type[Artifact], artifact_ids: Sequence[int] = ())\n</code></pre> <p>               Bases: <code>BaseChannel</code></p> <p>Dummy channel for testing.</p> METHOD DESCRIPTION <code>as_optional</code> <p>Creates an optional version of self.</p> <code>future</code> <code>get_data_dependent_node_ids</code> <p>Get data dependent nodes of this channel.</p> <code>no_trigger</code> <code>trigger_by_property</code> ATTRIBUTE DESCRIPTION <code>artifact_ids</code> <p> </p> <code>input_trigger</code> <p> TYPE: <code>_InputTrigger</code> </p> <code>is_optional</code> <p>If this is an \"optional\" channel. Changes Pipeline runtime behavior.</p> <p> TYPE: <code>Optional[bool]</code> </p> <code>type</code> <p> TYPE: <code>Type[_AT]</code> </p> <code>type_name</code> <p>Name of the artifact type class that Channel takes.</p> <p> </p> Source code in <code>tfx/types/channel_utils.py</code> <pre><code>def __init__(\n    self,\n    artifact_type: Type[artifact.Artifact],\n    artifact_ids: Sequence[int] = (),\n):\n  super().__init__(artifact_type)\n  self.artifact_ids = artifact_ids\n</code></pre>"},{"location":"api/v1/testing/#tfx.v1.testing.Channel-attributes","title":"Attributes","text":""},{"location":"api/v1/testing/#tfx.v1.testing.Channel.artifact_ids","title":"artifact_ids  <code>instance-attribute</code>","text":"<pre><code>artifact_ids = artifact_ids\n</code></pre>"},{"location":"api/v1/testing/#tfx.v1.testing.Channel.input_trigger","title":"input_trigger  <code>property</code>","text":"<pre><code>input_trigger: _InputTrigger\n</code></pre>"},{"location":"api/v1/testing/#tfx.v1.testing.Channel.is_optional","title":"is_optional  <code>property</code>","text":"<pre><code>is_optional: Optional[bool]\n</code></pre> <p>If this is an \"optional\" channel. Changes Pipeline runtime behavior.</p>"},{"location":"api/v1/testing/#tfx.v1.testing.Channel.type","title":"type  <code>property</code> <code>writable</code>","text":"<pre><code>type: Type[_AT]\n</code></pre>"},{"location":"api/v1/testing/#tfx.v1.testing.Channel.type_name","title":"type_name  <code>property</code>","text":"<pre><code>type_name\n</code></pre> <p>Name of the artifact type class that Channel takes.</p>"},{"location":"api/v1/testing/#tfx.v1.testing.Channel-functions","title":"Functions","text":""},{"location":"api/v1/testing/#tfx.v1.testing.Channel.as_optional","title":"as_optional","text":"<pre><code>as_optional() -&gt; Self\n</code></pre> <p>Creates an optional version of self.</p> <p>By default component input channels are considered required, meaning if the channel does not contain at least 1 artifact, the component will be skipped. Making channel optional disables this requirement and allows componenst to be executed with no artifacts from this channel.</p> RETURNS DESCRIPTION <code>Self</code> <p>A copy of self which is optional.</p> Source code in <code>tfx/types/channel.py</code> <pre><code>def as_optional(self) -&gt; typing_extensions.Self:\n  \"\"\"Creates an optional version of self.\n\n  By default component input channels are considered required, meaning\n  if the channel does not contain at least 1 artifact, the component\n  will be skipped. Making channel optional disables this requirement and\n  allows componenst to be executed with no artifacts from this channel.\n\n  Returns:\n    A copy of self which is optional.\n  \"\"\"\n  new_channel = copy.copy(self)\n  new_channel._is_optional = True  # pylint: disable=protected-access\n  return new_channel\n</code></pre>"},{"location":"api/v1/testing/#tfx.v1.testing.Channel.future","title":"future","text":"<pre><code>future() -&gt; ChannelWrappedPlaceholder\n</code></pre> Source code in <code>tfx/types/channel_utils.py</code> <pre><code>def future(self) -&gt; channel.ChannelWrappedPlaceholder:\n  return channel.ChannelWrappedPlaceholder(self)\n</code></pre>"},{"location":"api/v1/testing/#tfx.v1.testing.Channel.get_data_dependent_node_ids","title":"get_data_dependent_node_ids","text":"<pre><code>get_data_dependent_node_ids() -&gt; Set[str]\n</code></pre> <p>Get data dependent nodes of this channel.</p> <p>Currently only the <code>OutputChannel</code> directly imposes the data dependency, but other channels can also indirectly have a data dependency if they depend on the OutputChannel. Use this abstract method to define transitive data dependency.</p> RETURNS DESCRIPTION <code>Set[str]</code> <p>A set of data-dependent node IDs.</p> Source code in <code>tfx/types/channel_utils.py</code> <pre><code>def get_data_dependent_node_ids(self) -&gt; Set[str]:\n  return set()\n</code></pre>"},{"location":"api/v1/testing/#tfx.v1.testing.Channel.no_trigger","title":"no_trigger","text":"<pre><code>no_trigger()\n</code></pre> Source code in <code>tfx/types/channel.py</code> <pre><code>@doc_controls.do_not_generate_docs\ndef no_trigger(self):\n  return self._with_input_trigger(NoTrigger())\n</code></pre>"},{"location":"api/v1/testing/#tfx.v1.testing.Channel.trigger_by_property","title":"trigger_by_property","text":"<pre><code>trigger_by_property(*property_keys: str)\n</code></pre> Source code in <code>tfx/types/channel.py</code> <pre><code>@doc_controls.do_not_generate_docs\ndef trigger_by_property(self, *property_keys: str):\n  return self._with_input_trigger(TriggerByProperty(property_keys))\n</code></pre>"},{"location":"api/v1/types/","title":"Types","text":""},{"location":"api/v1/types/#tfx.v1.types","title":"tfx.v1.types","text":"<p>TFX types module.</p> MODULE DESCRIPTION <code>standard_artifacts</code> <p>Public API for standard_artifacts.</p> CLASS DESCRIPTION <code>BaseBeamComponent</code> <p>Base class for a TFX Beam pipeline component.</p> <code>BaseChannel</code> <p>An abstraction for component (BaseNode) artifact inputs.</p> <code>BaseComponent</code> <p>Base class for a TFX pipeline component.</p> <code>BaseFunctionalComponent</code> <p>Base class for functional components.</p> <code>BaseFunctionalComponentFactory</code> <p>Serves to declare the return type below.</p> <code>BaseNode</code> <p>Base class for a node in TFX pipeline.</p>"},{"location":"api/v1/types/#tfx.v1.types-classes","title":"Classes","text":""},{"location":"api/v1/types/#tfx.v1.types.BaseBeamComponent","title":"BaseBeamComponent","text":"<pre><code>BaseBeamComponent(spec: ComponentSpec, custom_executor_spec: Optional[ExecutorSpec] = None)\n</code></pre> <p>               Bases: <code>BaseComponent</code></p> <p>Base class for a TFX Beam pipeline component.</p> <p>An instance of a subclass of BaseBaseComponent represents the parameters for a single execution of that TFX Beam pipeline component.</p> <p>Beam based components should subclass BaseBeamComponent instead of BaseComponent in order to inherit Beam related SDKs. All subclasses of BaseBeamComponent should override the required class level attributes specified in BaseComponent.</p> <p>Initialize a component.</p> PARAMETER DESCRIPTION <code>spec</code> <p>types.ComponentSpec object for this component instance.</p> <p> TYPE: <code>ComponentSpec</code> </p> <code>custom_executor_spec</code> <p>Optional custom executor spec overriding the default executor specified in the component attribute.</p> <p> TYPE: <code>Optional[ExecutorSpec]</code> DEFAULT: <code>None</code> </p> METHOD DESCRIPTION <code>add_downstream_node</code> <p>Experimental: Add another component that must run after this one.</p> <code>add_downstream_nodes</code> <p>Experimental: Add another component that must run after this one.</p> <code>add_upstream_node</code> <p>Experimental: Add another component that must run before this one.</p> <code>add_upstream_nodes</code> <p>Experimental: Add components that must run before this one.</p> <code>from_json_dict</code> <p>Convert from dictionary data to an object.</p> <code>to_json_dict</code> <p>Convert from an object to a JSON serializable dictionary.</p> <code>with_beam_pipeline_args</code> <p>Add per component Beam pipeline args.</p> <code>with_platform_config</code> <p>Attaches a proto-form platform config to a component.</p> ATTRIBUTE DESCRIPTION <code>id</code> <p>Node id, unique across all TFX nodes in a pipeline.</p> <p> TYPE: <code>str</code> </p> <code>outputs</code> <p>Component's output channel dict.</p> <p> TYPE: <code>Dict[str, OutputChannel]</code> </p> Source code in <code>tfx/dsl/components/base/base_component.py</code> <pre><code>def __init__(\n    self,\n    spec: types.ComponentSpec,\n    custom_executor_spec: Optional[executor_spec.ExecutorSpec] = None):\n  \"\"\"Initialize a component.\n\n  Args:\n    spec: types.ComponentSpec object for this component instance.\n    custom_executor_spec: Optional custom executor spec overriding the default\n      executor specified in the component attribute.\n  \"\"\"\n  if custom_executor_spec:\n    if not isinstance(custom_executor_spec, executor_spec.ExecutorSpec):\n      raise TypeError(\n          ('Custom executor spec override %s for %s should be an instance of '\n           'ExecutorSpec') % (custom_executor_spec, self.__class__))\n\n  executor_spec_obj = custom_executor_spec or self.__class__.EXECUTOR_SPEC\n  # TODO(b/171742415): Remove this try-catch block once we migrate Beam\n  # DAG runner to IR-based stack. The deep copy will only fail for function\n  # based components due to pickle workaround we created in ExecutorClassSpec.\n  try:\n    executor_spec_obj = executor_spec_obj.copy()\n  except Exception as e:  # pylint:disable = bare-except\n    # This will only happen for function based components, which is fine.\n    raise ValueError(\n        f'The executor spec {executor_spec_obj!r} of {self.__class__} class '\n        'is not copyable.'\n    ) from e\n\n  driver_class = self.__class__.DRIVER_CLASS\n  # Set self.spec before super.__init__() where node registration happens.\n  # This enable node input checking on node context registration.\n  self._validate_spec(spec)\n  spec.migrate_output_channels(self)\n  self.spec = spec\n  super().__init__(\n      executor_spec=executor_spec_obj,\n      driver_class=driver_class,\n  )\n  self._validate_component_class()\n  self.platform_config = None\n  self._pip_dependencies = []\n</code></pre>"},{"location":"api/v1/types/#tfx.v1.types.BaseBeamComponent-attributes","title":"Attributes","text":""},{"location":"api/v1/types/#tfx.v1.types.BaseBeamComponent.id","title":"id  <code>property</code> <code>writable</code>","text":"<pre><code>id: str\n</code></pre> <p>Node id, unique across all TFX nodes in a pipeline.</p> <p>If <code>id</code> is set by the user, return it directly. Otherwise, return .</p> RETURNS DESCRIPTION <code>str</code> <p>node id.</p>"},{"location":"api/v1/types/#tfx.v1.types.BaseBeamComponent.outputs","title":"outputs  <code>property</code>","text":"<pre><code>outputs: Dict[str, OutputChannel]\n</code></pre> <p>Component's output channel dict.</p>"},{"location":"api/v1/types/#tfx.v1.types.BaseBeamComponent-functions","title":"Functions","text":""},{"location":"api/v1/types/#tfx.v1.types.BaseBeamComponent.add_downstream_node","title":"add_downstream_node","text":"<pre><code>add_downstream_node(downstream_node)\n</code></pre> <p>Experimental: Add another component that must run after this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> <p>It is symmetric with <code>add_upstream_node</code>.</p> PARAMETER DESCRIPTION <code>downstream_node</code> <p>a component that must run after this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_downstream_node(self, downstream_node):\n  \"\"\"Experimental: Add another component that must run after this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n  It is symmetric with `add_upstream_node`.\n\n  Args:\n    downstream_node: a component that must run after this node.\n  \"\"\"\n  self._downstream_nodes.add(downstream_node)\n  if self not in downstream_node.upstream_nodes:\n    downstream_node.add_upstream_node(self)\n</code></pre>"},{"location":"api/v1/types/#tfx.v1.types.BaseBeamComponent.add_downstream_nodes","title":"add_downstream_nodes","text":"<pre><code>add_downstream_nodes(downstream_nodes)\n</code></pre> <p>Experimental: Add another component that must run after this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> <p>It is symmetric with <code>add_upstream_nodes</code>.</p> PARAMETER DESCRIPTION <code>downstream_nodes</code> <p>a list of components that must run after this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_downstream_nodes(self, downstream_nodes):\n  \"\"\"Experimental: Add another component that must run after this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n  It is symmetric with `add_upstream_nodes`.\n\n  Args:\n    downstream_nodes: a list of components that must run after this node.\n  \"\"\"\n  self._downstream_nodes.update(downstream_nodes)\n  for downstream_node in downstream_nodes:\n    if self not in downstream_node.upstream_nodes:\n      downstream_node.add_upstream_node(self)\n</code></pre>"},{"location":"api/v1/types/#tfx.v1.types.BaseBeamComponent.add_upstream_node","title":"add_upstream_node","text":"<pre><code>add_upstream_node(upstream_node)\n</code></pre> <p>Experimental: Add another component that must run before this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> <p>It is symmetric with <code>add_downstream_node</code>.</p> PARAMETER DESCRIPTION <code>upstream_node</code> <p>a component that must run before this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_upstream_node(self, upstream_node):\n  \"\"\"Experimental: Add another component that must run before this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n  It is symmetric with `add_downstream_node`.\n\n  Args:\n    upstream_node: a component that must run before this node.\n  \"\"\"\n  self._upstream_nodes.add(upstream_node)\n  if self not in upstream_node.downstream_nodes:\n    upstream_node.add_downstream_node(self)\n</code></pre>"},{"location":"api/v1/types/#tfx.v1.types.BaseBeamComponent.add_upstream_nodes","title":"add_upstream_nodes","text":"<pre><code>add_upstream_nodes(upstream_nodes)\n</code></pre> <p>Experimental: Add components that must run before this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> PARAMETER DESCRIPTION <code>upstream_nodes</code> <p>a list of components that must run before this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_upstream_nodes(self, upstream_nodes):\n  \"\"\"Experimental: Add components that must run before this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n\n  Args:\n    upstream_nodes: a list of components that must run before this node.\n  \"\"\"\n  self._upstream_nodes.update(upstream_nodes)\n  for upstream_node in upstream_nodes:\n    if self not in upstream_node.downstream_nodes:\n      upstream_node.add_downstream_node(self)\n</code></pre>"},{"location":"api/v1/types/#tfx.v1.types.BaseBeamComponent.from_json_dict","title":"from_json_dict  <code>classmethod</code>","text":"<pre><code>from_json_dict(dict_data: Dict[str, Any]) -&gt; Any\n</code></pre> <p>Convert from dictionary data to an object.</p> Source code in <code>tfx/utils/json_utils.py</code> <pre><code>@classmethod\n@doc_controls.do_not_doc_in_subclasses\ndef from_json_dict(cls, dict_data: Dict[str, Any]) -&gt; Any:\n  \"\"\"Convert from dictionary data to an object.\"\"\"\n  instance = cls.__new__(cls)\n  instance.__dict__ = dict_data\n  return instance\n</code></pre>"},{"location":"api/v1/types/#tfx.v1.types.BaseBeamComponent.to_json_dict","title":"to_json_dict","text":"<pre><code>to_json_dict() -&gt; Dict[str, Any]\n</code></pre> <p>Convert from an object to a JSON serializable dictionary.</p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef to_json_dict(self) -&gt; Dict[str, Any]:\n  \"\"\"Convert from an object to a JSON serializable dictionary.\"\"\"\n  return dict((k, v)\n              for k, v in self.__dict__.items()\n              if k not in ['_upstream_nodes', '_downstream_nodes'])\n</code></pre>"},{"location":"api/v1/types/#tfx.v1.types.BaseBeamComponent.with_beam_pipeline_args","title":"with_beam_pipeline_args","text":"<pre><code>with_beam_pipeline_args(beam_pipeline_args: Iterable[Union[str, Placeholder]]) -&gt; BaseBeamComponent\n</code></pre> <p>Add per component Beam pipeline args.</p> PARAMETER DESCRIPTION <code>beam_pipeline_args</code> <p>List of Beam pipeline args to be added to the Beam executor spec.</p> <p> TYPE: <code>Iterable[Union[str, Placeholder]]</code> </p> RETURNS DESCRIPTION <code>BaseBeamComponent</code> <p>the same component itself.</p> Source code in <code>tfx/dsl/components/base/base_beam_component.py</code> <pre><code>def with_beam_pipeline_args(\n    self, beam_pipeline_args: Iterable[Union[str, placeholder.Placeholder]]\n) -&gt; 'BaseBeamComponent':\n  \"\"\"Add per component Beam pipeline args.\n\n  Args:\n    beam_pipeline_args: List of Beam pipeline args to be added to the Beam\n      executor spec.\n\n  Returns:\n    the same component itself.\n  \"\"\"\n  cast(executor_spec.BeamExecutorSpec,\n       self.executor_spec).add_beam_pipeline_args(beam_pipeline_args)\n  return self\n</code></pre>"},{"location":"api/v1/types/#tfx.v1.types.BaseBeamComponent.with_platform_config","title":"with_platform_config","text":"<pre><code>with_platform_config(config: Message) -&gt; Self\n</code></pre> <p>Attaches a proto-form platform config to a component.</p> <p>The config will be a per-node platform-specific config.</p> PARAMETER DESCRIPTION <code>config</code> <p>platform config to attach to the component.</p> <p> TYPE: <code>Message</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>the same component itself.</p> Source code in <code>tfx/dsl/components/base/base_component.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef with_platform_config(\n    self, config: message.Message\n) -&gt; typing_extensions.Self:\n  \"\"\"Attaches a proto-form platform config to a component.\n\n  The config will be a per-node platform-specific config.\n\n  Args:\n    config: platform config to attach to the component.\n\n  Returns:\n    the same component itself.\n  \"\"\"\n  self.platform_config = config\n  return self\n</code></pre>"},{"location":"api/v1/types/#tfx.v1.types.BaseChannel","title":"BaseChannel","text":"<pre><code>BaseChannel(type: Type[_AT], is_optional: Optional[bool] = None)\n</code></pre> <p>               Bases: <code>ABC</code>, <code>Generic[_AT]</code></p> <p>An abstraction for component (BaseNode) artifact inputs.</p> <p><code>BaseChannel</code> is often interchangeably used with the term 'channel' (not capital <code>Channel</code> which points to the legacy class name).</p> <p>Component takes artifact inputs distinguished by each \"input key\". For example:</p> <pre><code>trainer = Trainer(\n    examples=example_gen.outputs['examples'],\n) # ^^^^^^^^\n  # input key\n           # ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n           # channel\n</code></pre> <p>Here \"examples\" is the input key of the <code>Examples</code> artifact type. <code>example_gen.outputs[\"examples\"]</code> is a channel. Typically a single channel refers to a list of <code>Artifact</code> of a homogeneous type. Since channel is a declarative abstraction it is not strictly bound to the actual artifact, but is more of an input selector.</p> <p>The most commonly used channel type is an <code>OutputChannel</code> (in the form of <code>component.outputs[\"key\"]</code>, which selects the artifact produced by the component in the same pipeline run (in synchronous execution mode; more information on OutputChannel docstring), and is typically a single artifact.</p> ATTRIBUTE DESCRIPTION <code>type</code> <p>The artifact type class that the Channel takes.</p> <p> TYPE: <code>Type[_AT]</code> </p> <code>is_optional</code> <p>If this channel is optional (e.g. may trigger components at run time if there are no artifacts in the channel). None if not explicetely set.</p> <p> TYPE: <code>Optional[bool]</code> </p> METHOD DESCRIPTION <code>as_optional</code> <p>Creates an optional version of self.</p> <code>get_data_dependent_node_ids</code> <p>Get data dependent nodes of this channel.</p> Source code in <code>tfx/types/channel.py</code> <pre><code>def __init__(self, type: Type[_AT], is_optional: Optional[bool] = None):  # pylint: disable=redefined-builtin\n  if not _is_artifact_type(type):\n    raise ValueError(\n        'Argument \"type\" of BaseChannel constructor must be a subclass of '\n        f'tfx.Artifact (got {type}).')\n  self._artifact_type = type\n  self._input_trigger = None\n  self._original_channel = None\n  self._is_optional = is_optional\n</code></pre>"},{"location":"api/v1/types/#tfx.v1.types.BaseChannel-attributes","title":"Attributes","text":""},{"location":"api/v1/types/#tfx.v1.types.BaseChannel.is_optional","title":"is_optional  <code>property</code>","text":"<pre><code>is_optional: Optional[bool]\n</code></pre> <p>If this is an \"optional\" channel. Changes Pipeline runtime behavior.</p>"},{"location":"api/v1/types/#tfx.v1.types.BaseChannel.type_name","title":"type_name  <code>property</code>","text":"<pre><code>type_name\n</code></pre> <p>Name of the artifact type class that Channel takes.</p>"},{"location":"api/v1/types/#tfx.v1.types.BaseChannel-functions","title":"Functions","text":""},{"location":"api/v1/types/#tfx.v1.types.BaseChannel.as_optional","title":"as_optional","text":"<pre><code>as_optional() -&gt; Self\n</code></pre> <p>Creates an optional version of self.</p> <p>By default component input channels are considered required, meaning if the channel does not contain at least 1 artifact, the component will be skipped. Making channel optional disables this requirement and allows componenst to be executed with no artifacts from this channel.</p> RETURNS DESCRIPTION <code>Self</code> <p>A copy of self which is optional.</p> Source code in <code>tfx/types/channel.py</code> <pre><code>def as_optional(self) -&gt; typing_extensions.Self:\n  \"\"\"Creates an optional version of self.\n\n  By default component input channels are considered required, meaning\n  if the channel does not contain at least 1 artifact, the component\n  will be skipped. Making channel optional disables this requirement and\n  allows componenst to be executed with no artifacts from this channel.\n\n  Returns:\n    A copy of self which is optional.\n  \"\"\"\n  new_channel = copy.copy(self)\n  new_channel._is_optional = True  # pylint: disable=protected-access\n  return new_channel\n</code></pre>"},{"location":"api/v1/types/#tfx.v1.types.BaseChannel.get_data_dependent_node_ids","title":"get_data_dependent_node_ids  <code>abstractmethod</code>","text":"<pre><code>get_data_dependent_node_ids() -&gt; Set[str]\n</code></pre> <p>Get data dependent nodes of this channel.</p> <p>Currently only the <code>OutputChannel</code> directly imposes the data dependency, but other channels can also indirectly have a data dependency if they depend on the OutputChannel. Use this abstract method to define transitive data dependency.</p> RETURNS DESCRIPTION <code>Set[str]</code> <p>A set of data-dependent node IDs.</p> Source code in <code>tfx/types/channel.py</code> <pre><code>@abc.abstractmethod\ndef get_data_dependent_node_ids(self) -&gt; Set[str]:\n  \"\"\"Get data dependent nodes of this channel.\n\n  Currently only the `OutputChannel` directly imposes the data dependency,\n  but other channels can also indirectly have a data dependency if they depend\n  on the OutputChannel. Use this abstract method to define transitive data\n  dependency.\n\n  Returns:\n    A set of data-dependent node IDs.\n  \"\"\"\n</code></pre>"},{"location":"api/v1/types/#tfx.v1.types.BaseComponent","title":"BaseComponent","text":"<pre><code>BaseComponent(spec: ComponentSpec, custom_executor_spec: Optional[ExecutorSpec] = None)\n</code></pre> <p>               Bases: <code>BaseNode</code>, <code>ABC</code></p> <p>Base class for a TFX pipeline component.</p> <p>An instance of a subclass of BaseComponent represents the parameters for a single execution of that TFX pipeline component.</p> <p>All subclasses of BaseComponent must override the SPEC_CLASS field with the ComponentSpec subclass that defines the interface of this component.</p> ATTRIBUTE DESCRIPTION <code>SPEC_CLASS</code> <p>a subclass of types.ComponentSpec used by this component (required). This is a class level value.</p> <p> </p> <code>EXECUTOR_SPEC</code> <p>an instance of executor_spec.ExecutorSpec which describes how to execute this component (required). This is a class level value.</p> <p> </p> <code>DRIVER_CLASS</code> <p>a subclass of base_driver.BaseDriver as a custom driver for this component (optional, defaults to base_driver.BaseDriver). This is a class level value.</p> <p> </p> <code>PRE_EXECUTABLE_SPEC</code> <p>an optional PythonClassExecutableSpec of pre-execution hook.</p> <p> </p> <code>spec</code> <p>an instance of <code>SPEC_CLASS</code>. See types.ComponentSpec for more details.</p> <p> </p> <code>platform_config</code> <p>a protobuf message representing platform config for a component instance.</p> <p> </p> <p>Initialize a component.</p> PARAMETER DESCRIPTION <code>spec</code> <p>types.ComponentSpec object for this component instance.</p> <p> TYPE: <code>ComponentSpec</code> </p> <code>custom_executor_spec</code> <p>Optional custom executor spec overriding the default executor specified in the component attribute.</p> <p> TYPE: <code>Optional[ExecutorSpec]</code> DEFAULT: <code>None</code> </p> METHOD DESCRIPTION <code>add_downstream_node</code> <p>Experimental: Add another component that must run after this one.</p> <code>add_downstream_nodes</code> <p>Experimental: Add another component that must run after this one.</p> <code>add_upstream_node</code> <p>Experimental: Add another component that must run before this one.</p> <code>add_upstream_nodes</code> <p>Experimental: Add components that must run before this one.</p> <code>from_json_dict</code> <p>Convert from dictionary data to an object.</p> <code>to_json_dict</code> <p>Convert from an object to a JSON serializable dictionary.</p> <code>with_platform_config</code> <p>Attaches a proto-form platform config to a component.</p> Source code in <code>tfx/dsl/components/base/base_component.py</code> <pre><code>def __init__(\n    self,\n    spec: types.ComponentSpec,\n    custom_executor_spec: Optional[executor_spec.ExecutorSpec] = None):\n  \"\"\"Initialize a component.\n\n  Args:\n    spec: types.ComponentSpec object for this component instance.\n    custom_executor_spec: Optional custom executor spec overriding the default\n      executor specified in the component attribute.\n  \"\"\"\n  if custom_executor_spec:\n    if not isinstance(custom_executor_spec, executor_spec.ExecutorSpec):\n      raise TypeError(\n          ('Custom executor spec override %s for %s should be an instance of '\n           'ExecutorSpec') % (custom_executor_spec, self.__class__))\n\n  executor_spec_obj = custom_executor_spec or self.__class__.EXECUTOR_SPEC\n  # TODO(b/171742415): Remove this try-catch block once we migrate Beam\n  # DAG runner to IR-based stack. The deep copy will only fail for function\n  # based components due to pickle workaround we created in ExecutorClassSpec.\n  try:\n    executor_spec_obj = executor_spec_obj.copy()\n  except Exception as e:  # pylint:disable = bare-except\n    # This will only happen for function based components, which is fine.\n    raise ValueError(\n        f'The executor spec {executor_spec_obj!r} of {self.__class__} class '\n        'is not copyable.'\n    ) from e\n\n  driver_class = self.__class__.DRIVER_CLASS\n  # Set self.spec before super.__init__() where node registration happens.\n  # This enable node input checking on node context registration.\n  self._validate_spec(spec)\n  spec.migrate_output_channels(self)\n  self.spec = spec\n  super().__init__(\n      executor_spec=executor_spec_obj,\n      driver_class=driver_class,\n  )\n  self._validate_component_class()\n  self.platform_config = None\n  self._pip_dependencies = []\n</code></pre>"},{"location":"api/v1/types/#tfx.v1.types.BaseComponent-attributes","title":"Attributes","text":""},{"location":"api/v1/types/#tfx.v1.types.BaseComponent.id","title":"id  <code>property</code> <code>writable</code>","text":"<pre><code>id: str\n</code></pre> <p>Node id, unique across all TFX nodes in a pipeline.</p> <p>If <code>id</code> is set by the user, return it directly. Otherwise, return .</p> RETURNS DESCRIPTION <code>str</code> <p>node id.</p>"},{"location":"api/v1/types/#tfx.v1.types.BaseComponent.outputs","title":"outputs  <code>property</code>","text":"<pre><code>outputs: Dict[str, OutputChannel]\n</code></pre> <p>Component's output channel dict.</p>"},{"location":"api/v1/types/#tfx.v1.types.BaseComponent-functions","title":"Functions","text":""},{"location":"api/v1/types/#tfx.v1.types.BaseComponent.add_downstream_node","title":"add_downstream_node","text":"<pre><code>add_downstream_node(downstream_node)\n</code></pre> <p>Experimental: Add another component that must run after this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> <p>It is symmetric with <code>add_upstream_node</code>.</p> PARAMETER DESCRIPTION <code>downstream_node</code> <p>a component that must run after this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_downstream_node(self, downstream_node):\n  \"\"\"Experimental: Add another component that must run after this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n  It is symmetric with `add_upstream_node`.\n\n  Args:\n    downstream_node: a component that must run after this node.\n  \"\"\"\n  self._downstream_nodes.add(downstream_node)\n  if self not in downstream_node.upstream_nodes:\n    downstream_node.add_upstream_node(self)\n</code></pre>"},{"location":"api/v1/types/#tfx.v1.types.BaseComponent.add_downstream_nodes","title":"add_downstream_nodes","text":"<pre><code>add_downstream_nodes(downstream_nodes)\n</code></pre> <p>Experimental: Add another component that must run after this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> <p>It is symmetric with <code>add_upstream_nodes</code>.</p> PARAMETER DESCRIPTION <code>downstream_nodes</code> <p>a list of components that must run after this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_downstream_nodes(self, downstream_nodes):\n  \"\"\"Experimental: Add another component that must run after this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n  It is symmetric with `add_upstream_nodes`.\n\n  Args:\n    downstream_nodes: a list of components that must run after this node.\n  \"\"\"\n  self._downstream_nodes.update(downstream_nodes)\n  for downstream_node in downstream_nodes:\n    if self not in downstream_node.upstream_nodes:\n      downstream_node.add_upstream_node(self)\n</code></pre>"},{"location":"api/v1/types/#tfx.v1.types.BaseComponent.add_upstream_node","title":"add_upstream_node","text":"<pre><code>add_upstream_node(upstream_node)\n</code></pre> <p>Experimental: Add another component that must run before this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> <p>It is symmetric with <code>add_downstream_node</code>.</p> PARAMETER DESCRIPTION <code>upstream_node</code> <p>a component that must run before this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_upstream_node(self, upstream_node):\n  \"\"\"Experimental: Add another component that must run before this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n  It is symmetric with `add_downstream_node`.\n\n  Args:\n    upstream_node: a component that must run before this node.\n  \"\"\"\n  self._upstream_nodes.add(upstream_node)\n  if self not in upstream_node.downstream_nodes:\n    upstream_node.add_downstream_node(self)\n</code></pre>"},{"location":"api/v1/types/#tfx.v1.types.BaseComponent.add_upstream_nodes","title":"add_upstream_nodes","text":"<pre><code>add_upstream_nodes(upstream_nodes)\n</code></pre> <p>Experimental: Add components that must run before this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> PARAMETER DESCRIPTION <code>upstream_nodes</code> <p>a list of components that must run before this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_upstream_nodes(self, upstream_nodes):\n  \"\"\"Experimental: Add components that must run before this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n\n  Args:\n    upstream_nodes: a list of components that must run before this node.\n  \"\"\"\n  self._upstream_nodes.update(upstream_nodes)\n  for upstream_node in upstream_nodes:\n    if self not in upstream_node.downstream_nodes:\n      upstream_node.add_downstream_node(self)\n</code></pre>"},{"location":"api/v1/types/#tfx.v1.types.BaseComponent.from_json_dict","title":"from_json_dict  <code>classmethod</code>","text":"<pre><code>from_json_dict(dict_data: Dict[str, Any]) -&gt; Any\n</code></pre> <p>Convert from dictionary data to an object.</p> Source code in <code>tfx/utils/json_utils.py</code> <pre><code>@classmethod\n@doc_controls.do_not_doc_in_subclasses\ndef from_json_dict(cls, dict_data: Dict[str, Any]) -&gt; Any:\n  \"\"\"Convert from dictionary data to an object.\"\"\"\n  instance = cls.__new__(cls)\n  instance.__dict__ = dict_data\n  return instance\n</code></pre>"},{"location":"api/v1/types/#tfx.v1.types.BaseComponent.to_json_dict","title":"to_json_dict","text":"<pre><code>to_json_dict() -&gt; Dict[str, Any]\n</code></pre> <p>Convert from an object to a JSON serializable dictionary.</p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef to_json_dict(self) -&gt; Dict[str, Any]:\n  \"\"\"Convert from an object to a JSON serializable dictionary.\"\"\"\n  return dict((k, v)\n              for k, v in self.__dict__.items()\n              if k not in ['_upstream_nodes', '_downstream_nodes'])\n</code></pre>"},{"location":"api/v1/types/#tfx.v1.types.BaseComponent.with_platform_config","title":"with_platform_config","text":"<pre><code>with_platform_config(config: Message) -&gt; Self\n</code></pre> <p>Attaches a proto-form platform config to a component.</p> <p>The config will be a per-node platform-specific config.</p> PARAMETER DESCRIPTION <code>config</code> <p>platform config to attach to the component.</p> <p> TYPE: <code>Message</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>the same component itself.</p> Source code in <code>tfx/dsl/components/base/base_component.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef with_platform_config(\n    self, config: message.Message\n) -&gt; typing_extensions.Self:\n  \"\"\"Attaches a proto-form platform config to a component.\n\n  The config will be a per-node platform-specific config.\n\n  Args:\n    config: platform config to attach to the component.\n\n  Returns:\n    the same component itself.\n  \"\"\"\n  self.platform_config = config\n  return self\n</code></pre>"},{"location":"api/v1/types/#tfx.v1.types.BaseFunctionalComponent","title":"BaseFunctionalComponent","text":"<pre><code>BaseFunctionalComponent(spec: ComponentSpec, custom_executor_spec: Optional[ExecutorSpec] = None)\n</code></pre> <p>               Bases: <code>BaseComponent</code></p> <p>Base class for functional components.</p> <p>Initialize a component.</p> PARAMETER DESCRIPTION <code>spec</code> <p>types.ComponentSpec object for this component instance.</p> <p> TYPE: <code>ComponentSpec</code> </p> <code>custom_executor_spec</code> <p>Optional custom executor spec overriding the default executor specified in the component attribute.</p> <p> TYPE: <code>Optional[ExecutorSpec]</code> DEFAULT: <code>None</code> </p> METHOD DESCRIPTION <code>add_downstream_node</code> <p>Experimental: Add another component that must run after this one.</p> <code>add_downstream_nodes</code> <p>Experimental: Add another component that must run after this one.</p> <code>add_upstream_node</code> <p>Experimental: Add another component that must run before this one.</p> <code>add_upstream_nodes</code> <p>Experimental: Add components that must run before this one.</p> <code>from_json_dict</code> <p>Convert from dictionary data to an object.</p> <code>to_json_dict</code> <p>Convert from an object to a JSON serializable dictionary.</p> <code>with_platform_config</code> <p>Attaches a proto-form platform config to a component.</p> ATTRIBUTE DESCRIPTION <code>id</code> <p>Node id, unique across all TFX nodes in a pipeline.</p> <p> TYPE: <code>str</code> </p> <code>outputs</code> <p>Component's output channel dict.</p> <p> TYPE: <code>Dict[str, OutputChannel]</code> </p> Source code in <code>tfx/dsl/components/base/base_component.py</code> <pre><code>def __init__(\n    self,\n    spec: types.ComponentSpec,\n    custom_executor_spec: Optional[executor_spec.ExecutorSpec] = None):\n  \"\"\"Initialize a component.\n\n  Args:\n    spec: types.ComponentSpec object for this component instance.\n    custom_executor_spec: Optional custom executor spec overriding the default\n      executor specified in the component attribute.\n  \"\"\"\n  if custom_executor_spec:\n    if not isinstance(custom_executor_spec, executor_spec.ExecutorSpec):\n      raise TypeError(\n          ('Custom executor spec override %s for %s should be an instance of '\n           'ExecutorSpec') % (custom_executor_spec, self.__class__))\n\n  executor_spec_obj = custom_executor_spec or self.__class__.EXECUTOR_SPEC\n  # TODO(b/171742415): Remove this try-catch block once we migrate Beam\n  # DAG runner to IR-based stack. The deep copy will only fail for function\n  # based components due to pickle workaround we created in ExecutorClassSpec.\n  try:\n    executor_spec_obj = executor_spec_obj.copy()\n  except Exception as e:  # pylint:disable = bare-except\n    # This will only happen for function based components, which is fine.\n    raise ValueError(\n        f'The executor spec {executor_spec_obj!r} of {self.__class__} class '\n        'is not copyable.'\n    ) from e\n\n  driver_class = self.__class__.DRIVER_CLASS\n  # Set self.spec before super.__init__() where node registration happens.\n  # This enable node input checking on node context registration.\n  self._validate_spec(spec)\n  spec.migrate_output_channels(self)\n  self.spec = spec\n  super().__init__(\n      executor_spec=executor_spec_obj,\n      driver_class=driver_class,\n  )\n  self._validate_component_class()\n  self.platform_config = None\n  self._pip_dependencies = []\n</code></pre>"},{"location":"api/v1/types/#tfx.v1.types.BaseFunctionalComponent-attributes","title":"Attributes","text":""},{"location":"api/v1/types/#tfx.v1.types.BaseFunctionalComponent.id","title":"id  <code>property</code> <code>writable</code>","text":"<pre><code>id: str\n</code></pre> <p>Node id, unique across all TFX nodes in a pipeline.</p> <p>If <code>id</code> is set by the user, return it directly. Otherwise, return .</p> RETURNS DESCRIPTION <code>str</code> <p>node id.</p>"},{"location":"api/v1/types/#tfx.v1.types.BaseFunctionalComponent.outputs","title":"outputs  <code>property</code>","text":"<pre><code>outputs: Dict[str, OutputChannel]\n</code></pre> <p>Component's output channel dict.</p>"},{"location":"api/v1/types/#tfx.v1.types.BaseFunctionalComponent-functions","title":"Functions","text":""},{"location":"api/v1/types/#tfx.v1.types.BaseFunctionalComponent.add_downstream_node","title":"add_downstream_node","text":"<pre><code>add_downstream_node(downstream_node)\n</code></pre> <p>Experimental: Add another component that must run after this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> <p>It is symmetric with <code>add_upstream_node</code>.</p> PARAMETER DESCRIPTION <code>downstream_node</code> <p>a component that must run after this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_downstream_node(self, downstream_node):\n  \"\"\"Experimental: Add another component that must run after this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n  It is symmetric with `add_upstream_node`.\n\n  Args:\n    downstream_node: a component that must run after this node.\n  \"\"\"\n  self._downstream_nodes.add(downstream_node)\n  if self not in downstream_node.upstream_nodes:\n    downstream_node.add_upstream_node(self)\n</code></pre>"},{"location":"api/v1/types/#tfx.v1.types.BaseFunctionalComponent.add_downstream_nodes","title":"add_downstream_nodes","text":"<pre><code>add_downstream_nodes(downstream_nodes)\n</code></pre> <p>Experimental: Add another component that must run after this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> <p>It is symmetric with <code>add_upstream_nodes</code>.</p> PARAMETER DESCRIPTION <code>downstream_nodes</code> <p>a list of components that must run after this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_downstream_nodes(self, downstream_nodes):\n  \"\"\"Experimental: Add another component that must run after this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n  It is symmetric with `add_upstream_nodes`.\n\n  Args:\n    downstream_nodes: a list of components that must run after this node.\n  \"\"\"\n  self._downstream_nodes.update(downstream_nodes)\n  for downstream_node in downstream_nodes:\n    if self not in downstream_node.upstream_nodes:\n      downstream_node.add_upstream_node(self)\n</code></pre>"},{"location":"api/v1/types/#tfx.v1.types.BaseFunctionalComponent.add_upstream_node","title":"add_upstream_node","text":"<pre><code>add_upstream_node(upstream_node)\n</code></pre> <p>Experimental: Add another component that must run before this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> <p>It is symmetric with <code>add_downstream_node</code>.</p> PARAMETER DESCRIPTION <code>upstream_node</code> <p>a component that must run before this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_upstream_node(self, upstream_node):\n  \"\"\"Experimental: Add another component that must run before this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n  It is symmetric with `add_downstream_node`.\n\n  Args:\n    upstream_node: a component that must run before this node.\n  \"\"\"\n  self._upstream_nodes.add(upstream_node)\n  if self not in upstream_node.downstream_nodes:\n    upstream_node.add_downstream_node(self)\n</code></pre>"},{"location":"api/v1/types/#tfx.v1.types.BaseFunctionalComponent.add_upstream_nodes","title":"add_upstream_nodes","text":"<pre><code>add_upstream_nodes(upstream_nodes)\n</code></pre> <p>Experimental: Add components that must run before this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> PARAMETER DESCRIPTION <code>upstream_nodes</code> <p>a list of components that must run before this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_upstream_nodes(self, upstream_nodes):\n  \"\"\"Experimental: Add components that must run before this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n\n  Args:\n    upstream_nodes: a list of components that must run before this node.\n  \"\"\"\n  self._upstream_nodes.update(upstream_nodes)\n  for upstream_node in upstream_nodes:\n    if self not in upstream_node.downstream_nodes:\n      upstream_node.add_downstream_node(self)\n</code></pre>"},{"location":"api/v1/types/#tfx.v1.types.BaseFunctionalComponent.from_json_dict","title":"from_json_dict  <code>classmethod</code>","text":"<pre><code>from_json_dict(dict_data: Dict[str, Any]) -&gt; Any\n</code></pre> <p>Convert from dictionary data to an object.</p> Source code in <code>tfx/utils/json_utils.py</code> <pre><code>@classmethod\n@doc_controls.do_not_doc_in_subclasses\ndef from_json_dict(cls, dict_data: Dict[str, Any]) -&gt; Any:\n  \"\"\"Convert from dictionary data to an object.\"\"\"\n  instance = cls.__new__(cls)\n  instance.__dict__ = dict_data\n  return instance\n</code></pre>"},{"location":"api/v1/types/#tfx.v1.types.BaseFunctionalComponent.to_json_dict","title":"to_json_dict","text":"<pre><code>to_json_dict() -&gt; Dict[str, Any]\n</code></pre> <p>Convert from an object to a JSON serializable dictionary.</p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef to_json_dict(self) -&gt; Dict[str, Any]:\n  \"\"\"Convert from an object to a JSON serializable dictionary.\"\"\"\n  return dict((k, v)\n              for k, v in self.__dict__.items()\n              if k not in ['_upstream_nodes', '_downstream_nodes'])\n</code></pre>"},{"location":"api/v1/types/#tfx.v1.types.BaseFunctionalComponent.with_platform_config","title":"with_platform_config","text":"<pre><code>with_platform_config(config: Message) -&gt; Self\n</code></pre> <p>Attaches a proto-form platform config to a component.</p> <p>The config will be a per-node platform-specific config.</p> PARAMETER DESCRIPTION <code>config</code> <p>platform config to attach to the component.</p> <p> TYPE: <code>Message</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>the same component itself.</p> Source code in <code>tfx/dsl/components/base/base_component.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef with_platform_config(\n    self, config: message.Message\n) -&gt; typing_extensions.Self:\n  \"\"\"Attaches a proto-form platform config to a component.\n\n  The config will be a per-node platform-specific config.\n\n  Args:\n    config: platform config to attach to the component.\n\n  Returns:\n    the same component itself.\n  \"\"\"\n  self.platform_config = config\n  return self\n</code></pre>"},{"location":"api/v1/types/#tfx.v1.types.BaseFunctionalComponentFactory","title":"BaseFunctionalComponentFactory","text":"<p>               Bases: <code>Protocol</code></p> <p>Serves to declare the return type below.</p> METHOD DESCRIPTION <code>__call__</code> <p>This corresponds to BaseFunctionalComponent.init.</p> <code>test_call</code> <p>This corresponds to the static BaseFunctionalComponent.test_call().</p>"},{"location":"api/v1/types/#tfx.v1.types.BaseFunctionalComponentFactory-functions","title":"Functions","text":""},{"location":"api/v1/types/#tfx.v1.types.BaseFunctionalComponentFactory.__call__","title":"__call__","text":"<pre><code>__call__(*args: Any, **kwargs: Any) -&gt; BaseFunctionalComponent\n</code></pre> <p>This corresponds to BaseFunctionalComponent.init.</p> Source code in <code>tfx/dsl/component/experimental/decorators.py</code> <pre><code>def __call__(self, *args: Any, **kwargs: Any) -&gt; BaseFunctionalComponent:\n  \"\"\"This corresponds to BaseFunctionalComponent.__init__.\"\"\"\n  ...\n</code></pre>"},{"location":"api/v1/types/#tfx.v1.types.BaseFunctionalComponentFactory.test_call","title":"test_call","text":"<pre><code>test_call(*args: Any, **kwargs: Any) -&gt; Any\n</code></pre> <p>This corresponds to the static BaseFunctionalComponent.test_call().</p> Source code in <code>tfx/dsl/component/experimental/decorators.py</code> <pre><code>def test_call(self, *args: Any, **kwargs: Any) -&gt; Any:\n  \"\"\"This corresponds to the static BaseFunctionalComponent.test_call().\"\"\"\n  ...\n</code></pre>"},{"location":"api/v1/types/#tfx.v1.types.BaseNode","title":"BaseNode","text":"<pre><code>BaseNode(executor_spec: Optional[ExecutorSpec] = None, driver_class: Optional[Type[BaseDriver]] = None)\n</code></pre> <p>               Bases: <code>Jsonable</code>, <code>ABC</code></p> <p>Base class for a node in TFX pipeline.</p> <p>Initialize a node.</p> PARAMETER DESCRIPTION <code>executor_spec</code> <p>Optional instance of executor_spec.ExecutorSpec which describes how to execute this node (optional, defaults to an empty executor indicates no-op.</p> <p> TYPE: <code>Optional[ExecutorSpec]</code> DEFAULT: <code>None</code> </p> <code>driver_class</code> <p>Optional subclass of base_driver.BaseDriver as a custom driver for this node (optional, defaults to base_driver.BaseDriver). Nodes usually use the default driver class, but may override it.</p> <p> TYPE: <code>Optional[Type[BaseDriver]]</code> DEFAULT: <code>None</code> </p> METHOD DESCRIPTION <code>add_downstream_node</code> <p>Experimental: Add another component that must run after this one.</p> <code>add_downstream_nodes</code> <p>Experimental: Add another component that must run after this one.</p> <code>add_upstream_node</code> <p>Experimental: Add another component that must run before this one.</p> <code>add_upstream_nodes</code> <p>Experimental: Add components that must run before this one.</p> <code>from_json_dict</code> <p>Convert from dictionary data to an object.</p> <code>to_json_dict</code> <p>Convert from an object to a JSON serializable dictionary.</p> ATTRIBUTE DESCRIPTION <code>id</code> <p>Node id, unique across all TFX nodes in a pipeline.</p> <p> TYPE: <code>str</code> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>def __init__(\n    self,\n    executor_spec: Optional[executor_spec_module.ExecutorSpec] = None,\n    driver_class: Optional[Type[base_driver.BaseDriver]] = None,\n):\n  \"\"\"Initialize a node.\n\n  Args:\n    executor_spec: Optional instance of executor_spec.ExecutorSpec which\n      describes how to execute this node (optional, defaults to an empty\n      executor indicates no-op.\n    driver_class: Optional subclass of base_driver.BaseDriver as a custom\n      driver for this node (optional, defaults to base_driver.BaseDriver).\n      Nodes usually use the default driver class, but may override it.\n  \"\"\"\n  if executor_spec is None:\n    executor_spec = executor_spec_module.ExecutorClassSpec(\n        base_executor.EmptyExecutor)\n  if driver_class is None:\n    driver_class = base_driver.BaseDriver\n  self.executor_spec = executor_spec\n  self.driver_class = driver_class\n  self._upstream_nodes = set()\n  self._downstream_nodes = set()\n  self._id = None\n  self._node_execution_options: Optional[utils.NodeExecutionOptions] = None\n  dsl_context_registry.get().put_node(self)\n</code></pre>"},{"location":"api/v1/types/#tfx.v1.types.BaseNode-attributes","title":"Attributes","text":""},{"location":"api/v1/types/#tfx.v1.types.BaseNode.id","title":"id  <code>property</code> <code>writable</code>","text":"<pre><code>id: str\n</code></pre> <p>Node id, unique across all TFX nodes in a pipeline.</p> <p>If <code>id</code> is set by the user, return it directly. Otherwise, return .</p> RETURNS DESCRIPTION <code>str</code> <p>node id.</p>"},{"location":"api/v1/types/#tfx.v1.types.BaseNode-functions","title":"Functions","text":""},{"location":"api/v1/types/#tfx.v1.types.BaseNode.add_downstream_node","title":"add_downstream_node","text":"<pre><code>add_downstream_node(downstream_node)\n</code></pre> <p>Experimental: Add another component that must run after this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> <p>It is symmetric with <code>add_upstream_node</code>.</p> PARAMETER DESCRIPTION <code>downstream_node</code> <p>a component that must run after this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_downstream_node(self, downstream_node):\n  \"\"\"Experimental: Add another component that must run after this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n  It is symmetric with `add_upstream_node`.\n\n  Args:\n    downstream_node: a component that must run after this node.\n  \"\"\"\n  self._downstream_nodes.add(downstream_node)\n  if self not in downstream_node.upstream_nodes:\n    downstream_node.add_upstream_node(self)\n</code></pre>"},{"location":"api/v1/types/#tfx.v1.types.BaseNode.add_downstream_nodes","title":"add_downstream_nodes","text":"<pre><code>add_downstream_nodes(downstream_nodes)\n</code></pre> <p>Experimental: Add another component that must run after this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> <p>It is symmetric with <code>add_upstream_nodes</code>.</p> PARAMETER DESCRIPTION <code>downstream_nodes</code> <p>a list of components that must run after this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_downstream_nodes(self, downstream_nodes):\n  \"\"\"Experimental: Add another component that must run after this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n  It is symmetric with `add_upstream_nodes`.\n\n  Args:\n    downstream_nodes: a list of components that must run after this node.\n  \"\"\"\n  self._downstream_nodes.update(downstream_nodes)\n  for downstream_node in downstream_nodes:\n    if self not in downstream_node.upstream_nodes:\n      downstream_node.add_upstream_node(self)\n</code></pre>"},{"location":"api/v1/types/#tfx.v1.types.BaseNode.add_upstream_node","title":"add_upstream_node","text":"<pre><code>add_upstream_node(upstream_node)\n</code></pre> <p>Experimental: Add another component that must run before this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> <p>It is symmetric with <code>add_downstream_node</code>.</p> PARAMETER DESCRIPTION <code>upstream_node</code> <p>a component that must run before this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_upstream_node(self, upstream_node):\n  \"\"\"Experimental: Add another component that must run before this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n  It is symmetric with `add_downstream_node`.\n\n  Args:\n    upstream_node: a component that must run before this node.\n  \"\"\"\n  self._upstream_nodes.add(upstream_node)\n  if self not in upstream_node.downstream_nodes:\n    upstream_node.add_downstream_node(self)\n</code></pre>"},{"location":"api/v1/types/#tfx.v1.types.BaseNode.add_upstream_nodes","title":"add_upstream_nodes","text":"<pre><code>add_upstream_nodes(upstream_nodes)\n</code></pre> <p>Experimental: Add components that must run before this one.</p> <p>This method enables task-based dependencies by enforcing execution order for synchronous pipelines on supported platforms. Currently, the supported platforms are Airflow, Beam, and Kubeflow Pipelines.</p> <p>Note that this API call should be considered experimental, and may not work with asynchronous pipelines, sub-pipelines and pipelines with conditional nodes. We also recommend relying on data for capturing dependencies where possible to ensure data lineage is fully captured within MLMD.</p> PARAMETER DESCRIPTION <code>upstream_nodes</code> <p>a list of components that must run before this node.</p> <p> </p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef add_upstream_nodes(self, upstream_nodes):\n  \"\"\"Experimental: Add components that must run before this one.\n\n  This method enables task-based dependencies by enforcing execution order for\n  synchronous pipelines on supported platforms. Currently, the supported\n  platforms are Airflow, Beam, and Kubeflow Pipelines.\n\n  Note that this API call should be considered experimental, and may not work\n  with asynchronous pipelines, sub-pipelines and pipelines with conditional\n  nodes. We also recommend relying on data for capturing dependencies where\n  possible to ensure data lineage is fully captured within MLMD.\n\n\n  Args:\n    upstream_nodes: a list of components that must run before this node.\n  \"\"\"\n  self._upstream_nodes.update(upstream_nodes)\n  for upstream_node in upstream_nodes:\n    if self not in upstream_node.downstream_nodes:\n      upstream_node.add_downstream_node(self)\n</code></pre>"},{"location":"api/v1/types/#tfx.v1.types.BaseNode.from_json_dict","title":"from_json_dict  <code>classmethod</code>","text":"<pre><code>from_json_dict(dict_data: Dict[str, Any]) -&gt; Any\n</code></pre> <p>Convert from dictionary data to an object.</p> Source code in <code>tfx/utils/json_utils.py</code> <pre><code>@classmethod\n@doc_controls.do_not_doc_in_subclasses\ndef from_json_dict(cls, dict_data: Dict[str, Any]) -&gt; Any:\n  \"\"\"Convert from dictionary data to an object.\"\"\"\n  instance = cls.__new__(cls)\n  instance.__dict__ = dict_data\n  return instance\n</code></pre>"},{"location":"api/v1/types/#tfx.v1.types.BaseNode.to_json_dict","title":"to_json_dict","text":"<pre><code>to_json_dict() -&gt; Dict[str, Any]\n</code></pre> <p>Convert from an object to a JSON serializable dictionary.</p> Source code in <code>tfx/dsl/components/base/base_node.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef to_json_dict(self) -&gt; Dict[str, Any]:\n  \"\"\"Convert from an object to a JSON serializable dictionary.\"\"\"\n  return dict((k, v)\n              for k, v in self.__dict__.items()\n              if k not in ['_upstream_nodes', '_downstream_nodes'])\n</code></pre>"},{"location":"api/v1/types/#tfx.v1.types-modules","title":"Modules","text":""},{"location":"api/v1/types/#tfx.v1.types.standard_artifacts","title":"standard_artifacts","text":"<p>Public API for standard_artifacts.</p> CLASS DESCRIPTION <code>Boolean</code> <p>Artifacts representing a boolean.</p> <code>Bytes</code> <p>Artifacts representing raw bytes.</p> <code>ExampleAnomalies</code> <p>TFX first-party component artifact definition.</p> <code>ExampleStatistics</code> <p>TFX first-party component artifact definition.</p> <code>Examples</code> <p>Artifact that contains the training data.</p> <code>Float</code> <p>Float-typed artifact.</p> <code>HyperParameters</code> <p>TFX first-party component artifact definition.</p> <code>InferenceResult</code> <p>TFX first-party component artifact definition.</p> <code>InfraBlessing</code> <p>TFX first-party component artifact definition.</p> <code>Integer</code> <p>Integer-typed artifact.</p> <code>JsonValue</code> <p>Artifacts representing a Jsonable value.</p> <code>Model</code> <p>Artifact that contains the actual persisted model.</p> <code>ModelBlessing</code> <p>Artifact that contains the evaluation of a trained model.</p> <code>ModelEvaluation</code> <p>TFX first-party component artifact definition.</p> <code>ModelRun</code> <p>TFX first-party component artifact definition.</p> <code>PushedModel</code> <p>TFX first-party component artifact definition.</p> <code>Schema</code> <p>Artifact that contains the schema of the data.</p> <code>String</code> <p>String-typed artifact.</p> <code>TransformCache</code> <p>TFX first-party component artifact definition.</p> <code>TransformGraph</code> <p>TFX first-party component artifact definition.</p> <code>TunerResults</code> <p>TFX first-party component artifact definition.</p>"},{"location":"api/v1/types/#tfx.v1.types.standard_artifacts-classes","title":"Classes","text":""},{"location":"api/v1/types/#tfx.v1.types.standard_artifacts.Boolean","title":"Boolean","text":"<pre><code>Boolean(*args, **kwargs)\n</code></pre> <p>               Bases: <code>ValueArtifact</code></p> <p>Artifacts representing a boolean.</p> <p>Boolean value artifacts are encoded as \"1\" for True and \"0\" for False.</p> <p>Initializes ValueArtifact.</p> METHOD DESCRIPTION <code>annotate_as</code> <p>Annotate the value artifact type with a system artifact class.</p> <code>copy_from</code> <p>Set uri, properties and custom properties from a given Artifact.</p> <code>decode</code> <p>Method decoding the file content. Implemented by subclasses.</p> <code>encode</code> <p>Method encoding the file content. Implemented by subclasses.</p> <code>from_json_dict</code> <p>Convert from dictionary data to an object.</p> <code>get_bool_custom_property</code> <p>Get a custom property of bool type.</p> <code>get_custom_property</code> <p>Gets a custom property with key. Return None if not found.</p> <code>get_float_custom_property</code> <p>Gets a custom property of float type.</p> <code>get_int_custom_property</code> <p>Get a custom property of int type.</p> <code>get_json_value_custom_property</code> <p>Get a custom property of JSON type.</p> <code>get_proto_custom_property</code> <p>Get a custom property of proto type.</p> <code>get_string_custom_property</code> <p>Get a custom property of string type.</p> <code>set_bool_custom_property</code> <p>Sets a custom property of bool type.</p> <code>set_float_custom_property</code> <p>Sets a custom property of float type.</p> <code>set_int_custom_property</code> <p>Set a custom property of int type.</p> <code>set_json_value_custom_property</code> <p>Sets a custom property of JSON type.</p> <code>set_mlmd_artifact</code> <p>Replace the MLMD artifact object on this artifact.</p> <code>set_mlmd_artifact_type</code> <p>Set entire ArtifactType in this object.</p> <code>set_proto_custom_property</code> <p>Sets a custom property of proto type.</p> <code>set_string_custom_property</code> <p>Set a custom property of string type.</p> <code>to_json_dict</code> <p>Convert from an object to a JSON serializable dictionary.</p> ATTRIBUTE DESCRIPTION <code>artifact_type</code> <p>Type of the underlying mlmd artifact.</p> <p> </p> <code>external_id</code> <p>external id of the underlying artifact.</p> <p> TYPE: <code>str</code> </p> <code>id</code> <p>Id of the underlying mlmd artifact.</p> <p> TYPE: <code>int</code> </p> <code>is_external</code> <p>Returns true if the artifact is external.</p> <p> TYPE: <code>bool</code> </p> <code>mlmd_artifact</code> <p>Underlying mlmd artifact.</p> <p> </p> <code>name</code> <p>Name of the underlying mlmd artifact.</p> <p> TYPE: <code>str</code> </p> <code>pipeline_name</code> <p>Name of the pipeline that produce the artifact.</p> <p> TYPE: <code>str</code> </p> <code>producer_component</code> <p>Producer component of the artifact.</p> <p> TYPE: <code>str</code> </p> <code>state</code> <p>State of the underlying mlmd artifact.</p> <p> TYPE: <code>str</code> </p> <code>type</code> <p>Type of the artifact.</p> <p> </p> <code>type_id</code> <p>Type id of the underlying mlmd artifact.</p> <p> TYPE: <code>int</code> </p> <code>type_name</code> <p>Type name of the underlying mlmd artifact.</p> <p> </p> <code>uri</code> <p>Artifact URI.</p> <p> TYPE: <code>str</code> </p> <code>value</code> <p>Value stored in the artifact.</p> <p> </p> Source code in <code>tfx/types/value_artifact.py</code> <pre><code>def __init__(self, *args, **kwargs):\n  \"\"\"Initializes ValueArtifact.\"\"\"\n  self._has_value = False\n  self._modified = False\n  self._value = None\n  super().__init__(*args, **kwargs)\n</code></pre> Attributes\u00b6 <code></code> artifact_type <code>property</code> \u00b6 <pre><code>artifact_type\n</code></pre> <p>Type of the underlying mlmd artifact.</p> <code></code> external_id <code>property</code> \u00b6 <pre><code>external_id: str\n</code></pre> <p>external id of the underlying artifact.</p> <code></code> id <code>property</code> <code>writable</code> \u00b6 <pre><code>id: int\n</code></pre> <p>Id of the underlying mlmd artifact.</p> <code></code> is_external <code>property</code> <code>writable</code> \u00b6 <pre><code>is_external: bool\n</code></pre> <p>Returns true if the artifact is external.</p> <code></code> mlmd_artifact <code>property</code> \u00b6 <pre><code>mlmd_artifact\n</code></pre> <p>Underlying mlmd artifact.</p> <code></code> name <code>property</code> <code>writable</code> \u00b6 <pre><code>name: str\n</code></pre> <p>Name of the underlying mlmd artifact.</p> <code></code> pipeline_name <code>property</code> <code>writable</code> \u00b6 <pre><code>pipeline_name: str\n</code></pre> <p>Name of the pipeline that produce the artifact.</p> <code></code> producer_component <code>property</code> <code>writable</code> \u00b6 <pre><code>producer_component: str\n</code></pre> <p>Producer component of the artifact.</p> <code></code> state <code>property</code> <code>writable</code> \u00b6 <pre><code>state: str\n</code></pre> <p>State of the underlying mlmd artifact.</p> <code></code> type <code>property</code> \u00b6 <pre><code>type\n</code></pre> <p>Type of the artifact.</p> <code></code> type_id <code>property</code> <code>writable</code> \u00b6 <pre><code>type_id: int\n</code></pre> <p>Type id of the underlying mlmd artifact.</p> <code></code> type_name <code>property</code> \u00b6 <pre><code>type_name\n</code></pre> <p>Type name of the underlying mlmd artifact.</p> <code></code> uri <code>property</code> <code>writable</code> \u00b6 <pre><code>uri: str\n</code></pre> <p>Artifact URI.</p> <code></code> value <code>property</code> <code>writable</code> \u00b6 <pre><code>value\n</code></pre> <p>Value stored in the artifact.</p> Functions\u00b6 <code></code> annotate_as <code>classmethod</code> \u00b6 <pre><code>annotate_as(type_annotation: Optional[Type[SystemArtifact]] = None)\n</code></pre> <p>Annotate the value artifact type with a system artifact class.</p> <p>Example usage</p> <pre><code>from tfx import v1 as tfx\n\nOutputArtifact = tfx.dsl.components.OutputArtifact\nString = tfx.types.standard_artifacts.String\nModel = tfx.dsl.standard_annotations.Model\n\n\n@tfx.dsl.components.component\ndef MyTrainer(model: OutputArtifact[String.annotate_as(Model)]): ...\n</code></pre> PARAMETER DESCRIPTION <code>type_annotation</code> <p>the standard annotations used to annotate the value artifact type. The possible values are in <code>tfx.v1.dsl.standard_annotations</code>.</p> <p> TYPE: <code>Optional[Type[SystemArtifact]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <p>A subclass of the method caller class (e.g., <code>standard_artifacts.String</code>, <code>standard_artifacts.Float</code>) with TYPE_ANNOTATION attribute set to be <code>type_annotation</code>; returns the original class if<code>type_annotation</code> is None.</p> Source code in <code>tfx/types/value_artifact.py</code> <pre><code>@classmethod\ndef annotate_as(cls, type_annotation: Optional[Type[SystemArtifact]] = None):\n  \"\"\"Annotate the value artifact type with a system artifact class.\n\n  !!! example \"Example usage\"\n\n      ```python\n      from tfx import v1 as tfx\n\n      OutputArtifact = tfx.dsl.components.OutputArtifact\n      String = tfx.types.standard_artifacts.String\n      Model = tfx.dsl.standard_annotations.Model\n\n\n      @tfx.dsl.components.component\n      def MyTrainer(model: OutputArtifact[String.annotate_as(Model)]): ...\n      ```\n\n  Args:\n    type_annotation: the standard annotations used to annotate the value\n      artifact type. The possible values are in\n      `tfx.v1.dsl.standard_annotations`.\n\n  Returns:\n    A subclass of the method caller class (e.g., [`standard_artifacts.String`][tfx.v1.types.standard_artifacts.String],\n      [`standard_artifacts.Float`][tfx.v1.types.standard_artifacts.Float]) with TYPE_ANNOTATION attribute set to be\n      `type_annotation`; returns the original class if`type_annotation` is None.\n  \"\"\"\n  if not type_annotation:\n    return cls\n  if not issubclass(type_annotation, SystemArtifact):\n    raise ValueError(\n        'type_annotation %s is not a subclass of SystemArtifact.' %\n        type_annotation)\n  type_annotation_str = str(type_annotation.__name__)\n  return type(\n      str(cls.__name__) + '_' + type_annotation_str,\n      (cls,),\n      {\n          'TYPE_NAME': str(cls.TYPE_NAME) + '_' + type_annotation_str,\n          'TYPE_ANNOTATION': type_annotation,\n          '__module__': cls.__module__,\n      },\n  )\n</code></pre> <code></code> copy_from \u00b6 <pre><code>copy_from(other: Artifact)\n</code></pre> <p>Set uri, properties and custom properties from a given Artifact.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef copy_from(self, other: 'Artifact'):\n  \"\"\"Set uri, properties and custom properties from a given Artifact.\"\"\"\n  assert self.type is other.type, (\n      'Unable to set properties from an artifact of different type: {} vs {}'\n      .format(self.type_name, other.type_name))\n  self.uri = other.uri\n  if other.artifact_type.HasField('id'):\n    self.type_id = other.artifact_type.id\n\n  self._artifact.properties.clear()\n  self._artifact.properties.MergeFrom(other._artifact.properties)  # pylint: disable=protected-access\n  self._artifact.custom_properties.clear()\n  self._artifact.custom_properties.MergeFrom(\n      other._artifact.custom_properties)  # pylint: disable=protected-access\n  self._cached_modifiable_properties = copy.deepcopy(\n      other._cached_modifiable_properties)  # pylint: disable=protected-access\n  self._cached_modifiable_custom_properties = copy.deepcopy(\n      other._cached_modifiable_custom_properties)  # pylint: disable=protected-access\n</code></pre> <code></code> decode \u00b6 <pre><code>decode(serialized_value: bytes)\n</code></pre> <p>Method decoding the file content. Implemented by subclasses.</p> Source code in <code>tfx/types/standard_artifacts.py</code> <pre><code>def decode(self, serialized_value: bytes):\n    return int(serialized_value) != 0\n</code></pre> <code></code> encode \u00b6 <pre><code>encode(value: bool)\n</code></pre> <p>Method encoding the file content. Implemented by subclasses.</p> Source code in <code>tfx/types/standard_artifacts.py</code> <pre><code>def encode(self, value: bool):\n    if not isinstance(value, bool):\n        raise TypeError(\n            f\"Expecting bytes but got value {value} of type {type(value)}\"\n        )\n    return b\"1\" if value else b\"0\"\n</code></pre> <code></code> from_json_dict <code>classmethod</code> \u00b6 <pre><code>from_json_dict(dict_data: Dict[str, Any]) -&gt; Any\n</code></pre> <p>Convert from dictionary data to an object.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@classmethod\n@doc_controls.do_not_doc_inheritable\ndef from_json_dict(cls, dict_data: Dict[str, Any]) -&gt; Any:\n  module_name = dict_data['__artifact_class_module__']\n  class_name = dict_data['__artifact_class_name__']\n  artifact = metadata_store_pb2.Artifact()\n  artifact_type = metadata_store_pb2.ArtifactType()\n  json_format.Parse(json.dumps(dict_data['artifact']), artifact)\n  json_format.Parse(json.dumps(dict_data['artifact_type']), artifact_type)\n\n  # First, try to resolve the specific class used for the artifact; if this\n  # is not possible, use a generic artifact.Artifact object.\n  result = None\n  try:\n    artifact_cls = getattr(importlib.import_module(module_name), class_name)\n    # If the artifact type is the base Artifact class, do not construct the\n    # object here since that constructor requires the mlmd_artifact_type\n    # argument.\n    if artifact_cls != Artifact:\n      result = artifact_cls()\n  except (AttributeError, ImportError, ValueError):\n    logging.warning((\n        'Could not load artifact class %s.%s; using fallback deserialization '\n        'for the relevant artifact. Please make sure that any artifact '\n        'classes can be imported within your container or environment.'),\n                    module_name, class_name)\n  if not result:\n    result = Artifact(mlmd_artifact_type=artifact_type)\n  result.set_mlmd_artifact_type(artifact_type)\n  result.set_mlmd_artifact(artifact)\n  return result\n</code></pre> <code></code> get_bool_custom_property \u00b6 <pre><code>get_bool_custom_property(key: str) -&gt; bool\n</code></pre> <p>Get a custom property of bool type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_bool_custom_property(self, key: str) -&gt; bool:\n  \"\"\"Get a custom property of bool type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return False\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, bool):\n    return json_value\n  return self._artifact.custom_properties[key].bool_value\n</code></pre> <code></code> get_custom_property \u00b6 <pre><code>get_custom_property(key: str) -&gt; Optional[Union[int, float, str, bool, JsonValueType]]\n</code></pre> <p>Gets a custom property with key. Return None if not found.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_custom_property(\n    self, key: str\n) -&gt; Optional[Union[int, float, str, bool, JsonValueType]]:\n  \"\"\"Gets a custom property with key. Return None if not found.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return None\n\n  json_value = self.get_json_value_custom_property(key)\n  if json_value:\n    return json_value\n\n  mlmd_value = self._artifact.custom_properties[key]\n  if mlmd_value.HasField('int_value'):\n    return mlmd_value.int_value\n  elif mlmd_value.HasField('double_value'):\n    return mlmd_value.double_value\n  elif mlmd_value.HasField('string_value'):\n    return mlmd_value.string_value\n  elif mlmd_value.HasField('bool_value'):\n    return mlmd_value.bool_value\n  return None\n</code></pre> <code></code> get_float_custom_property \u00b6 <pre><code>get_float_custom_property(key: str) -&gt; float\n</code></pre> <p>Gets a custom property of float type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_float_custom_property(self, key: str) -&gt; float:\n  \"\"\"Gets a custom property of float type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return 0.0\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, float):\n    return json_value\n  return self._artifact.custom_properties[key].double_value\n</code></pre> <code></code> get_int_custom_property \u00b6 <pre><code>get_int_custom_property(key: str) -&gt; int\n</code></pre> <p>Get a custom property of int type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_int_custom_property(self, key: str) -&gt; int:\n  \"\"\"Get a custom property of int type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return 0\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, float):\n    return int(json_value)\n  return self._artifact.custom_properties[key].int_value\n</code></pre> <code></code> get_json_value_custom_property \u00b6 <pre><code>get_json_value_custom_property(key: str) -&gt; JsonValueType\n</code></pre> <p>Get a custom property of JSON type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef get_json_value_custom_property(self, key: str) -&gt; JsonValueType:\n  \"\"\"Get a custom property of JSON type.\"\"\"\n  if key in self._cached_modifiable_custom_properties:\n    return self._cached_modifiable_custom_properties[key]\n  if (key not in self._artifact.custom_properties or\n      not self._artifact.custom_properties[key].HasField('struct_value')):\n    return None\n  value = _decode_struct_value(\n      self._artifact.custom_properties[key].struct_value)\n  # We must cache the decoded lists or dictionaries returned here so that\n  # if their recursive contents are modified, the Metadata proto message\n  # can be updated to reflect this.\n  if isinstance(value, (dict, list)):\n    self._cached_modifiable_custom_properties[key] = value\n  return value\n</code></pre> <code></code> get_proto_custom_property \u00b6 <pre><code>get_proto_custom_property(key: str) -&gt; Optional[Message]\n</code></pre> <p>Get a custom property of proto type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef get_proto_custom_property(self, key: str) -&gt; Optional[message.Message]:\n  \"\"\"Get a custom property of proto type.\"\"\"\n  if key in self._cached_modifiable_custom_properties:\n    return self._cached_modifiable_custom_properties[key]\n  if (key not in self._artifact.custom_properties or\n      not self._artifact.custom_properties[key].HasField('proto_value')):\n    return None\n  value = proto_utils.unpack_proto_any(\n      self._artifact.custom_properties[key].proto_value)\n  # We must cache the protobuf message here so that if its contents are\n  # modified, the Metadata proto message can be updated to reflect this.\n  if isinstance(value, message.Message):\n    self._cached_modifiable_custom_properties[key] = value\n  return value\n</code></pre> <code></code> get_string_custom_property \u00b6 <pre><code>get_string_custom_property(key: str) -&gt; str\n</code></pre> <p>Get a custom property of string type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_string_custom_property(self, key: str) -&gt; str:\n  \"\"\"Get a custom property of string type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return ''\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, str):\n    return json_value\n  return self._artifact.custom_properties[key].string_value\n</code></pre> <code></code> set_bool_custom_property \u00b6 <pre><code>set_bool_custom_property(key: str, value: bool)\n</code></pre> <p>Sets a custom property of bool type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_bool_custom_property(self, key: str, value: bool):\n  \"\"\"Sets a custom property of bool type.\"\"\"\n  self._artifact.custom_properties[key].bool_value = value\n</code></pre> <code></code> set_float_custom_property \u00b6 <pre><code>set_float_custom_property(key: str, value: float)\n</code></pre> <p>Sets a custom property of float type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_float_custom_property(self, key: str, value: float):\n  \"\"\"Sets a custom property of float type.\"\"\"\n  self._artifact.custom_properties[key].double_value = builtins.float(value)\n</code></pre> <code></code> set_int_custom_property \u00b6 <pre><code>set_int_custom_property(key: str, value: int)\n</code></pre> <p>Set a custom property of int type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_int_custom_property(self, key: str, value: int):\n  \"\"\"Set a custom property of int type.\"\"\"\n  self._artifact.custom_properties[key].int_value = builtins.int(value)\n</code></pre> <code></code> set_json_value_custom_property \u00b6 <pre><code>set_json_value_custom_property(key: str, value: JsonValueType)\n</code></pre> <p>Sets a custom property of JSON type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_json_value_custom_property(self, key: str, value: JsonValueType):\n  \"\"\"Sets a custom property of JSON type.\"\"\"\n  self._cached_modifiable_custom_properties[key] = value\n</code></pre> <code></code> set_mlmd_artifact \u00b6 <pre><code>set_mlmd_artifact(artifact: Artifact)\n</code></pre> <p>Replace the MLMD artifact object on this artifact.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_mlmd_artifact(self, artifact: metadata_store_pb2.Artifact):\n  \"\"\"Replace the MLMD artifact object on this artifact.\"\"\"\n  if not isinstance(artifact, metadata_store_pb2.Artifact):\n    raise ValueError(\n        ('Expected instance of metadata_store_pb2.Artifact, got %s '\n         'instead.') % (artifact,))\n  self._artifact = artifact\n  self._cached_modifiable_properties = {}\n  self._cached_modifiable_custom_properties = {}\n</code></pre> <code></code> set_mlmd_artifact_type \u00b6 <pre><code>set_mlmd_artifact_type(artifact_type: ArtifactType)\n</code></pre> <p>Set entire ArtifactType in this object.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_mlmd_artifact_type(self,\n                           artifact_type: metadata_store_pb2.ArtifactType):\n  \"\"\"Set entire ArtifactType in this object.\"\"\"\n  if not isinstance(artifact_type, metadata_store_pb2.ArtifactType):\n    raise ValueError(\n        ('Expected instance of metadata_store_pb2.ArtifactType, got %s '\n         'instead.') % (artifact_type,))\n  self._artifact_type = artifact_type\n  self._artifact.type_id = artifact_type.id\n</code></pre> <code></code> set_proto_custom_property \u00b6 <pre><code>set_proto_custom_property(key: str, value: Message)\n</code></pre> <p>Sets a custom property of proto type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_proto_custom_property(self, key: str, value: message.Message):\n  \"\"\"Sets a custom property of proto type.\"\"\"\n  self._cached_modifiable_custom_properties[key] = value\n</code></pre> <code></code> set_string_custom_property \u00b6 <pre><code>set_string_custom_property(key: str, value: str)\n</code></pre> <p>Set a custom property of string type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_string_custom_property(self, key: str, value: str):\n  \"\"\"Set a custom property of string type.\"\"\"\n  self._artifact.custom_properties[key].string_value = value\n</code></pre> <code></code> to_json_dict \u00b6 <pre><code>to_json_dict() -&gt; Dict[str, Any]\n</code></pre> <p>Convert from an object to a JSON serializable dictionary.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef to_json_dict(self) -&gt; Dict[str, Any]:\n  return {\n      'artifact':\n          json.loads(\n              json_format.MessageToJson(\n                  message=self.mlmd_artifact,\n                  preserving_proto_field_name=True)),\n      'artifact_type':\n          json.loads(\n              json_format.MessageToJson(\n                  message=self._artifact_type,\n                  preserving_proto_field_name=True)),\n      '__artifact_class_module__':\n          self.__class__.__module__,\n      '__artifact_class_name__':\n          self.__class__.__name__,\n  }\n</code></pre>"},{"location":"api/v1/types/#tfx.v1.types.standard_artifacts.Bytes","title":"Bytes","text":"<pre><code>Bytes(*args, **kwargs)\n</code></pre> <p>               Bases: <code>ValueArtifact</code></p> <p>Artifacts representing raw bytes.</p> <p>Initializes ValueArtifact.</p> METHOD DESCRIPTION <code>annotate_as</code> <p>Annotate the value artifact type with a system artifact class.</p> <code>copy_from</code> <p>Set uri, properties and custom properties from a given Artifact.</p> <code>decode</code> <p>Method decoding the file content. Implemented by subclasses.</p> <code>encode</code> <p>Method encoding the file content. Implemented by subclasses.</p> <code>from_json_dict</code> <p>Convert from dictionary data to an object.</p> <code>get_bool_custom_property</code> <p>Get a custom property of bool type.</p> <code>get_custom_property</code> <p>Gets a custom property with key. Return None if not found.</p> <code>get_float_custom_property</code> <p>Gets a custom property of float type.</p> <code>get_int_custom_property</code> <p>Get a custom property of int type.</p> <code>get_json_value_custom_property</code> <p>Get a custom property of JSON type.</p> <code>get_proto_custom_property</code> <p>Get a custom property of proto type.</p> <code>get_string_custom_property</code> <p>Get a custom property of string type.</p> <code>set_bool_custom_property</code> <p>Sets a custom property of bool type.</p> <code>set_float_custom_property</code> <p>Sets a custom property of float type.</p> <code>set_int_custom_property</code> <p>Set a custom property of int type.</p> <code>set_json_value_custom_property</code> <p>Sets a custom property of JSON type.</p> <code>set_mlmd_artifact</code> <p>Replace the MLMD artifact object on this artifact.</p> <code>set_mlmd_artifact_type</code> <p>Set entire ArtifactType in this object.</p> <code>set_proto_custom_property</code> <p>Sets a custom property of proto type.</p> <code>set_string_custom_property</code> <p>Set a custom property of string type.</p> <code>to_json_dict</code> <p>Convert from an object to a JSON serializable dictionary.</p> ATTRIBUTE DESCRIPTION <code>artifact_type</code> <p>Type of the underlying mlmd artifact.</p> <p> </p> <code>external_id</code> <p>external id of the underlying artifact.</p> <p> TYPE: <code>str</code> </p> <code>id</code> <p>Id of the underlying mlmd artifact.</p> <p> TYPE: <code>int</code> </p> <code>is_external</code> <p>Returns true if the artifact is external.</p> <p> TYPE: <code>bool</code> </p> <code>mlmd_artifact</code> <p>Underlying mlmd artifact.</p> <p> </p> <code>name</code> <p>Name of the underlying mlmd artifact.</p> <p> TYPE: <code>str</code> </p> <code>pipeline_name</code> <p>Name of the pipeline that produce the artifact.</p> <p> TYPE: <code>str</code> </p> <code>producer_component</code> <p>Producer component of the artifact.</p> <p> TYPE: <code>str</code> </p> <code>state</code> <p>State of the underlying mlmd artifact.</p> <p> TYPE: <code>str</code> </p> <code>type</code> <p>Type of the artifact.</p> <p> </p> <code>type_id</code> <p>Type id of the underlying mlmd artifact.</p> <p> TYPE: <code>int</code> </p> <code>type_name</code> <p>Type name of the underlying mlmd artifact.</p> <p> </p> <code>uri</code> <p>Artifact URI.</p> <p> TYPE: <code>str</code> </p> <code>value</code> <p>Value stored in the artifact.</p> <p> </p> Source code in <code>tfx/types/value_artifact.py</code> <pre><code>def __init__(self, *args, **kwargs):\n  \"\"\"Initializes ValueArtifact.\"\"\"\n  self._has_value = False\n  self._modified = False\n  self._value = None\n  super().__init__(*args, **kwargs)\n</code></pre> Attributes\u00b6 <code></code> artifact_type <code>property</code> \u00b6 <pre><code>artifact_type\n</code></pre> <p>Type of the underlying mlmd artifact.</p> <code></code> external_id <code>property</code> \u00b6 <pre><code>external_id: str\n</code></pre> <p>external id of the underlying artifact.</p> <code></code> id <code>property</code> <code>writable</code> \u00b6 <pre><code>id: int\n</code></pre> <p>Id of the underlying mlmd artifact.</p> <code></code> is_external <code>property</code> <code>writable</code> \u00b6 <pre><code>is_external: bool\n</code></pre> <p>Returns true if the artifact is external.</p> <code></code> mlmd_artifact <code>property</code> \u00b6 <pre><code>mlmd_artifact\n</code></pre> <p>Underlying mlmd artifact.</p> <code></code> name <code>property</code> <code>writable</code> \u00b6 <pre><code>name: str\n</code></pre> <p>Name of the underlying mlmd artifact.</p> <code></code> pipeline_name <code>property</code> <code>writable</code> \u00b6 <pre><code>pipeline_name: str\n</code></pre> <p>Name of the pipeline that produce the artifact.</p> <code></code> producer_component <code>property</code> <code>writable</code> \u00b6 <pre><code>producer_component: str\n</code></pre> <p>Producer component of the artifact.</p> <code></code> state <code>property</code> <code>writable</code> \u00b6 <pre><code>state: str\n</code></pre> <p>State of the underlying mlmd artifact.</p> <code></code> type <code>property</code> \u00b6 <pre><code>type\n</code></pre> <p>Type of the artifact.</p> <code></code> type_id <code>property</code> <code>writable</code> \u00b6 <pre><code>type_id: int\n</code></pre> <p>Type id of the underlying mlmd artifact.</p> <code></code> type_name <code>property</code> \u00b6 <pre><code>type_name\n</code></pre> <p>Type name of the underlying mlmd artifact.</p> <code></code> uri <code>property</code> <code>writable</code> \u00b6 <pre><code>uri: str\n</code></pre> <p>Artifact URI.</p> <code></code> value <code>property</code> <code>writable</code> \u00b6 <pre><code>value\n</code></pre> <p>Value stored in the artifact.</p> Functions\u00b6 <code></code> annotate_as <code>classmethod</code> \u00b6 <pre><code>annotate_as(type_annotation: Optional[Type[SystemArtifact]] = None)\n</code></pre> <p>Annotate the value artifact type with a system artifact class.</p> <p>Example usage</p> <pre><code>from tfx import v1 as tfx\n\nOutputArtifact = tfx.dsl.components.OutputArtifact\nString = tfx.types.standard_artifacts.String\nModel = tfx.dsl.standard_annotations.Model\n\n\n@tfx.dsl.components.component\ndef MyTrainer(model: OutputArtifact[String.annotate_as(Model)]): ...\n</code></pre> PARAMETER DESCRIPTION <code>type_annotation</code> <p>the standard annotations used to annotate the value artifact type. The possible values are in <code>tfx.v1.dsl.standard_annotations</code>.</p> <p> TYPE: <code>Optional[Type[SystemArtifact]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <p>A subclass of the method caller class (e.g., <code>standard_artifacts.String</code>, <code>standard_artifacts.Float</code>) with TYPE_ANNOTATION attribute set to be <code>type_annotation</code>; returns the original class if<code>type_annotation</code> is None.</p> Source code in <code>tfx/types/value_artifact.py</code> <pre><code>@classmethod\ndef annotate_as(cls, type_annotation: Optional[Type[SystemArtifact]] = None):\n  \"\"\"Annotate the value artifact type with a system artifact class.\n\n  !!! example \"Example usage\"\n\n      ```python\n      from tfx import v1 as tfx\n\n      OutputArtifact = tfx.dsl.components.OutputArtifact\n      String = tfx.types.standard_artifacts.String\n      Model = tfx.dsl.standard_annotations.Model\n\n\n      @tfx.dsl.components.component\n      def MyTrainer(model: OutputArtifact[String.annotate_as(Model)]): ...\n      ```\n\n  Args:\n    type_annotation: the standard annotations used to annotate the value\n      artifact type. The possible values are in\n      `tfx.v1.dsl.standard_annotations`.\n\n  Returns:\n    A subclass of the method caller class (e.g., [`standard_artifacts.String`][tfx.v1.types.standard_artifacts.String],\n      [`standard_artifacts.Float`][tfx.v1.types.standard_artifacts.Float]) with TYPE_ANNOTATION attribute set to be\n      `type_annotation`; returns the original class if`type_annotation` is None.\n  \"\"\"\n  if not type_annotation:\n    return cls\n  if not issubclass(type_annotation, SystemArtifact):\n    raise ValueError(\n        'type_annotation %s is not a subclass of SystemArtifact.' %\n        type_annotation)\n  type_annotation_str = str(type_annotation.__name__)\n  return type(\n      str(cls.__name__) + '_' + type_annotation_str,\n      (cls,),\n      {\n          'TYPE_NAME': str(cls.TYPE_NAME) + '_' + type_annotation_str,\n          'TYPE_ANNOTATION': type_annotation,\n          '__module__': cls.__module__,\n      },\n  )\n</code></pre> <code></code> copy_from \u00b6 <pre><code>copy_from(other: Artifact)\n</code></pre> <p>Set uri, properties and custom properties from a given Artifact.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef copy_from(self, other: 'Artifact'):\n  \"\"\"Set uri, properties and custom properties from a given Artifact.\"\"\"\n  assert self.type is other.type, (\n      'Unable to set properties from an artifact of different type: {} vs {}'\n      .format(self.type_name, other.type_name))\n  self.uri = other.uri\n  if other.artifact_type.HasField('id'):\n    self.type_id = other.artifact_type.id\n\n  self._artifact.properties.clear()\n  self._artifact.properties.MergeFrom(other._artifact.properties)  # pylint: disable=protected-access\n  self._artifact.custom_properties.clear()\n  self._artifact.custom_properties.MergeFrom(\n      other._artifact.custom_properties)  # pylint: disable=protected-access\n  self._cached_modifiable_properties = copy.deepcopy(\n      other._cached_modifiable_properties)  # pylint: disable=protected-access\n  self._cached_modifiable_custom_properties = copy.deepcopy(\n      other._cached_modifiable_custom_properties)  # pylint: disable=protected-access\n</code></pre> <code></code> decode \u00b6 <pre><code>decode(serialized_value: bytes)\n</code></pre> <p>Method decoding the file content. Implemented by subclasses.</p> Source code in <code>tfx/types/standard_artifacts.py</code> <pre><code>def decode(self, serialized_value: bytes):\n    return serialized_value\n</code></pre> <code></code> encode \u00b6 <pre><code>encode(value: bytes)\n</code></pre> <p>Method encoding the file content. Implemented by subclasses.</p> Source code in <code>tfx/types/standard_artifacts.py</code> <pre><code>def encode(self, value: bytes):\n    if not isinstance(value, bytes):\n        raise TypeError(\n            \"Expecting bytes but got value %s of type %s\"\n            % (str(value), type(value))\n        )\n    return value\n</code></pre> <code></code> from_json_dict <code>classmethod</code> \u00b6 <pre><code>from_json_dict(dict_data: Dict[str, Any]) -&gt; Any\n</code></pre> <p>Convert from dictionary data to an object.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@classmethod\n@doc_controls.do_not_doc_inheritable\ndef from_json_dict(cls, dict_data: Dict[str, Any]) -&gt; Any:\n  module_name = dict_data['__artifact_class_module__']\n  class_name = dict_data['__artifact_class_name__']\n  artifact = metadata_store_pb2.Artifact()\n  artifact_type = metadata_store_pb2.ArtifactType()\n  json_format.Parse(json.dumps(dict_data['artifact']), artifact)\n  json_format.Parse(json.dumps(dict_data['artifact_type']), artifact_type)\n\n  # First, try to resolve the specific class used for the artifact; if this\n  # is not possible, use a generic artifact.Artifact object.\n  result = None\n  try:\n    artifact_cls = getattr(importlib.import_module(module_name), class_name)\n    # If the artifact type is the base Artifact class, do not construct the\n    # object here since that constructor requires the mlmd_artifact_type\n    # argument.\n    if artifact_cls != Artifact:\n      result = artifact_cls()\n  except (AttributeError, ImportError, ValueError):\n    logging.warning((\n        'Could not load artifact class %s.%s; using fallback deserialization '\n        'for the relevant artifact. Please make sure that any artifact '\n        'classes can be imported within your container or environment.'),\n                    module_name, class_name)\n  if not result:\n    result = Artifact(mlmd_artifact_type=artifact_type)\n  result.set_mlmd_artifact_type(artifact_type)\n  result.set_mlmd_artifact(artifact)\n  return result\n</code></pre> <code></code> get_bool_custom_property \u00b6 <pre><code>get_bool_custom_property(key: str) -&gt; bool\n</code></pre> <p>Get a custom property of bool type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_bool_custom_property(self, key: str) -&gt; bool:\n  \"\"\"Get a custom property of bool type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return False\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, bool):\n    return json_value\n  return self._artifact.custom_properties[key].bool_value\n</code></pre> <code></code> get_custom_property \u00b6 <pre><code>get_custom_property(key: str) -&gt; Optional[Union[int, float, str, bool, JsonValueType]]\n</code></pre> <p>Gets a custom property with key. Return None if not found.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_custom_property(\n    self, key: str\n) -&gt; Optional[Union[int, float, str, bool, JsonValueType]]:\n  \"\"\"Gets a custom property with key. Return None if not found.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return None\n\n  json_value = self.get_json_value_custom_property(key)\n  if json_value:\n    return json_value\n\n  mlmd_value = self._artifact.custom_properties[key]\n  if mlmd_value.HasField('int_value'):\n    return mlmd_value.int_value\n  elif mlmd_value.HasField('double_value'):\n    return mlmd_value.double_value\n  elif mlmd_value.HasField('string_value'):\n    return mlmd_value.string_value\n  elif mlmd_value.HasField('bool_value'):\n    return mlmd_value.bool_value\n  return None\n</code></pre> <code></code> get_float_custom_property \u00b6 <pre><code>get_float_custom_property(key: str) -&gt; float\n</code></pre> <p>Gets a custom property of float type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_float_custom_property(self, key: str) -&gt; float:\n  \"\"\"Gets a custom property of float type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return 0.0\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, float):\n    return json_value\n  return self._artifact.custom_properties[key].double_value\n</code></pre> <code></code> get_int_custom_property \u00b6 <pre><code>get_int_custom_property(key: str) -&gt; int\n</code></pre> <p>Get a custom property of int type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_int_custom_property(self, key: str) -&gt; int:\n  \"\"\"Get a custom property of int type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return 0\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, float):\n    return int(json_value)\n  return self._artifact.custom_properties[key].int_value\n</code></pre> <code></code> get_json_value_custom_property \u00b6 <pre><code>get_json_value_custom_property(key: str) -&gt; JsonValueType\n</code></pre> <p>Get a custom property of JSON type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef get_json_value_custom_property(self, key: str) -&gt; JsonValueType:\n  \"\"\"Get a custom property of JSON type.\"\"\"\n  if key in self._cached_modifiable_custom_properties:\n    return self._cached_modifiable_custom_properties[key]\n  if (key not in self._artifact.custom_properties or\n      not self._artifact.custom_properties[key].HasField('struct_value')):\n    return None\n  value = _decode_struct_value(\n      self._artifact.custom_properties[key].struct_value)\n  # We must cache the decoded lists or dictionaries returned here so that\n  # if their recursive contents are modified, the Metadata proto message\n  # can be updated to reflect this.\n  if isinstance(value, (dict, list)):\n    self._cached_modifiable_custom_properties[key] = value\n  return value\n</code></pre> <code></code> get_proto_custom_property \u00b6 <pre><code>get_proto_custom_property(key: str) -&gt; Optional[Message]\n</code></pre> <p>Get a custom property of proto type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef get_proto_custom_property(self, key: str) -&gt; Optional[message.Message]:\n  \"\"\"Get a custom property of proto type.\"\"\"\n  if key in self._cached_modifiable_custom_properties:\n    return self._cached_modifiable_custom_properties[key]\n  if (key not in self._artifact.custom_properties or\n      not self._artifact.custom_properties[key].HasField('proto_value')):\n    return None\n  value = proto_utils.unpack_proto_any(\n      self._artifact.custom_properties[key].proto_value)\n  # We must cache the protobuf message here so that if its contents are\n  # modified, the Metadata proto message can be updated to reflect this.\n  if isinstance(value, message.Message):\n    self._cached_modifiable_custom_properties[key] = value\n  return value\n</code></pre> <code></code> get_string_custom_property \u00b6 <pre><code>get_string_custom_property(key: str) -&gt; str\n</code></pre> <p>Get a custom property of string type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_string_custom_property(self, key: str) -&gt; str:\n  \"\"\"Get a custom property of string type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return ''\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, str):\n    return json_value\n  return self._artifact.custom_properties[key].string_value\n</code></pre> <code></code> set_bool_custom_property \u00b6 <pre><code>set_bool_custom_property(key: str, value: bool)\n</code></pre> <p>Sets a custom property of bool type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_bool_custom_property(self, key: str, value: bool):\n  \"\"\"Sets a custom property of bool type.\"\"\"\n  self._artifact.custom_properties[key].bool_value = value\n</code></pre> <code></code> set_float_custom_property \u00b6 <pre><code>set_float_custom_property(key: str, value: float)\n</code></pre> <p>Sets a custom property of float type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_float_custom_property(self, key: str, value: float):\n  \"\"\"Sets a custom property of float type.\"\"\"\n  self._artifact.custom_properties[key].double_value = builtins.float(value)\n</code></pre> <code></code> set_int_custom_property \u00b6 <pre><code>set_int_custom_property(key: str, value: int)\n</code></pre> <p>Set a custom property of int type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_int_custom_property(self, key: str, value: int):\n  \"\"\"Set a custom property of int type.\"\"\"\n  self._artifact.custom_properties[key].int_value = builtins.int(value)\n</code></pre> <code></code> set_json_value_custom_property \u00b6 <pre><code>set_json_value_custom_property(key: str, value: JsonValueType)\n</code></pre> <p>Sets a custom property of JSON type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_json_value_custom_property(self, key: str, value: JsonValueType):\n  \"\"\"Sets a custom property of JSON type.\"\"\"\n  self._cached_modifiable_custom_properties[key] = value\n</code></pre> <code></code> set_mlmd_artifact \u00b6 <pre><code>set_mlmd_artifact(artifact: Artifact)\n</code></pre> <p>Replace the MLMD artifact object on this artifact.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_mlmd_artifact(self, artifact: metadata_store_pb2.Artifact):\n  \"\"\"Replace the MLMD artifact object on this artifact.\"\"\"\n  if not isinstance(artifact, metadata_store_pb2.Artifact):\n    raise ValueError(\n        ('Expected instance of metadata_store_pb2.Artifact, got %s '\n         'instead.') % (artifact,))\n  self._artifact = artifact\n  self._cached_modifiable_properties = {}\n  self._cached_modifiable_custom_properties = {}\n</code></pre> <code></code> set_mlmd_artifact_type \u00b6 <pre><code>set_mlmd_artifact_type(artifact_type: ArtifactType)\n</code></pre> <p>Set entire ArtifactType in this object.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_mlmd_artifact_type(self,\n                           artifact_type: metadata_store_pb2.ArtifactType):\n  \"\"\"Set entire ArtifactType in this object.\"\"\"\n  if not isinstance(artifact_type, metadata_store_pb2.ArtifactType):\n    raise ValueError(\n        ('Expected instance of metadata_store_pb2.ArtifactType, got %s '\n         'instead.') % (artifact_type,))\n  self._artifact_type = artifact_type\n  self._artifact.type_id = artifact_type.id\n</code></pre> <code></code> set_proto_custom_property \u00b6 <pre><code>set_proto_custom_property(key: str, value: Message)\n</code></pre> <p>Sets a custom property of proto type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_proto_custom_property(self, key: str, value: message.Message):\n  \"\"\"Sets a custom property of proto type.\"\"\"\n  self._cached_modifiable_custom_properties[key] = value\n</code></pre> <code></code> set_string_custom_property \u00b6 <pre><code>set_string_custom_property(key: str, value: str)\n</code></pre> <p>Set a custom property of string type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_string_custom_property(self, key: str, value: str):\n  \"\"\"Set a custom property of string type.\"\"\"\n  self._artifact.custom_properties[key].string_value = value\n</code></pre> <code></code> to_json_dict \u00b6 <pre><code>to_json_dict() -&gt; Dict[str, Any]\n</code></pre> <p>Convert from an object to a JSON serializable dictionary.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef to_json_dict(self) -&gt; Dict[str, Any]:\n  return {\n      'artifact':\n          json.loads(\n              json_format.MessageToJson(\n                  message=self.mlmd_artifact,\n                  preserving_proto_field_name=True)),\n      'artifact_type':\n          json.loads(\n              json_format.MessageToJson(\n                  message=self._artifact_type,\n                  preserving_proto_field_name=True)),\n      '__artifact_class_module__':\n          self.__class__.__module__,\n      '__artifact_class_name__':\n          self.__class__.__name__,\n  }\n</code></pre>"},{"location":"api/v1/types/#tfx.v1.types.standard_artifacts.ExampleAnomalies","title":"ExampleAnomalies","text":"<pre><code>ExampleAnomalies(*args, **kwargs)\n</code></pre> <p>               Bases: <code>_TfxArtifact</code></p> <p>TFX first-party component artifact definition.</p> <p>Construct TFX first-party component artifact.</p> METHOD DESCRIPTION <code>copy_from</code> <p>Set uri, properties and custom properties from a given Artifact.</p> <code>from_json_dict</code> <p>Convert from dictionary data to an object.</p> <code>get_bool_custom_property</code> <p>Get a custom property of bool type.</p> <code>get_custom_property</code> <p>Gets a custom property with key. Return None if not found.</p> <code>get_float_custom_property</code> <p>Gets a custom property of float type.</p> <code>get_int_custom_property</code> <p>Get a custom property of int type.</p> <code>get_json_value_custom_property</code> <p>Get a custom property of JSON type.</p> <code>get_proto_custom_property</code> <p>Get a custom property of proto type.</p> <code>get_string_custom_property</code> <p>Get a custom property of string type.</p> <code>set_bool_custom_property</code> <p>Sets a custom property of bool type.</p> <code>set_float_custom_property</code> <p>Sets a custom property of float type.</p> <code>set_int_custom_property</code> <p>Set a custom property of int type.</p> <code>set_json_value_custom_property</code> <p>Sets a custom property of JSON type.</p> <code>set_mlmd_artifact</code> <p>Replace the MLMD artifact object on this artifact.</p> <code>set_mlmd_artifact_type</code> <p>Set entire ArtifactType in this object.</p> <code>set_proto_custom_property</code> <p>Sets a custom property of proto type.</p> <code>set_string_custom_property</code> <p>Set a custom property of string type.</p> <code>to_json_dict</code> <p>Convert from an object to a JSON serializable dictionary.</p> ATTRIBUTE DESCRIPTION <code>artifact_type</code> <p>Type of the underlying mlmd artifact.</p> <p> </p> <code>external_id</code> <p>external id of the underlying artifact.</p> <p> TYPE: <code>str</code> </p> <code>id</code> <p>Id of the underlying mlmd artifact.</p> <p> TYPE: <code>int</code> </p> <code>is_external</code> <p>Returns true if the artifact is external.</p> <p> TYPE: <code>bool</code> </p> <code>mlmd_artifact</code> <p>Underlying mlmd artifact.</p> <p> </p> <code>name</code> <p>Name of the underlying mlmd artifact.</p> <p> TYPE: <code>str</code> </p> <code>pipeline_name</code> <p>Name of the pipeline that produce the artifact.</p> <p> TYPE: <code>str</code> </p> <code>producer_component</code> <p>Producer component of the artifact.</p> <p> TYPE: <code>str</code> </p> <code>state</code> <p>State of the underlying mlmd artifact.</p> <p> TYPE: <code>str</code> </p> <code>type</code> <p>Type of the artifact.</p> <p> </p> <code>type_id</code> <p>Type id of the underlying mlmd artifact.</p> <p> TYPE: <code>int</code> </p> <code>type_name</code> <p>Type name of the underlying mlmd artifact.</p> <p> </p> <code>uri</code> <p>Artifact URI.</p> <p> TYPE: <code>str</code> </p> Source code in <code>tfx/types/standard_artifacts.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    \"\"\"Construct TFX first-party component artifact.\"\"\"\n    # TODO(b/176795331): Refactor directory structure to make it clearer that\n    # TFX-specific artifacts require the full \"tfx\" package be installed.\n    #\n    # Do not allow usage of TFX-specific artifact if only the core pipeline\n    # SDK package is installed.\n    try:\n        import setuptools # pytype: disable=import-error  # noqa: F401\n\n        # Test import only when setuptools is available.\n        try:\n            # `extensions` is not included in ml_pipelines_sdk and doesn't have any\n            # transitive import.\n            import tfx.extensions as _  # type: ignore  # noqa: F401 # pylint: disable=g-import-not-at-top\n        except ModuleNotFoundError as err:\n            # The following condition detects exactly whether only the DSL package\n            # is installed, and is bypassed when tests run in Bazel.\n            raise RuntimeError(\n                'The \"tfx\" and all dependent packages need to be '\n                \"installed to use this functionality.\"\n            ) from err\n    except ModuleNotFoundError:\n        pass\n\n    super().__init__(*args, **kwargs)\n</code></pre> Attributes\u00b6 <code></code> artifact_type <code>property</code> \u00b6 <pre><code>artifact_type\n</code></pre> <p>Type of the underlying mlmd artifact.</p> <code></code> external_id <code>property</code> \u00b6 <pre><code>external_id: str\n</code></pre> <p>external id of the underlying artifact.</p> <code></code> id <code>property</code> <code>writable</code> \u00b6 <pre><code>id: int\n</code></pre> <p>Id of the underlying mlmd artifact.</p> <code></code> is_external <code>property</code> <code>writable</code> \u00b6 <pre><code>is_external: bool\n</code></pre> <p>Returns true if the artifact is external.</p> <code></code> mlmd_artifact <code>property</code> \u00b6 <pre><code>mlmd_artifact\n</code></pre> <p>Underlying mlmd artifact.</p> <code></code> name <code>property</code> <code>writable</code> \u00b6 <pre><code>name: str\n</code></pre> <p>Name of the underlying mlmd artifact.</p> <code></code> pipeline_name <code>property</code> <code>writable</code> \u00b6 <pre><code>pipeline_name: str\n</code></pre> <p>Name of the pipeline that produce the artifact.</p> <code></code> producer_component <code>property</code> <code>writable</code> \u00b6 <pre><code>producer_component: str\n</code></pre> <p>Producer component of the artifact.</p> <code></code> state <code>property</code> <code>writable</code> \u00b6 <pre><code>state: str\n</code></pre> <p>State of the underlying mlmd artifact.</p> <code></code> type <code>property</code> \u00b6 <pre><code>type\n</code></pre> <p>Type of the artifact.</p> <code></code> type_id <code>property</code> <code>writable</code> \u00b6 <pre><code>type_id: int\n</code></pre> <p>Type id of the underlying mlmd artifact.</p> <code></code> type_name <code>property</code> \u00b6 <pre><code>type_name\n</code></pre> <p>Type name of the underlying mlmd artifact.</p> <code></code> uri <code>property</code> <code>writable</code> \u00b6 <pre><code>uri: str\n</code></pre> <p>Artifact URI.</p> Functions\u00b6 <code></code> copy_from \u00b6 <pre><code>copy_from(other: Artifact)\n</code></pre> <p>Set uri, properties and custom properties from a given Artifact.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef copy_from(self, other: 'Artifact'):\n  \"\"\"Set uri, properties and custom properties from a given Artifact.\"\"\"\n  assert self.type is other.type, (\n      'Unable to set properties from an artifact of different type: {} vs {}'\n      .format(self.type_name, other.type_name))\n  self.uri = other.uri\n  if other.artifact_type.HasField('id'):\n    self.type_id = other.artifact_type.id\n\n  self._artifact.properties.clear()\n  self._artifact.properties.MergeFrom(other._artifact.properties)  # pylint: disable=protected-access\n  self._artifact.custom_properties.clear()\n  self._artifact.custom_properties.MergeFrom(\n      other._artifact.custom_properties)  # pylint: disable=protected-access\n  self._cached_modifiable_properties = copy.deepcopy(\n      other._cached_modifiable_properties)  # pylint: disable=protected-access\n  self._cached_modifiable_custom_properties = copy.deepcopy(\n      other._cached_modifiable_custom_properties)  # pylint: disable=protected-access\n</code></pre> <code></code> from_json_dict <code>classmethod</code> \u00b6 <pre><code>from_json_dict(dict_data: Dict[str, Any]) -&gt; Any\n</code></pre> <p>Convert from dictionary data to an object.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@classmethod\n@doc_controls.do_not_doc_inheritable\ndef from_json_dict(cls, dict_data: Dict[str, Any]) -&gt; Any:\n  module_name = dict_data['__artifact_class_module__']\n  class_name = dict_data['__artifact_class_name__']\n  artifact = metadata_store_pb2.Artifact()\n  artifact_type = metadata_store_pb2.ArtifactType()\n  json_format.Parse(json.dumps(dict_data['artifact']), artifact)\n  json_format.Parse(json.dumps(dict_data['artifact_type']), artifact_type)\n\n  # First, try to resolve the specific class used for the artifact; if this\n  # is not possible, use a generic artifact.Artifact object.\n  result = None\n  try:\n    artifact_cls = getattr(importlib.import_module(module_name), class_name)\n    # If the artifact type is the base Artifact class, do not construct the\n    # object here since that constructor requires the mlmd_artifact_type\n    # argument.\n    if artifact_cls != Artifact:\n      result = artifact_cls()\n  except (AttributeError, ImportError, ValueError):\n    logging.warning((\n        'Could not load artifact class %s.%s; using fallback deserialization '\n        'for the relevant artifact. Please make sure that any artifact '\n        'classes can be imported within your container or environment.'),\n                    module_name, class_name)\n  if not result:\n    result = Artifact(mlmd_artifact_type=artifact_type)\n  result.set_mlmd_artifact_type(artifact_type)\n  result.set_mlmd_artifact(artifact)\n  return result\n</code></pre> <code></code> get_bool_custom_property \u00b6 <pre><code>get_bool_custom_property(key: str) -&gt; bool\n</code></pre> <p>Get a custom property of bool type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_bool_custom_property(self, key: str) -&gt; bool:\n  \"\"\"Get a custom property of bool type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return False\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, bool):\n    return json_value\n  return self._artifact.custom_properties[key].bool_value\n</code></pre> <code></code> get_custom_property \u00b6 <pre><code>get_custom_property(key: str) -&gt; Optional[Union[int, float, str, bool, JsonValueType]]\n</code></pre> <p>Gets a custom property with key. Return None if not found.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_custom_property(\n    self, key: str\n) -&gt; Optional[Union[int, float, str, bool, JsonValueType]]:\n  \"\"\"Gets a custom property with key. Return None if not found.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return None\n\n  json_value = self.get_json_value_custom_property(key)\n  if json_value:\n    return json_value\n\n  mlmd_value = self._artifact.custom_properties[key]\n  if mlmd_value.HasField('int_value'):\n    return mlmd_value.int_value\n  elif mlmd_value.HasField('double_value'):\n    return mlmd_value.double_value\n  elif mlmd_value.HasField('string_value'):\n    return mlmd_value.string_value\n  elif mlmd_value.HasField('bool_value'):\n    return mlmd_value.bool_value\n  return None\n</code></pre> <code></code> get_float_custom_property \u00b6 <pre><code>get_float_custom_property(key: str) -&gt; float\n</code></pre> <p>Gets a custom property of float type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_float_custom_property(self, key: str) -&gt; float:\n  \"\"\"Gets a custom property of float type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return 0.0\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, float):\n    return json_value\n  return self._artifact.custom_properties[key].double_value\n</code></pre> <code></code> get_int_custom_property \u00b6 <pre><code>get_int_custom_property(key: str) -&gt; int\n</code></pre> <p>Get a custom property of int type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_int_custom_property(self, key: str) -&gt; int:\n  \"\"\"Get a custom property of int type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return 0\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, float):\n    return int(json_value)\n  return self._artifact.custom_properties[key].int_value\n</code></pre> <code></code> get_json_value_custom_property \u00b6 <pre><code>get_json_value_custom_property(key: str) -&gt; JsonValueType\n</code></pre> <p>Get a custom property of JSON type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef get_json_value_custom_property(self, key: str) -&gt; JsonValueType:\n  \"\"\"Get a custom property of JSON type.\"\"\"\n  if key in self._cached_modifiable_custom_properties:\n    return self._cached_modifiable_custom_properties[key]\n  if (key not in self._artifact.custom_properties or\n      not self._artifact.custom_properties[key].HasField('struct_value')):\n    return None\n  value = _decode_struct_value(\n      self._artifact.custom_properties[key].struct_value)\n  # We must cache the decoded lists or dictionaries returned here so that\n  # if their recursive contents are modified, the Metadata proto message\n  # can be updated to reflect this.\n  if isinstance(value, (dict, list)):\n    self._cached_modifiable_custom_properties[key] = value\n  return value\n</code></pre> <code></code> get_proto_custom_property \u00b6 <pre><code>get_proto_custom_property(key: str) -&gt; Optional[Message]\n</code></pre> <p>Get a custom property of proto type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef get_proto_custom_property(self, key: str) -&gt; Optional[message.Message]:\n  \"\"\"Get a custom property of proto type.\"\"\"\n  if key in self._cached_modifiable_custom_properties:\n    return self._cached_modifiable_custom_properties[key]\n  if (key not in self._artifact.custom_properties or\n      not self._artifact.custom_properties[key].HasField('proto_value')):\n    return None\n  value = proto_utils.unpack_proto_any(\n      self._artifact.custom_properties[key].proto_value)\n  # We must cache the protobuf message here so that if its contents are\n  # modified, the Metadata proto message can be updated to reflect this.\n  if isinstance(value, message.Message):\n    self._cached_modifiable_custom_properties[key] = value\n  return value\n</code></pre> <code></code> get_string_custom_property \u00b6 <pre><code>get_string_custom_property(key: str) -&gt; str\n</code></pre> <p>Get a custom property of string type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_string_custom_property(self, key: str) -&gt; str:\n  \"\"\"Get a custom property of string type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return ''\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, str):\n    return json_value\n  return self._artifact.custom_properties[key].string_value\n</code></pre> <code></code> set_bool_custom_property \u00b6 <pre><code>set_bool_custom_property(key: str, value: bool)\n</code></pre> <p>Sets a custom property of bool type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_bool_custom_property(self, key: str, value: bool):\n  \"\"\"Sets a custom property of bool type.\"\"\"\n  self._artifact.custom_properties[key].bool_value = value\n</code></pre> <code></code> set_float_custom_property \u00b6 <pre><code>set_float_custom_property(key: str, value: float)\n</code></pre> <p>Sets a custom property of float type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_float_custom_property(self, key: str, value: float):\n  \"\"\"Sets a custom property of float type.\"\"\"\n  self._artifact.custom_properties[key].double_value = builtins.float(value)\n</code></pre> <code></code> set_int_custom_property \u00b6 <pre><code>set_int_custom_property(key: str, value: int)\n</code></pre> <p>Set a custom property of int type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_int_custom_property(self, key: str, value: int):\n  \"\"\"Set a custom property of int type.\"\"\"\n  self._artifact.custom_properties[key].int_value = builtins.int(value)\n</code></pre> <code></code> set_json_value_custom_property \u00b6 <pre><code>set_json_value_custom_property(key: str, value: JsonValueType)\n</code></pre> <p>Sets a custom property of JSON type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_json_value_custom_property(self, key: str, value: JsonValueType):\n  \"\"\"Sets a custom property of JSON type.\"\"\"\n  self._cached_modifiable_custom_properties[key] = value\n</code></pre> <code></code> set_mlmd_artifact \u00b6 <pre><code>set_mlmd_artifact(artifact: Artifact)\n</code></pre> <p>Replace the MLMD artifact object on this artifact.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_mlmd_artifact(self, artifact: metadata_store_pb2.Artifact):\n  \"\"\"Replace the MLMD artifact object on this artifact.\"\"\"\n  if not isinstance(artifact, metadata_store_pb2.Artifact):\n    raise ValueError(\n        ('Expected instance of metadata_store_pb2.Artifact, got %s '\n         'instead.') % (artifact,))\n  self._artifact = artifact\n  self._cached_modifiable_properties = {}\n  self._cached_modifiable_custom_properties = {}\n</code></pre> <code></code> set_mlmd_artifact_type \u00b6 <pre><code>set_mlmd_artifact_type(artifact_type: ArtifactType)\n</code></pre> <p>Set entire ArtifactType in this object.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_mlmd_artifact_type(self,\n                           artifact_type: metadata_store_pb2.ArtifactType):\n  \"\"\"Set entire ArtifactType in this object.\"\"\"\n  if not isinstance(artifact_type, metadata_store_pb2.ArtifactType):\n    raise ValueError(\n        ('Expected instance of metadata_store_pb2.ArtifactType, got %s '\n         'instead.') % (artifact_type,))\n  self._artifact_type = artifact_type\n  self._artifact.type_id = artifact_type.id\n</code></pre> <code></code> set_proto_custom_property \u00b6 <pre><code>set_proto_custom_property(key: str, value: Message)\n</code></pre> <p>Sets a custom property of proto type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_proto_custom_property(self, key: str, value: message.Message):\n  \"\"\"Sets a custom property of proto type.\"\"\"\n  self._cached_modifiable_custom_properties[key] = value\n</code></pre> <code></code> set_string_custom_property \u00b6 <pre><code>set_string_custom_property(key: str, value: str)\n</code></pre> <p>Set a custom property of string type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_string_custom_property(self, key: str, value: str):\n  \"\"\"Set a custom property of string type.\"\"\"\n  self._artifact.custom_properties[key].string_value = value\n</code></pre> <code></code> to_json_dict \u00b6 <pre><code>to_json_dict() -&gt; Dict[str, Any]\n</code></pre> <p>Convert from an object to a JSON serializable dictionary.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef to_json_dict(self) -&gt; Dict[str, Any]:\n  return {\n      'artifact':\n          json.loads(\n              json_format.MessageToJson(\n                  message=self.mlmd_artifact,\n                  preserving_proto_field_name=True)),\n      'artifact_type':\n          json.loads(\n              json_format.MessageToJson(\n                  message=self._artifact_type,\n                  preserving_proto_field_name=True)),\n      '__artifact_class_module__':\n          self.__class__.__module__,\n      '__artifact_class_name__':\n          self.__class__.__name__,\n  }\n</code></pre>"},{"location":"api/v1/types/#tfx.v1.types.standard_artifacts.ExampleStatistics","title":"ExampleStatistics","text":"<pre><code>ExampleStatistics(*args, **kwargs)\n</code></pre> <p>               Bases: <code>_TfxArtifact</code></p> <p>TFX first-party component artifact definition.</p> <p>Construct TFX first-party component artifact.</p> METHOD DESCRIPTION <code>copy_from</code> <p>Set uri, properties and custom properties from a given Artifact.</p> <code>from_json_dict</code> <p>Convert from dictionary data to an object.</p> <code>get_bool_custom_property</code> <p>Get a custom property of bool type.</p> <code>get_custom_property</code> <p>Gets a custom property with key. Return None if not found.</p> <code>get_float_custom_property</code> <p>Gets a custom property of float type.</p> <code>get_int_custom_property</code> <p>Get a custom property of int type.</p> <code>get_json_value_custom_property</code> <p>Get a custom property of JSON type.</p> <code>get_proto_custom_property</code> <p>Get a custom property of proto type.</p> <code>get_string_custom_property</code> <p>Get a custom property of string type.</p> <code>set_bool_custom_property</code> <p>Sets a custom property of bool type.</p> <code>set_float_custom_property</code> <p>Sets a custom property of float type.</p> <code>set_int_custom_property</code> <p>Set a custom property of int type.</p> <code>set_json_value_custom_property</code> <p>Sets a custom property of JSON type.</p> <code>set_mlmd_artifact</code> <p>Replace the MLMD artifact object on this artifact.</p> <code>set_mlmd_artifact_type</code> <p>Set entire ArtifactType in this object.</p> <code>set_proto_custom_property</code> <p>Sets a custom property of proto type.</p> <code>set_string_custom_property</code> <p>Set a custom property of string type.</p> <code>to_json_dict</code> <p>Convert from an object to a JSON serializable dictionary.</p> ATTRIBUTE DESCRIPTION <code>artifact_type</code> <p>Type of the underlying mlmd artifact.</p> <p> </p> <code>external_id</code> <p>external id of the underlying artifact.</p> <p> TYPE: <code>str</code> </p> <code>id</code> <p>Id of the underlying mlmd artifact.</p> <p> TYPE: <code>int</code> </p> <code>is_external</code> <p>Returns true if the artifact is external.</p> <p> TYPE: <code>bool</code> </p> <code>mlmd_artifact</code> <p>Underlying mlmd artifact.</p> <p> </p> <code>name</code> <p>Name of the underlying mlmd artifact.</p> <p> TYPE: <code>str</code> </p> <code>pipeline_name</code> <p>Name of the pipeline that produce the artifact.</p> <p> TYPE: <code>str</code> </p> <code>producer_component</code> <p>Producer component of the artifact.</p> <p> TYPE: <code>str</code> </p> <code>state</code> <p>State of the underlying mlmd artifact.</p> <p> TYPE: <code>str</code> </p> <code>type</code> <p>Type of the artifact.</p> <p> </p> <code>type_id</code> <p>Type id of the underlying mlmd artifact.</p> <p> TYPE: <code>int</code> </p> <code>type_name</code> <p>Type name of the underlying mlmd artifact.</p> <p> </p> <code>uri</code> <p>Artifact URI.</p> <p> TYPE: <code>str</code> </p> Source code in <code>tfx/types/standard_artifacts.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    \"\"\"Construct TFX first-party component artifact.\"\"\"\n    # TODO(b/176795331): Refactor directory structure to make it clearer that\n    # TFX-specific artifacts require the full \"tfx\" package be installed.\n    #\n    # Do not allow usage of TFX-specific artifact if only the core pipeline\n    # SDK package is installed.\n    try:\n        import setuptools # pytype: disable=import-error  # noqa: F401\n\n        # Test import only when setuptools is available.\n        try:\n            # `extensions` is not included in ml_pipelines_sdk and doesn't have any\n            # transitive import.\n            import tfx.extensions as _  # type: ignore  # noqa: F401 # pylint: disable=g-import-not-at-top\n        except ModuleNotFoundError as err:\n            # The following condition detects exactly whether only the DSL package\n            # is installed, and is bypassed when tests run in Bazel.\n            raise RuntimeError(\n                'The \"tfx\" and all dependent packages need to be '\n                \"installed to use this functionality.\"\n            ) from err\n    except ModuleNotFoundError:\n        pass\n\n    super().__init__(*args, **kwargs)\n</code></pre> Attributes\u00b6 <code></code> artifact_type <code>property</code> \u00b6 <pre><code>artifact_type\n</code></pre> <p>Type of the underlying mlmd artifact.</p> <code></code> external_id <code>property</code> \u00b6 <pre><code>external_id: str\n</code></pre> <p>external id of the underlying artifact.</p> <code></code> id <code>property</code> <code>writable</code> \u00b6 <pre><code>id: int\n</code></pre> <p>Id of the underlying mlmd artifact.</p> <code></code> is_external <code>property</code> <code>writable</code> \u00b6 <pre><code>is_external: bool\n</code></pre> <p>Returns true if the artifact is external.</p> <code></code> mlmd_artifact <code>property</code> \u00b6 <pre><code>mlmd_artifact\n</code></pre> <p>Underlying mlmd artifact.</p> <code></code> name <code>property</code> <code>writable</code> \u00b6 <pre><code>name: str\n</code></pre> <p>Name of the underlying mlmd artifact.</p> <code></code> pipeline_name <code>property</code> <code>writable</code> \u00b6 <pre><code>pipeline_name: str\n</code></pre> <p>Name of the pipeline that produce the artifact.</p> <code></code> producer_component <code>property</code> <code>writable</code> \u00b6 <pre><code>producer_component: str\n</code></pre> <p>Producer component of the artifact.</p> <code></code> state <code>property</code> <code>writable</code> \u00b6 <pre><code>state: str\n</code></pre> <p>State of the underlying mlmd artifact.</p> <code></code> type <code>property</code> \u00b6 <pre><code>type\n</code></pre> <p>Type of the artifact.</p> <code></code> type_id <code>property</code> <code>writable</code> \u00b6 <pre><code>type_id: int\n</code></pre> <p>Type id of the underlying mlmd artifact.</p> <code></code> type_name <code>property</code> \u00b6 <pre><code>type_name\n</code></pre> <p>Type name of the underlying mlmd artifact.</p> <code></code> uri <code>property</code> <code>writable</code> \u00b6 <pre><code>uri: str\n</code></pre> <p>Artifact URI.</p> Functions\u00b6 <code></code> copy_from \u00b6 <pre><code>copy_from(other: Artifact)\n</code></pre> <p>Set uri, properties and custom properties from a given Artifact.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef copy_from(self, other: 'Artifact'):\n  \"\"\"Set uri, properties and custom properties from a given Artifact.\"\"\"\n  assert self.type is other.type, (\n      'Unable to set properties from an artifact of different type: {} vs {}'\n      .format(self.type_name, other.type_name))\n  self.uri = other.uri\n  if other.artifact_type.HasField('id'):\n    self.type_id = other.artifact_type.id\n\n  self._artifact.properties.clear()\n  self._artifact.properties.MergeFrom(other._artifact.properties)  # pylint: disable=protected-access\n  self._artifact.custom_properties.clear()\n  self._artifact.custom_properties.MergeFrom(\n      other._artifact.custom_properties)  # pylint: disable=protected-access\n  self._cached_modifiable_properties = copy.deepcopy(\n      other._cached_modifiable_properties)  # pylint: disable=protected-access\n  self._cached_modifiable_custom_properties = copy.deepcopy(\n      other._cached_modifiable_custom_properties)  # pylint: disable=protected-access\n</code></pre> <code></code> from_json_dict <code>classmethod</code> \u00b6 <pre><code>from_json_dict(dict_data: Dict[str, Any]) -&gt; Any\n</code></pre> <p>Convert from dictionary data to an object.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@classmethod\n@doc_controls.do_not_doc_inheritable\ndef from_json_dict(cls, dict_data: Dict[str, Any]) -&gt; Any:\n  module_name = dict_data['__artifact_class_module__']\n  class_name = dict_data['__artifact_class_name__']\n  artifact = metadata_store_pb2.Artifact()\n  artifact_type = metadata_store_pb2.ArtifactType()\n  json_format.Parse(json.dumps(dict_data['artifact']), artifact)\n  json_format.Parse(json.dumps(dict_data['artifact_type']), artifact_type)\n\n  # First, try to resolve the specific class used for the artifact; if this\n  # is not possible, use a generic artifact.Artifact object.\n  result = None\n  try:\n    artifact_cls = getattr(importlib.import_module(module_name), class_name)\n    # If the artifact type is the base Artifact class, do not construct the\n    # object here since that constructor requires the mlmd_artifact_type\n    # argument.\n    if artifact_cls != Artifact:\n      result = artifact_cls()\n  except (AttributeError, ImportError, ValueError):\n    logging.warning((\n        'Could not load artifact class %s.%s; using fallback deserialization '\n        'for the relevant artifact. Please make sure that any artifact '\n        'classes can be imported within your container or environment.'),\n                    module_name, class_name)\n  if not result:\n    result = Artifact(mlmd_artifact_type=artifact_type)\n  result.set_mlmd_artifact_type(artifact_type)\n  result.set_mlmd_artifact(artifact)\n  return result\n</code></pre> <code></code> get_bool_custom_property \u00b6 <pre><code>get_bool_custom_property(key: str) -&gt; bool\n</code></pre> <p>Get a custom property of bool type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_bool_custom_property(self, key: str) -&gt; bool:\n  \"\"\"Get a custom property of bool type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return False\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, bool):\n    return json_value\n  return self._artifact.custom_properties[key].bool_value\n</code></pre> <code></code> get_custom_property \u00b6 <pre><code>get_custom_property(key: str) -&gt; Optional[Union[int, float, str, bool, JsonValueType]]\n</code></pre> <p>Gets a custom property with key. Return None if not found.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_custom_property(\n    self, key: str\n) -&gt; Optional[Union[int, float, str, bool, JsonValueType]]:\n  \"\"\"Gets a custom property with key. Return None if not found.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return None\n\n  json_value = self.get_json_value_custom_property(key)\n  if json_value:\n    return json_value\n\n  mlmd_value = self._artifact.custom_properties[key]\n  if mlmd_value.HasField('int_value'):\n    return mlmd_value.int_value\n  elif mlmd_value.HasField('double_value'):\n    return mlmd_value.double_value\n  elif mlmd_value.HasField('string_value'):\n    return mlmd_value.string_value\n  elif mlmd_value.HasField('bool_value'):\n    return mlmd_value.bool_value\n  return None\n</code></pre> <code></code> get_float_custom_property \u00b6 <pre><code>get_float_custom_property(key: str) -&gt; float\n</code></pre> <p>Gets a custom property of float type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_float_custom_property(self, key: str) -&gt; float:\n  \"\"\"Gets a custom property of float type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return 0.0\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, float):\n    return json_value\n  return self._artifact.custom_properties[key].double_value\n</code></pre> <code></code> get_int_custom_property \u00b6 <pre><code>get_int_custom_property(key: str) -&gt; int\n</code></pre> <p>Get a custom property of int type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_int_custom_property(self, key: str) -&gt; int:\n  \"\"\"Get a custom property of int type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return 0\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, float):\n    return int(json_value)\n  return self._artifact.custom_properties[key].int_value\n</code></pre> <code></code> get_json_value_custom_property \u00b6 <pre><code>get_json_value_custom_property(key: str) -&gt; JsonValueType\n</code></pre> <p>Get a custom property of JSON type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef get_json_value_custom_property(self, key: str) -&gt; JsonValueType:\n  \"\"\"Get a custom property of JSON type.\"\"\"\n  if key in self._cached_modifiable_custom_properties:\n    return self._cached_modifiable_custom_properties[key]\n  if (key not in self._artifact.custom_properties or\n      not self._artifact.custom_properties[key].HasField('struct_value')):\n    return None\n  value = _decode_struct_value(\n      self._artifact.custom_properties[key].struct_value)\n  # We must cache the decoded lists or dictionaries returned here so that\n  # if their recursive contents are modified, the Metadata proto message\n  # can be updated to reflect this.\n  if isinstance(value, (dict, list)):\n    self._cached_modifiable_custom_properties[key] = value\n  return value\n</code></pre> <code></code> get_proto_custom_property \u00b6 <pre><code>get_proto_custom_property(key: str) -&gt; Optional[Message]\n</code></pre> <p>Get a custom property of proto type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef get_proto_custom_property(self, key: str) -&gt; Optional[message.Message]:\n  \"\"\"Get a custom property of proto type.\"\"\"\n  if key in self._cached_modifiable_custom_properties:\n    return self._cached_modifiable_custom_properties[key]\n  if (key not in self._artifact.custom_properties or\n      not self._artifact.custom_properties[key].HasField('proto_value')):\n    return None\n  value = proto_utils.unpack_proto_any(\n      self._artifact.custom_properties[key].proto_value)\n  # We must cache the protobuf message here so that if its contents are\n  # modified, the Metadata proto message can be updated to reflect this.\n  if isinstance(value, message.Message):\n    self._cached_modifiable_custom_properties[key] = value\n  return value\n</code></pre> <code></code> get_string_custom_property \u00b6 <pre><code>get_string_custom_property(key: str) -&gt; str\n</code></pre> <p>Get a custom property of string type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_string_custom_property(self, key: str) -&gt; str:\n  \"\"\"Get a custom property of string type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return ''\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, str):\n    return json_value\n  return self._artifact.custom_properties[key].string_value\n</code></pre> <code></code> set_bool_custom_property \u00b6 <pre><code>set_bool_custom_property(key: str, value: bool)\n</code></pre> <p>Sets a custom property of bool type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_bool_custom_property(self, key: str, value: bool):\n  \"\"\"Sets a custom property of bool type.\"\"\"\n  self._artifact.custom_properties[key].bool_value = value\n</code></pre> <code></code> set_float_custom_property \u00b6 <pre><code>set_float_custom_property(key: str, value: float)\n</code></pre> <p>Sets a custom property of float type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_float_custom_property(self, key: str, value: float):\n  \"\"\"Sets a custom property of float type.\"\"\"\n  self._artifact.custom_properties[key].double_value = builtins.float(value)\n</code></pre> <code></code> set_int_custom_property \u00b6 <pre><code>set_int_custom_property(key: str, value: int)\n</code></pre> <p>Set a custom property of int type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_int_custom_property(self, key: str, value: int):\n  \"\"\"Set a custom property of int type.\"\"\"\n  self._artifact.custom_properties[key].int_value = builtins.int(value)\n</code></pre> <code></code> set_json_value_custom_property \u00b6 <pre><code>set_json_value_custom_property(key: str, value: JsonValueType)\n</code></pre> <p>Sets a custom property of JSON type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_json_value_custom_property(self, key: str, value: JsonValueType):\n  \"\"\"Sets a custom property of JSON type.\"\"\"\n  self._cached_modifiable_custom_properties[key] = value\n</code></pre> <code></code> set_mlmd_artifact \u00b6 <pre><code>set_mlmd_artifact(artifact: Artifact)\n</code></pre> <p>Replace the MLMD artifact object on this artifact.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_mlmd_artifact(self, artifact: metadata_store_pb2.Artifact):\n  \"\"\"Replace the MLMD artifact object on this artifact.\"\"\"\n  if not isinstance(artifact, metadata_store_pb2.Artifact):\n    raise ValueError(\n        ('Expected instance of metadata_store_pb2.Artifact, got %s '\n         'instead.') % (artifact,))\n  self._artifact = artifact\n  self._cached_modifiable_properties = {}\n  self._cached_modifiable_custom_properties = {}\n</code></pre> <code></code> set_mlmd_artifact_type \u00b6 <pre><code>set_mlmd_artifact_type(artifact_type: ArtifactType)\n</code></pre> <p>Set entire ArtifactType in this object.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_mlmd_artifact_type(self,\n                           artifact_type: metadata_store_pb2.ArtifactType):\n  \"\"\"Set entire ArtifactType in this object.\"\"\"\n  if not isinstance(artifact_type, metadata_store_pb2.ArtifactType):\n    raise ValueError(\n        ('Expected instance of metadata_store_pb2.ArtifactType, got %s '\n         'instead.') % (artifact_type,))\n  self._artifact_type = artifact_type\n  self._artifact.type_id = artifact_type.id\n</code></pre> <code></code> set_proto_custom_property \u00b6 <pre><code>set_proto_custom_property(key: str, value: Message)\n</code></pre> <p>Sets a custom property of proto type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_proto_custom_property(self, key: str, value: message.Message):\n  \"\"\"Sets a custom property of proto type.\"\"\"\n  self._cached_modifiable_custom_properties[key] = value\n</code></pre> <code></code> set_string_custom_property \u00b6 <pre><code>set_string_custom_property(key: str, value: str)\n</code></pre> <p>Set a custom property of string type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_string_custom_property(self, key: str, value: str):\n  \"\"\"Set a custom property of string type.\"\"\"\n  self._artifact.custom_properties[key].string_value = value\n</code></pre> <code></code> to_json_dict \u00b6 <pre><code>to_json_dict() -&gt; Dict[str, Any]\n</code></pre> <p>Convert from an object to a JSON serializable dictionary.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef to_json_dict(self) -&gt; Dict[str, Any]:\n  return {\n      'artifact':\n          json.loads(\n              json_format.MessageToJson(\n                  message=self.mlmd_artifact,\n                  preserving_proto_field_name=True)),\n      'artifact_type':\n          json.loads(\n              json_format.MessageToJson(\n                  message=self._artifact_type,\n                  preserving_proto_field_name=True)),\n      '__artifact_class_module__':\n          self.__class__.__module__,\n      '__artifact_class_name__':\n          self.__class__.__name__,\n  }\n</code></pre>"},{"location":"api/v1/types/#tfx.v1.types.standard_artifacts.Examples","title":"Examples","text":"<pre><code>Examples(*args, **kwargs)\n</code></pre> <p>               Bases: <code>_TfxArtifact</code></p> <p>Artifact that contains the training data.</p> <p>Training data should be brought in to the TFX pipeline using components like ExampleGen. Data in Examples artifact is split and stored separately. The file and payload format must be specified as optional custom properties if not using default formats. Please see the <code>ExampleGen</code> guide to understand about span, version and splits.</p> <ul> <li>Properties:</li> <li><code>span</code>: Integer to distinguish group of Examples.</li> <li><code>version</code>: Integer to represent updated data.</li> <li> <p><code>splits</code>: A list of split names. For example, <code>[\"train\", \"test\"]</code>.</p> </li> <li> <p>File structure:</p> </li> <li> <p><code>{uri}/</code></p> <ul> <li><code>Split-{split_name1}/</code>: Files for split<ul> <li>All direct children files are recognized as the data.</li> <li>File format and payload format are determined by custom properties.</li> </ul> </li> <li><code>Split-{split_name2}/</code>: Another split...</li> </ul> </li> <li> <p>Commonly used custom properties of the Examples artifact:</p> </li> <li><code>file_format</code>: a string that represents the file format. See     tfx/components/util/tfxio_utils.py:make_tfxio for     available values.</li> <li><code>payload_format</code>: int (enum) value of the data payload format.     See tfx/proto/example_gen.proto:PayloadFormat for available formats.</li> </ul> <p>Construct TFX first-party component artifact.</p> METHOD DESCRIPTION <code>copy_from</code> <p>Set uri, properties and custom properties from a given Artifact.</p> <code>from_json_dict</code> <p>Convert from dictionary data to an object.</p> <code>get_bool_custom_property</code> <p>Get a custom property of bool type.</p> <code>get_custom_property</code> <p>Gets a custom property with key. Return None if not found.</p> <code>get_float_custom_property</code> <p>Gets a custom property of float type.</p> <code>get_int_custom_property</code> <p>Get a custom property of int type.</p> <code>get_json_value_custom_property</code> <p>Get a custom property of JSON type.</p> <code>get_proto_custom_property</code> <p>Get a custom property of proto type.</p> <code>get_string_custom_property</code> <p>Get a custom property of string type.</p> <code>path</code> <p>Path to the artifact URI's split subdirectory.</p> <code>set_bool_custom_property</code> <p>Sets a custom property of bool type.</p> <code>set_float_custom_property</code> <p>Sets a custom property of float type.</p> <code>set_int_custom_property</code> <p>Set a custom property of int type.</p> <code>set_json_value_custom_property</code> <p>Sets a custom property of JSON type.</p> <code>set_mlmd_artifact</code> <p>Replace the MLMD artifact object on this artifact.</p> <code>set_mlmd_artifact_type</code> <p>Set entire ArtifactType in this object.</p> <code>set_proto_custom_property</code> <p>Sets a custom property of proto type.</p> <code>set_string_custom_property</code> <p>Set a custom property of string type.</p> <code>to_json_dict</code> <p>Convert from an object to a JSON serializable dictionary.</p> ATTRIBUTE DESCRIPTION <code>artifact_type</code> <p>Type of the underlying mlmd artifact.</p> <p> </p> <code>external_id</code> <p>external id of the underlying artifact.</p> <p> TYPE: <code>str</code> </p> <code>id</code> <p>Id of the underlying mlmd artifact.</p> <p> TYPE: <code>int</code> </p> <code>is_external</code> <p>Returns true if the artifact is external.</p> <p> TYPE: <code>bool</code> </p> <code>mlmd_artifact</code> <p>Underlying mlmd artifact.</p> <p> </p> <code>name</code> <p>Name of the underlying mlmd artifact.</p> <p> TYPE: <code>str</code> </p> <code>pipeline_name</code> <p>Name of the pipeline that produce the artifact.</p> <p> TYPE: <code>str</code> </p> <code>producer_component</code> <p>Producer component of the artifact.</p> <p> TYPE: <code>str</code> </p> <code>state</code> <p>State of the underlying mlmd artifact.</p> <p> TYPE: <code>str</code> </p> <code>type</code> <p>Type of the artifact.</p> <p> </p> <code>type_id</code> <p>Type id of the underlying mlmd artifact.</p> <p> TYPE: <code>int</code> </p> <code>type_name</code> <p>Type name of the underlying mlmd artifact.</p> <p> </p> <code>uri</code> <p>Artifact URI.</p> <p> TYPE: <code>str</code> </p> Source code in <code>tfx/types/standard_artifacts.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    \"\"\"Construct TFX first-party component artifact.\"\"\"\n    # TODO(b/176795331): Refactor directory structure to make it clearer that\n    # TFX-specific artifacts require the full \"tfx\" package be installed.\n    #\n    # Do not allow usage of TFX-specific artifact if only the core pipeline\n    # SDK package is installed.\n    try:\n        import setuptools # pytype: disable=import-error  # noqa: F401\n\n        # Test import only when setuptools is available.\n        try:\n            # `extensions` is not included in ml_pipelines_sdk and doesn't have any\n            # transitive import.\n            import tfx.extensions as _  # type: ignore  # noqa: F401 # pylint: disable=g-import-not-at-top\n        except ModuleNotFoundError as err:\n            # The following condition detects exactly whether only the DSL package\n            # is installed, and is bypassed when tests run in Bazel.\n            raise RuntimeError(\n                'The \"tfx\" and all dependent packages need to be '\n                \"installed to use this functionality.\"\n            ) from err\n    except ModuleNotFoundError:\n        pass\n\n    super().__init__(*args, **kwargs)\n</code></pre> Attributes\u00b6 <code></code> artifact_type <code>property</code> \u00b6 <pre><code>artifact_type\n</code></pre> <p>Type of the underlying mlmd artifact.</p> <code></code> external_id <code>property</code> \u00b6 <pre><code>external_id: str\n</code></pre> <p>external id of the underlying artifact.</p> <code></code> id <code>property</code> <code>writable</code> \u00b6 <pre><code>id: int\n</code></pre> <p>Id of the underlying mlmd artifact.</p> <code></code> is_external <code>property</code> <code>writable</code> \u00b6 <pre><code>is_external: bool\n</code></pre> <p>Returns true if the artifact is external.</p> <code></code> mlmd_artifact <code>property</code> \u00b6 <pre><code>mlmd_artifact\n</code></pre> <p>Underlying mlmd artifact.</p> <code></code> name <code>property</code> <code>writable</code> \u00b6 <pre><code>name: str\n</code></pre> <p>Name of the underlying mlmd artifact.</p> <code></code> pipeline_name <code>property</code> <code>writable</code> \u00b6 <pre><code>pipeline_name: str\n</code></pre> <p>Name of the pipeline that produce the artifact.</p> <code></code> producer_component <code>property</code> <code>writable</code> \u00b6 <pre><code>producer_component: str\n</code></pre> <p>Producer component of the artifact.</p> <code></code> state <code>property</code> <code>writable</code> \u00b6 <pre><code>state: str\n</code></pre> <p>State of the underlying mlmd artifact.</p> <code></code> type <code>property</code> \u00b6 <pre><code>type\n</code></pre> <p>Type of the artifact.</p> <code></code> type_id <code>property</code> <code>writable</code> \u00b6 <pre><code>type_id: int\n</code></pre> <p>Type id of the underlying mlmd artifact.</p> <code></code> type_name <code>property</code> \u00b6 <pre><code>type_name\n</code></pre> <p>Type name of the underlying mlmd artifact.</p> <code></code> uri <code>property</code> <code>writable</code> \u00b6 <pre><code>uri: str\n</code></pre> <p>Artifact URI.</p> Functions\u00b6 <code></code> copy_from \u00b6 <pre><code>copy_from(other: Artifact)\n</code></pre> <p>Set uri, properties and custom properties from a given Artifact.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef copy_from(self, other: 'Artifact'):\n  \"\"\"Set uri, properties and custom properties from a given Artifact.\"\"\"\n  assert self.type is other.type, (\n      'Unable to set properties from an artifact of different type: {} vs {}'\n      .format(self.type_name, other.type_name))\n  self.uri = other.uri\n  if other.artifact_type.HasField('id'):\n    self.type_id = other.artifact_type.id\n\n  self._artifact.properties.clear()\n  self._artifact.properties.MergeFrom(other._artifact.properties)  # pylint: disable=protected-access\n  self._artifact.custom_properties.clear()\n  self._artifact.custom_properties.MergeFrom(\n      other._artifact.custom_properties)  # pylint: disable=protected-access\n  self._cached_modifiable_properties = copy.deepcopy(\n      other._cached_modifiable_properties)  # pylint: disable=protected-access\n  self._cached_modifiable_custom_properties = copy.deepcopy(\n      other._cached_modifiable_custom_properties)  # pylint: disable=protected-access\n</code></pre> <code></code> from_json_dict <code>classmethod</code> \u00b6 <pre><code>from_json_dict(dict_data: Dict[str, Any]) -&gt; Any\n</code></pre> <p>Convert from dictionary data to an object.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@classmethod\n@doc_controls.do_not_doc_inheritable\ndef from_json_dict(cls, dict_data: Dict[str, Any]) -&gt; Any:\n  module_name = dict_data['__artifact_class_module__']\n  class_name = dict_data['__artifact_class_name__']\n  artifact = metadata_store_pb2.Artifact()\n  artifact_type = metadata_store_pb2.ArtifactType()\n  json_format.Parse(json.dumps(dict_data['artifact']), artifact)\n  json_format.Parse(json.dumps(dict_data['artifact_type']), artifact_type)\n\n  # First, try to resolve the specific class used for the artifact; if this\n  # is not possible, use a generic artifact.Artifact object.\n  result = None\n  try:\n    artifact_cls = getattr(importlib.import_module(module_name), class_name)\n    # If the artifact type is the base Artifact class, do not construct the\n    # object here since that constructor requires the mlmd_artifact_type\n    # argument.\n    if artifact_cls != Artifact:\n      result = artifact_cls()\n  except (AttributeError, ImportError, ValueError):\n    logging.warning((\n        'Could not load artifact class %s.%s; using fallback deserialization '\n        'for the relevant artifact. Please make sure that any artifact '\n        'classes can be imported within your container or environment.'),\n                    module_name, class_name)\n  if not result:\n    result = Artifact(mlmd_artifact_type=artifact_type)\n  result.set_mlmd_artifact_type(artifact_type)\n  result.set_mlmd_artifact(artifact)\n  return result\n</code></pre> <code></code> get_bool_custom_property \u00b6 <pre><code>get_bool_custom_property(key: str) -&gt; bool\n</code></pre> <p>Get a custom property of bool type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_bool_custom_property(self, key: str) -&gt; bool:\n  \"\"\"Get a custom property of bool type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return False\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, bool):\n    return json_value\n  return self._artifact.custom_properties[key].bool_value\n</code></pre> <code></code> get_custom_property \u00b6 <pre><code>get_custom_property(key: str) -&gt; Optional[Union[int, float, str, bool, JsonValueType]]\n</code></pre> <p>Gets a custom property with key. Return None if not found.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_custom_property(\n    self, key: str\n) -&gt; Optional[Union[int, float, str, bool, JsonValueType]]:\n  \"\"\"Gets a custom property with key. Return None if not found.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return None\n\n  json_value = self.get_json_value_custom_property(key)\n  if json_value:\n    return json_value\n\n  mlmd_value = self._artifact.custom_properties[key]\n  if mlmd_value.HasField('int_value'):\n    return mlmd_value.int_value\n  elif mlmd_value.HasField('double_value'):\n    return mlmd_value.double_value\n  elif mlmd_value.HasField('string_value'):\n    return mlmd_value.string_value\n  elif mlmd_value.HasField('bool_value'):\n    return mlmd_value.bool_value\n  return None\n</code></pre> <code></code> get_float_custom_property \u00b6 <pre><code>get_float_custom_property(key: str) -&gt; float\n</code></pre> <p>Gets a custom property of float type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_float_custom_property(self, key: str) -&gt; float:\n  \"\"\"Gets a custom property of float type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return 0.0\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, float):\n    return json_value\n  return self._artifact.custom_properties[key].double_value\n</code></pre> <code></code> get_int_custom_property \u00b6 <pre><code>get_int_custom_property(key: str) -&gt; int\n</code></pre> <p>Get a custom property of int type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_int_custom_property(self, key: str) -&gt; int:\n  \"\"\"Get a custom property of int type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return 0\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, float):\n    return int(json_value)\n  return self._artifact.custom_properties[key].int_value\n</code></pre> <code></code> get_json_value_custom_property \u00b6 <pre><code>get_json_value_custom_property(key: str) -&gt; JsonValueType\n</code></pre> <p>Get a custom property of JSON type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef get_json_value_custom_property(self, key: str) -&gt; JsonValueType:\n  \"\"\"Get a custom property of JSON type.\"\"\"\n  if key in self._cached_modifiable_custom_properties:\n    return self._cached_modifiable_custom_properties[key]\n  if (key not in self._artifact.custom_properties or\n      not self._artifact.custom_properties[key].HasField('struct_value')):\n    return None\n  value = _decode_struct_value(\n      self._artifact.custom_properties[key].struct_value)\n  # We must cache the decoded lists or dictionaries returned here so that\n  # if their recursive contents are modified, the Metadata proto message\n  # can be updated to reflect this.\n  if isinstance(value, (dict, list)):\n    self._cached_modifiable_custom_properties[key] = value\n  return value\n</code></pre> <code></code> get_proto_custom_property \u00b6 <pre><code>get_proto_custom_property(key: str) -&gt; Optional[Message]\n</code></pre> <p>Get a custom property of proto type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef get_proto_custom_property(self, key: str) -&gt; Optional[message.Message]:\n  \"\"\"Get a custom property of proto type.\"\"\"\n  if key in self._cached_modifiable_custom_properties:\n    return self._cached_modifiable_custom_properties[key]\n  if (key not in self._artifact.custom_properties or\n      not self._artifact.custom_properties[key].HasField('proto_value')):\n    return None\n  value = proto_utils.unpack_proto_any(\n      self._artifact.custom_properties[key].proto_value)\n  # We must cache the protobuf message here so that if its contents are\n  # modified, the Metadata proto message can be updated to reflect this.\n  if isinstance(value, message.Message):\n    self._cached_modifiable_custom_properties[key] = value\n  return value\n</code></pre> <code></code> get_string_custom_property \u00b6 <pre><code>get_string_custom_property(key: str) -&gt; str\n</code></pre> <p>Get a custom property of string type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_string_custom_property(self, key: str) -&gt; str:\n  \"\"\"Get a custom property of string type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return ''\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, str):\n    return json_value\n  return self._artifact.custom_properties[key].string_value\n</code></pre> <code></code> path \u00b6 <pre><code>path(*, split: str) -&gt; str\n</code></pre> <p>Path to the artifact URI's split subdirectory.</p> <p>This method DOES NOT create a directory path it returns; caller must make a directory of the returned path value before writing.</p> PARAMETER DESCRIPTION <code>split</code> <p>A name of the split, e.g. <code>\"train\"</code>, <code>\"validation\"</code>, <code>\"test\"</code>.</p> <p> TYPE: <code>str</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>if the <code>split</code> is not in the <code>self.splits</code>.</p> RETURNS DESCRIPTION <code>str</code> <p>A path to <code>{self.uri}/Split-{split}</code>.</p> Source code in <code>tfx/types/standard_artifacts.py</code> <pre><code>def path(self, *, split: str) -&gt; str:\n    \"\"\"Path to the artifact URI's split subdirectory.\n\n    This method DOES NOT create a directory path it returns; caller must make\n    a directory of the returned path value before writing.\n\n    Args:\n      split: A name of the split, e.g. `\"train\"`, `\"validation\"`, `\"test\"`.\n\n    Raises:\n      ValueError: if the `split` is not in the `self.splits`.\n\n    Returns:\n      A path to `{self.uri}/Split-{split}`.\n    \"\"\"\n    if split not in self.splits:\n        raise ValueError(\n            f\"Split {split} not found in {self.splits=}. Did you forget to update\"\n            \" Examples.splits first?\"\n        )\n    return standard_artifact_utils.get_split_uris([self], split)[0]\n</code></pre> <code></code> set_bool_custom_property \u00b6 <pre><code>set_bool_custom_property(key: str, value: bool)\n</code></pre> <p>Sets a custom property of bool type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_bool_custom_property(self, key: str, value: bool):\n  \"\"\"Sets a custom property of bool type.\"\"\"\n  self._artifact.custom_properties[key].bool_value = value\n</code></pre> <code></code> set_float_custom_property \u00b6 <pre><code>set_float_custom_property(key: str, value: float)\n</code></pre> <p>Sets a custom property of float type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_float_custom_property(self, key: str, value: float):\n  \"\"\"Sets a custom property of float type.\"\"\"\n  self._artifact.custom_properties[key].double_value = builtins.float(value)\n</code></pre> <code></code> set_int_custom_property \u00b6 <pre><code>set_int_custom_property(key: str, value: int)\n</code></pre> <p>Set a custom property of int type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_int_custom_property(self, key: str, value: int):\n  \"\"\"Set a custom property of int type.\"\"\"\n  self._artifact.custom_properties[key].int_value = builtins.int(value)\n</code></pre> <code></code> set_json_value_custom_property \u00b6 <pre><code>set_json_value_custom_property(key: str, value: JsonValueType)\n</code></pre> <p>Sets a custom property of JSON type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_json_value_custom_property(self, key: str, value: JsonValueType):\n  \"\"\"Sets a custom property of JSON type.\"\"\"\n  self._cached_modifiable_custom_properties[key] = value\n</code></pre> <code></code> set_mlmd_artifact \u00b6 <pre><code>set_mlmd_artifact(artifact: Artifact)\n</code></pre> <p>Replace the MLMD artifact object on this artifact.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_mlmd_artifact(self, artifact: metadata_store_pb2.Artifact):\n  \"\"\"Replace the MLMD artifact object on this artifact.\"\"\"\n  if not isinstance(artifact, metadata_store_pb2.Artifact):\n    raise ValueError(\n        ('Expected instance of metadata_store_pb2.Artifact, got %s '\n         'instead.') % (artifact,))\n  self._artifact = artifact\n  self._cached_modifiable_properties = {}\n  self._cached_modifiable_custom_properties = {}\n</code></pre> <code></code> set_mlmd_artifact_type \u00b6 <pre><code>set_mlmd_artifact_type(artifact_type: ArtifactType)\n</code></pre> <p>Set entire ArtifactType in this object.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_mlmd_artifact_type(self,\n                           artifact_type: metadata_store_pb2.ArtifactType):\n  \"\"\"Set entire ArtifactType in this object.\"\"\"\n  if not isinstance(artifact_type, metadata_store_pb2.ArtifactType):\n    raise ValueError(\n        ('Expected instance of metadata_store_pb2.ArtifactType, got %s '\n         'instead.') % (artifact_type,))\n  self._artifact_type = artifact_type\n  self._artifact.type_id = artifact_type.id\n</code></pre> <code></code> set_proto_custom_property \u00b6 <pre><code>set_proto_custom_property(key: str, value: Message)\n</code></pre> <p>Sets a custom property of proto type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_proto_custom_property(self, key: str, value: message.Message):\n  \"\"\"Sets a custom property of proto type.\"\"\"\n  self._cached_modifiable_custom_properties[key] = value\n</code></pre> <code></code> set_string_custom_property \u00b6 <pre><code>set_string_custom_property(key: str, value: str)\n</code></pre> <p>Set a custom property of string type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_string_custom_property(self, key: str, value: str):\n  \"\"\"Set a custom property of string type.\"\"\"\n  self._artifact.custom_properties[key].string_value = value\n</code></pre> <code></code> to_json_dict \u00b6 <pre><code>to_json_dict() -&gt; Dict[str, Any]\n</code></pre> <p>Convert from an object to a JSON serializable dictionary.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef to_json_dict(self) -&gt; Dict[str, Any]:\n  return {\n      'artifact':\n          json.loads(\n              json_format.MessageToJson(\n                  message=self.mlmd_artifact,\n                  preserving_proto_field_name=True)),\n      'artifact_type':\n          json.loads(\n              json_format.MessageToJson(\n                  message=self._artifact_type,\n                  preserving_proto_field_name=True)),\n      '__artifact_class_module__':\n          self.__class__.__module__,\n      '__artifact_class_name__':\n          self.__class__.__name__,\n  }\n</code></pre>"},{"location":"api/v1/types/#tfx.v1.types.standard_artifacts.Float","title":"Float","text":"<pre><code>Float(*args, **kwargs)\n</code></pre> <p>               Bases: <code>ValueArtifact</code></p> <p>Float-typed artifact.</p> <p>Float value artifacts are encoded using Python str() class. However, Nan and Infinity are handled separately. See string constants in the class.</p> <p>Initializes ValueArtifact.</p> METHOD DESCRIPTION <code>annotate_as</code> <p>Annotate the value artifact type with a system artifact class.</p> <code>copy_from</code> <p>Set uri, properties and custom properties from a given Artifact.</p> <code>decode</code> <p>Method decoding the file content. Implemented by subclasses.</p> <code>encode</code> <p>Method encoding the file content. Implemented by subclasses.</p> <code>from_json_dict</code> <p>Convert from dictionary data to an object.</p> <code>get_bool_custom_property</code> <p>Get a custom property of bool type.</p> <code>get_custom_property</code> <p>Gets a custom property with key. Return None if not found.</p> <code>get_float_custom_property</code> <p>Gets a custom property of float type.</p> <code>get_int_custom_property</code> <p>Get a custom property of int type.</p> <code>get_json_value_custom_property</code> <p>Get a custom property of JSON type.</p> <code>get_proto_custom_property</code> <p>Get a custom property of proto type.</p> <code>get_string_custom_property</code> <p>Get a custom property of string type.</p> <code>set_bool_custom_property</code> <p>Sets a custom property of bool type.</p> <code>set_float_custom_property</code> <p>Sets a custom property of float type.</p> <code>set_int_custom_property</code> <p>Set a custom property of int type.</p> <code>set_json_value_custom_property</code> <p>Sets a custom property of JSON type.</p> <code>set_mlmd_artifact</code> <p>Replace the MLMD artifact object on this artifact.</p> <code>set_mlmd_artifact_type</code> <p>Set entire ArtifactType in this object.</p> <code>set_proto_custom_property</code> <p>Sets a custom property of proto type.</p> <code>set_string_custom_property</code> <p>Set a custom property of string type.</p> <code>to_json_dict</code> <p>Convert from an object to a JSON serializable dictionary.</p> ATTRIBUTE DESCRIPTION <code>artifact_type</code> <p>Type of the underlying mlmd artifact.</p> <p> </p> <code>external_id</code> <p>external id of the underlying artifact.</p> <p> TYPE: <code>str</code> </p> <code>id</code> <p>Id of the underlying mlmd artifact.</p> <p> TYPE: <code>int</code> </p> <code>is_external</code> <p>Returns true if the artifact is external.</p> <p> TYPE: <code>bool</code> </p> <code>mlmd_artifact</code> <p>Underlying mlmd artifact.</p> <p> </p> <code>name</code> <p>Name of the underlying mlmd artifact.</p> <p> TYPE: <code>str</code> </p> <code>pipeline_name</code> <p>Name of the pipeline that produce the artifact.</p> <p> TYPE: <code>str</code> </p> <code>producer_component</code> <p>Producer component of the artifact.</p> <p> TYPE: <code>str</code> </p> <code>state</code> <p>State of the underlying mlmd artifact.</p> <p> TYPE: <code>str</code> </p> <code>type</code> <p>Type of the artifact.</p> <p> </p> <code>type_id</code> <p>Type id of the underlying mlmd artifact.</p> <p> TYPE: <code>int</code> </p> <code>type_name</code> <p>Type name of the underlying mlmd artifact.</p> <p> </p> <code>uri</code> <p>Artifact URI.</p> <p> TYPE: <code>str</code> </p> <code>value</code> <p>Value stored in the artifact.</p> <p> </p> Source code in <code>tfx/types/value_artifact.py</code> <pre><code>def __init__(self, *args, **kwargs):\n  \"\"\"Initializes ValueArtifact.\"\"\"\n  self._has_value = False\n  self._modified = False\n  self._value = None\n  super().__init__(*args, **kwargs)\n</code></pre> Attributes\u00b6 <code></code> artifact_type <code>property</code> \u00b6 <pre><code>artifact_type\n</code></pre> <p>Type of the underlying mlmd artifact.</p> <code></code> external_id <code>property</code> \u00b6 <pre><code>external_id: str\n</code></pre> <p>external id of the underlying artifact.</p> <code></code> id <code>property</code> <code>writable</code> \u00b6 <pre><code>id: int\n</code></pre> <p>Id of the underlying mlmd artifact.</p> <code></code> is_external <code>property</code> <code>writable</code> \u00b6 <pre><code>is_external: bool\n</code></pre> <p>Returns true if the artifact is external.</p> <code></code> mlmd_artifact <code>property</code> \u00b6 <pre><code>mlmd_artifact\n</code></pre> <p>Underlying mlmd artifact.</p> <code></code> name <code>property</code> <code>writable</code> \u00b6 <pre><code>name: str\n</code></pre> <p>Name of the underlying mlmd artifact.</p> <code></code> pipeline_name <code>property</code> <code>writable</code> \u00b6 <pre><code>pipeline_name: str\n</code></pre> <p>Name of the pipeline that produce the artifact.</p> <code></code> producer_component <code>property</code> <code>writable</code> \u00b6 <pre><code>producer_component: str\n</code></pre> <p>Producer component of the artifact.</p> <code></code> state <code>property</code> <code>writable</code> \u00b6 <pre><code>state: str\n</code></pre> <p>State of the underlying mlmd artifact.</p> <code></code> type <code>property</code> \u00b6 <pre><code>type\n</code></pre> <p>Type of the artifact.</p> <code></code> type_id <code>property</code> <code>writable</code> \u00b6 <pre><code>type_id: int\n</code></pre> <p>Type id of the underlying mlmd artifact.</p> <code></code> type_name <code>property</code> \u00b6 <pre><code>type_name\n</code></pre> <p>Type name of the underlying mlmd artifact.</p> <code></code> uri <code>property</code> <code>writable</code> \u00b6 <pre><code>uri: str\n</code></pre> <p>Artifact URI.</p> <code></code> value <code>property</code> <code>writable</code> \u00b6 <pre><code>value\n</code></pre> <p>Value stored in the artifact.</p> Functions\u00b6 <code></code> annotate_as <code>classmethod</code> \u00b6 <pre><code>annotate_as(type_annotation: Optional[Type[SystemArtifact]] = None)\n</code></pre> <p>Annotate the value artifact type with a system artifact class.</p> <p>Example usage</p> <pre><code>from tfx import v1 as tfx\n\nOutputArtifact = tfx.dsl.components.OutputArtifact\nString = tfx.types.standard_artifacts.String\nModel = tfx.dsl.standard_annotations.Model\n\n\n@tfx.dsl.components.component\ndef MyTrainer(model: OutputArtifact[String.annotate_as(Model)]): ...\n</code></pre> PARAMETER DESCRIPTION <code>type_annotation</code> <p>the standard annotations used to annotate the value artifact type. The possible values are in <code>tfx.v1.dsl.standard_annotations</code>.</p> <p> TYPE: <code>Optional[Type[SystemArtifact]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <p>A subclass of the method caller class (e.g., <code>standard_artifacts.String</code>, <code>standard_artifacts.Float</code>) with TYPE_ANNOTATION attribute set to be <code>type_annotation</code>; returns the original class if<code>type_annotation</code> is None.</p> Source code in <code>tfx/types/value_artifact.py</code> <pre><code>@classmethod\ndef annotate_as(cls, type_annotation: Optional[Type[SystemArtifact]] = None):\n  \"\"\"Annotate the value artifact type with a system artifact class.\n\n  !!! example \"Example usage\"\n\n      ```python\n      from tfx import v1 as tfx\n\n      OutputArtifact = tfx.dsl.components.OutputArtifact\n      String = tfx.types.standard_artifacts.String\n      Model = tfx.dsl.standard_annotations.Model\n\n\n      @tfx.dsl.components.component\n      def MyTrainer(model: OutputArtifact[String.annotate_as(Model)]): ...\n      ```\n\n  Args:\n    type_annotation: the standard annotations used to annotate the value\n      artifact type. The possible values are in\n      `tfx.v1.dsl.standard_annotations`.\n\n  Returns:\n    A subclass of the method caller class (e.g., [`standard_artifacts.String`][tfx.v1.types.standard_artifacts.String],\n      [`standard_artifacts.Float`][tfx.v1.types.standard_artifacts.Float]) with TYPE_ANNOTATION attribute set to be\n      `type_annotation`; returns the original class if`type_annotation` is None.\n  \"\"\"\n  if not type_annotation:\n    return cls\n  if not issubclass(type_annotation, SystemArtifact):\n    raise ValueError(\n        'type_annotation %s is not a subclass of SystemArtifact.' %\n        type_annotation)\n  type_annotation_str = str(type_annotation.__name__)\n  return type(\n      str(cls.__name__) + '_' + type_annotation_str,\n      (cls,),\n      {\n          'TYPE_NAME': str(cls.TYPE_NAME) + '_' + type_annotation_str,\n          'TYPE_ANNOTATION': type_annotation,\n          '__module__': cls.__module__,\n      },\n  )\n</code></pre> <code></code> copy_from \u00b6 <pre><code>copy_from(other: Artifact)\n</code></pre> <p>Set uri, properties and custom properties from a given Artifact.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef copy_from(self, other: 'Artifact'):\n  \"\"\"Set uri, properties and custom properties from a given Artifact.\"\"\"\n  assert self.type is other.type, (\n      'Unable to set properties from an artifact of different type: {} vs {}'\n      .format(self.type_name, other.type_name))\n  self.uri = other.uri\n  if other.artifact_type.HasField('id'):\n    self.type_id = other.artifact_type.id\n\n  self._artifact.properties.clear()\n  self._artifact.properties.MergeFrom(other._artifact.properties)  # pylint: disable=protected-access\n  self._artifact.custom_properties.clear()\n  self._artifact.custom_properties.MergeFrom(\n      other._artifact.custom_properties)  # pylint: disable=protected-access\n  self._cached_modifiable_properties = copy.deepcopy(\n      other._cached_modifiable_properties)  # pylint: disable=protected-access\n  self._cached_modifiable_custom_properties = copy.deepcopy(\n      other._cached_modifiable_custom_properties)  # pylint: disable=protected-access\n</code></pre> <code></code> decode \u00b6 <pre><code>decode(serialized_value: bytes) -&gt; float\n</code></pre> <p>Method decoding the file content. Implemented by subclasses.</p> Source code in <code>tfx/types/standard_artifacts.py</code> <pre><code>def decode(self, serialized_value: bytes) -&gt; float:\n    result = float(serialized_value)\n\n    # Check that the decoded value exactly matches the encoded string.\n    # Note that float() can handle bytes, but Decimal() cannot.\n    serialized_string = serialized_value.decode(\"utf-8\")\n    reserialized_string = str(result)\n    is_exact = decimal.Decimal(serialized_string) == decimal.Decimal(\n        reserialized_string\n    )\n    if not is_exact:\n        logging.warning(\n            'The number \"%s\" has lost precision when converted to float \"%s\"',\n            serialized_value,\n            reserialized_string,\n        )\n\n    return result\n</code></pre> <code></code> encode \u00b6 <pre><code>encode(value: float) -&gt; bytes\n</code></pre> <p>Method encoding the file content. Implemented by subclasses.</p> Source code in <code>tfx/types/standard_artifacts.py</code> <pre><code>def encode(self, value: float) -&gt; bytes:\n    if not isinstance(value, float):\n        raise TypeError(\n            f\"Expecting float but got value {value} of type {type(value)}\"\n        )\n    if math.isinf(value) or math.isnan(value):\n        logging.warning(\n            '! The number \"%s\" may be unsupported by non-python components.', value\n        )\n    str_value = str(value)\n    # Special encoding for infinities and NaN to increase comatibility with\n    # other languages.\n    # Decoding works automatically.\n    if math.isinf(value):\n        if value &gt;= 0:\n            str_value = Float._ENCODED_POSITIVE_INFINITY\n        else:\n            str_value = Float._ENCODED_NEGATIVE_INFINITY\n    if math.isnan(value):\n        str_value = Float._ENCODED_NAN\n\n    return str_value.encode(\"utf-8\")\n</code></pre> <code></code> from_json_dict <code>classmethod</code> \u00b6 <pre><code>from_json_dict(dict_data: Dict[str, Any]) -&gt; Any\n</code></pre> <p>Convert from dictionary data to an object.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@classmethod\n@doc_controls.do_not_doc_inheritable\ndef from_json_dict(cls, dict_data: Dict[str, Any]) -&gt; Any:\n  module_name = dict_data['__artifact_class_module__']\n  class_name = dict_data['__artifact_class_name__']\n  artifact = metadata_store_pb2.Artifact()\n  artifact_type = metadata_store_pb2.ArtifactType()\n  json_format.Parse(json.dumps(dict_data['artifact']), artifact)\n  json_format.Parse(json.dumps(dict_data['artifact_type']), artifact_type)\n\n  # First, try to resolve the specific class used for the artifact; if this\n  # is not possible, use a generic artifact.Artifact object.\n  result = None\n  try:\n    artifact_cls = getattr(importlib.import_module(module_name), class_name)\n    # If the artifact type is the base Artifact class, do not construct the\n    # object here since that constructor requires the mlmd_artifact_type\n    # argument.\n    if artifact_cls != Artifact:\n      result = artifact_cls()\n  except (AttributeError, ImportError, ValueError):\n    logging.warning((\n        'Could not load artifact class %s.%s; using fallback deserialization '\n        'for the relevant artifact. Please make sure that any artifact '\n        'classes can be imported within your container or environment.'),\n                    module_name, class_name)\n  if not result:\n    result = Artifact(mlmd_artifact_type=artifact_type)\n  result.set_mlmd_artifact_type(artifact_type)\n  result.set_mlmd_artifact(artifact)\n  return result\n</code></pre> <code></code> get_bool_custom_property \u00b6 <pre><code>get_bool_custom_property(key: str) -&gt; bool\n</code></pre> <p>Get a custom property of bool type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_bool_custom_property(self, key: str) -&gt; bool:\n  \"\"\"Get a custom property of bool type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return False\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, bool):\n    return json_value\n  return self._artifact.custom_properties[key].bool_value\n</code></pre> <code></code> get_custom_property \u00b6 <pre><code>get_custom_property(key: str) -&gt; Optional[Union[int, float, str, bool, JsonValueType]]\n</code></pre> <p>Gets a custom property with key. Return None if not found.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_custom_property(\n    self, key: str\n) -&gt; Optional[Union[int, float, str, bool, JsonValueType]]:\n  \"\"\"Gets a custom property with key. Return None if not found.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return None\n\n  json_value = self.get_json_value_custom_property(key)\n  if json_value:\n    return json_value\n\n  mlmd_value = self._artifact.custom_properties[key]\n  if mlmd_value.HasField('int_value'):\n    return mlmd_value.int_value\n  elif mlmd_value.HasField('double_value'):\n    return mlmd_value.double_value\n  elif mlmd_value.HasField('string_value'):\n    return mlmd_value.string_value\n  elif mlmd_value.HasField('bool_value'):\n    return mlmd_value.bool_value\n  return None\n</code></pre> <code></code> get_float_custom_property \u00b6 <pre><code>get_float_custom_property(key: str) -&gt; float\n</code></pre> <p>Gets a custom property of float type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_float_custom_property(self, key: str) -&gt; float:\n  \"\"\"Gets a custom property of float type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return 0.0\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, float):\n    return json_value\n  return self._artifact.custom_properties[key].double_value\n</code></pre> <code></code> get_int_custom_property \u00b6 <pre><code>get_int_custom_property(key: str) -&gt; int\n</code></pre> <p>Get a custom property of int type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_int_custom_property(self, key: str) -&gt; int:\n  \"\"\"Get a custom property of int type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return 0\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, float):\n    return int(json_value)\n  return self._artifact.custom_properties[key].int_value\n</code></pre> <code></code> get_json_value_custom_property \u00b6 <pre><code>get_json_value_custom_property(key: str) -&gt; JsonValueType\n</code></pre> <p>Get a custom property of JSON type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef get_json_value_custom_property(self, key: str) -&gt; JsonValueType:\n  \"\"\"Get a custom property of JSON type.\"\"\"\n  if key in self._cached_modifiable_custom_properties:\n    return self._cached_modifiable_custom_properties[key]\n  if (key not in self._artifact.custom_properties or\n      not self._artifact.custom_properties[key].HasField('struct_value')):\n    return None\n  value = _decode_struct_value(\n      self._artifact.custom_properties[key].struct_value)\n  # We must cache the decoded lists or dictionaries returned here so that\n  # if their recursive contents are modified, the Metadata proto message\n  # can be updated to reflect this.\n  if isinstance(value, (dict, list)):\n    self._cached_modifiable_custom_properties[key] = value\n  return value\n</code></pre> <code></code> get_proto_custom_property \u00b6 <pre><code>get_proto_custom_property(key: str) -&gt; Optional[Message]\n</code></pre> <p>Get a custom property of proto type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef get_proto_custom_property(self, key: str) -&gt; Optional[message.Message]:\n  \"\"\"Get a custom property of proto type.\"\"\"\n  if key in self._cached_modifiable_custom_properties:\n    return self._cached_modifiable_custom_properties[key]\n  if (key not in self._artifact.custom_properties or\n      not self._artifact.custom_properties[key].HasField('proto_value')):\n    return None\n  value = proto_utils.unpack_proto_any(\n      self._artifact.custom_properties[key].proto_value)\n  # We must cache the protobuf message here so that if its contents are\n  # modified, the Metadata proto message can be updated to reflect this.\n  if isinstance(value, message.Message):\n    self._cached_modifiable_custom_properties[key] = value\n  return value\n</code></pre> <code></code> get_string_custom_property \u00b6 <pre><code>get_string_custom_property(key: str) -&gt; str\n</code></pre> <p>Get a custom property of string type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_string_custom_property(self, key: str) -&gt; str:\n  \"\"\"Get a custom property of string type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return ''\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, str):\n    return json_value\n  return self._artifact.custom_properties[key].string_value\n</code></pre> <code></code> set_bool_custom_property \u00b6 <pre><code>set_bool_custom_property(key: str, value: bool)\n</code></pre> <p>Sets a custom property of bool type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_bool_custom_property(self, key: str, value: bool):\n  \"\"\"Sets a custom property of bool type.\"\"\"\n  self._artifact.custom_properties[key].bool_value = value\n</code></pre> <code></code> set_float_custom_property \u00b6 <pre><code>set_float_custom_property(key: str, value: float)\n</code></pre> <p>Sets a custom property of float type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_float_custom_property(self, key: str, value: float):\n  \"\"\"Sets a custom property of float type.\"\"\"\n  self._artifact.custom_properties[key].double_value = builtins.float(value)\n</code></pre> <code></code> set_int_custom_property \u00b6 <pre><code>set_int_custom_property(key: str, value: int)\n</code></pre> <p>Set a custom property of int type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_int_custom_property(self, key: str, value: int):\n  \"\"\"Set a custom property of int type.\"\"\"\n  self._artifact.custom_properties[key].int_value = builtins.int(value)\n</code></pre> <code></code> set_json_value_custom_property \u00b6 <pre><code>set_json_value_custom_property(key: str, value: JsonValueType)\n</code></pre> <p>Sets a custom property of JSON type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_json_value_custom_property(self, key: str, value: JsonValueType):\n  \"\"\"Sets a custom property of JSON type.\"\"\"\n  self._cached_modifiable_custom_properties[key] = value\n</code></pre> <code></code> set_mlmd_artifact \u00b6 <pre><code>set_mlmd_artifact(artifact: Artifact)\n</code></pre> <p>Replace the MLMD artifact object on this artifact.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_mlmd_artifact(self, artifact: metadata_store_pb2.Artifact):\n  \"\"\"Replace the MLMD artifact object on this artifact.\"\"\"\n  if not isinstance(artifact, metadata_store_pb2.Artifact):\n    raise ValueError(\n        ('Expected instance of metadata_store_pb2.Artifact, got %s '\n         'instead.') % (artifact,))\n  self._artifact = artifact\n  self._cached_modifiable_properties = {}\n  self._cached_modifiable_custom_properties = {}\n</code></pre> <code></code> set_mlmd_artifact_type \u00b6 <pre><code>set_mlmd_artifact_type(artifact_type: ArtifactType)\n</code></pre> <p>Set entire ArtifactType in this object.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_mlmd_artifact_type(self,\n                           artifact_type: metadata_store_pb2.ArtifactType):\n  \"\"\"Set entire ArtifactType in this object.\"\"\"\n  if not isinstance(artifact_type, metadata_store_pb2.ArtifactType):\n    raise ValueError(\n        ('Expected instance of metadata_store_pb2.ArtifactType, got %s '\n         'instead.') % (artifact_type,))\n  self._artifact_type = artifact_type\n  self._artifact.type_id = artifact_type.id\n</code></pre> <code></code> set_proto_custom_property \u00b6 <pre><code>set_proto_custom_property(key: str, value: Message)\n</code></pre> <p>Sets a custom property of proto type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_proto_custom_property(self, key: str, value: message.Message):\n  \"\"\"Sets a custom property of proto type.\"\"\"\n  self._cached_modifiable_custom_properties[key] = value\n</code></pre> <code></code> set_string_custom_property \u00b6 <pre><code>set_string_custom_property(key: str, value: str)\n</code></pre> <p>Set a custom property of string type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_string_custom_property(self, key: str, value: str):\n  \"\"\"Set a custom property of string type.\"\"\"\n  self._artifact.custom_properties[key].string_value = value\n</code></pre> <code></code> to_json_dict \u00b6 <pre><code>to_json_dict() -&gt; Dict[str, Any]\n</code></pre> <p>Convert from an object to a JSON serializable dictionary.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef to_json_dict(self) -&gt; Dict[str, Any]:\n  return {\n      'artifact':\n          json.loads(\n              json_format.MessageToJson(\n                  message=self.mlmd_artifact,\n                  preserving_proto_field_name=True)),\n      'artifact_type':\n          json.loads(\n              json_format.MessageToJson(\n                  message=self._artifact_type,\n                  preserving_proto_field_name=True)),\n      '__artifact_class_module__':\n          self.__class__.__module__,\n      '__artifact_class_name__':\n          self.__class__.__name__,\n  }\n</code></pre>"},{"location":"api/v1/types/#tfx.v1.types.standard_artifacts.HyperParameters","title":"HyperParameters","text":"<pre><code>HyperParameters(*args, **kwargs)\n</code></pre> <p>               Bases: <code>_TfxArtifact</code></p> <p>TFX first-party component artifact definition.</p> <p>Construct TFX first-party component artifact.</p> METHOD DESCRIPTION <code>copy_from</code> <p>Set uri, properties and custom properties from a given Artifact.</p> <code>from_json_dict</code> <p>Convert from dictionary data to an object.</p> <code>get_bool_custom_property</code> <p>Get a custom property of bool type.</p> <code>get_custom_property</code> <p>Gets a custom property with key. Return None if not found.</p> <code>get_float_custom_property</code> <p>Gets a custom property of float type.</p> <code>get_int_custom_property</code> <p>Get a custom property of int type.</p> <code>get_json_value_custom_property</code> <p>Get a custom property of JSON type.</p> <code>get_proto_custom_property</code> <p>Get a custom property of proto type.</p> <code>get_string_custom_property</code> <p>Get a custom property of string type.</p> <code>set_bool_custom_property</code> <p>Sets a custom property of bool type.</p> <code>set_float_custom_property</code> <p>Sets a custom property of float type.</p> <code>set_int_custom_property</code> <p>Set a custom property of int type.</p> <code>set_json_value_custom_property</code> <p>Sets a custom property of JSON type.</p> <code>set_mlmd_artifact</code> <p>Replace the MLMD artifact object on this artifact.</p> <code>set_mlmd_artifact_type</code> <p>Set entire ArtifactType in this object.</p> <code>set_proto_custom_property</code> <p>Sets a custom property of proto type.</p> <code>set_string_custom_property</code> <p>Set a custom property of string type.</p> <code>to_json_dict</code> <p>Convert from an object to a JSON serializable dictionary.</p> ATTRIBUTE DESCRIPTION <code>artifact_type</code> <p>Type of the underlying mlmd artifact.</p> <p> </p> <code>external_id</code> <p>external id of the underlying artifact.</p> <p> TYPE: <code>str</code> </p> <code>id</code> <p>Id of the underlying mlmd artifact.</p> <p> TYPE: <code>int</code> </p> <code>is_external</code> <p>Returns true if the artifact is external.</p> <p> TYPE: <code>bool</code> </p> <code>mlmd_artifact</code> <p>Underlying mlmd artifact.</p> <p> </p> <code>name</code> <p>Name of the underlying mlmd artifact.</p> <p> TYPE: <code>str</code> </p> <code>pipeline_name</code> <p>Name of the pipeline that produce the artifact.</p> <p> TYPE: <code>str</code> </p> <code>producer_component</code> <p>Producer component of the artifact.</p> <p> TYPE: <code>str</code> </p> <code>state</code> <p>State of the underlying mlmd artifact.</p> <p> TYPE: <code>str</code> </p> <code>type</code> <p>Type of the artifact.</p> <p> </p> <code>type_id</code> <p>Type id of the underlying mlmd artifact.</p> <p> TYPE: <code>int</code> </p> <code>type_name</code> <p>Type name of the underlying mlmd artifact.</p> <p> </p> <code>uri</code> <p>Artifact URI.</p> <p> TYPE: <code>str</code> </p> Source code in <code>tfx/types/standard_artifacts.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    \"\"\"Construct TFX first-party component artifact.\"\"\"\n    # TODO(b/176795331): Refactor directory structure to make it clearer that\n    # TFX-specific artifacts require the full \"tfx\" package be installed.\n    #\n    # Do not allow usage of TFX-specific artifact if only the core pipeline\n    # SDK package is installed.\n    try:\n        import setuptools # pytype: disable=import-error  # noqa: F401\n\n        # Test import only when setuptools is available.\n        try:\n            # `extensions` is not included in ml_pipelines_sdk and doesn't have any\n            # transitive import.\n            import tfx.extensions as _  # type: ignore  # noqa: F401 # pylint: disable=g-import-not-at-top\n        except ModuleNotFoundError as err:\n            # The following condition detects exactly whether only the DSL package\n            # is installed, and is bypassed when tests run in Bazel.\n            raise RuntimeError(\n                'The \"tfx\" and all dependent packages need to be '\n                \"installed to use this functionality.\"\n            ) from err\n    except ModuleNotFoundError:\n        pass\n\n    super().__init__(*args, **kwargs)\n</code></pre> Attributes\u00b6 <code></code> artifact_type <code>property</code> \u00b6 <pre><code>artifact_type\n</code></pre> <p>Type of the underlying mlmd artifact.</p> <code></code> external_id <code>property</code> \u00b6 <pre><code>external_id: str\n</code></pre> <p>external id of the underlying artifact.</p> <code></code> id <code>property</code> <code>writable</code> \u00b6 <pre><code>id: int\n</code></pre> <p>Id of the underlying mlmd artifact.</p> <code></code> is_external <code>property</code> <code>writable</code> \u00b6 <pre><code>is_external: bool\n</code></pre> <p>Returns true if the artifact is external.</p> <code></code> mlmd_artifact <code>property</code> \u00b6 <pre><code>mlmd_artifact\n</code></pre> <p>Underlying mlmd artifact.</p> <code></code> name <code>property</code> <code>writable</code> \u00b6 <pre><code>name: str\n</code></pre> <p>Name of the underlying mlmd artifact.</p> <code></code> pipeline_name <code>property</code> <code>writable</code> \u00b6 <pre><code>pipeline_name: str\n</code></pre> <p>Name of the pipeline that produce the artifact.</p> <code></code> producer_component <code>property</code> <code>writable</code> \u00b6 <pre><code>producer_component: str\n</code></pre> <p>Producer component of the artifact.</p> <code></code> state <code>property</code> <code>writable</code> \u00b6 <pre><code>state: str\n</code></pre> <p>State of the underlying mlmd artifact.</p> <code></code> type <code>property</code> \u00b6 <pre><code>type\n</code></pre> <p>Type of the artifact.</p> <code></code> type_id <code>property</code> <code>writable</code> \u00b6 <pre><code>type_id: int\n</code></pre> <p>Type id of the underlying mlmd artifact.</p> <code></code> type_name <code>property</code> \u00b6 <pre><code>type_name\n</code></pre> <p>Type name of the underlying mlmd artifact.</p> <code></code> uri <code>property</code> <code>writable</code> \u00b6 <pre><code>uri: str\n</code></pre> <p>Artifact URI.</p> Functions\u00b6 <code></code> copy_from \u00b6 <pre><code>copy_from(other: Artifact)\n</code></pre> <p>Set uri, properties and custom properties from a given Artifact.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef copy_from(self, other: 'Artifact'):\n  \"\"\"Set uri, properties and custom properties from a given Artifact.\"\"\"\n  assert self.type is other.type, (\n      'Unable to set properties from an artifact of different type: {} vs {}'\n      .format(self.type_name, other.type_name))\n  self.uri = other.uri\n  if other.artifact_type.HasField('id'):\n    self.type_id = other.artifact_type.id\n\n  self._artifact.properties.clear()\n  self._artifact.properties.MergeFrom(other._artifact.properties)  # pylint: disable=protected-access\n  self._artifact.custom_properties.clear()\n  self._artifact.custom_properties.MergeFrom(\n      other._artifact.custom_properties)  # pylint: disable=protected-access\n  self._cached_modifiable_properties = copy.deepcopy(\n      other._cached_modifiable_properties)  # pylint: disable=protected-access\n  self._cached_modifiable_custom_properties = copy.deepcopy(\n      other._cached_modifiable_custom_properties)  # pylint: disable=protected-access\n</code></pre> <code></code> from_json_dict <code>classmethod</code> \u00b6 <pre><code>from_json_dict(dict_data: Dict[str, Any]) -&gt; Any\n</code></pre> <p>Convert from dictionary data to an object.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@classmethod\n@doc_controls.do_not_doc_inheritable\ndef from_json_dict(cls, dict_data: Dict[str, Any]) -&gt; Any:\n  module_name = dict_data['__artifact_class_module__']\n  class_name = dict_data['__artifact_class_name__']\n  artifact = metadata_store_pb2.Artifact()\n  artifact_type = metadata_store_pb2.ArtifactType()\n  json_format.Parse(json.dumps(dict_data['artifact']), artifact)\n  json_format.Parse(json.dumps(dict_data['artifact_type']), artifact_type)\n\n  # First, try to resolve the specific class used for the artifact; if this\n  # is not possible, use a generic artifact.Artifact object.\n  result = None\n  try:\n    artifact_cls = getattr(importlib.import_module(module_name), class_name)\n    # If the artifact type is the base Artifact class, do not construct the\n    # object here since that constructor requires the mlmd_artifact_type\n    # argument.\n    if artifact_cls != Artifact:\n      result = artifact_cls()\n  except (AttributeError, ImportError, ValueError):\n    logging.warning((\n        'Could not load artifact class %s.%s; using fallback deserialization '\n        'for the relevant artifact. Please make sure that any artifact '\n        'classes can be imported within your container or environment.'),\n                    module_name, class_name)\n  if not result:\n    result = Artifact(mlmd_artifact_type=artifact_type)\n  result.set_mlmd_artifact_type(artifact_type)\n  result.set_mlmd_artifact(artifact)\n  return result\n</code></pre> <code></code> get_bool_custom_property \u00b6 <pre><code>get_bool_custom_property(key: str) -&gt; bool\n</code></pre> <p>Get a custom property of bool type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_bool_custom_property(self, key: str) -&gt; bool:\n  \"\"\"Get a custom property of bool type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return False\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, bool):\n    return json_value\n  return self._artifact.custom_properties[key].bool_value\n</code></pre> <code></code> get_custom_property \u00b6 <pre><code>get_custom_property(key: str) -&gt; Optional[Union[int, float, str, bool, JsonValueType]]\n</code></pre> <p>Gets a custom property with key. Return None if not found.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_custom_property(\n    self, key: str\n) -&gt; Optional[Union[int, float, str, bool, JsonValueType]]:\n  \"\"\"Gets a custom property with key. Return None if not found.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return None\n\n  json_value = self.get_json_value_custom_property(key)\n  if json_value:\n    return json_value\n\n  mlmd_value = self._artifact.custom_properties[key]\n  if mlmd_value.HasField('int_value'):\n    return mlmd_value.int_value\n  elif mlmd_value.HasField('double_value'):\n    return mlmd_value.double_value\n  elif mlmd_value.HasField('string_value'):\n    return mlmd_value.string_value\n  elif mlmd_value.HasField('bool_value'):\n    return mlmd_value.bool_value\n  return None\n</code></pre> <code></code> get_float_custom_property \u00b6 <pre><code>get_float_custom_property(key: str) -&gt; float\n</code></pre> <p>Gets a custom property of float type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_float_custom_property(self, key: str) -&gt; float:\n  \"\"\"Gets a custom property of float type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return 0.0\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, float):\n    return json_value\n  return self._artifact.custom_properties[key].double_value\n</code></pre> <code></code> get_int_custom_property \u00b6 <pre><code>get_int_custom_property(key: str) -&gt; int\n</code></pre> <p>Get a custom property of int type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_int_custom_property(self, key: str) -&gt; int:\n  \"\"\"Get a custom property of int type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return 0\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, float):\n    return int(json_value)\n  return self._artifact.custom_properties[key].int_value\n</code></pre> <code></code> get_json_value_custom_property \u00b6 <pre><code>get_json_value_custom_property(key: str) -&gt; JsonValueType\n</code></pre> <p>Get a custom property of JSON type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef get_json_value_custom_property(self, key: str) -&gt; JsonValueType:\n  \"\"\"Get a custom property of JSON type.\"\"\"\n  if key in self._cached_modifiable_custom_properties:\n    return self._cached_modifiable_custom_properties[key]\n  if (key not in self._artifact.custom_properties or\n      not self._artifact.custom_properties[key].HasField('struct_value')):\n    return None\n  value = _decode_struct_value(\n      self._artifact.custom_properties[key].struct_value)\n  # We must cache the decoded lists or dictionaries returned here so that\n  # if their recursive contents are modified, the Metadata proto message\n  # can be updated to reflect this.\n  if isinstance(value, (dict, list)):\n    self._cached_modifiable_custom_properties[key] = value\n  return value\n</code></pre> <code></code> get_proto_custom_property \u00b6 <pre><code>get_proto_custom_property(key: str) -&gt; Optional[Message]\n</code></pre> <p>Get a custom property of proto type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef get_proto_custom_property(self, key: str) -&gt; Optional[message.Message]:\n  \"\"\"Get a custom property of proto type.\"\"\"\n  if key in self._cached_modifiable_custom_properties:\n    return self._cached_modifiable_custom_properties[key]\n  if (key not in self._artifact.custom_properties or\n      not self._artifact.custom_properties[key].HasField('proto_value')):\n    return None\n  value = proto_utils.unpack_proto_any(\n      self._artifact.custom_properties[key].proto_value)\n  # We must cache the protobuf message here so that if its contents are\n  # modified, the Metadata proto message can be updated to reflect this.\n  if isinstance(value, message.Message):\n    self._cached_modifiable_custom_properties[key] = value\n  return value\n</code></pre> <code></code> get_string_custom_property \u00b6 <pre><code>get_string_custom_property(key: str) -&gt; str\n</code></pre> <p>Get a custom property of string type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_string_custom_property(self, key: str) -&gt; str:\n  \"\"\"Get a custom property of string type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return ''\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, str):\n    return json_value\n  return self._artifact.custom_properties[key].string_value\n</code></pre> <code></code> set_bool_custom_property \u00b6 <pre><code>set_bool_custom_property(key: str, value: bool)\n</code></pre> <p>Sets a custom property of bool type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_bool_custom_property(self, key: str, value: bool):\n  \"\"\"Sets a custom property of bool type.\"\"\"\n  self._artifact.custom_properties[key].bool_value = value\n</code></pre> <code></code> set_float_custom_property \u00b6 <pre><code>set_float_custom_property(key: str, value: float)\n</code></pre> <p>Sets a custom property of float type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_float_custom_property(self, key: str, value: float):\n  \"\"\"Sets a custom property of float type.\"\"\"\n  self._artifact.custom_properties[key].double_value = builtins.float(value)\n</code></pre> <code></code> set_int_custom_property \u00b6 <pre><code>set_int_custom_property(key: str, value: int)\n</code></pre> <p>Set a custom property of int type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_int_custom_property(self, key: str, value: int):\n  \"\"\"Set a custom property of int type.\"\"\"\n  self._artifact.custom_properties[key].int_value = builtins.int(value)\n</code></pre> <code></code> set_json_value_custom_property \u00b6 <pre><code>set_json_value_custom_property(key: str, value: JsonValueType)\n</code></pre> <p>Sets a custom property of JSON type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_json_value_custom_property(self, key: str, value: JsonValueType):\n  \"\"\"Sets a custom property of JSON type.\"\"\"\n  self._cached_modifiable_custom_properties[key] = value\n</code></pre> <code></code> set_mlmd_artifact \u00b6 <pre><code>set_mlmd_artifact(artifact: Artifact)\n</code></pre> <p>Replace the MLMD artifact object on this artifact.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_mlmd_artifact(self, artifact: metadata_store_pb2.Artifact):\n  \"\"\"Replace the MLMD artifact object on this artifact.\"\"\"\n  if not isinstance(artifact, metadata_store_pb2.Artifact):\n    raise ValueError(\n        ('Expected instance of metadata_store_pb2.Artifact, got %s '\n         'instead.') % (artifact,))\n  self._artifact = artifact\n  self._cached_modifiable_properties = {}\n  self._cached_modifiable_custom_properties = {}\n</code></pre> <code></code> set_mlmd_artifact_type \u00b6 <pre><code>set_mlmd_artifact_type(artifact_type: ArtifactType)\n</code></pre> <p>Set entire ArtifactType in this object.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_mlmd_artifact_type(self,\n                           artifact_type: metadata_store_pb2.ArtifactType):\n  \"\"\"Set entire ArtifactType in this object.\"\"\"\n  if not isinstance(artifact_type, metadata_store_pb2.ArtifactType):\n    raise ValueError(\n        ('Expected instance of metadata_store_pb2.ArtifactType, got %s '\n         'instead.') % (artifact_type,))\n  self._artifact_type = artifact_type\n  self._artifact.type_id = artifact_type.id\n</code></pre> <code></code> set_proto_custom_property \u00b6 <pre><code>set_proto_custom_property(key: str, value: Message)\n</code></pre> <p>Sets a custom property of proto type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_proto_custom_property(self, key: str, value: message.Message):\n  \"\"\"Sets a custom property of proto type.\"\"\"\n  self._cached_modifiable_custom_properties[key] = value\n</code></pre> <code></code> set_string_custom_property \u00b6 <pre><code>set_string_custom_property(key: str, value: str)\n</code></pre> <p>Set a custom property of string type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_string_custom_property(self, key: str, value: str):\n  \"\"\"Set a custom property of string type.\"\"\"\n  self._artifact.custom_properties[key].string_value = value\n</code></pre> <code></code> to_json_dict \u00b6 <pre><code>to_json_dict() -&gt; Dict[str, Any]\n</code></pre> <p>Convert from an object to a JSON serializable dictionary.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef to_json_dict(self) -&gt; Dict[str, Any]:\n  return {\n      'artifact':\n          json.loads(\n              json_format.MessageToJson(\n                  message=self.mlmd_artifact,\n                  preserving_proto_field_name=True)),\n      'artifact_type':\n          json.loads(\n              json_format.MessageToJson(\n                  message=self._artifact_type,\n                  preserving_proto_field_name=True)),\n      '__artifact_class_module__':\n          self.__class__.__module__,\n      '__artifact_class_name__':\n          self.__class__.__name__,\n  }\n</code></pre>"},{"location":"api/v1/types/#tfx.v1.types.standard_artifacts.InferenceResult","title":"InferenceResult","text":"<pre><code>InferenceResult(*args, **kwargs)\n</code></pre> <p>               Bases: <code>_TfxArtifact</code></p> <p>TFX first-party component artifact definition.</p> <p>Construct TFX first-party component artifact.</p> METHOD DESCRIPTION <code>copy_from</code> <p>Set uri, properties and custom properties from a given Artifact.</p> <code>from_json_dict</code> <p>Convert from dictionary data to an object.</p> <code>get_bool_custom_property</code> <p>Get a custom property of bool type.</p> <code>get_custom_property</code> <p>Gets a custom property with key. Return None if not found.</p> <code>get_float_custom_property</code> <p>Gets a custom property of float type.</p> <code>get_int_custom_property</code> <p>Get a custom property of int type.</p> <code>get_json_value_custom_property</code> <p>Get a custom property of JSON type.</p> <code>get_proto_custom_property</code> <p>Get a custom property of proto type.</p> <code>get_string_custom_property</code> <p>Get a custom property of string type.</p> <code>set_bool_custom_property</code> <p>Sets a custom property of bool type.</p> <code>set_float_custom_property</code> <p>Sets a custom property of float type.</p> <code>set_int_custom_property</code> <p>Set a custom property of int type.</p> <code>set_json_value_custom_property</code> <p>Sets a custom property of JSON type.</p> <code>set_mlmd_artifact</code> <p>Replace the MLMD artifact object on this artifact.</p> <code>set_mlmd_artifact_type</code> <p>Set entire ArtifactType in this object.</p> <code>set_proto_custom_property</code> <p>Sets a custom property of proto type.</p> <code>set_string_custom_property</code> <p>Set a custom property of string type.</p> <code>to_json_dict</code> <p>Convert from an object to a JSON serializable dictionary.</p> ATTRIBUTE DESCRIPTION <code>artifact_type</code> <p>Type of the underlying mlmd artifact.</p> <p> </p> <code>external_id</code> <p>external id of the underlying artifact.</p> <p> TYPE: <code>str</code> </p> <code>id</code> <p>Id of the underlying mlmd artifact.</p> <p> TYPE: <code>int</code> </p> <code>is_external</code> <p>Returns true if the artifact is external.</p> <p> TYPE: <code>bool</code> </p> <code>mlmd_artifact</code> <p>Underlying mlmd artifact.</p> <p> </p> <code>name</code> <p>Name of the underlying mlmd artifact.</p> <p> TYPE: <code>str</code> </p> <code>pipeline_name</code> <p>Name of the pipeline that produce the artifact.</p> <p> TYPE: <code>str</code> </p> <code>producer_component</code> <p>Producer component of the artifact.</p> <p> TYPE: <code>str</code> </p> <code>state</code> <p>State of the underlying mlmd artifact.</p> <p> TYPE: <code>str</code> </p> <code>type</code> <p>Type of the artifact.</p> <p> </p> <code>type_id</code> <p>Type id of the underlying mlmd artifact.</p> <p> TYPE: <code>int</code> </p> <code>type_name</code> <p>Type name of the underlying mlmd artifact.</p> <p> </p> <code>uri</code> <p>Artifact URI.</p> <p> TYPE: <code>str</code> </p> Source code in <code>tfx/types/standard_artifacts.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    \"\"\"Construct TFX first-party component artifact.\"\"\"\n    # TODO(b/176795331): Refactor directory structure to make it clearer that\n    # TFX-specific artifacts require the full \"tfx\" package be installed.\n    #\n    # Do not allow usage of TFX-specific artifact if only the core pipeline\n    # SDK package is installed.\n    try:\n        import setuptools # pytype: disable=import-error  # noqa: F401\n\n        # Test import only when setuptools is available.\n        try:\n            # `extensions` is not included in ml_pipelines_sdk and doesn't have any\n            # transitive import.\n            import tfx.extensions as _  # type: ignore  # noqa: F401 # pylint: disable=g-import-not-at-top\n        except ModuleNotFoundError as err:\n            # The following condition detects exactly whether only the DSL package\n            # is installed, and is bypassed when tests run in Bazel.\n            raise RuntimeError(\n                'The \"tfx\" and all dependent packages need to be '\n                \"installed to use this functionality.\"\n            ) from err\n    except ModuleNotFoundError:\n        pass\n\n    super().__init__(*args, **kwargs)\n</code></pre> Attributes\u00b6 <code></code> artifact_type <code>property</code> \u00b6 <pre><code>artifact_type\n</code></pre> <p>Type of the underlying mlmd artifact.</p> <code></code> external_id <code>property</code> \u00b6 <pre><code>external_id: str\n</code></pre> <p>external id of the underlying artifact.</p> <code></code> id <code>property</code> <code>writable</code> \u00b6 <pre><code>id: int\n</code></pre> <p>Id of the underlying mlmd artifact.</p> <code></code> is_external <code>property</code> <code>writable</code> \u00b6 <pre><code>is_external: bool\n</code></pre> <p>Returns true if the artifact is external.</p> <code></code> mlmd_artifact <code>property</code> \u00b6 <pre><code>mlmd_artifact\n</code></pre> <p>Underlying mlmd artifact.</p> <code></code> name <code>property</code> <code>writable</code> \u00b6 <pre><code>name: str\n</code></pre> <p>Name of the underlying mlmd artifact.</p> <code></code> pipeline_name <code>property</code> <code>writable</code> \u00b6 <pre><code>pipeline_name: str\n</code></pre> <p>Name of the pipeline that produce the artifact.</p> <code></code> producer_component <code>property</code> <code>writable</code> \u00b6 <pre><code>producer_component: str\n</code></pre> <p>Producer component of the artifact.</p> <code></code> state <code>property</code> <code>writable</code> \u00b6 <pre><code>state: str\n</code></pre> <p>State of the underlying mlmd artifact.</p> <code></code> type <code>property</code> \u00b6 <pre><code>type\n</code></pre> <p>Type of the artifact.</p> <code></code> type_id <code>property</code> <code>writable</code> \u00b6 <pre><code>type_id: int\n</code></pre> <p>Type id of the underlying mlmd artifact.</p> <code></code> type_name <code>property</code> \u00b6 <pre><code>type_name\n</code></pre> <p>Type name of the underlying mlmd artifact.</p> <code></code> uri <code>property</code> <code>writable</code> \u00b6 <pre><code>uri: str\n</code></pre> <p>Artifact URI.</p> Functions\u00b6 <code></code> copy_from \u00b6 <pre><code>copy_from(other: Artifact)\n</code></pre> <p>Set uri, properties and custom properties from a given Artifact.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef copy_from(self, other: 'Artifact'):\n  \"\"\"Set uri, properties and custom properties from a given Artifact.\"\"\"\n  assert self.type is other.type, (\n      'Unable to set properties from an artifact of different type: {} vs {}'\n      .format(self.type_name, other.type_name))\n  self.uri = other.uri\n  if other.artifact_type.HasField('id'):\n    self.type_id = other.artifact_type.id\n\n  self._artifact.properties.clear()\n  self._artifact.properties.MergeFrom(other._artifact.properties)  # pylint: disable=protected-access\n  self._artifact.custom_properties.clear()\n  self._artifact.custom_properties.MergeFrom(\n      other._artifact.custom_properties)  # pylint: disable=protected-access\n  self._cached_modifiable_properties = copy.deepcopy(\n      other._cached_modifiable_properties)  # pylint: disable=protected-access\n  self._cached_modifiable_custom_properties = copy.deepcopy(\n      other._cached_modifiable_custom_properties)  # pylint: disable=protected-access\n</code></pre> <code></code> from_json_dict <code>classmethod</code> \u00b6 <pre><code>from_json_dict(dict_data: Dict[str, Any]) -&gt; Any\n</code></pre> <p>Convert from dictionary data to an object.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@classmethod\n@doc_controls.do_not_doc_inheritable\ndef from_json_dict(cls, dict_data: Dict[str, Any]) -&gt; Any:\n  module_name = dict_data['__artifact_class_module__']\n  class_name = dict_data['__artifact_class_name__']\n  artifact = metadata_store_pb2.Artifact()\n  artifact_type = metadata_store_pb2.ArtifactType()\n  json_format.Parse(json.dumps(dict_data['artifact']), artifact)\n  json_format.Parse(json.dumps(dict_data['artifact_type']), artifact_type)\n\n  # First, try to resolve the specific class used for the artifact; if this\n  # is not possible, use a generic artifact.Artifact object.\n  result = None\n  try:\n    artifact_cls = getattr(importlib.import_module(module_name), class_name)\n    # If the artifact type is the base Artifact class, do not construct the\n    # object here since that constructor requires the mlmd_artifact_type\n    # argument.\n    if artifact_cls != Artifact:\n      result = artifact_cls()\n  except (AttributeError, ImportError, ValueError):\n    logging.warning((\n        'Could not load artifact class %s.%s; using fallback deserialization '\n        'for the relevant artifact. Please make sure that any artifact '\n        'classes can be imported within your container or environment.'),\n                    module_name, class_name)\n  if not result:\n    result = Artifact(mlmd_artifact_type=artifact_type)\n  result.set_mlmd_artifact_type(artifact_type)\n  result.set_mlmd_artifact(artifact)\n  return result\n</code></pre> <code></code> get_bool_custom_property \u00b6 <pre><code>get_bool_custom_property(key: str) -&gt; bool\n</code></pre> <p>Get a custom property of bool type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_bool_custom_property(self, key: str) -&gt; bool:\n  \"\"\"Get a custom property of bool type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return False\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, bool):\n    return json_value\n  return self._artifact.custom_properties[key].bool_value\n</code></pre> <code></code> get_custom_property \u00b6 <pre><code>get_custom_property(key: str) -&gt; Optional[Union[int, float, str, bool, JsonValueType]]\n</code></pre> <p>Gets a custom property with key. Return None if not found.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_custom_property(\n    self, key: str\n) -&gt; Optional[Union[int, float, str, bool, JsonValueType]]:\n  \"\"\"Gets a custom property with key. Return None if not found.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return None\n\n  json_value = self.get_json_value_custom_property(key)\n  if json_value:\n    return json_value\n\n  mlmd_value = self._artifact.custom_properties[key]\n  if mlmd_value.HasField('int_value'):\n    return mlmd_value.int_value\n  elif mlmd_value.HasField('double_value'):\n    return mlmd_value.double_value\n  elif mlmd_value.HasField('string_value'):\n    return mlmd_value.string_value\n  elif mlmd_value.HasField('bool_value'):\n    return mlmd_value.bool_value\n  return None\n</code></pre> <code></code> get_float_custom_property \u00b6 <pre><code>get_float_custom_property(key: str) -&gt; float\n</code></pre> <p>Gets a custom property of float type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_float_custom_property(self, key: str) -&gt; float:\n  \"\"\"Gets a custom property of float type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return 0.0\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, float):\n    return json_value\n  return self._artifact.custom_properties[key].double_value\n</code></pre> <code></code> get_int_custom_property \u00b6 <pre><code>get_int_custom_property(key: str) -&gt; int\n</code></pre> <p>Get a custom property of int type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_int_custom_property(self, key: str) -&gt; int:\n  \"\"\"Get a custom property of int type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return 0\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, float):\n    return int(json_value)\n  return self._artifact.custom_properties[key].int_value\n</code></pre> <code></code> get_json_value_custom_property \u00b6 <pre><code>get_json_value_custom_property(key: str) -&gt; JsonValueType\n</code></pre> <p>Get a custom property of JSON type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef get_json_value_custom_property(self, key: str) -&gt; JsonValueType:\n  \"\"\"Get a custom property of JSON type.\"\"\"\n  if key in self._cached_modifiable_custom_properties:\n    return self._cached_modifiable_custom_properties[key]\n  if (key not in self._artifact.custom_properties or\n      not self._artifact.custom_properties[key].HasField('struct_value')):\n    return None\n  value = _decode_struct_value(\n      self._artifact.custom_properties[key].struct_value)\n  # We must cache the decoded lists or dictionaries returned here so that\n  # if their recursive contents are modified, the Metadata proto message\n  # can be updated to reflect this.\n  if isinstance(value, (dict, list)):\n    self._cached_modifiable_custom_properties[key] = value\n  return value\n</code></pre> <code></code> get_proto_custom_property \u00b6 <pre><code>get_proto_custom_property(key: str) -&gt; Optional[Message]\n</code></pre> <p>Get a custom property of proto type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef get_proto_custom_property(self, key: str) -&gt; Optional[message.Message]:\n  \"\"\"Get a custom property of proto type.\"\"\"\n  if key in self._cached_modifiable_custom_properties:\n    return self._cached_modifiable_custom_properties[key]\n  if (key not in self._artifact.custom_properties or\n      not self._artifact.custom_properties[key].HasField('proto_value')):\n    return None\n  value = proto_utils.unpack_proto_any(\n      self._artifact.custom_properties[key].proto_value)\n  # We must cache the protobuf message here so that if its contents are\n  # modified, the Metadata proto message can be updated to reflect this.\n  if isinstance(value, message.Message):\n    self._cached_modifiable_custom_properties[key] = value\n  return value\n</code></pre> <code></code> get_string_custom_property \u00b6 <pre><code>get_string_custom_property(key: str) -&gt; str\n</code></pre> <p>Get a custom property of string type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_string_custom_property(self, key: str) -&gt; str:\n  \"\"\"Get a custom property of string type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return ''\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, str):\n    return json_value\n  return self._artifact.custom_properties[key].string_value\n</code></pre> <code></code> set_bool_custom_property \u00b6 <pre><code>set_bool_custom_property(key: str, value: bool)\n</code></pre> <p>Sets a custom property of bool type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_bool_custom_property(self, key: str, value: bool):\n  \"\"\"Sets a custom property of bool type.\"\"\"\n  self._artifact.custom_properties[key].bool_value = value\n</code></pre> <code></code> set_float_custom_property \u00b6 <pre><code>set_float_custom_property(key: str, value: float)\n</code></pre> <p>Sets a custom property of float type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_float_custom_property(self, key: str, value: float):\n  \"\"\"Sets a custom property of float type.\"\"\"\n  self._artifact.custom_properties[key].double_value = builtins.float(value)\n</code></pre> <code></code> set_int_custom_property \u00b6 <pre><code>set_int_custom_property(key: str, value: int)\n</code></pre> <p>Set a custom property of int type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_int_custom_property(self, key: str, value: int):\n  \"\"\"Set a custom property of int type.\"\"\"\n  self._artifact.custom_properties[key].int_value = builtins.int(value)\n</code></pre> <code></code> set_json_value_custom_property \u00b6 <pre><code>set_json_value_custom_property(key: str, value: JsonValueType)\n</code></pre> <p>Sets a custom property of JSON type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_json_value_custom_property(self, key: str, value: JsonValueType):\n  \"\"\"Sets a custom property of JSON type.\"\"\"\n  self._cached_modifiable_custom_properties[key] = value\n</code></pre> <code></code> set_mlmd_artifact \u00b6 <pre><code>set_mlmd_artifact(artifact: Artifact)\n</code></pre> <p>Replace the MLMD artifact object on this artifact.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_mlmd_artifact(self, artifact: metadata_store_pb2.Artifact):\n  \"\"\"Replace the MLMD artifact object on this artifact.\"\"\"\n  if not isinstance(artifact, metadata_store_pb2.Artifact):\n    raise ValueError(\n        ('Expected instance of metadata_store_pb2.Artifact, got %s '\n         'instead.') % (artifact,))\n  self._artifact = artifact\n  self._cached_modifiable_properties = {}\n  self._cached_modifiable_custom_properties = {}\n</code></pre> <code></code> set_mlmd_artifact_type \u00b6 <pre><code>set_mlmd_artifact_type(artifact_type: ArtifactType)\n</code></pre> <p>Set entire ArtifactType in this object.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_mlmd_artifact_type(self,\n                           artifact_type: metadata_store_pb2.ArtifactType):\n  \"\"\"Set entire ArtifactType in this object.\"\"\"\n  if not isinstance(artifact_type, metadata_store_pb2.ArtifactType):\n    raise ValueError(\n        ('Expected instance of metadata_store_pb2.ArtifactType, got %s '\n         'instead.') % (artifact_type,))\n  self._artifact_type = artifact_type\n  self._artifact.type_id = artifact_type.id\n</code></pre> <code></code> set_proto_custom_property \u00b6 <pre><code>set_proto_custom_property(key: str, value: Message)\n</code></pre> <p>Sets a custom property of proto type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_proto_custom_property(self, key: str, value: message.Message):\n  \"\"\"Sets a custom property of proto type.\"\"\"\n  self._cached_modifiable_custom_properties[key] = value\n</code></pre> <code></code> set_string_custom_property \u00b6 <pre><code>set_string_custom_property(key: str, value: str)\n</code></pre> <p>Set a custom property of string type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_string_custom_property(self, key: str, value: str):\n  \"\"\"Set a custom property of string type.\"\"\"\n  self._artifact.custom_properties[key].string_value = value\n</code></pre> <code></code> to_json_dict \u00b6 <pre><code>to_json_dict() -&gt; Dict[str, Any]\n</code></pre> <p>Convert from an object to a JSON serializable dictionary.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef to_json_dict(self) -&gt; Dict[str, Any]:\n  return {\n      'artifact':\n          json.loads(\n              json_format.MessageToJson(\n                  message=self.mlmd_artifact,\n                  preserving_proto_field_name=True)),\n      'artifact_type':\n          json.loads(\n              json_format.MessageToJson(\n                  message=self._artifact_type,\n                  preserving_proto_field_name=True)),\n      '__artifact_class_module__':\n          self.__class__.__module__,\n      '__artifact_class_name__':\n          self.__class__.__name__,\n  }\n</code></pre>"},{"location":"api/v1/types/#tfx.v1.types.standard_artifacts.InfraBlessing","title":"InfraBlessing","text":"<pre><code>InfraBlessing(*args, **kwargs)\n</code></pre> <p>               Bases: <code>_TfxArtifact</code></p> <p>TFX first-party component artifact definition.</p> <p>Construct TFX first-party component artifact.</p> METHOD DESCRIPTION <code>copy_from</code> <p>Set uri, properties and custom properties from a given Artifact.</p> <code>from_json_dict</code> <p>Convert from dictionary data to an object.</p> <code>get_bool_custom_property</code> <p>Get a custom property of bool type.</p> <code>get_custom_property</code> <p>Gets a custom property with key. Return None if not found.</p> <code>get_float_custom_property</code> <p>Gets a custom property of float type.</p> <code>get_int_custom_property</code> <p>Get a custom property of int type.</p> <code>get_json_value_custom_property</code> <p>Get a custom property of JSON type.</p> <code>get_proto_custom_property</code> <p>Get a custom property of proto type.</p> <code>get_string_custom_property</code> <p>Get a custom property of string type.</p> <code>set_bool_custom_property</code> <p>Sets a custom property of bool type.</p> <code>set_float_custom_property</code> <p>Sets a custom property of float type.</p> <code>set_int_custom_property</code> <p>Set a custom property of int type.</p> <code>set_json_value_custom_property</code> <p>Sets a custom property of JSON type.</p> <code>set_mlmd_artifact</code> <p>Replace the MLMD artifact object on this artifact.</p> <code>set_mlmd_artifact_type</code> <p>Set entire ArtifactType in this object.</p> <code>set_proto_custom_property</code> <p>Sets a custom property of proto type.</p> <code>set_string_custom_property</code> <p>Set a custom property of string type.</p> <code>to_json_dict</code> <p>Convert from an object to a JSON serializable dictionary.</p> ATTRIBUTE DESCRIPTION <code>artifact_type</code> <p>Type of the underlying mlmd artifact.</p> <p> </p> <code>external_id</code> <p>external id of the underlying artifact.</p> <p> TYPE: <code>str</code> </p> <code>id</code> <p>Id of the underlying mlmd artifact.</p> <p> TYPE: <code>int</code> </p> <code>is_external</code> <p>Returns true if the artifact is external.</p> <p> TYPE: <code>bool</code> </p> <code>mlmd_artifact</code> <p>Underlying mlmd artifact.</p> <p> </p> <code>name</code> <p>Name of the underlying mlmd artifact.</p> <p> TYPE: <code>str</code> </p> <code>pipeline_name</code> <p>Name of the pipeline that produce the artifact.</p> <p> TYPE: <code>str</code> </p> <code>producer_component</code> <p>Producer component of the artifact.</p> <p> TYPE: <code>str</code> </p> <code>state</code> <p>State of the underlying mlmd artifact.</p> <p> TYPE: <code>str</code> </p> <code>type</code> <p>Type of the artifact.</p> <p> </p> <code>type_id</code> <p>Type id of the underlying mlmd artifact.</p> <p> TYPE: <code>int</code> </p> <code>type_name</code> <p>Type name of the underlying mlmd artifact.</p> <p> </p> <code>uri</code> <p>Artifact URI.</p> <p> TYPE: <code>str</code> </p> Source code in <code>tfx/types/standard_artifacts.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    \"\"\"Construct TFX first-party component artifact.\"\"\"\n    # TODO(b/176795331): Refactor directory structure to make it clearer that\n    # TFX-specific artifacts require the full \"tfx\" package be installed.\n    #\n    # Do not allow usage of TFX-specific artifact if only the core pipeline\n    # SDK package is installed.\n    try:\n        import setuptools # pytype: disable=import-error  # noqa: F401\n\n        # Test import only when setuptools is available.\n        try:\n            # `extensions` is not included in ml_pipelines_sdk and doesn't have any\n            # transitive import.\n            import tfx.extensions as _  # type: ignore  # noqa: F401 # pylint: disable=g-import-not-at-top\n        except ModuleNotFoundError as err:\n            # The following condition detects exactly whether only the DSL package\n            # is installed, and is bypassed when tests run in Bazel.\n            raise RuntimeError(\n                'The \"tfx\" and all dependent packages need to be '\n                \"installed to use this functionality.\"\n            ) from err\n    except ModuleNotFoundError:\n        pass\n\n    super().__init__(*args, **kwargs)\n</code></pre> Attributes\u00b6 <code></code> artifact_type <code>property</code> \u00b6 <pre><code>artifact_type\n</code></pre> <p>Type of the underlying mlmd artifact.</p> <code></code> external_id <code>property</code> \u00b6 <pre><code>external_id: str\n</code></pre> <p>external id of the underlying artifact.</p> <code></code> id <code>property</code> <code>writable</code> \u00b6 <pre><code>id: int\n</code></pre> <p>Id of the underlying mlmd artifact.</p> <code></code> is_external <code>property</code> <code>writable</code> \u00b6 <pre><code>is_external: bool\n</code></pre> <p>Returns true if the artifact is external.</p> <code></code> mlmd_artifact <code>property</code> \u00b6 <pre><code>mlmd_artifact\n</code></pre> <p>Underlying mlmd artifact.</p> <code></code> name <code>property</code> <code>writable</code> \u00b6 <pre><code>name: str\n</code></pre> <p>Name of the underlying mlmd artifact.</p> <code></code> pipeline_name <code>property</code> <code>writable</code> \u00b6 <pre><code>pipeline_name: str\n</code></pre> <p>Name of the pipeline that produce the artifact.</p> <code></code> producer_component <code>property</code> <code>writable</code> \u00b6 <pre><code>producer_component: str\n</code></pre> <p>Producer component of the artifact.</p> <code></code> state <code>property</code> <code>writable</code> \u00b6 <pre><code>state: str\n</code></pre> <p>State of the underlying mlmd artifact.</p> <code></code> type <code>property</code> \u00b6 <pre><code>type\n</code></pre> <p>Type of the artifact.</p> <code></code> type_id <code>property</code> <code>writable</code> \u00b6 <pre><code>type_id: int\n</code></pre> <p>Type id of the underlying mlmd artifact.</p> <code></code> type_name <code>property</code> \u00b6 <pre><code>type_name\n</code></pre> <p>Type name of the underlying mlmd artifact.</p> <code></code> uri <code>property</code> <code>writable</code> \u00b6 <pre><code>uri: str\n</code></pre> <p>Artifact URI.</p> Functions\u00b6 <code></code> copy_from \u00b6 <pre><code>copy_from(other: Artifact)\n</code></pre> <p>Set uri, properties and custom properties from a given Artifact.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef copy_from(self, other: 'Artifact'):\n  \"\"\"Set uri, properties and custom properties from a given Artifact.\"\"\"\n  assert self.type is other.type, (\n      'Unable to set properties from an artifact of different type: {} vs {}'\n      .format(self.type_name, other.type_name))\n  self.uri = other.uri\n  if other.artifact_type.HasField('id'):\n    self.type_id = other.artifact_type.id\n\n  self._artifact.properties.clear()\n  self._artifact.properties.MergeFrom(other._artifact.properties)  # pylint: disable=protected-access\n  self._artifact.custom_properties.clear()\n  self._artifact.custom_properties.MergeFrom(\n      other._artifact.custom_properties)  # pylint: disable=protected-access\n  self._cached_modifiable_properties = copy.deepcopy(\n      other._cached_modifiable_properties)  # pylint: disable=protected-access\n  self._cached_modifiable_custom_properties = copy.deepcopy(\n      other._cached_modifiable_custom_properties)  # pylint: disable=protected-access\n</code></pre> <code></code> from_json_dict <code>classmethod</code> \u00b6 <pre><code>from_json_dict(dict_data: Dict[str, Any]) -&gt; Any\n</code></pre> <p>Convert from dictionary data to an object.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@classmethod\n@doc_controls.do_not_doc_inheritable\ndef from_json_dict(cls, dict_data: Dict[str, Any]) -&gt; Any:\n  module_name = dict_data['__artifact_class_module__']\n  class_name = dict_data['__artifact_class_name__']\n  artifact = metadata_store_pb2.Artifact()\n  artifact_type = metadata_store_pb2.ArtifactType()\n  json_format.Parse(json.dumps(dict_data['artifact']), artifact)\n  json_format.Parse(json.dumps(dict_data['artifact_type']), artifact_type)\n\n  # First, try to resolve the specific class used for the artifact; if this\n  # is not possible, use a generic artifact.Artifact object.\n  result = None\n  try:\n    artifact_cls = getattr(importlib.import_module(module_name), class_name)\n    # If the artifact type is the base Artifact class, do not construct the\n    # object here since that constructor requires the mlmd_artifact_type\n    # argument.\n    if artifact_cls != Artifact:\n      result = artifact_cls()\n  except (AttributeError, ImportError, ValueError):\n    logging.warning((\n        'Could not load artifact class %s.%s; using fallback deserialization '\n        'for the relevant artifact. Please make sure that any artifact '\n        'classes can be imported within your container or environment.'),\n                    module_name, class_name)\n  if not result:\n    result = Artifact(mlmd_artifact_type=artifact_type)\n  result.set_mlmd_artifact_type(artifact_type)\n  result.set_mlmd_artifact(artifact)\n  return result\n</code></pre> <code></code> get_bool_custom_property \u00b6 <pre><code>get_bool_custom_property(key: str) -&gt; bool\n</code></pre> <p>Get a custom property of bool type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_bool_custom_property(self, key: str) -&gt; bool:\n  \"\"\"Get a custom property of bool type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return False\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, bool):\n    return json_value\n  return self._artifact.custom_properties[key].bool_value\n</code></pre> <code></code> get_custom_property \u00b6 <pre><code>get_custom_property(key: str) -&gt; Optional[Union[int, float, str, bool, JsonValueType]]\n</code></pre> <p>Gets a custom property with key. Return None if not found.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_custom_property(\n    self, key: str\n) -&gt; Optional[Union[int, float, str, bool, JsonValueType]]:\n  \"\"\"Gets a custom property with key. Return None if not found.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return None\n\n  json_value = self.get_json_value_custom_property(key)\n  if json_value:\n    return json_value\n\n  mlmd_value = self._artifact.custom_properties[key]\n  if mlmd_value.HasField('int_value'):\n    return mlmd_value.int_value\n  elif mlmd_value.HasField('double_value'):\n    return mlmd_value.double_value\n  elif mlmd_value.HasField('string_value'):\n    return mlmd_value.string_value\n  elif mlmd_value.HasField('bool_value'):\n    return mlmd_value.bool_value\n  return None\n</code></pre> <code></code> get_float_custom_property \u00b6 <pre><code>get_float_custom_property(key: str) -&gt; float\n</code></pre> <p>Gets a custom property of float type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_float_custom_property(self, key: str) -&gt; float:\n  \"\"\"Gets a custom property of float type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return 0.0\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, float):\n    return json_value\n  return self._artifact.custom_properties[key].double_value\n</code></pre> <code></code> get_int_custom_property \u00b6 <pre><code>get_int_custom_property(key: str) -&gt; int\n</code></pre> <p>Get a custom property of int type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_int_custom_property(self, key: str) -&gt; int:\n  \"\"\"Get a custom property of int type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return 0\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, float):\n    return int(json_value)\n  return self._artifact.custom_properties[key].int_value\n</code></pre> <code></code> get_json_value_custom_property \u00b6 <pre><code>get_json_value_custom_property(key: str) -&gt; JsonValueType\n</code></pre> <p>Get a custom property of JSON type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef get_json_value_custom_property(self, key: str) -&gt; JsonValueType:\n  \"\"\"Get a custom property of JSON type.\"\"\"\n  if key in self._cached_modifiable_custom_properties:\n    return self._cached_modifiable_custom_properties[key]\n  if (key not in self._artifact.custom_properties or\n      not self._artifact.custom_properties[key].HasField('struct_value')):\n    return None\n  value = _decode_struct_value(\n      self._artifact.custom_properties[key].struct_value)\n  # We must cache the decoded lists or dictionaries returned here so that\n  # if their recursive contents are modified, the Metadata proto message\n  # can be updated to reflect this.\n  if isinstance(value, (dict, list)):\n    self._cached_modifiable_custom_properties[key] = value\n  return value\n</code></pre> <code></code> get_proto_custom_property \u00b6 <pre><code>get_proto_custom_property(key: str) -&gt; Optional[Message]\n</code></pre> <p>Get a custom property of proto type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef get_proto_custom_property(self, key: str) -&gt; Optional[message.Message]:\n  \"\"\"Get a custom property of proto type.\"\"\"\n  if key in self._cached_modifiable_custom_properties:\n    return self._cached_modifiable_custom_properties[key]\n  if (key not in self._artifact.custom_properties or\n      not self._artifact.custom_properties[key].HasField('proto_value')):\n    return None\n  value = proto_utils.unpack_proto_any(\n      self._artifact.custom_properties[key].proto_value)\n  # We must cache the protobuf message here so that if its contents are\n  # modified, the Metadata proto message can be updated to reflect this.\n  if isinstance(value, message.Message):\n    self._cached_modifiable_custom_properties[key] = value\n  return value\n</code></pre> <code></code> get_string_custom_property \u00b6 <pre><code>get_string_custom_property(key: str) -&gt; str\n</code></pre> <p>Get a custom property of string type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_string_custom_property(self, key: str) -&gt; str:\n  \"\"\"Get a custom property of string type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return ''\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, str):\n    return json_value\n  return self._artifact.custom_properties[key].string_value\n</code></pre> <code></code> set_bool_custom_property \u00b6 <pre><code>set_bool_custom_property(key: str, value: bool)\n</code></pre> <p>Sets a custom property of bool type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_bool_custom_property(self, key: str, value: bool):\n  \"\"\"Sets a custom property of bool type.\"\"\"\n  self._artifact.custom_properties[key].bool_value = value\n</code></pre> <code></code> set_float_custom_property \u00b6 <pre><code>set_float_custom_property(key: str, value: float)\n</code></pre> <p>Sets a custom property of float type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_float_custom_property(self, key: str, value: float):\n  \"\"\"Sets a custom property of float type.\"\"\"\n  self._artifact.custom_properties[key].double_value = builtins.float(value)\n</code></pre> <code></code> set_int_custom_property \u00b6 <pre><code>set_int_custom_property(key: str, value: int)\n</code></pre> <p>Set a custom property of int type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_int_custom_property(self, key: str, value: int):\n  \"\"\"Set a custom property of int type.\"\"\"\n  self._artifact.custom_properties[key].int_value = builtins.int(value)\n</code></pre> <code></code> set_json_value_custom_property \u00b6 <pre><code>set_json_value_custom_property(key: str, value: JsonValueType)\n</code></pre> <p>Sets a custom property of JSON type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_json_value_custom_property(self, key: str, value: JsonValueType):\n  \"\"\"Sets a custom property of JSON type.\"\"\"\n  self._cached_modifiable_custom_properties[key] = value\n</code></pre> <code></code> set_mlmd_artifact \u00b6 <pre><code>set_mlmd_artifact(artifact: Artifact)\n</code></pre> <p>Replace the MLMD artifact object on this artifact.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_mlmd_artifact(self, artifact: metadata_store_pb2.Artifact):\n  \"\"\"Replace the MLMD artifact object on this artifact.\"\"\"\n  if not isinstance(artifact, metadata_store_pb2.Artifact):\n    raise ValueError(\n        ('Expected instance of metadata_store_pb2.Artifact, got %s '\n         'instead.') % (artifact,))\n  self._artifact = artifact\n  self._cached_modifiable_properties = {}\n  self._cached_modifiable_custom_properties = {}\n</code></pre> <code></code> set_mlmd_artifact_type \u00b6 <pre><code>set_mlmd_artifact_type(artifact_type: ArtifactType)\n</code></pre> <p>Set entire ArtifactType in this object.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_mlmd_artifact_type(self,\n                           artifact_type: metadata_store_pb2.ArtifactType):\n  \"\"\"Set entire ArtifactType in this object.\"\"\"\n  if not isinstance(artifact_type, metadata_store_pb2.ArtifactType):\n    raise ValueError(\n        ('Expected instance of metadata_store_pb2.ArtifactType, got %s '\n         'instead.') % (artifact_type,))\n  self._artifact_type = artifact_type\n  self._artifact.type_id = artifact_type.id\n</code></pre> <code></code> set_proto_custom_property \u00b6 <pre><code>set_proto_custom_property(key: str, value: Message)\n</code></pre> <p>Sets a custom property of proto type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_proto_custom_property(self, key: str, value: message.Message):\n  \"\"\"Sets a custom property of proto type.\"\"\"\n  self._cached_modifiable_custom_properties[key] = value\n</code></pre> <code></code> set_string_custom_property \u00b6 <pre><code>set_string_custom_property(key: str, value: str)\n</code></pre> <p>Set a custom property of string type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_string_custom_property(self, key: str, value: str):\n  \"\"\"Set a custom property of string type.\"\"\"\n  self._artifact.custom_properties[key].string_value = value\n</code></pre> <code></code> to_json_dict \u00b6 <pre><code>to_json_dict() -&gt; Dict[str, Any]\n</code></pre> <p>Convert from an object to a JSON serializable dictionary.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef to_json_dict(self) -&gt; Dict[str, Any]:\n  return {\n      'artifact':\n          json.loads(\n              json_format.MessageToJson(\n                  message=self.mlmd_artifact,\n                  preserving_proto_field_name=True)),\n      'artifact_type':\n          json.loads(\n              json_format.MessageToJson(\n                  message=self._artifact_type,\n                  preserving_proto_field_name=True)),\n      '__artifact_class_module__':\n          self.__class__.__module__,\n      '__artifact_class_name__':\n          self.__class__.__name__,\n  }\n</code></pre>"},{"location":"api/v1/types/#tfx.v1.types.standard_artifacts.Integer","title":"Integer","text":"<pre><code>Integer(*args, **kwargs)\n</code></pre> <p>               Bases: <code>ValueArtifact</code></p> <p>Integer-typed artifact.</p> <p>Integer value artifacts are encoded as a decimal string.</p> <p>Initializes ValueArtifact.</p> METHOD DESCRIPTION <code>annotate_as</code> <p>Annotate the value artifact type with a system artifact class.</p> <code>copy_from</code> <p>Set uri, properties and custom properties from a given Artifact.</p> <code>decode</code> <p>Method decoding the file content. Implemented by subclasses.</p> <code>encode</code> <p>Method encoding the file content. Implemented by subclasses.</p> <code>from_json_dict</code> <p>Convert from dictionary data to an object.</p> <code>get_bool_custom_property</code> <p>Get a custom property of bool type.</p> <code>get_custom_property</code> <p>Gets a custom property with key. Return None if not found.</p> <code>get_float_custom_property</code> <p>Gets a custom property of float type.</p> <code>get_int_custom_property</code> <p>Get a custom property of int type.</p> <code>get_json_value_custom_property</code> <p>Get a custom property of JSON type.</p> <code>get_proto_custom_property</code> <p>Get a custom property of proto type.</p> <code>get_string_custom_property</code> <p>Get a custom property of string type.</p> <code>set_bool_custom_property</code> <p>Sets a custom property of bool type.</p> <code>set_float_custom_property</code> <p>Sets a custom property of float type.</p> <code>set_int_custom_property</code> <p>Set a custom property of int type.</p> <code>set_json_value_custom_property</code> <p>Sets a custom property of JSON type.</p> <code>set_mlmd_artifact</code> <p>Replace the MLMD artifact object on this artifact.</p> <code>set_mlmd_artifact_type</code> <p>Set entire ArtifactType in this object.</p> <code>set_proto_custom_property</code> <p>Sets a custom property of proto type.</p> <code>set_string_custom_property</code> <p>Set a custom property of string type.</p> <code>to_json_dict</code> <p>Convert from an object to a JSON serializable dictionary.</p> ATTRIBUTE DESCRIPTION <code>artifact_type</code> <p>Type of the underlying mlmd artifact.</p> <p> </p> <code>external_id</code> <p>external id of the underlying artifact.</p> <p> TYPE: <code>str</code> </p> <code>id</code> <p>Id of the underlying mlmd artifact.</p> <p> TYPE: <code>int</code> </p> <code>is_external</code> <p>Returns true if the artifact is external.</p> <p> TYPE: <code>bool</code> </p> <code>mlmd_artifact</code> <p>Underlying mlmd artifact.</p> <p> </p> <code>name</code> <p>Name of the underlying mlmd artifact.</p> <p> TYPE: <code>str</code> </p> <code>pipeline_name</code> <p>Name of the pipeline that produce the artifact.</p> <p> TYPE: <code>str</code> </p> <code>producer_component</code> <p>Producer component of the artifact.</p> <p> TYPE: <code>str</code> </p> <code>state</code> <p>State of the underlying mlmd artifact.</p> <p> TYPE: <code>str</code> </p> <code>type</code> <p>Type of the artifact.</p> <p> </p> <code>type_id</code> <p>Type id of the underlying mlmd artifact.</p> <p> TYPE: <code>int</code> </p> <code>type_name</code> <p>Type name of the underlying mlmd artifact.</p> <p> </p> <code>uri</code> <p>Artifact URI.</p> <p> TYPE: <code>str</code> </p> <code>value</code> <p>Value stored in the artifact.</p> <p> </p> Source code in <code>tfx/types/value_artifact.py</code> <pre><code>def __init__(self, *args, **kwargs):\n  \"\"\"Initializes ValueArtifact.\"\"\"\n  self._has_value = False\n  self._modified = False\n  self._value = None\n  super().__init__(*args, **kwargs)\n</code></pre> Attributes\u00b6 <code></code> artifact_type <code>property</code> \u00b6 <pre><code>artifact_type\n</code></pre> <p>Type of the underlying mlmd artifact.</p> <code></code> external_id <code>property</code> \u00b6 <pre><code>external_id: str\n</code></pre> <p>external id of the underlying artifact.</p> <code></code> id <code>property</code> <code>writable</code> \u00b6 <pre><code>id: int\n</code></pre> <p>Id of the underlying mlmd artifact.</p> <code></code> is_external <code>property</code> <code>writable</code> \u00b6 <pre><code>is_external: bool\n</code></pre> <p>Returns true if the artifact is external.</p> <code></code> mlmd_artifact <code>property</code> \u00b6 <pre><code>mlmd_artifact\n</code></pre> <p>Underlying mlmd artifact.</p> <code></code> name <code>property</code> <code>writable</code> \u00b6 <pre><code>name: str\n</code></pre> <p>Name of the underlying mlmd artifact.</p> <code></code> pipeline_name <code>property</code> <code>writable</code> \u00b6 <pre><code>pipeline_name: str\n</code></pre> <p>Name of the pipeline that produce the artifact.</p> <code></code> producer_component <code>property</code> <code>writable</code> \u00b6 <pre><code>producer_component: str\n</code></pre> <p>Producer component of the artifact.</p> <code></code> state <code>property</code> <code>writable</code> \u00b6 <pre><code>state: str\n</code></pre> <p>State of the underlying mlmd artifact.</p> <code></code> type <code>property</code> \u00b6 <pre><code>type\n</code></pre> <p>Type of the artifact.</p> <code></code> type_id <code>property</code> <code>writable</code> \u00b6 <pre><code>type_id: int\n</code></pre> <p>Type id of the underlying mlmd artifact.</p> <code></code> type_name <code>property</code> \u00b6 <pre><code>type_name\n</code></pre> <p>Type name of the underlying mlmd artifact.</p> <code></code> uri <code>property</code> <code>writable</code> \u00b6 <pre><code>uri: str\n</code></pre> <p>Artifact URI.</p> <code></code> value <code>property</code> <code>writable</code> \u00b6 <pre><code>value\n</code></pre> <p>Value stored in the artifact.</p> Functions\u00b6 <code></code> annotate_as <code>classmethod</code> \u00b6 <pre><code>annotate_as(type_annotation: Optional[Type[SystemArtifact]] = None)\n</code></pre> <p>Annotate the value artifact type with a system artifact class.</p> <p>Example usage</p> <pre><code>from tfx import v1 as tfx\n\nOutputArtifact = tfx.dsl.components.OutputArtifact\nString = tfx.types.standard_artifacts.String\nModel = tfx.dsl.standard_annotations.Model\n\n\n@tfx.dsl.components.component\ndef MyTrainer(model: OutputArtifact[String.annotate_as(Model)]): ...\n</code></pre> PARAMETER DESCRIPTION <code>type_annotation</code> <p>the standard annotations used to annotate the value artifact type. The possible values are in <code>tfx.v1.dsl.standard_annotations</code>.</p> <p> TYPE: <code>Optional[Type[SystemArtifact]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <p>A subclass of the method caller class (e.g., <code>standard_artifacts.String</code>, <code>standard_artifacts.Float</code>) with TYPE_ANNOTATION attribute set to be <code>type_annotation</code>; returns the original class if<code>type_annotation</code> is None.</p> Source code in <code>tfx/types/value_artifact.py</code> <pre><code>@classmethod\ndef annotate_as(cls, type_annotation: Optional[Type[SystemArtifact]] = None):\n  \"\"\"Annotate the value artifact type with a system artifact class.\n\n  !!! example \"Example usage\"\n\n      ```python\n      from tfx import v1 as tfx\n\n      OutputArtifact = tfx.dsl.components.OutputArtifact\n      String = tfx.types.standard_artifacts.String\n      Model = tfx.dsl.standard_annotations.Model\n\n\n      @tfx.dsl.components.component\n      def MyTrainer(model: OutputArtifact[String.annotate_as(Model)]): ...\n      ```\n\n  Args:\n    type_annotation: the standard annotations used to annotate the value\n      artifact type. The possible values are in\n      `tfx.v1.dsl.standard_annotations`.\n\n  Returns:\n    A subclass of the method caller class (e.g., [`standard_artifacts.String`][tfx.v1.types.standard_artifacts.String],\n      [`standard_artifacts.Float`][tfx.v1.types.standard_artifacts.Float]) with TYPE_ANNOTATION attribute set to be\n      `type_annotation`; returns the original class if`type_annotation` is None.\n  \"\"\"\n  if not type_annotation:\n    return cls\n  if not issubclass(type_annotation, SystemArtifact):\n    raise ValueError(\n        'type_annotation %s is not a subclass of SystemArtifact.' %\n        type_annotation)\n  type_annotation_str = str(type_annotation.__name__)\n  return type(\n      str(cls.__name__) + '_' + type_annotation_str,\n      (cls,),\n      {\n          'TYPE_NAME': str(cls.TYPE_NAME) + '_' + type_annotation_str,\n          'TYPE_ANNOTATION': type_annotation,\n          '__module__': cls.__module__,\n      },\n  )\n</code></pre> <code></code> copy_from \u00b6 <pre><code>copy_from(other: Artifact)\n</code></pre> <p>Set uri, properties and custom properties from a given Artifact.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef copy_from(self, other: 'Artifact'):\n  \"\"\"Set uri, properties and custom properties from a given Artifact.\"\"\"\n  assert self.type is other.type, (\n      'Unable to set properties from an artifact of different type: {} vs {}'\n      .format(self.type_name, other.type_name))\n  self.uri = other.uri\n  if other.artifact_type.HasField('id'):\n    self.type_id = other.artifact_type.id\n\n  self._artifact.properties.clear()\n  self._artifact.properties.MergeFrom(other._artifact.properties)  # pylint: disable=protected-access\n  self._artifact.custom_properties.clear()\n  self._artifact.custom_properties.MergeFrom(\n      other._artifact.custom_properties)  # pylint: disable=protected-access\n  self._cached_modifiable_properties = copy.deepcopy(\n      other._cached_modifiable_properties)  # pylint: disable=protected-access\n  self._cached_modifiable_custom_properties = copy.deepcopy(\n      other._cached_modifiable_custom_properties)  # pylint: disable=protected-access\n</code></pre> <code></code> decode \u00b6 <pre><code>decode(serialized_value: bytes) -&gt; int\n</code></pre> <p>Method decoding the file content. Implemented by subclasses.</p> Source code in <code>tfx/types/standard_artifacts.py</code> <pre><code>def decode(self, serialized_value: bytes) -&gt; int:\n    return int(serialized_value)\n</code></pre> <code></code> encode \u00b6 <pre><code>encode(value: int) -&gt; bytes\n</code></pre> <p>Method encoding the file content. Implemented by subclasses.</p> Source code in <code>tfx/types/standard_artifacts.py</code> <pre><code>def encode(self, value: int) -&gt; bytes:\n    if not isinstance(value, int):\n        raise TypeError(\n            f\"Expecting int but got value {value} of type {type(value)}\"\n        )\n    return str(value).encode(\"utf-8\")\n</code></pre> <code></code> from_json_dict <code>classmethod</code> \u00b6 <pre><code>from_json_dict(dict_data: Dict[str, Any]) -&gt; Any\n</code></pre> <p>Convert from dictionary data to an object.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@classmethod\n@doc_controls.do_not_doc_inheritable\ndef from_json_dict(cls, dict_data: Dict[str, Any]) -&gt; Any:\n  module_name = dict_data['__artifact_class_module__']\n  class_name = dict_data['__artifact_class_name__']\n  artifact = metadata_store_pb2.Artifact()\n  artifact_type = metadata_store_pb2.ArtifactType()\n  json_format.Parse(json.dumps(dict_data['artifact']), artifact)\n  json_format.Parse(json.dumps(dict_data['artifact_type']), artifact_type)\n\n  # First, try to resolve the specific class used for the artifact; if this\n  # is not possible, use a generic artifact.Artifact object.\n  result = None\n  try:\n    artifact_cls = getattr(importlib.import_module(module_name), class_name)\n    # If the artifact type is the base Artifact class, do not construct the\n    # object here since that constructor requires the mlmd_artifact_type\n    # argument.\n    if artifact_cls != Artifact:\n      result = artifact_cls()\n  except (AttributeError, ImportError, ValueError):\n    logging.warning((\n        'Could not load artifact class %s.%s; using fallback deserialization '\n        'for the relevant artifact. Please make sure that any artifact '\n        'classes can be imported within your container or environment.'),\n                    module_name, class_name)\n  if not result:\n    result = Artifact(mlmd_artifact_type=artifact_type)\n  result.set_mlmd_artifact_type(artifact_type)\n  result.set_mlmd_artifact(artifact)\n  return result\n</code></pre> <code></code> get_bool_custom_property \u00b6 <pre><code>get_bool_custom_property(key: str) -&gt; bool\n</code></pre> <p>Get a custom property of bool type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_bool_custom_property(self, key: str) -&gt; bool:\n  \"\"\"Get a custom property of bool type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return False\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, bool):\n    return json_value\n  return self._artifact.custom_properties[key].bool_value\n</code></pre> <code></code> get_custom_property \u00b6 <pre><code>get_custom_property(key: str) -&gt; Optional[Union[int, float, str, bool, JsonValueType]]\n</code></pre> <p>Gets a custom property with key. Return None if not found.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_custom_property(\n    self, key: str\n) -&gt; Optional[Union[int, float, str, bool, JsonValueType]]:\n  \"\"\"Gets a custom property with key. Return None if not found.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return None\n\n  json_value = self.get_json_value_custom_property(key)\n  if json_value:\n    return json_value\n\n  mlmd_value = self._artifact.custom_properties[key]\n  if mlmd_value.HasField('int_value'):\n    return mlmd_value.int_value\n  elif mlmd_value.HasField('double_value'):\n    return mlmd_value.double_value\n  elif mlmd_value.HasField('string_value'):\n    return mlmd_value.string_value\n  elif mlmd_value.HasField('bool_value'):\n    return mlmd_value.bool_value\n  return None\n</code></pre> <code></code> get_float_custom_property \u00b6 <pre><code>get_float_custom_property(key: str) -&gt; float\n</code></pre> <p>Gets a custom property of float type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_float_custom_property(self, key: str) -&gt; float:\n  \"\"\"Gets a custom property of float type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return 0.0\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, float):\n    return json_value\n  return self._artifact.custom_properties[key].double_value\n</code></pre> <code></code> get_int_custom_property \u00b6 <pre><code>get_int_custom_property(key: str) -&gt; int\n</code></pre> <p>Get a custom property of int type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_int_custom_property(self, key: str) -&gt; int:\n  \"\"\"Get a custom property of int type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return 0\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, float):\n    return int(json_value)\n  return self._artifact.custom_properties[key].int_value\n</code></pre> <code></code> get_json_value_custom_property \u00b6 <pre><code>get_json_value_custom_property(key: str) -&gt; JsonValueType\n</code></pre> <p>Get a custom property of JSON type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef get_json_value_custom_property(self, key: str) -&gt; JsonValueType:\n  \"\"\"Get a custom property of JSON type.\"\"\"\n  if key in self._cached_modifiable_custom_properties:\n    return self._cached_modifiable_custom_properties[key]\n  if (key not in self._artifact.custom_properties or\n      not self._artifact.custom_properties[key].HasField('struct_value')):\n    return None\n  value = _decode_struct_value(\n      self._artifact.custom_properties[key].struct_value)\n  # We must cache the decoded lists or dictionaries returned here so that\n  # if their recursive contents are modified, the Metadata proto message\n  # can be updated to reflect this.\n  if isinstance(value, (dict, list)):\n    self._cached_modifiable_custom_properties[key] = value\n  return value\n</code></pre> <code></code> get_proto_custom_property \u00b6 <pre><code>get_proto_custom_property(key: str) -&gt; Optional[Message]\n</code></pre> <p>Get a custom property of proto type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef get_proto_custom_property(self, key: str) -&gt; Optional[message.Message]:\n  \"\"\"Get a custom property of proto type.\"\"\"\n  if key in self._cached_modifiable_custom_properties:\n    return self._cached_modifiable_custom_properties[key]\n  if (key not in self._artifact.custom_properties or\n      not self._artifact.custom_properties[key].HasField('proto_value')):\n    return None\n  value = proto_utils.unpack_proto_any(\n      self._artifact.custom_properties[key].proto_value)\n  # We must cache the protobuf message here so that if its contents are\n  # modified, the Metadata proto message can be updated to reflect this.\n  if isinstance(value, message.Message):\n    self._cached_modifiable_custom_properties[key] = value\n  return value\n</code></pre> <code></code> get_string_custom_property \u00b6 <pre><code>get_string_custom_property(key: str) -&gt; str\n</code></pre> <p>Get a custom property of string type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_string_custom_property(self, key: str) -&gt; str:\n  \"\"\"Get a custom property of string type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return ''\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, str):\n    return json_value\n  return self._artifact.custom_properties[key].string_value\n</code></pre> <code></code> set_bool_custom_property \u00b6 <pre><code>set_bool_custom_property(key: str, value: bool)\n</code></pre> <p>Sets a custom property of bool type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_bool_custom_property(self, key: str, value: bool):\n  \"\"\"Sets a custom property of bool type.\"\"\"\n  self._artifact.custom_properties[key].bool_value = value\n</code></pre> <code></code> set_float_custom_property \u00b6 <pre><code>set_float_custom_property(key: str, value: float)\n</code></pre> <p>Sets a custom property of float type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_float_custom_property(self, key: str, value: float):\n  \"\"\"Sets a custom property of float type.\"\"\"\n  self._artifact.custom_properties[key].double_value = builtins.float(value)\n</code></pre> <code></code> set_int_custom_property \u00b6 <pre><code>set_int_custom_property(key: str, value: int)\n</code></pre> <p>Set a custom property of int type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_int_custom_property(self, key: str, value: int):\n  \"\"\"Set a custom property of int type.\"\"\"\n  self._artifact.custom_properties[key].int_value = builtins.int(value)\n</code></pre> <code></code> set_json_value_custom_property \u00b6 <pre><code>set_json_value_custom_property(key: str, value: JsonValueType)\n</code></pre> <p>Sets a custom property of JSON type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_json_value_custom_property(self, key: str, value: JsonValueType):\n  \"\"\"Sets a custom property of JSON type.\"\"\"\n  self._cached_modifiable_custom_properties[key] = value\n</code></pre> <code></code> set_mlmd_artifact \u00b6 <pre><code>set_mlmd_artifact(artifact: Artifact)\n</code></pre> <p>Replace the MLMD artifact object on this artifact.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_mlmd_artifact(self, artifact: metadata_store_pb2.Artifact):\n  \"\"\"Replace the MLMD artifact object on this artifact.\"\"\"\n  if not isinstance(artifact, metadata_store_pb2.Artifact):\n    raise ValueError(\n        ('Expected instance of metadata_store_pb2.Artifact, got %s '\n         'instead.') % (artifact,))\n  self._artifact = artifact\n  self._cached_modifiable_properties = {}\n  self._cached_modifiable_custom_properties = {}\n</code></pre> <code></code> set_mlmd_artifact_type \u00b6 <pre><code>set_mlmd_artifact_type(artifact_type: ArtifactType)\n</code></pre> <p>Set entire ArtifactType in this object.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_mlmd_artifact_type(self,\n                           artifact_type: metadata_store_pb2.ArtifactType):\n  \"\"\"Set entire ArtifactType in this object.\"\"\"\n  if not isinstance(artifact_type, metadata_store_pb2.ArtifactType):\n    raise ValueError(\n        ('Expected instance of metadata_store_pb2.ArtifactType, got %s '\n         'instead.') % (artifact_type,))\n  self._artifact_type = artifact_type\n  self._artifact.type_id = artifact_type.id\n</code></pre> <code></code> set_proto_custom_property \u00b6 <pre><code>set_proto_custom_property(key: str, value: Message)\n</code></pre> <p>Sets a custom property of proto type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_proto_custom_property(self, key: str, value: message.Message):\n  \"\"\"Sets a custom property of proto type.\"\"\"\n  self._cached_modifiable_custom_properties[key] = value\n</code></pre> <code></code> set_string_custom_property \u00b6 <pre><code>set_string_custom_property(key: str, value: str)\n</code></pre> <p>Set a custom property of string type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_string_custom_property(self, key: str, value: str):\n  \"\"\"Set a custom property of string type.\"\"\"\n  self._artifact.custom_properties[key].string_value = value\n</code></pre> <code></code> to_json_dict \u00b6 <pre><code>to_json_dict() -&gt; Dict[str, Any]\n</code></pre> <p>Convert from an object to a JSON serializable dictionary.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef to_json_dict(self) -&gt; Dict[str, Any]:\n  return {\n      'artifact':\n          json.loads(\n              json_format.MessageToJson(\n                  message=self.mlmd_artifact,\n                  preserving_proto_field_name=True)),\n      'artifact_type':\n          json.loads(\n              json_format.MessageToJson(\n                  message=self._artifact_type,\n                  preserving_proto_field_name=True)),\n      '__artifact_class_module__':\n          self.__class__.__module__,\n      '__artifact_class_name__':\n          self.__class__.__name__,\n  }\n</code></pre>"},{"location":"api/v1/types/#tfx.v1.types.standard_artifacts.JsonValue","title":"JsonValue","text":"<pre><code>JsonValue(*args, **kwargs)\n</code></pre> <p>               Bases: <code>ValueArtifact</code></p> <p>Artifacts representing a Jsonable value.</p> <p>Initializes ValueArtifact.</p> METHOD DESCRIPTION <code>annotate_as</code> <p>Annotate the value artifact type with a system artifact class.</p> <code>copy_from</code> <p>Set uri, properties and custom properties from a given Artifact.</p> <code>decode</code> <p>Method decoding the file content. Implemented by subclasses.</p> <code>encode</code> <p>Method encoding the file content. Implemented by subclasses.</p> <code>from_json_dict</code> <p>Convert from dictionary data to an object.</p> <code>get_bool_custom_property</code> <p>Get a custom property of bool type.</p> <code>get_custom_property</code> <p>Gets a custom property with key. Return None if not found.</p> <code>get_float_custom_property</code> <p>Gets a custom property of float type.</p> <code>get_int_custom_property</code> <p>Get a custom property of int type.</p> <code>get_json_value_custom_property</code> <p>Get a custom property of JSON type.</p> <code>get_proto_custom_property</code> <p>Get a custom property of proto type.</p> <code>get_string_custom_property</code> <p>Get a custom property of string type.</p> <code>set_bool_custom_property</code> <p>Sets a custom property of bool type.</p> <code>set_float_custom_property</code> <p>Sets a custom property of float type.</p> <code>set_int_custom_property</code> <p>Set a custom property of int type.</p> <code>set_json_value_custom_property</code> <p>Sets a custom property of JSON type.</p> <code>set_mlmd_artifact</code> <p>Replace the MLMD artifact object on this artifact.</p> <code>set_mlmd_artifact_type</code> <p>Set entire ArtifactType in this object.</p> <code>set_proto_custom_property</code> <p>Sets a custom property of proto type.</p> <code>set_string_custom_property</code> <p>Set a custom property of string type.</p> <code>to_json_dict</code> <p>Convert from an object to a JSON serializable dictionary.</p> ATTRIBUTE DESCRIPTION <code>artifact_type</code> <p>Type of the underlying mlmd artifact.</p> <p> </p> <code>external_id</code> <p>external id of the underlying artifact.</p> <p> TYPE: <code>str</code> </p> <code>id</code> <p>Id of the underlying mlmd artifact.</p> <p> TYPE: <code>int</code> </p> <code>is_external</code> <p>Returns true if the artifact is external.</p> <p> TYPE: <code>bool</code> </p> <code>mlmd_artifact</code> <p>Underlying mlmd artifact.</p> <p> </p> <code>name</code> <p>Name of the underlying mlmd artifact.</p> <p> TYPE: <code>str</code> </p> <code>pipeline_name</code> <p>Name of the pipeline that produce the artifact.</p> <p> TYPE: <code>str</code> </p> <code>producer_component</code> <p>Producer component of the artifact.</p> <p> TYPE: <code>str</code> </p> <code>state</code> <p>State of the underlying mlmd artifact.</p> <p> TYPE: <code>str</code> </p> <code>type</code> <p>Type of the artifact.</p> <p> </p> <code>type_id</code> <p>Type id of the underlying mlmd artifact.</p> <p> TYPE: <code>int</code> </p> <code>type_name</code> <p>Type name of the underlying mlmd artifact.</p> <p> </p> <code>uri</code> <p>Artifact URI.</p> <p> TYPE: <code>str</code> </p> <code>value</code> <p>Value stored in the artifact.</p> <p> </p> Source code in <code>tfx/types/value_artifact.py</code> <pre><code>def __init__(self, *args, **kwargs):\n  \"\"\"Initializes ValueArtifact.\"\"\"\n  self._has_value = False\n  self._modified = False\n  self._value = None\n  super().__init__(*args, **kwargs)\n</code></pre> Attributes\u00b6 <code></code> artifact_type <code>property</code> \u00b6 <pre><code>artifact_type\n</code></pre> <p>Type of the underlying mlmd artifact.</p> <code></code> external_id <code>property</code> \u00b6 <pre><code>external_id: str\n</code></pre> <p>external id of the underlying artifact.</p> <code></code> id <code>property</code> <code>writable</code> \u00b6 <pre><code>id: int\n</code></pre> <p>Id of the underlying mlmd artifact.</p> <code></code> is_external <code>property</code> <code>writable</code> \u00b6 <pre><code>is_external: bool\n</code></pre> <p>Returns true if the artifact is external.</p> <code></code> mlmd_artifact <code>property</code> \u00b6 <pre><code>mlmd_artifact\n</code></pre> <p>Underlying mlmd artifact.</p> <code></code> name <code>property</code> <code>writable</code> \u00b6 <pre><code>name: str\n</code></pre> <p>Name of the underlying mlmd artifact.</p> <code></code> pipeline_name <code>property</code> <code>writable</code> \u00b6 <pre><code>pipeline_name: str\n</code></pre> <p>Name of the pipeline that produce the artifact.</p> <code></code> producer_component <code>property</code> <code>writable</code> \u00b6 <pre><code>producer_component: str\n</code></pre> <p>Producer component of the artifact.</p> <code></code> state <code>property</code> <code>writable</code> \u00b6 <pre><code>state: str\n</code></pre> <p>State of the underlying mlmd artifact.</p> <code></code> type <code>property</code> \u00b6 <pre><code>type\n</code></pre> <p>Type of the artifact.</p> <code></code> type_id <code>property</code> <code>writable</code> \u00b6 <pre><code>type_id: int\n</code></pre> <p>Type id of the underlying mlmd artifact.</p> <code></code> type_name <code>property</code> \u00b6 <pre><code>type_name\n</code></pre> <p>Type name of the underlying mlmd artifact.</p> <code></code> uri <code>property</code> <code>writable</code> \u00b6 <pre><code>uri: str\n</code></pre> <p>Artifact URI.</p> <code></code> value <code>property</code> <code>writable</code> \u00b6 <pre><code>value\n</code></pre> <p>Value stored in the artifact.</p> Functions\u00b6 <code></code> annotate_as <code>classmethod</code> \u00b6 <pre><code>annotate_as(type_annotation: Optional[Type[SystemArtifact]] = None)\n</code></pre> <p>Annotate the value artifact type with a system artifact class.</p> <p>Example usage</p> <pre><code>from tfx import v1 as tfx\n\nOutputArtifact = tfx.dsl.components.OutputArtifact\nString = tfx.types.standard_artifacts.String\nModel = tfx.dsl.standard_annotations.Model\n\n\n@tfx.dsl.components.component\ndef MyTrainer(model: OutputArtifact[String.annotate_as(Model)]): ...\n</code></pre> PARAMETER DESCRIPTION <code>type_annotation</code> <p>the standard annotations used to annotate the value artifact type. The possible values are in <code>tfx.v1.dsl.standard_annotations</code>.</p> <p> TYPE: <code>Optional[Type[SystemArtifact]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <p>A subclass of the method caller class (e.g., <code>standard_artifacts.String</code>, <code>standard_artifacts.Float</code>) with TYPE_ANNOTATION attribute set to be <code>type_annotation</code>; returns the original class if<code>type_annotation</code> is None.</p> Source code in <code>tfx/types/value_artifact.py</code> <pre><code>@classmethod\ndef annotate_as(cls, type_annotation: Optional[Type[SystemArtifact]] = None):\n  \"\"\"Annotate the value artifact type with a system artifact class.\n\n  !!! example \"Example usage\"\n\n      ```python\n      from tfx import v1 as tfx\n\n      OutputArtifact = tfx.dsl.components.OutputArtifact\n      String = tfx.types.standard_artifacts.String\n      Model = tfx.dsl.standard_annotations.Model\n\n\n      @tfx.dsl.components.component\n      def MyTrainer(model: OutputArtifact[String.annotate_as(Model)]): ...\n      ```\n\n  Args:\n    type_annotation: the standard annotations used to annotate the value\n      artifact type. The possible values are in\n      `tfx.v1.dsl.standard_annotations`.\n\n  Returns:\n    A subclass of the method caller class (e.g., [`standard_artifacts.String`][tfx.v1.types.standard_artifacts.String],\n      [`standard_artifacts.Float`][tfx.v1.types.standard_artifacts.Float]) with TYPE_ANNOTATION attribute set to be\n      `type_annotation`; returns the original class if`type_annotation` is None.\n  \"\"\"\n  if not type_annotation:\n    return cls\n  if not issubclass(type_annotation, SystemArtifact):\n    raise ValueError(\n        'type_annotation %s is not a subclass of SystemArtifact.' %\n        type_annotation)\n  type_annotation_str = str(type_annotation.__name__)\n  return type(\n      str(cls.__name__) + '_' + type_annotation_str,\n      (cls,),\n      {\n          'TYPE_NAME': str(cls.TYPE_NAME) + '_' + type_annotation_str,\n          'TYPE_ANNOTATION': type_annotation,\n          '__module__': cls.__module__,\n      },\n  )\n</code></pre> <code></code> copy_from \u00b6 <pre><code>copy_from(other: Artifact)\n</code></pre> <p>Set uri, properties and custom properties from a given Artifact.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef copy_from(self, other: 'Artifact'):\n  \"\"\"Set uri, properties and custom properties from a given Artifact.\"\"\"\n  assert self.type is other.type, (\n      'Unable to set properties from an artifact of different type: {} vs {}'\n      .format(self.type_name, other.type_name))\n  self.uri = other.uri\n  if other.artifact_type.HasField('id'):\n    self.type_id = other.artifact_type.id\n\n  self._artifact.properties.clear()\n  self._artifact.properties.MergeFrom(other._artifact.properties)  # pylint: disable=protected-access\n  self._artifact.custom_properties.clear()\n  self._artifact.custom_properties.MergeFrom(\n      other._artifact.custom_properties)  # pylint: disable=protected-access\n  self._cached_modifiable_properties = copy.deepcopy(\n      other._cached_modifiable_properties)  # pylint: disable=protected-access\n  self._cached_modifiable_custom_properties = copy.deepcopy(\n      other._cached_modifiable_custom_properties)  # pylint: disable=protected-access\n</code></pre> <code></code> decode \u00b6 <pre><code>decode(serialized_value: str) -&gt; JsonableType\n</code></pre> <p>Method decoding the file content. Implemented by subclasses.</p> Source code in <code>tfx/types/standard_artifacts.py</code> <pre><code>def decode(self, serialized_value: str) -&gt; json_utils.JsonableType:\n    return json_utils.loads(serialized_value)\n</code></pre> <code></code> encode \u00b6 <pre><code>encode(value: JsonableType) -&gt; str\n</code></pre> <p>Method encoding the file content. Implemented by subclasses.</p> Source code in <code>tfx/types/standard_artifacts.py</code> <pre><code>def encode(self, value: json_utils.JsonableType) -&gt; str:\n    return json_utils.dumps(value)\n</code></pre> <code></code> from_json_dict <code>classmethod</code> \u00b6 <pre><code>from_json_dict(dict_data: Dict[str, Any]) -&gt; Any\n</code></pre> <p>Convert from dictionary data to an object.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@classmethod\n@doc_controls.do_not_doc_inheritable\ndef from_json_dict(cls, dict_data: Dict[str, Any]) -&gt; Any:\n  module_name = dict_data['__artifact_class_module__']\n  class_name = dict_data['__artifact_class_name__']\n  artifact = metadata_store_pb2.Artifact()\n  artifact_type = metadata_store_pb2.ArtifactType()\n  json_format.Parse(json.dumps(dict_data['artifact']), artifact)\n  json_format.Parse(json.dumps(dict_data['artifact_type']), artifact_type)\n\n  # First, try to resolve the specific class used for the artifact; if this\n  # is not possible, use a generic artifact.Artifact object.\n  result = None\n  try:\n    artifact_cls = getattr(importlib.import_module(module_name), class_name)\n    # If the artifact type is the base Artifact class, do not construct the\n    # object here since that constructor requires the mlmd_artifact_type\n    # argument.\n    if artifact_cls != Artifact:\n      result = artifact_cls()\n  except (AttributeError, ImportError, ValueError):\n    logging.warning((\n        'Could not load artifact class %s.%s; using fallback deserialization '\n        'for the relevant artifact. Please make sure that any artifact '\n        'classes can be imported within your container or environment.'),\n                    module_name, class_name)\n  if not result:\n    result = Artifact(mlmd_artifact_type=artifact_type)\n  result.set_mlmd_artifact_type(artifact_type)\n  result.set_mlmd_artifact(artifact)\n  return result\n</code></pre> <code></code> get_bool_custom_property \u00b6 <pre><code>get_bool_custom_property(key: str) -&gt; bool\n</code></pre> <p>Get a custom property of bool type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_bool_custom_property(self, key: str) -&gt; bool:\n  \"\"\"Get a custom property of bool type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return False\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, bool):\n    return json_value\n  return self._artifact.custom_properties[key].bool_value\n</code></pre> <code></code> get_custom_property \u00b6 <pre><code>get_custom_property(key: str) -&gt; Optional[Union[int, float, str, bool, JsonValueType]]\n</code></pre> <p>Gets a custom property with key. Return None if not found.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_custom_property(\n    self, key: str\n) -&gt; Optional[Union[int, float, str, bool, JsonValueType]]:\n  \"\"\"Gets a custom property with key. Return None if not found.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return None\n\n  json_value = self.get_json_value_custom_property(key)\n  if json_value:\n    return json_value\n\n  mlmd_value = self._artifact.custom_properties[key]\n  if mlmd_value.HasField('int_value'):\n    return mlmd_value.int_value\n  elif mlmd_value.HasField('double_value'):\n    return mlmd_value.double_value\n  elif mlmd_value.HasField('string_value'):\n    return mlmd_value.string_value\n  elif mlmd_value.HasField('bool_value'):\n    return mlmd_value.bool_value\n  return None\n</code></pre> <code></code> get_float_custom_property \u00b6 <pre><code>get_float_custom_property(key: str) -&gt; float\n</code></pre> <p>Gets a custom property of float type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_float_custom_property(self, key: str) -&gt; float:\n  \"\"\"Gets a custom property of float type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return 0.0\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, float):\n    return json_value\n  return self._artifact.custom_properties[key].double_value\n</code></pre> <code></code> get_int_custom_property \u00b6 <pre><code>get_int_custom_property(key: str) -&gt; int\n</code></pre> <p>Get a custom property of int type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_int_custom_property(self, key: str) -&gt; int:\n  \"\"\"Get a custom property of int type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return 0\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, float):\n    return int(json_value)\n  return self._artifact.custom_properties[key].int_value\n</code></pre> <code></code> get_json_value_custom_property \u00b6 <pre><code>get_json_value_custom_property(key: str) -&gt; JsonValueType\n</code></pre> <p>Get a custom property of JSON type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef get_json_value_custom_property(self, key: str) -&gt; JsonValueType:\n  \"\"\"Get a custom property of JSON type.\"\"\"\n  if key in self._cached_modifiable_custom_properties:\n    return self._cached_modifiable_custom_properties[key]\n  if (key not in self._artifact.custom_properties or\n      not self._artifact.custom_properties[key].HasField('struct_value')):\n    return None\n  value = _decode_struct_value(\n      self._artifact.custom_properties[key].struct_value)\n  # We must cache the decoded lists or dictionaries returned here so that\n  # if their recursive contents are modified, the Metadata proto message\n  # can be updated to reflect this.\n  if isinstance(value, (dict, list)):\n    self._cached_modifiable_custom_properties[key] = value\n  return value\n</code></pre> <code></code> get_proto_custom_property \u00b6 <pre><code>get_proto_custom_property(key: str) -&gt; Optional[Message]\n</code></pre> <p>Get a custom property of proto type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef get_proto_custom_property(self, key: str) -&gt; Optional[message.Message]:\n  \"\"\"Get a custom property of proto type.\"\"\"\n  if key in self._cached_modifiable_custom_properties:\n    return self._cached_modifiable_custom_properties[key]\n  if (key not in self._artifact.custom_properties or\n      not self._artifact.custom_properties[key].HasField('proto_value')):\n    return None\n  value = proto_utils.unpack_proto_any(\n      self._artifact.custom_properties[key].proto_value)\n  # We must cache the protobuf message here so that if its contents are\n  # modified, the Metadata proto message can be updated to reflect this.\n  if isinstance(value, message.Message):\n    self._cached_modifiable_custom_properties[key] = value\n  return value\n</code></pre> <code></code> get_string_custom_property \u00b6 <pre><code>get_string_custom_property(key: str) -&gt; str\n</code></pre> <p>Get a custom property of string type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_string_custom_property(self, key: str) -&gt; str:\n  \"\"\"Get a custom property of string type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return ''\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, str):\n    return json_value\n  return self._artifact.custom_properties[key].string_value\n</code></pre> <code></code> set_bool_custom_property \u00b6 <pre><code>set_bool_custom_property(key: str, value: bool)\n</code></pre> <p>Sets a custom property of bool type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_bool_custom_property(self, key: str, value: bool):\n  \"\"\"Sets a custom property of bool type.\"\"\"\n  self._artifact.custom_properties[key].bool_value = value\n</code></pre> <code></code> set_float_custom_property \u00b6 <pre><code>set_float_custom_property(key: str, value: float)\n</code></pre> <p>Sets a custom property of float type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_float_custom_property(self, key: str, value: float):\n  \"\"\"Sets a custom property of float type.\"\"\"\n  self._artifact.custom_properties[key].double_value = builtins.float(value)\n</code></pre> <code></code> set_int_custom_property \u00b6 <pre><code>set_int_custom_property(key: str, value: int)\n</code></pre> <p>Set a custom property of int type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_int_custom_property(self, key: str, value: int):\n  \"\"\"Set a custom property of int type.\"\"\"\n  self._artifact.custom_properties[key].int_value = builtins.int(value)\n</code></pre> <code></code> set_json_value_custom_property \u00b6 <pre><code>set_json_value_custom_property(key: str, value: JsonValueType)\n</code></pre> <p>Sets a custom property of JSON type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_json_value_custom_property(self, key: str, value: JsonValueType):\n  \"\"\"Sets a custom property of JSON type.\"\"\"\n  self._cached_modifiable_custom_properties[key] = value\n</code></pre> <code></code> set_mlmd_artifact \u00b6 <pre><code>set_mlmd_artifact(artifact: Artifact)\n</code></pre> <p>Replace the MLMD artifact object on this artifact.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_mlmd_artifact(self, artifact: metadata_store_pb2.Artifact):\n  \"\"\"Replace the MLMD artifact object on this artifact.\"\"\"\n  if not isinstance(artifact, metadata_store_pb2.Artifact):\n    raise ValueError(\n        ('Expected instance of metadata_store_pb2.Artifact, got %s '\n         'instead.') % (artifact,))\n  self._artifact = artifact\n  self._cached_modifiable_properties = {}\n  self._cached_modifiable_custom_properties = {}\n</code></pre> <code></code> set_mlmd_artifact_type \u00b6 <pre><code>set_mlmd_artifact_type(artifact_type: ArtifactType)\n</code></pre> <p>Set entire ArtifactType in this object.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_mlmd_artifact_type(self,\n                           artifact_type: metadata_store_pb2.ArtifactType):\n  \"\"\"Set entire ArtifactType in this object.\"\"\"\n  if not isinstance(artifact_type, metadata_store_pb2.ArtifactType):\n    raise ValueError(\n        ('Expected instance of metadata_store_pb2.ArtifactType, got %s '\n         'instead.') % (artifact_type,))\n  self._artifact_type = artifact_type\n  self._artifact.type_id = artifact_type.id\n</code></pre> <code></code> set_proto_custom_property \u00b6 <pre><code>set_proto_custom_property(key: str, value: Message)\n</code></pre> <p>Sets a custom property of proto type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_proto_custom_property(self, key: str, value: message.Message):\n  \"\"\"Sets a custom property of proto type.\"\"\"\n  self._cached_modifiable_custom_properties[key] = value\n</code></pre> <code></code> set_string_custom_property \u00b6 <pre><code>set_string_custom_property(key: str, value: str)\n</code></pre> <p>Set a custom property of string type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_string_custom_property(self, key: str, value: str):\n  \"\"\"Set a custom property of string type.\"\"\"\n  self._artifact.custom_properties[key].string_value = value\n</code></pre> <code></code> to_json_dict \u00b6 <pre><code>to_json_dict() -&gt; Dict[str, Any]\n</code></pre> <p>Convert from an object to a JSON serializable dictionary.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef to_json_dict(self) -&gt; Dict[str, Any]:\n  return {\n      'artifact':\n          json.loads(\n              json_format.MessageToJson(\n                  message=self.mlmd_artifact,\n                  preserving_proto_field_name=True)),\n      'artifact_type':\n          json.loads(\n              json_format.MessageToJson(\n                  message=self._artifact_type,\n                  preserving_proto_field_name=True)),\n      '__artifact_class_module__':\n          self.__class__.__module__,\n      '__artifact_class_name__':\n          self.__class__.__name__,\n  }\n</code></pre>"},{"location":"api/v1/types/#tfx.v1.types.standard_artifacts.Model","title":"Model","text":"<pre><code>Model(*args, **kwargs)\n</code></pre> <p>               Bases: <code>_TfxArtifact</code></p> <p>Artifact that contains the actual persisted model.</p> <p>Training components stores the trained model like a saved model in this artifact. A <code>Model</code> artifact contains serialization of the trained model in one or more formats, each suitable for different usage (e.g. serving, evaluation), and serving environments.</p> <ul> <li>File structure:</li> <li> <p><code>{uri}/</code></p> <ul> <li><code>Format-Serving/</code>: Model exported for serving.<ul> <li><code>saved_model.pb</code></li> <li>Other actual model files.</li> </ul> </li> <li><code>Format-TFMA/</code>: Model exported for evaluation.<ul> <li><code>saved_model.pb</code></li> <li>Other actual model files.</li> </ul> </li> </ul> </li> <li> <p>Commonly used custom properties of the Model artifact:</p> </li> </ul> <p>Construct TFX first-party component artifact.</p> METHOD DESCRIPTION <code>copy_from</code> <p>Set uri, properties and custom properties from a given Artifact.</p> <code>from_json_dict</code> <p>Convert from dictionary data to an object.</p> <code>get_bool_custom_property</code> <p>Get a custom property of bool type.</p> <code>get_custom_property</code> <p>Gets a custom property with key. Return None if not found.</p> <code>get_float_custom_property</code> <p>Gets a custom property of float type.</p> <code>get_int_custom_property</code> <p>Get a custom property of int type.</p> <code>get_json_value_custom_property</code> <p>Get a custom property of JSON type.</p> <code>get_proto_custom_property</code> <p>Get a custom property of proto type.</p> <code>get_string_custom_property</code> <p>Get a custom property of string type.</p> <code>set_bool_custom_property</code> <p>Sets a custom property of bool type.</p> <code>set_float_custom_property</code> <p>Sets a custom property of float type.</p> <code>set_int_custom_property</code> <p>Set a custom property of int type.</p> <code>set_json_value_custom_property</code> <p>Sets a custom property of JSON type.</p> <code>set_mlmd_artifact</code> <p>Replace the MLMD artifact object on this artifact.</p> <code>set_mlmd_artifact_type</code> <p>Set entire ArtifactType in this object.</p> <code>set_proto_custom_property</code> <p>Sets a custom property of proto type.</p> <code>set_string_custom_property</code> <p>Set a custom property of string type.</p> <code>to_json_dict</code> <p>Convert from an object to a JSON serializable dictionary.</p> ATTRIBUTE DESCRIPTION <code>artifact_type</code> <p>Type of the underlying mlmd artifact.</p> <p> </p> <code>external_id</code> <p>external id of the underlying artifact.</p> <p> TYPE: <code>str</code> </p> <code>id</code> <p>Id of the underlying mlmd artifact.</p> <p> TYPE: <code>int</code> </p> <code>is_external</code> <p>Returns true if the artifact is external.</p> <p> TYPE: <code>bool</code> </p> <code>mlmd_artifact</code> <p>Underlying mlmd artifact.</p> <p> </p> <code>name</code> <p>Name of the underlying mlmd artifact.</p> <p> TYPE: <code>str</code> </p> <code>pipeline_name</code> <p>Name of the pipeline that produce the artifact.</p> <p> TYPE: <code>str</code> </p> <code>producer_component</code> <p>Producer component of the artifact.</p> <p> TYPE: <code>str</code> </p> <code>state</code> <p>State of the underlying mlmd artifact.</p> <p> TYPE: <code>str</code> </p> <code>type</code> <p>Type of the artifact.</p> <p> </p> <code>type_id</code> <p>Type id of the underlying mlmd artifact.</p> <p> TYPE: <code>int</code> </p> <code>type_name</code> <p>Type name of the underlying mlmd artifact.</p> <p> </p> <code>uri</code> <p>Artifact URI.</p> <p> TYPE: <code>str</code> </p> Source code in <code>tfx/types/standard_artifacts.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    \"\"\"Construct TFX first-party component artifact.\"\"\"\n    # TODO(b/176795331): Refactor directory structure to make it clearer that\n    # TFX-specific artifacts require the full \"tfx\" package be installed.\n    #\n    # Do not allow usage of TFX-specific artifact if only the core pipeline\n    # SDK package is installed.\n    try:\n        import setuptools # pytype: disable=import-error  # noqa: F401\n\n        # Test import only when setuptools is available.\n        try:\n            # `extensions` is not included in ml_pipelines_sdk and doesn't have any\n            # transitive import.\n            import tfx.extensions as _  # type: ignore  # noqa: F401 # pylint: disable=g-import-not-at-top\n        except ModuleNotFoundError as err:\n            # The following condition detects exactly whether only the DSL package\n            # is installed, and is bypassed when tests run in Bazel.\n            raise RuntimeError(\n                'The \"tfx\" and all dependent packages need to be '\n                \"installed to use this functionality.\"\n            ) from err\n    except ModuleNotFoundError:\n        pass\n\n    super().__init__(*args, **kwargs)\n</code></pre> Attributes\u00b6 <code></code> artifact_type <code>property</code> \u00b6 <pre><code>artifact_type\n</code></pre> <p>Type of the underlying mlmd artifact.</p> <code></code> external_id <code>property</code> \u00b6 <pre><code>external_id: str\n</code></pre> <p>external id of the underlying artifact.</p> <code></code> id <code>property</code> <code>writable</code> \u00b6 <pre><code>id: int\n</code></pre> <p>Id of the underlying mlmd artifact.</p> <code></code> is_external <code>property</code> <code>writable</code> \u00b6 <pre><code>is_external: bool\n</code></pre> <p>Returns true if the artifact is external.</p> <code></code> mlmd_artifact <code>property</code> \u00b6 <pre><code>mlmd_artifact\n</code></pre> <p>Underlying mlmd artifact.</p> <code></code> name <code>property</code> <code>writable</code> \u00b6 <pre><code>name: str\n</code></pre> <p>Name of the underlying mlmd artifact.</p> <code></code> pipeline_name <code>property</code> <code>writable</code> \u00b6 <pre><code>pipeline_name: str\n</code></pre> <p>Name of the pipeline that produce the artifact.</p> <code></code> producer_component <code>property</code> <code>writable</code> \u00b6 <pre><code>producer_component: str\n</code></pre> <p>Producer component of the artifact.</p> <code></code> state <code>property</code> <code>writable</code> \u00b6 <pre><code>state: str\n</code></pre> <p>State of the underlying mlmd artifact.</p> <code></code> type <code>property</code> \u00b6 <pre><code>type\n</code></pre> <p>Type of the artifact.</p> <code></code> type_id <code>property</code> <code>writable</code> \u00b6 <pre><code>type_id: int\n</code></pre> <p>Type id of the underlying mlmd artifact.</p> <code></code> type_name <code>property</code> \u00b6 <pre><code>type_name\n</code></pre> <p>Type name of the underlying mlmd artifact.</p> <code></code> uri <code>property</code> <code>writable</code> \u00b6 <pre><code>uri: str\n</code></pre> <p>Artifact URI.</p> Functions\u00b6 <code></code> copy_from \u00b6 <pre><code>copy_from(other: Artifact)\n</code></pre> <p>Set uri, properties and custom properties from a given Artifact.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef copy_from(self, other: 'Artifact'):\n  \"\"\"Set uri, properties and custom properties from a given Artifact.\"\"\"\n  assert self.type is other.type, (\n      'Unable to set properties from an artifact of different type: {} vs {}'\n      .format(self.type_name, other.type_name))\n  self.uri = other.uri\n  if other.artifact_type.HasField('id'):\n    self.type_id = other.artifact_type.id\n\n  self._artifact.properties.clear()\n  self._artifact.properties.MergeFrom(other._artifact.properties)  # pylint: disable=protected-access\n  self._artifact.custom_properties.clear()\n  self._artifact.custom_properties.MergeFrom(\n      other._artifact.custom_properties)  # pylint: disable=protected-access\n  self._cached_modifiable_properties = copy.deepcopy(\n      other._cached_modifiable_properties)  # pylint: disable=protected-access\n  self._cached_modifiable_custom_properties = copy.deepcopy(\n      other._cached_modifiable_custom_properties)  # pylint: disable=protected-access\n</code></pre> <code></code> from_json_dict <code>classmethod</code> \u00b6 <pre><code>from_json_dict(dict_data: Dict[str, Any]) -&gt; Any\n</code></pre> <p>Convert from dictionary data to an object.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@classmethod\n@doc_controls.do_not_doc_inheritable\ndef from_json_dict(cls, dict_data: Dict[str, Any]) -&gt; Any:\n  module_name = dict_data['__artifact_class_module__']\n  class_name = dict_data['__artifact_class_name__']\n  artifact = metadata_store_pb2.Artifact()\n  artifact_type = metadata_store_pb2.ArtifactType()\n  json_format.Parse(json.dumps(dict_data['artifact']), artifact)\n  json_format.Parse(json.dumps(dict_data['artifact_type']), artifact_type)\n\n  # First, try to resolve the specific class used for the artifact; if this\n  # is not possible, use a generic artifact.Artifact object.\n  result = None\n  try:\n    artifact_cls = getattr(importlib.import_module(module_name), class_name)\n    # If the artifact type is the base Artifact class, do not construct the\n    # object here since that constructor requires the mlmd_artifact_type\n    # argument.\n    if artifact_cls != Artifact:\n      result = artifact_cls()\n  except (AttributeError, ImportError, ValueError):\n    logging.warning((\n        'Could not load artifact class %s.%s; using fallback deserialization '\n        'for the relevant artifact. Please make sure that any artifact '\n        'classes can be imported within your container or environment.'),\n                    module_name, class_name)\n  if not result:\n    result = Artifact(mlmd_artifact_type=artifact_type)\n  result.set_mlmd_artifact_type(artifact_type)\n  result.set_mlmd_artifact(artifact)\n  return result\n</code></pre> <code></code> get_bool_custom_property \u00b6 <pre><code>get_bool_custom_property(key: str) -&gt; bool\n</code></pre> <p>Get a custom property of bool type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_bool_custom_property(self, key: str) -&gt; bool:\n  \"\"\"Get a custom property of bool type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return False\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, bool):\n    return json_value\n  return self._artifact.custom_properties[key].bool_value\n</code></pre> <code></code> get_custom_property \u00b6 <pre><code>get_custom_property(key: str) -&gt; Optional[Union[int, float, str, bool, JsonValueType]]\n</code></pre> <p>Gets a custom property with key. Return None if not found.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_custom_property(\n    self, key: str\n) -&gt; Optional[Union[int, float, str, bool, JsonValueType]]:\n  \"\"\"Gets a custom property with key. Return None if not found.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return None\n\n  json_value = self.get_json_value_custom_property(key)\n  if json_value:\n    return json_value\n\n  mlmd_value = self._artifact.custom_properties[key]\n  if mlmd_value.HasField('int_value'):\n    return mlmd_value.int_value\n  elif mlmd_value.HasField('double_value'):\n    return mlmd_value.double_value\n  elif mlmd_value.HasField('string_value'):\n    return mlmd_value.string_value\n  elif mlmd_value.HasField('bool_value'):\n    return mlmd_value.bool_value\n  return None\n</code></pre> <code></code> get_float_custom_property \u00b6 <pre><code>get_float_custom_property(key: str) -&gt; float\n</code></pre> <p>Gets a custom property of float type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_float_custom_property(self, key: str) -&gt; float:\n  \"\"\"Gets a custom property of float type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return 0.0\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, float):\n    return json_value\n  return self._artifact.custom_properties[key].double_value\n</code></pre> <code></code> get_int_custom_property \u00b6 <pre><code>get_int_custom_property(key: str) -&gt; int\n</code></pre> <p>Get a custom property of int type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_int_custom_property(self, key: str) -&gt; int:\n  \"\"\"Get a custom property of int type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return 0\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, float):\n    return int(json_value)\n  return self._artifact.custom_properties[key].int_value\n</code></pre> <code></code> get_json_value_custom_property \u00b6 <pre><code>get_json_value_custom_property(key: str) -&gt; JsonValueType\n</code></pre> <p>Get a custom property of JSON type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef get_json_value_custom_property(self, key: str) -&gt; JsonValueType:\n  \"\"\"Get a custom property of JSON type.\"\"\"\n  if key in self._cached_modifiable_custom_properties:\n    return self._cached_modifiable_custom_properties[key]\n  if (key not in self._artifact.custom_properties or\n      not self._artifact.custom_properties[key].HasField('struct_value')):\n    return None\n  value = _decode_struct_value(\n      self._artifact.custom_properties[key].struct_value)\n  # We must cache the decoded lists or dictionaries returned here so that\n  # if their recursive contents are modified, the Metadata proto message\n  # can be updated to reflect this.\n  if isinstance(value, (dict, list)):\n    self._cached_modifiable_custom_properties[key] = value\n  return value\n</code></pre> <code></code> get_proto_custom_property \u00b6 <pre><code>get_proto_custom_property(key: str) -&gt; Optional[Message]\n</code></pre> <p>Get a custom property of proto type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef get_proto_custom_property(self, key: str) -&gt; Optional[message.Message]:\n  \"\"\"Get a custom property of proto type.\"\"\"\n  if key in self._cached_modifiable_custom_properties:\n    return self._cached_modifiable_custom_properties[key]\n  if (key not in self._artifact.custom_properties or\n      not self._artifact.custom_properties[key].HasField('proto_value')):\n    return None\n  value = proto_utils.unpack_proto_any(\n      self._artifact.custom_properties[key].proto_value)\n  # We must cache the protobuf message here so that if its contents are\n  # modified, the Metadata proto message can be updated to reflect this.\n  if isinstance(value, message.Message):\n    self._cached_modifiable_custom_properties[key] = value\n  return value\n</code></pre> <code></code> get_string_custom_property \u00b6 <pre><code>get_string_custom_property(key: str) -&gt; str\n</code></pre> <p>Get a custom property of string type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_string_custom_property(self, key: str) -&gt; str:\n  \"\"\"Get a custom property of string type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return ''\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, str):\n    return json_value\n  return self._artifact.custom_properties[key].string_value\n</code></pre> <code></code> set_bool_custom_property \u00b6 <pre><code>set_bool_custom_property(key: str, value: bool)\n</code></pre> <p>Sets a custom property of bool type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_bool_custom_property(self, key: str, value: bool):\n  \"\"\"Sets a custom property of bool type.\"\"\"\n  self._artifact.custom_properties[key].bool_value = value\n</code></pre> <code></code> set_float_custom_property \u00b6 <pre><code>set_float_custom_property(key: str, value: float)\n</code></pre> <p>Sets a custom property of float type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_float_custom_property(self, key: str, value: float):\n  \"\"\"Sets a custom property of float type.\"\"\"\n  self._artifact.custom_properties[key].double_value = builtins.float(value)\n</code></pre> <code></code> set_int_custom_property \u00b6 <pre><code>set_int_custom_property(key: str, value: int)\n</code></pre> <p>Set a custom property of int type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_int_custom_property(self, key: str, value: int):\n  \"\"\"Set a custom property of int type.\"\"\"\n  self._artifact.custom_properties[key].int_value = builtins.int(value)\n</code></pre> <code></code> set_json_value_custom_property \u00b6 <pre><code>set_json_value_custom_property(key: str, value: JsonValueType)\n</code></pre> <p>Sets a custom property of JSON type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_json_value_custom_property(self, key: str, value: JsonValueType):\n  \"\"\"Sets a custom property of JSON type.\"\"\"\n  self._cached_modifiable_custom_properties[key] = value\n</code></pre> <code></code> set_mlmd_artifact \u00b6 <pre><code>set_mlmd_artifact(artifact: Artifact)\n</code></pre> <p>Replace the MLMD artifact object on this artifact.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_mlmd_artifact(self, artifact: metadata_store_pb2.Artifact):\n  \"\"\"Replace the MLMD artifact object on this artifact.\"\"\"\n  if not isinstance(artifact, metadata_store_pb2.Artifact):\n    raise ValueError(\n        ('Expected instance of metadata_store_pb2.Artifact, got %s '\n         'instead.') % (artifact,))\n  self._artifact = artifact\n  self._cached_modifiable_properties = {}\n  self._cached_modifiable_custom_properties = {}\n</code></pre> <code></code> set_mlmd_artifact_type \u00b6 <pre><code>set_mlmd_artifact_type(artifact_type: ArtifactType)\n</code></pre> <p>Set entire ArtifactType in this object.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_mlmd_artifact_type(self,\n                           artifact_type: metadata_store_pb2.ArtifactType):\n  \"\"\"Set entire ArtifactType in this object.\"\"\"\n  if not isinstance(artifact_type, metadata_store_pb2.ArtifactType):\n    raise ValueError(\n        ('Expected instance of metadata_store_pb2.ArtifactType, got %s '\n         'instead.') % (artifact_type,))\n  self._artifact_type = artifact_type\n  self._artifact.type_id = artifact_type.id\n</code></pre> <code></code> set_proto_custom_property \u00b6 <pre><code>set_proto_custom_property(key: str, value: Message)\n</code></pre> <p>Sets a custom property of proto type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_proto_custom_property(self, key: str, value: message.Message):\n  \"\"\"Sets a custom property of proto type.\"\"\"\n  self._cached_modifiable_custom_properties[key] = value\n</code></pre> <code></code> set_string_custom_property \u00b6 <pre><code>set_string_custom_property(key: str, value: str)\n</code></pre> <p>Set a custom property of string type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_string_custom_property(self, key: str, value: str):\n  \"\"\"Set a custom property of string type.\"\"\"\n  self._artifact.custom_properties[key].string_value = value\n</code></pre> <code></code> to_json_dict \u00b6 <pre><code>to_json_dict() -&gt; Dict[str, Any]\n</code></pre> <p>Convert from an object to a JSON serializable dictionary.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef to_json_dict(self) -&gt; Dict[str, Any]:\n  return {\n      'artifact':\n          json.loads(\n              json_format.MessageToJson(\n                  message=self.mlmd_artifact,\n                  preserving_proto_field_name=True)),\n      'artifact_type':\n          json.loads(\n              json_format.MessageToJson(\n                  message=self._artifact_type,\n                  preserving_proto_field_name=True)),\n      '__artifact_class_module__':\n          self.__class__.__module__,\n      '__artifact_class_name__':\n          self.__class__.__name__,\n  }\n</code></pre>"},{"location":"api/v1/types/#tfx.v1.types.standard_artifacts.ModelBlessing","title":"ModelBlessing","text":"<pre><code>ModelBlessing(*args, **kwargs)\n</code></pre> <p>               Bases: <code>_TfxArtifact</code></p> <p>Artifact that contains the evaluation of a trained model.</p> <p>This artifact is usually used with Conditional when determining whether to push this model on service or not.</p> <pre><code># Run pusher if evaluator has blessed the model.\nwith tfx.dsl.Cond(evaluator.outputs['blessing'].future()\n                  [0].custom_property('blessed') == 1):\n  pusher = Pusher(...)\n</code></pre> <ul> <li>File structure:</li> <li><code>{uri}/</code><ul> <li><code>BLESSED</code>: if the evaluator has blessed the model.</li> <li><code>NOT_BLESSED</code>: if the evaluator has not blessed the model.</li> </ul> </li> <li> <p>See tfx/components/evaluator/executor.py for how to write   ModelBlessing.</p> </li> <li> <p>Commonly used custom properties of the ModelBlessing artifact:</p> </li> <li><code>blessed</code>: int value that represents whether the evaluator has blessed its   model or not.</li> </ul> <p>Construct TFX first-party component artifact.</p> METHOD DESCRIPTION <code>copy_from</code> <p>Set uri, properties and custom properties from a given Artifact.</p> <code>from_json_dict</code> <p>Convert from dictionary data to an object.</p> <code>get_bool_custom_property</code> <p>Get a custom property of bool type.</p> <code>get_custom_property</code> <p>Gets a custom property with key. Return None if not found.</p> <code>get_float_custom_property</code> <p>Gets a custom property of float type.</p> <code>get_int_custom_property</code> <p>Get a custom property of int type.</p> <code>get_json_value_custom_property</code> <p>Get a custom property of JSON type.</p> <code>get_proto_custom_property</code> <p>Get a custom property of proto type.</p> <code>get_string_custom_property</code> <p>Get a custom property of string type.</p> <code>set_bool_custom_property</code> <p>Sets a custom property of bool type.</p> <code>set_float_custom_property</code> <p>Sets a custom property of float type.</p> <code>set_int_custom_property</code> <p>Set a custom property of int type.</p> <code>set_json_value_custom_property</code> <p>Sets a custom property of JSON type.</p> <code>set_mlmd_artifact</code> <p>Replace the MLMD artifact object on this artifact.</p> <code>set_mlmd_artifact_type</code> <p>Set entire ArtifactType in this object.</p> <code>set_proto_custom_property</code> <p>Sets a custom property of proto type.</p> <code>set_string_custom_property</code> <p>Set a custom property of string type.</p> <code>to_json_dict</code> <p>Convert from an object to a JSON serializable dictionary.</p> ATTRIBUTE DESCRIPTION <code>artifact_type</code> <p>Type of the underlying mlmd artifact.</p> <p> </p> <code>external_id</code> <p>external id of the underlying artifact.</p> <p> TYPE: <code>str</code> </p> <code>id</code> <p>Id of the underlying mlmd artifact.</p> <p> TYPE: <code>int</code> </p> <code>is_external</code> <p>Returns true if the artifact is external.</p> <p> TYPE: <code>bool</code> </p> <code>mlmd_artifact</code> <p>Underlying mlmd artifact.</p> <p> </p> <code>name</code> <p>Name of the underlying mlmd artifact.</p> <p> TYPE: <code>str</code> </p> <code>pipeline_name</code> <p>Name of the pipeline that produce the artifact.</p> <p> TYPE: <code>str</code> </p> <code>producer_component</code> <p>Producer component of the artifact.</p> <p> TYPE: <code>str</code> </p> <code>state</code> <p>State of the underlying mlmd artifact.</p> <p> TYPE: <code>str</code> </p> <code>type</code> <p>Type of the artifact.</p> <p> </p> <code>type_id</code> <p>Type id of the underlying mlmd artifact.</p> <p> TYPE: <code>int</code> </p> <code>type_name</code> <p>Type name of the underlying mlmd artifact.</p> <p> </p> <code>uri</code> <p>Artifact URI.</p> <p> TYPE: <code>str</code> </p> Source code in <code>tfx/types/standard_artifacts.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    \"\"\"Construct TFX first-party component artifact.\"\"\"\n    # TODO(b/176795331): Refactor directory structure to make it clearer that\n    # TFX-specific artifacts require the full \"tfx\" package be installed.\n    #\n    # Do not allow usage of TFX-specific artifact if only the core pipeline\n    # SDK package is installed.\n    try:\n        import setuptools # pytype: disable=import-error  # noqa: F401\n\n        # Test import only when setuptools is available.\n        try:\n            # `extensions` is not included in ml_pipelines_sdk and doesn't have any\n            # transitive import.\n            import tfx.extensions as _  # type: ignore  # noqa: F401 # pylint: disable=g-import-not-at-top\n        except ModuleNotFoundError as err:\n            # The following condition detects exactly whether only the DSL package\n            # is installed, and is bypassed when tests run in Bazel.\n            raise RuntimeError(\n                'The \"tfx\" and all dependent packages need to be '\n                \"installed to use this functionality.\"\n            ) from err\n    except ModuleNotFoundError:\n        pass\n\n    super().__init__(*args, **kwargs)\n</code></pre> Attributes\u00b6 <code></code> artifact_type <code>property</code> \u00b6 <pre><code>artifact_type\n</code></pre> <p>Type of the underlying mlmd artifact.</p> <code></code> external_id <code>property</code> \u00b6 <pre><code>external_id: str\n</code></pre> <p>external id of the underlying artifact.</p> <code></code> id <code>property</code> <code>writable</code> \u00b6 <pre><code>id: int\n</code></pre> <p>Id of the underlying mlmd artifact.</p> <code></code> is_external <code>property</code> <code>writable</code> \u00b6 <pre><code>is_external: bool\n</code></pre> <p>Returns true if the artifact is external.</p> <code></code> mlmd_artifact <code>property</code> \u00b6 <pre><code>mlmd_artifact\n</code></pre> <p>Underlying mlmd artifact.</p> <code></code> name <code>property</code> <code>writable</code> \u00b6 <pre><code>name: str\n</code></pre> <p>Name of the underlying mlmd artifact.</p> <code></code> pipeline_name <code>property</code> <code>writable</code> \u00b6 <pre><code>pipeline_name: str\n</code></pre> <p>Name of the pipeline that produce the artifact.</p> <code></code> producer_component <code>property</code> <code>writable</code> \u00b6 <pre><code>producer_component: str\n</code></pre> <p>Producer component of the artifact.</p> <code></code> state <code>property</code> <code>writable</code> \u00b6 <pre><code>state: str\n</code></pre> <p>State of the underlying mlmd artifact.</p> <code></code> type <code>property</code> \u00b6 <pre><code>type\n</code></pre> <p>Type of the artifact.</p> <code></code> type_id <code>property</code> <code>writable</code> \u00b6 <pre><code>type_id: int\n</code></pre> <p>Type id of the underlying mlmd artifact.</p> <code></code> type_name <code>property</code> \u00b6 <pre><code>type_name\n</code></pre> <p>Type name of the underlying mlmd artifact.</p> <code></code> uri <code>property</code> <code>writable</code> \u00b6 <pre><code>uri: str\n</code></pre> <p>Artifact URI.</p> Functions\u00b6 <code></code> copy_from \u00b6 <pre><code>copy_from(other: Artifact)\n</code></pre> <p>Set uri, properties and custom properties from a given Artifact.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef copy_from(self, other: 'Artifact'):\n  \"\"\"Set uri, properties and custom properties from a given Artifact.\"\"\"\n  assert self.type is other.type, (\n      'Unable to set properties from an artifact of different type: {} vs {}'\n      .format(self.type_name, other.type_name))\n  self.uri = other.uri\n  if other.artifact_type.HasField('id'):\n    self.type_id = other.artifact_type.id\n\n  self._artifact.properties.clear()\n  self._artifact.properties.MergeFrom(other._artifact.properties)  # pylint: disable=protected-access\n  self._artifact.custom_properties.clear()\n  self._artifact.custom_properties.MergeFrom(\n      other._artifact.custom_properties)  # pylint: disable=protected-access\n  self._cached_modifiable_properties = copy.deepcopy(\n      other._cached_modifiable_properties)  # pylint: disable=protected-access\n  self._cached_modifiable_custom_properties = copy.deepcopy(\n      other._cached_modifiable_custom_properties)  # pylint: disable=protected-access\n</code></pre> <code></code> from_json_dict <code>classmethod</code> \u00b6 <pre><code>from_json_dict(dict_data: Dict[str, Any]) -&gt; Any\n</code></pre> <p>Convert from dictionary data to an object.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@classmethod\n@doc_controls.do_not_doc_inheritable\ndef from_json_dict(cls, dict_data: Dict[str, Any]) -&gt; Any:\n  module_name = dict_data['__artifact_class_module__']\n  class_name = dict_data['__artifact_class_name__']\n  artifact = metadata_store_pb2.Artifact()\n  artifact_type = metadata_store_pb2.ArtifactType()\n  json_format.Parse(json.dumps(dict_data['artifact']), artifact)\n  json_format.Parse(json.dumps(dict_data['artifact_type']), artifact_type)\n\n  # First, try to resolve the specific class used for the artifact; if this\n  # is not possible, use a generic artifact.Artifact object.\n  result = None\n  try:\n    artifact_cls = getattr(importlib.import_module(module_name), class_name)\n    # If the artifact type is the base Artifact class, do not construct the\n    # object here since that constructor requires the mlmd_artifact_type\n    # argument.\n    if artifact_cls != Artifact:\n      result = artifact_cls()\n  except (AttributeError, ImportError, ValueError):\n    logging.warning((\n        'Could not load artifact class %s.%s; using fallback deserialization '\n        'for the relevant artifact. Please make sure that any artifact '\n        'classes can be imported within your container or environment.'),\n                    module_name, class_name)\n  if not result:\n    result = Artifact(mlmd_artifact_type=artifact_type)\n  result.set_mlmd_artifact_type(artifact_type)\n  result.set_mlmd_artifact(artifact)\n  return result\n</code></pre> <code></code> get_bool_custom_property \u00b6 <pre><code>get_bool_custom_property(key: str) -&gt; bool\n</code></pre> <p>Get a custom property of bool type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_bool_custom_property(self, key: str) -&gt; bool:\n  \"\"\"Get a custom property of bool type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return False\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, bool):\n    return json_value\n  return self._artifact.custom_properties[key].bool_value\n</code></pre> <code></code> get_custom_property \u00b6 <pre><code>get_custom_property(key: str) -&gt; Optional[Union[int, float, str, bool, JsonValueType]]\n</code></pre> <p>Gets a custom property with key. Return None if not found.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_custom_property(\n    self, key: str\n) -&gt; Optional[Union[int, float, str, bool, JsonValueType]]:\n  \"\"\"Gets a custom property with key. Return None if not found.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return None\n\n  json_value = self.get_json_value_custom_property(key)\n  if json_value:\n    return json_value\n\n  mlmd_value = self._artifact.custom_properties[key]\n  if mlmd_value.HasField('int_value'):\n    return mlmd_value.int_value\n  elif mlmd_value.HasField('double_value'):\n    return mlmd_value.double_value\n  elif mlmd_value.HasField('string_value'):\n    return mlmd_value.string_value\n  elif mlmd_value.HasField('bool_value'):\n    return mlmd_value.bool_value\n  return None\n</code></pre> <code></code> get_float_custom_property \u00b6 <pre><code>get_float_custom_property(key: str) -&gt; float\n</code></pre> <p>Gets a custom property of float type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_float_custom_property(self, key: str) -&gt; float:\n  \"\"\"Gets a custom property of float type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return 0.0\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, float):\n    return json_value\n  return self._artifact.custom_properties[key].double_value\n</code></pre> <code></code> get_int_custom_property \u00b6 <pre><code>get_int_custom_property(key: str) -&gt; int\n</code></pre> <p>Get a custom property of int type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_int_custom_property(self, key: str) -&gt; int:\n  \"\"\"Get a custom property of int type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return 0\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, float):\n    return int(json_value)\n  return self._artifact.custom_properties[key].int_value\n</code></pre> <code></code> get_json_value_custom_property \u00b6 <pre><code>get_json_value_custom_property(key: str) -&gt; JsonValueType\n</code></pre> <p>Get a custom property of JSON type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef get_json_value_custom_property(self, key: str) -&gt; JsonValueType:\n  \"\"\"Get a custom property of JSON type.\"\"\"\n  if key in self._cached_modifiable_custom_properties:\n    return self._cached_modifiable_custom_properties[key]\n  if (key not in self._artifact.custom_properties or\n      not self._artifact.custom_properties[key].HasField('struct_value')):\n    return None\n  value = _decode_struct_value(\n      self._artifact.custom_properties[key].struct_value)\n  # We must cache the decoded lists or dictionaries returned here so that\n  # if their recursive contents are modified, the Metadata proto message\n  # can be updated to reflect this.\n  if isinstance(value, (dict, list)):\n    self._cached_modifiable_custom_properties[key] = value\n  return value\n</code></pre> <code></code> get_proto_custom_property \u00b6 <pre><code>get_proto_custom_property(key: str) -&gt; Optional[Message]\n</code></pre> <p>Get a custom property of proto type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef get_proto_custom_property(self, key: str) -&gt; Optional[message.Message]:\n  \"\"\"Get a custom property of proto type.\"\"\"\n  if key in self._cached_modifiable_custom_properties:\n    return self._cached_modifiable_custom_properties[key]\n  if (key not in self._artifact.custom_properties or\n      not self._artifact.custom_properties[key].HasField('proto_value')):\n    return None\n  value = proto_utils.unpack_proto_any(\n      self._artifact.custom_properties[key].proto_value)\n  # We must cache the protobuf message here so that if its contents are\n  # modified, the Metadata proto message can be updated to reflect this.\n  if isinstance(value, message.Message):\n    self._cached_modifiable_custom_properties[key] = value\n  return value\n</code></pre> <code></code> get_string_custom_property \u00b6 <pre><code>get_string_custom_property(key: str) -&gt; str\n</code></pre> <p>Get a custom property of string type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_string_custom_property(self, key: str) -&gt; str:\n  \"\"\"Get a custom property of string type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return ''\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, str):\n    return json_value\n  return self._artifact.custom_properties[key].string_value\n</code></pre> <code></code> set_bool_custom_property \u00b6 <pre><code>set_bool_custom_property(key: str, value: bool)\n</code></pre> <p>Sets a custom property of bool type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_bool_custom_property(self, key: str, value: bool):\n  \"\"\"Sets a custom property of bool type.\"\"\"\n  self._artifact.custom_properties[key].bool_value = value\n</code></pre> <code></code> set_float_custom_property \u00b6 <pre><code>set_float_custom_property(key: str, value: float)\n</code></pre> <p>Sets a custom property of float type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_float_custom_property(self, key: str, value: float):\n  \"\"\"Sets a custom property of float type.\"\"\"\n  self._artifact.custom_properties[key].double_value = builtins.float(value)\n</code></pre> <code></code> set_int_custom_property \u00b6 <pre><code>set_int_custom_property(key: str, value: int)\n</code></pre> <p>Set a custom property of int type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_int_custom_property(self, key: str, value: int):\n  \"\"\"Set a custom property of int type.\"\"\"\n  self._artifact.custom_properties[key].int_value = builtins.int(value)\n</code></pre> <code></code> set_json_value_custom_property \u00b6 <pre><code>set_json_value_custom_property(key: str, value: JsonValueType)\n</code></pre> <p>Sets a custom property of JSON type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_json_value_custom_property(self, key: str, value: JsonValueType):\n  \"\"\"Sets a custom property of JSON type.\"\"\"\n  self._cached_modifiable_custom_properties[key] = value\n</code></pre> <code></code> set_mlmd_artifact \u00b6 <pre><code>set_mlmd_artifact(artifact: Artifact)\n</code></pre> <p>Replace the MLMD artifact object on this artifact.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_mlmd_artifact(self, artifact: metadata_store_pb2.Artifact):\n  \"\"\"Replace the MLMD artifact object on this artifact.\"\"\"\n  if not isinstance(artifact, metadata_store_pb2.Artifact):\n    raise ValueError(\n        ('Expected instance of metadata_store_pb2.Artifact, got %s '\n         'instead.') % (artifact,))\n  self._artifact = artifact\n  self._cached_modifiable_properties = {}\n  self._cached_modifiable_custom_properties = {}\n</code></pre> <code></code> set_mlmd_artifact_type \u00b6 <pre><code>set_mlmd_artifact_type(artifact_type: ArtifactType)\n</code></pre> <p>Set entire ArtifactType in this object.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_mlmd_artifact_type(self,\n                           artifact_type: metadata_store_pb2.ArtifactType):\n  \"\"\"Set entire ArtifactType in this object.\"\"\"\n  if not isinstance(artifact_type, metadata_store_pb2.ArtifactType):\n    raise ValueError(\n        ('Expected instance of metadata_store_pb2.ArtifactType, got %s '\n         'instead.') % (artifact_type,))\n  self._artifact_type = artifact_type\n  self._artifact.type_id = artifact_type.id\n</code></pre> <code></code> set_proto_custom_property \u00b6 <pre><code>set_proto_custom_property(key: str, value: Message)\n</code></pre> <p>Sets a custom property of proto type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_proto_custom_property(self, key: str, value: message.Message):\n  \"\"\"Sets a custom property of proto type.\"\"\"\n  self._cached_modifiable_custom_properties[key] = value\n</code></pre> <code></code> set_string_custom_property \u00b6 <pre><code>set_string_custom_property(key: str, value: str)\n</code></pre> <p>Set a custom property of string type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_string_custom_property(self, key: str, value: str):\n  \"\"\"Set a custom property of string type.\"\"\"\n  self._artifact.custom_properties[key].string_value = value\n</code></pre> <code></code> to_json_dict \u00b6 <pre><code>to_json_dict() -&gt; Dict[str, Any]\n</code></pre> <p>Convert from an object to a JSON serializable dictionary.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef to_json_dict(self) -&gt; Dict[str, Any]:\n  return {\n      'artifact':\n          json.loads(\n              json_format.MessageToJson(\n                  message=self.mlmd_artifact,\n                  preserving_proto_field_name=True)),\n      'artifact_type':\n          json.loads(\n              json_format.MessageToJson(\n                  message=self._artifact_type,\n                  preserving_proto_field_name=True)),\n      '__artifact_class_module__':\n          self.__class__.__module__,\n      '__artifact_class_name__':\n          self.__class__.__name__,\n  }\n</code></pre>"},{"location":"api/v1/types/#tfx.v1.types.standard_artifacts.ModelEvaluation","title":"ModelEvaluation","text":"<pre><code>ModelEvaluation(*args, **kwargs)\n</code></pre> <p>               Bases: <code>_TfxArtifact</code></p> <p>TFX first-party component artifact definition.</p> <p>Construct TFX first-party component artifact.</p> METHOD DESCRIPTION <code>copy_from</code> <p>Set uri, properties and custom properties from a given Artifact.</p> <code>from_json_dict</code> <p>Convert from dictionary data to an object.</p> <code>get_bool_custom_property</code> <p>Get a custom property of bool type.</p> <code>get_custom_property</code> <p>Gets a custom property with key. Return None if not found.</p> <code>get_float_custom_property</code> <p>Gets a custom property of float type.</p> <code>get_int_custom_property</code> <p>Get a custom property of int type.</p> <code>get_json_value_custom_property</code> <p>Get a custom property of JSON type.</p> <code>get_proto_custom_property</code> <p>Get a custom property of proto type.</p> <code>get_string_custom_property</code> <p>Get a custom property of string type.</p> <code>set_bool_custom_property</code> <p>Sets a custom property of bool type.</p> <code>set_float_custom_property</code> <p>Sets a custom property of float type.</p> <code>set_int_custom_property</code> <p>Set a custom property of int type.</p> <code>set_json_value_custom_property</code> <p>Sets a custom property of JSON type.</p> <code>set_mlmd_artifact</code> <p>Replace the MLMD artifact object on this artifact.</p> <code>set_mlmd_artifact_type</code> <p>Set entire ArtifactType in this object.</p> <code>set_proto_custom_property</code> <p>Sets a custom property of proto type.</p> <code>set_string_custom_property</code> <p>Set a custom property of string type.</p> <code>to_json_dict</code> <p>Convert from an object to a JSON serializable dictionary.</p> ATTRIBUTE DESCRIPTION <code>artifact_type</code> <p>Type of the underlying mlmd artifact.</p> <p> </p> <code>external_id</code> <p>external id of the underlying artifact.</p> <p> TYPE: <code>str</code> </p> <code>id</code> <p>Id of the underlying mlmd artifact.</p> <p> TYPE: <code>int</code> </p> <code>is_external</code> <p>Returns true if the artifact is external.</p> <p> TYPE: <code>bool</code> </p> <code>mlmd_artifact</code> <p>Underlying mlmd artifact.</p> <p> </p> <code>name</code> <p>Name of the underlying mlmd artifact.</p> <p> TYPE: <code>str</code> </p> <code>pipeline_name</code> <p>Name of the pipeline that produce the artifact.</p> <p> TYPE: <code>str</code> </p> <code>producer_component</code> <p>Producer component of the artifact.</p> <p> TYPE: <code>str</code> </p> <code>state</code> <p>State of the underlying mlmd artifact.</p> <p> TYPE: <code>str</code> </p> <code>type</code> <p>Type of the artifact.</p> <p> </p> <code>type_id</code> <p>Type id of the underlying mlmd artifact.</p> <p> TYPE: <code>int</code> </p> <code>type_name</code> <p>Type name of the underlying mlmd artifact.</p> <p> </p> <code>uri</code> <p>Artifact URI.</p> <p> TYPE: <code>str</code> </p> Source code in <code>tfx/types/standard_artifacts.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    \"\"\"Construct TFX first-party component artifact.\"\"\"\n    # TODO(b/176795331): Refactor directory structure to make it clearer that\n    # TFX-specific artifacts require the full \"tfx\" package be installed.\n    #\n    # Do not allow usage of TFX-specific artifact if only the core pipeline\n    # SDK package is installed.\n    try:\n        import setuptools # pytype: disable=import-error  # noqa: F401\n\n        # Test import only when setuptools is available.\n        try:\n            # `extensions` is not included in ml_pipelines_sdk and doesn't have any\n            # transitive import.\n            import tfx.extensions as _  # type: ignore  # noqa: F401 # pylint: disable=g-import-not-at-top\n        except ModuleNotFoundError as err:\n            # The following condition detects exactly whether only the DSL package\n            # is installed, and is bypassed when tests run in Bazel.\n            raise RuntimeError(\n                'The \"tfx\" and all dependent packages need to be '\n                \"installed to use this functionality.\"\n            ) from err\n    except ModuleNotFoundError:\n        pass\n\n    super().__init__(*args, **kwargs)\n</code></pre> Attributes\u00b6 <code></code> artifact_type <code>property</code> \u00b6 <pre><code>artifact_type\n</code></pre> <p>Type of the underlying mlmd artifact.</p> <code></code> external_id <code>property</code> \u00b6 <pre><code>external_id: str\n</code></pre> <p>external id of the underlying artifact.</p> <code></code> id <code>property</code> <code>writable</code> \u00b6 <pre><code>id: int\n</code></pre> <p>Id of the underlying mlmd artifact.</p> <code></code> is_external <code>property</code> <code>writable</code> \u00b6 <pre><code>is_external: bool\n</code></pre> <p>Returns true if the artifact is external.</p> <code></code> mlmd_artifact <code>property</code> \u00b6 <pre><code>mlmd_artifact\n</code></pre> <p>Underlying mlmd artifact.</p> <code></code> name <code>property</code> <code>writable</code> \u00b6 <pre><code>name: str\n</code></pre> <p>Name of the underlying mlmd artifact.</p> <code></code> pipeline_name <code>property</code> <code>writable</code> \u00b6 <pre><code>pipeline_name: str\n</code></pre> <p>Name of the pipeline that produce the artifact.</p> <code></code> producer_component <code>property</code> <code>writable</code> \u00b6 <pre><code>producer_component: str\n</code></pre> <p>Producer component of the artifact.</p> <code></code> state <code>property</code> <code>writable</code> \u00b6 <pre><code>state: str\n</code></pre> <p>State of the underlying mlmd artifact.</p> <code></code> type <code>property</code> \u00b6 <pre><code>type\n</code></pre> <p>Type of the artifact.</p> <code></code> type_id <code>property</code> <code>writable</code> \u00b6 <pre><code>type_id: int\n</code></pre> <p>Type id of the underlying mlmd artifact.</p> <code></code> type_name <code>property</code> \u00b6 <pre><code>type_name\n</code></pre> <p>Type name of the underlying mlmd artifact.</p> <code></code> uri <code>property</code> <code>writable</code> \u00b6 <pre><code>uri: str\n</code></pre> <p>Artifact URI.</p> Functions\u00b6 <code></code> copy_from \u00b6 <pre><code>copy_from(other: Artifact)\n</code></pre> <p>Set uri, properties and custom properties from a given Artifact.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef copy_from(self, other: 'Artifact'):\n  \"\"\"Set uri, properties and custom properties from a given Artifact.\"\"\"\n  assert self.type is other.type, (\n      'Unable to set properties from an artifact of different type: {} vs {}'\n      .format(self.type_name, other.type_name))\n  self.uri = other.uri\n  if other.artifact_type.HasField('id'):\n    self.type_id = other.artifact_type.id\n\n  self._artifact.properties.clear()\n  self._artifact.properties.MergeFrom(other._artifact.properties)  # pylint: disable=protected-access\n  self._artifact.custom_properties.clear()\n  self._artifact.custom_properties.MergeFrom(\n      other._artifact.custom_properties)  # pylint: disable=protected-access\n  self._cached_modifiable_properties = copy.deepcopy(\n      other._cached_modifiable_properties)  # pylint: disable=protected-access\n  self._cached_modifiable_custom_properties = copy.deepcopy(\n      other._cached_modifiable_custom_properties)  # pylint: disable=protected-access\n</code></pre> <code></code> from_json_dict <code>classmethod</code> \u00b6 <pre><code>from_json_dict(dict_data: Dict[str, Any]) -&gt; Any\n</code></pre> <p>Convert from dictionary data to an object.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@classmethod\n@doc_controls.do_not_doc_inheritable\ndef from_json_dict(cls, dict_data: Dict[str, Any]) -&gt; Any:\n  module_name = dict_data['__artifact_class_module__']\n  class_name = dict_data['__artifact_class_name__']\n  artifact = metadata_store_pb2.Artifact()\n  artifact_type = metadata_store_pb2.ArtifactType()\n  json_format.Parse(json.dumps(dict_data['artifact']), artifact)\n  json_format.Parse(json.dumps(dict_data['artifact_type']), artifact_type)\n\n  # First, try to resolve the specific class used for the artifact; if this\n  # is not possible, use a generic artifact.Artifact object.\n  result = None\n  try:\n    artifact_cls = getattr(importlib.import_module(module_name), class_name)\n    # If the artifact type is the base Artifact class, do not construct the\n    # object here since that constructor requires the mlmd_artifact_type\n    # argument.\n    if artifact_cls != Artifact:\n      result = artifact_cls()\n  except (AttributeError, ImportError, ValueError):\n    logging.warning((\n        'Could not load artifact class %s.%s; using fallback deserialization '\n        'for the relevant artifact. Please make sure that any artifact '\n        'classes can be imported within your container or environment.'),\n                    module_name, class_name)\n  if not result:\n    result = Artifact(mlmd_artifact_type=artifact_type)\n  result.set_mlmd_artifact_type(artifact_type)\n  result.set_mlmd_artifact(artifact)\n  return result\n</code></pre> <code></code> get_bool_custom_property \u00b6 <pre><code>get_bool_custom_property(key: str) -&gt; bool\n</code></pre> <p>Get a custom property of bool type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_bool_custom_property(self, key: str) -&gt; bool:\n  \"\"\"Get a custom property of bool type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return False\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, bool):\n    return json_value\n  return self._artifact.custom_properties[key].bool_value\n</code></pre> <code></code> get_custom_property \u00b6 <pre><code>get_custom_property(key: str) -&gt; Optional[Union[int, float, str, bool, JsonValueType]]\n</code></pre> <p>Gets a custom property with key. Return None if not found.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_custom_property(\n    self, key: str\n) -&gt; Optional[Union[int, float, str, bool, JsonValueType]]:\n  \"\"\"Gets a custom property with key. Return None if not found.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return None\n\n  json_value = self.get_json_value_custom_property(key)\n  if json_value:\n    return json_value\n\n  mlmd_value = self._artifact.custom_properties[key]\n  if mlmd_value.HasField('int_value'):\n    return mlmd_value.int_value\n  elif mlmd_value.HasField('double_value'):\n    return mlmd_value.double_value\n  elif mlmd_value.HasField('string_value'):\n    return mlmd_value.string_value\n  elif mlmd_value.HasField('bool_value'):\n    return mlmd_value.bool_value\n  return None\n</code></pre> <code></code> get_float_custom_property \u00b6 <pre><code>get_float_custom_property(key: str) -&gt; float\n</code></pre> <p>Gets a custom property of float type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_float_custom_property(self, key: str) -&gt; float:\n  \"\"\"Gets a custom property of float type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return 0.0\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, float):\n    return json_value\n  return self._artifact.custom_properties[key].double_value\n</code></pre> <code></code> get_int_custom_property \u00b6 <pre><code>get_int_custom_property(key: str) -&gt; int\n</code></pre> <p>Get a custom property of int type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_int_custom_property(self, key: str) -&gt; int:\n  \"\"\"Get a custom property of int type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return 0\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, float):\n    return int(json_value)\n  return self._artifact.custom_properties[key].int_value\n</code></pre> <code></code> get_json_value_custom_property \u00b6 <pre><code>get_json_value_custom_property(key: str) -&gt; JsonValueType\n</code></pre> <p>Get a custom property of JSON type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef get_json_value_custom_property(self, key: str) -&gt; JsonValueType:\n  \"\"\"Get a custom property of JSON type.\"\"\"\n  if key in self._cached_modifiable_custom_properties:\n    return self._cached_modifiable_custom_properties[key]\n  if (key not in self._artifact.custom_properties or\n      not self._artifact.custom_properties[key].HasField('struct_value')):\n    return None\n  value = _decode_struct_value(\n      self._artifact.custom_properties[key].struct_value)\n  # We must cache the decoded lists or dictionaries returned here so that\n  # if their recursive contents are modified, the Metadata proto message\n  # can be updated to reflect this.\n  if isinstance(value, (dict, list)):\n    self._cached_modifiable_custom_properties[key] = value\n  return value\n</code></pre> <code></code> get_proto_custom_property \u00b6 <pre><code>get_proto_custom_property(key: str) -&gt; Optional[Message]\n</code></pre> <p>Get a custom property of proto type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef get_proto_custom_property(self, key: str) -&gt; Optional[message.Message]:\n  \"\"\"Get a custom property of proto type.\"\"\"\n  if key in self._cached_modifiable_custom_properties:\n    return self._cached_modifiable_custom_properties[key]\n  if (key not in self._artifact.custom_properties or\n      not self._artifact.custom_properties[key].HasField('proto_value')):\n    return None\n  value = proto_utils.unpack_proto_any(\n      self._artifact.custom_properties[key].proto_value)\n  # We must cache the protobuf message here so that if its contents are\n  # modified, the Metadata proto message can be updated to reflect this.\n  if isinstance(value, message.Message):\n    self._cached_modifiable_custom_properties[key] = value\n  return value\n</code></pre> <code></code> get_string_custom_property \u00b6 <pre><code>get_string_custom_property(key: str) -&gt; str\n</code></pre> <p>Get a custom property of string type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_string_custom_property(self, key: str) -&gt; str:\n  \"\"\"Get a custom property of string type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return ''\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, str):\n    return json_value\n  return self._artifact.custom_properties[key].string_value\n</code></pre> <code></code> set_bool_custom_property \u00b6 <pre><code>set_bool_custom_property(key: str, value: bool)\n</code></pre> <p>Sets a custom property of bool type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_bool_custom_property(self, key: str, value: bool):\n  \"\"\"Sets a custom property of bool type.\"\"\"\n  self._artifact.custom_properties[key].bool_value = value\n</code></pre> <code></code> set_float_custom_property \u00b6 <pre><code>set_float_custom_property(key: str, value: float)\n</code></pre> <p>Sets a custom property of float type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_float_custom_property(self, key: str, value: float):\n  \"\"\"Sets a custom property of float type.\"\"\"\n  self._artifact.custom_properties[key].double_value = builtins.float(value)\n</code></pre> <code></code> set_int_custom_property \u00b6 <pre><code>set_int_custom_property(key: str, value: int)\n</code></pre> <p>Set a custom property of int type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_int_custom_property(self, key: str, value: int):\n  \"\"\"Set a custom property of int type.\"\"\"\n  self._artifact.custom_properties[key].int_value = builtins.int(value)\n</code></pre> <code></code> set_json_value_custom_property \u00b6 <pre><code>set_json_value_custom_property(key: str, value: JsonValueType)\n</code></pre> <p>Sets a custom property of JSON type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_json_value_custom_property(self, key: str, value: JsonValueType):\n  \"\"\"Sets a custom property of JSON type.\"\"\"\n  self._cached_modifiable_custom_properties[key] = value\n</code></pre> <code></code> set_mlmd_artifact \u00b6 <pre><code>set_mlmd_artifact(artifact: Artifact)\n</code></pre> <p>Replace the MLMD artifact object on this artifact.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_mlmd_artifact(self, artifact: metadata_store_pb2.Artifact):\n  \"\"\"Replace the MLMD artifact object on this artifact.\"\"\"\n  if not isinstance(artifact, metadata_store_pb2.Artifact):\n    raise ValueError(\n        ('Expected instance of metadata_store_pb2.Artifact, got %s '\n         'instead.') % (artifact,))\n  self._artifact = artifact\n  self._cached_modifiable_properties = {}\n  self._cached_modifiable_custom_properties = {}\n</code></pre> <code></code> set_mlmd_artifact_type \u00b6 <pre><code>set_mlmd_artifact_type(artifact_type: ArtifactType)\n</code></pre> <p>Set entire ArtifactType in this object.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_mlmd_artifact_type(self,\n                           artifact_type: metadata_store_pb2.ArtifactType):\n  \"\"\"Set entire ArtifactType in this object.\"\"\"\n  if not isinstance(artifact_type, metadata_store_pb2.ArtifactType):\n    raise ValueError(\n        ('Expected instance of metadata_store_pb2.ArtifactType, got %s '\n         'instead.') % (artifact_type,))\n  self._artifact_type = artifact_type\n  self._artifact.type_id = artifact_type.id\n</code></pre> <code></code> set_proto_custom_property \u00b6 <pre><code>set_proto_custom_property(key: str, value: Message)\n</code></pre> <p>Sets a custom property of proto type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_proto_custom_property(self, key: str, value: message.Message):\n  \"\"\"Sets a custom property of proto type.\"\"\"\n  self._cached_modifiable_custom_properties[key] = value\n</code></pre> <code></code> set_string_custom_property \u00b6 <pre><code>set_string_custom_property(key: str, value: str)\n</code></pre> <p>Set a custom property of string type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_string_custom_property(self, key: str, value: str):\n  \"\"\"Set a custom property of string type.\"\"\"\n  self._artifact.custom_properties[key].string_value = value\n</code></pre> <code></code> to_json_dict \u00b6 <pre><code>to_json_dict() -&gt; Dict[str, Any]\n</code></pre> <p>Convert from an object to a JSON serializable dictionary.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef to_json_dict(self) -&gt; Dict[str, Any]:\n  return {\n      'artifact':\n          json.loads(\n              json_format.MessageToJson(\n                  message=self.mlmd_artifact,\n                  preserving_proto_field_name=True)),\n      'artifact_type':\n          json.loads(\n              json_format.MessageToJson(\n                  message=self._artifact_type,\n                  preserving_proto_field_name=True)),\n      '__artifact_class_module__':\n          self.__class__.__module__,\n      '__artifact_class_name__':\n          self.__class__.__name__,\n  }\n</code></pre>"},{"location":"api/v1/types/#tfx.v1.types.standard_artifacts.ModelRun","title":"ModelRun","text":"<pre><code>ModelRun(*args, **kwargs)\n</code></pre> <p>               Bases: <code>_TfxArtifact</code></p> <p>TFX first-party component artifact definition.</p> <p>Construct TFX first-party component artifact.</p> METHOD DESCRIPTION <code>copy_from</code> <p>Set uri, properties and custom properties from a given Artifact.</p> <code>from_json_dict</code> <p>Convert from dictionary data to an object.</p> <code>get_bool_custom_property</code> <p>Get a custom property of bool type.</p> <code>get_custom_property</code> <p>Gets a custom property with key. Return None if not found.</p> <code>get_float_custom_property</code> <p>Gets a custom property of float type.</p> <code>get_int_custom_property</code> <p>Get a custom property of int type.</p> <code>get_json_value_custom_property</code> <p>Get a custom property of JSON type.</p> <code>get_proto_custom_property</code> <p>Get a custom property of proto type.</p> <code>get_string_custom_property</code> <p>Get a custom property of string type.</p> <code>set_bool_custom_property</code> <p>Sets a custom property of bool type.</p> <code>set_float_custom_property</code> <p>Sets a custom property of float type.</p> <code>set_int_custom_property</code> <p>Set a custom property of int type.</p> <code>set_json_value_custom_property</code> <p>Sets a custom property of JSON type.</p> <code>set_mlmd_artifact</code> <p>Replace the MLMD artifact object on this artifact.</p> <code>set_mlmd_artifact_type</code> <p>Set entire ArtifactType in this object.</p> <code>set_proto_custom_property</code> <p>Sets a custom property of proto type.</p> <code>set_string_custom_property</code> <p>Set a custom property of string type.</p> <code>to_json_dict</code> <p>Convert from an object to a JSON serializable dictionary.</p> ATTRIBUTE DESCRIPTION <code>artifact_type</code> <p>Type of the underlying mlmd artifact.</p> <p> </p> <code>external_id</code> <p>external id of the underlying artifact.</p> <p> TYPE: <code>str</code> </p> <code>id</code> <p>Id of the underlying mlmd artifact.</p> <p> TYPE: <code>int</code> </p> <code>is_external</code> <p>Returns true if the artifact is external.</p> <p> TYPE: <code>bool</code> </p> <code>mlmd_artifact</code> <p>Underlying mlmd artifact.</p> <p> </p> <code>name</code> <p>Name of the underlying mlmd artifact.</p> <p> TYPE: <code>str</code> </p> <code>pipeline_name</code> <p>Name of the pipeline that produce the artifact.</p> <p> TYPE: <code>str</code> </p> <code>producer_component</code> <p>Producer component of the artifact.</p> <p> TYPE: <code>str</code> </p> <code>state</code> <p>State of the underlying mlmd artifact.</p> <p> TYPE: <code>str</code> </p> <code>type</code> <p>Type of the artifact.</p> <p> </p> <code>type_id</code> <p>Type id of the underlying mlmd artifact.</p> <p> TYPE: <code>int</code> </p> <code>type_name</code> <p>Type name of the underlying mlmd artifact.</p> <p> </p> <code>uri</code> <p>Artifact URI.</p> <p> TYPE: <code>str</code> </p> Source code in <code>tfx/types/standard_artifacts.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    \"\"\"Construct TFX first-party component artifact.\"\"\"\n    # TODO(b/176795331): Refactor directory structure to make it clearer that\n    # TFX-specific artifacts require the full \"tfx\" package be installed.\n    #\n    # Do not allow usage of TFX-specific artifact if only the core pipeline\n    # SDK package is installed.\n    try:\n        import setuptools # pytype: disable=import-error  # noqa: F401\n\n        # Test import only when setuptools is available.\n        try:\n            # `extensions` is not included in ml_pipelines_sdk and doesn't have any\n            # transitive import.\n            import tfx.extensions as _  # type: ignore  # noqa: F401 # pylint: disable=g-import-not-at-top\n        except ModuleNotFoundError as err:\n            # The following condition detects exactly whether only the DSL package\n            # is installed, and is bypassed when tests run in Bazel.\n            raise RuntimeError(\n                'The \"tfx\" and all dependent packages need to be '\n                \"installed to use this functionality.\"\n            ) from err\n    except ModuleNotFoundError:\n        pass\n\n    super().__init__(*args, **kwargs)\n</code></pre> Attributes\u00b6 <code></code> artifact_type <code>property</code> \u00b6 <pre><code>artifact_type\n</code></pre> <p>Type of the underlying mlmd artifact.</p> <code></code> external_id <code>property</code> \u00b6 <pre><code>external_id: str\n</code></pre> <p>external id of the underlying artifact.</p> <code></code> id <code>property</code> <code>writable</code> \u00b6 <pre><code>id: int\n</code></pre> <p>Id of the underlying mlmd artifact.</p> <code></code> is_external <code>property</code> <code>writable</code> \u00b6 <pre><code>is_external: bool\n</code></pre> <p>Returns true if the artifact is external.</p> <code></code> mlmd_artifact <code>property</code> \u00b6 <pre><code>mlmd_artifact\n</code></pre> <p>Underlying mlmd artifact.</p> <code></code> name <code>property</code> <code>writable</code> \u00b6 <pre><code>name: str\n</code></pre> <p>Name of the underlying mlmd artifact.</p> <code></code> pipeline_name <code>property</code> <code>writable</code> \u00b6 <pre><code>pipeline_name: str\n</code></pre> <p>Name of the pipeline that produce the artifact.</p> <code></code> producer_component <code>property</code> <code>writable</code> \u00b6 <pre><code>producer_component: str\n</code></pre> <p>Producer component of the artifact.</p> <code></code> state <code>property</code> <code>writable</code> \u00b6 <pre><code>state: str\n</code></pre> <p>State of the underlying mlmd artifact.</p> <code></code> type <code>property</code> \u00b6 <pre><code>type\n</code></pre> <p>Type of the artifact.</p> <code></code> type_id <code>property</code> <code>writable</code> \u00b6 <pre><code>type_id: int\n</code></pre> <p>Type id of the underlying mlmd artifact.</p> <code></code> type_name <code>property</code> \u00b6 <pre><code>type_name\n</code></pre> <p>Type name of the underlying mlmd artifact.</p> <code></code> uri <code>property</code> <code>writable</code> \u00b6 <pre><code>uri: str\n</code></pre> <p>Artifact URI.</p> Functions\u00b6 <code></code> copy_from \u00b6 <pre><code>copy_from(other: Artifact)\n</code></pre> <p>Set uri, properties and custom properties from a given Artifact.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef copy_from(self, other: 'Artifact'):\n  \"\"\"Set uri, properties and custom properties from a given Artifact.\"\"\"\n  assert self.type is other.type, (\n      'Unable to set properties from an artifact of different type: {} vs {}'\n      .format(self.type_name, other.type_name))\n  self.uri = other.uri\n  if other.artifact_type.HasField('id'):\n    self.type_id = other.artifact_type.id\n\n  self._artifact.properties.clear()\n  self._artifact.properties.MergeFrom(other._artifact.properties)  # pylint: disable=protected-access\n  self._artifact.custom_properties.clear()\n  self._artifact.custom_properties.MergeFrom(\n      other._artifact.custom_properties)  # pylint: disable=protected-access\n  self._cached_modifiable_properties = copy.deepcopy(\n      other._cached_modifiable_properties)  # pylint: disable=protected-access\n  self._cached_modifiable_custom_properties = copy.deepcopy(\n      other._cached_modifiable_custom_properties)  # pylint: disable=protected-access\n</code></pre> <code></code> from_json_dict <code>classmethod</code> \u00b6 <pre><code>from_json_dict(dict_data: Dict[str, Any]) -&gt; Any\n</code></pre> <p>Convert from dictionary data to an object.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@classmethod\n@doc_controls.do_not_doc_inheritable\ndef from_json_dict(cls, dict_data: Dict[str, Any]) -&gt; Any:\n  module_name = dict_data['__artifact_class_module__']\n  class_name = dict_data['__artifact_class_name__']\n  artifact = metadata_store_pb2.Artifact()\n  artifact_type = metadata_store_pb2.ArtifactType()\n  json_format.Parse(json.dumps(dict_data['artifact']), artifact)\n  json_format.Parse(json.dumps(dict_data['artifact_type']), artifact_type)\n\n  # First, try to resolve the specific class used for the artifact; if this\n  # is not possible, use a generic artifact.Artifact object.\n  result = None\n  try:\n    artifact_cls = getattr(importlib.import_module(module_name), class_name)\n    # If the artifact type is the base Artifact class, do not construct the\n    # object here since that constructor requires the mlmd_artifact_type\n    # argument.\n    if artifact_cls != Artifact:\n      result = artifact_cls()\n  except (AttributeError, ImportError, ValueError):\n    logging.warning((\n        'Could not load artifact class %s.%s; using fallback deserialization '\n        'for the relevant artifact. Please make sure that any artifact '\n        'classes can be imported within your container or environment.'),\n                    module_name, class_name)\n  if not result:\n    result = Artifact(mlmd_artifact_type=artifact_type)\n  result.set_mlmd_artifact_type(artifact_type)\n  result.set_mlmd_artifact(artifact)\n  return result\n</code></pre> <code></code> get_bool_custom_property \u00b6 <pre><code>get_bool_custom_property(key: str) -&gt; bool\n</code></pre> <p>Get a custom property of bool type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_bool_custom_property(self, key: str) -&gt; bool:\n  \"\"\"Get a custom property of bool type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return False\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, bool):\n    return json_value\n  return self._artifact.custom_properties[key].bool_value\n</code></pre> <code></code> get_custom_property \u00b6 <pre><code>get_custom_property(key: str) -&gt; Optional[Union[int, float, str, bool, JsonValueType]]\n</code></pre> <p>Gets a custom property with key. Return None if not found.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_custom_property(\n    self, key: str\n) -&gt; Optional[Union[int, float, str, bool, JsonValueType]]:\n  \"\"\"Gets a custom property with key. Return None if not found.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return None\n\n  json_value = self.get_json_value_custom_property(key)\n  if json_value:\n    return json_value\n\n  mlmd_value = self._artifact.custom_properties[key]\n  if mlmd_value.HasField('int_value'):\n    return mlmd_value.int_value\n  elif mlmd_value.HasField('double_value'):\n    return mlmd_value.double_value\n  elif mlmd_value.HasField('string_value'):\n    return mlmd_value.string_value\n  elif mlmd_value.HasField('bool_value'):\n    return mlmd_value.bool_value\n  return None\n</code></pre> <code></code> get_float_custom_property \u00b6 <pre><code>get_float_custom_property(key: str) -&gt; float\n</code></pre> <p>Gets a custom property of float type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_float_custom_property(self, key: str) -&gt; float:\n  \"\"\"Gets a custom property of float type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return 0.0\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, float):\n    return json_value\n  return self._artifact.custom_properties[key].double_value\n</code></pre> <code></code> get_int_custom_property \u00b6 <pre><code>get_int_custom_property(key: str) -&gt; int\n</code></pre> <p>Get a custom property of int type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_int_custom_property(self, key: str) -&gt; int:\n  \"\"\"Get a custom property of int type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return 0\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, float):\n    return int(json_value)\n  return self._artifact.custom_properties[key].int_value\n</code></pre> <code></code> get_json_value_custom_property \u00b6 <pre><code>get_json_value_custom_property(key: str) -&gt; JsonValueType\n</code></pre> <p>Get a custom property of JSON type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef get_json_value_custom_property(self, key: str) -&gt; JsonValueType:\n  \"\"\"Get a custom property of JSON type.\"\"\"\n  if key in self._cached_modifiable_custom_properties:\n    return self._cached_modifiable_custom_properties[key]\n  if (key not in self._artifact.custom_properties or\n      not self._artifact.custom_properties[key].HasField('struct_value')):\n    return None\n  value = _decode_struct_value(\n      self._artifact.custom_properties[key].struct_value)\n  # We must cache the decoded lists or dictionaries returned here so that\n  # if their recursive contents are modified, the Metadata proto message\n  # can be updated to reflect this.\n  if isinstance(value, (dict, list)):\n    self._cached_modifiable_custom_properties[key] = value\n  return value\n</code></pre> <code></code> get_proto_custom_property \u00b6 <pre><code>get_proto_custom_property(key: str) -&gt; Optional[Message]\n</code></pre> <p>Get a custom property of proto type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef get_proto_custom_property(self, key: str) -&gt; Optional[message.Message]:\n  \"\"\"Get a custom property of proto type.\"\"\"\n  if key in self._cached_modifiable_custom_properties:\n    return self._cached_modifiable_custom_properties[key]\n  if (key not in self._artifact.custom_properties or\n      not self._artifact.custom_properties[key].HasField('proto_value')):\n    return None\n  value = proto_utils.unpack_proto_any(\n      self._artifact.custom_properties[key].proto_value)\n  # We must cache the protobuf message here so that if its contents are\n  # modified, the Metadata proto message can be updated to reflect this.\n  if isinstance(value, message.Message):\n    self._cached_modifiable_custom_properties[key] = value\n  return value\n</code></pre> <code></code> get_string_custom_property \u00b6 <pre><code>get_string_custom_property(key: str) -&gt; str\n</code></pre> <p>Get a custom property of string type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_string_custom_property(self, key: str) -&gt; str:\n  \"\"\"Get a custom property of string type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return ''\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, str):\n    return json_value\n  return self._artifact.custom_properties[key].string_value\n</code></pre> <code></code> set_bool_custom_property \u00b6 <pre><code>set_bool_custom_property(key: str, value: bool)\n</code></pre> <p>Sets a custom property of bool type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_bool_custom_property(self, key: str, value: bool):\n  \"\"\"Sets a custom property of bool type.\"\"\"\n  self._artifact.custom_properties[key].bool_value = value\n</code></pre> <code></code> set_float_custom_property \u00b6 <pre><code>set_float_custom_property(key: str, value: float)\n</code></pre> <p>Sets a custom property of float type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_float_custom_property(self, key: str, value: float):\n  \"\"\"Sets a custom property of float type.\"\"\"\n  self._artifact.custom_properties[key].double_value = builtins.float(value)\n</code></pre> <code></code> set_int_custom_property \u00b6 <pre><code>set_int_custom_property(key: str, value: int)\n</code></pre> <p>Set a custom property of int type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_int_custom_property(self, key: str, value: int):\n  \"\"\"Set a custom property of int type.\"\"\"\n  self._artifact.custom_properties[key].int_value = builtins.int(value)\n</code></pre> <code></code> set_json_value_custom_property \u00b6 <pre><code>set_json_value_custom_property(key: str, value: JsonValueType)\n</code></pre> <p>Sets a custom property of JSON type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_json_value_custom_property(self, key: str, value: JsonValueType):\n  \"\"\"Sets a custom property of JSON type.\"\"\"\n  self._cached_modifiable_custom_properties[key] = value\n</code></pre> <code></code> set_mlmd_artifact \u00b6 <pre><code>set_mlmd_artifact(artifact: Artifact)\n</code></pre> <p>Replace the MLMD artifact object on this artifact.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_mlmd_artifact(self, artifact: metadata_store_pb2.Artifact):\n  \"\"\"Replace the MLMD artifact object on this artifact.\"\"\"\n  if not isinstance(artifact, metadata_store_pb2.Artifact):\n    raise ValueError(\n        ('Expected instance of metadata_store_pb2.Artifact, got %s '\n         'instead.') % (artifact,))\n  self._artifact = artifact\n  self._cached_modifiable_properties = {}\n  self._cached_modifiable_custom_properties = {}\n</code></pre> <code></code> set_mlmd_artifact_type \u00b6 <pre><code>set_mlmd_artifact_type(artifact_type: ArtifactType)\n</code></pre> <p>Set entire ArtifactType in this object.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_mlmd_artifact_type(self,\n                           artifact_type: metadata_store_pb2.ArtifactType):\n  \"\"\"Set entire ArtifactType in this object.\"\"\"\n  if not isinstance(artifact_type, metadata_store_pb2.ArtifactType):\n    raise ValueError(\n        ('Expected instance of metadata_store_pb2.ArtifactType, got %s '\n         'instead.') % (artifact_type,))\n  self._artifact_type = artifact_type\n  self._artifact.type_id = artifact_type.id\n</code></pre> <code></code> set_proto_custom_property \u00b6 <pre><code>set_proto_custom_property(key: str, value: Message)\n</code></pre> <p>Sets a custom property of proto type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_proto_custom_property(self, key: str, value: message.Message):\n  \"\"\"Sets a custom property of proto type.\"\"\"\n  self._cached_modifiable_custom_properties[key] = value\n</code></pre> <code></code> set_string_custom_property \u00b6 <pre><code>set_string_custom_property(key: str, value: str)\n</code></pre> <p>Set a custom property of string type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_string_custom_property(self, key: str, value: str):\n  \"\"\"Set a custom property of string type.\"\"\"\n  self._artifact.custom_properties[key].string_value = value\n</code></pre> <code></code> to_json_dict \u00b6 <pre><code>to_json_dict() -&gt; Dict[str, Any]\n</code></pre> <p>Convert from an object to a JSON serializable dictionary.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef to_json_dict(self) -&gt; Dict[str, Any]:\n  return {\n      'artifact':\n          json.loads(\n              json_format.MessageToJson(\n                  message=self.mlmd_artifact,\n                  preserving_proto_field_name=True)),\n      'artifact_type':\n          json.loads(\n              json_format.MessageToJson(\n                  message=self._artifact_type,\n                  preserving_proto_field_name=True)),\n      '__artifact_class_module__':\n          self.__class__.__module__,\n      '__artifact_class_name__':\n          self.__class__.__name__,\n  }\n</code></pre>"},{"location":"api/v1/types/#tfx.v1.types.standard_artifacts.PushedModel","title":"PushedModel","text":"<pre><code>PushedModel(*args, **kwargs)\n</code></pre> <p>               Bases: <code>_TfxArtifact</code></p> <p>TFX first-party component artifact definition.</p> <p>Construct TFX first-party component artifact.</p> METHOD DESCRIPTION <code>copy_from</code> <p>Set uri, properties and custom properties from a given Artifact.</p> <code>from_json_dict</code> <p>Convert from dictionary data to an object.</p> <code>get_bool_custom_property</code> <p>Get a custom property of bool type.</p> <code>get_custom_property</code> <p>Gets a custom property with key. Return None if not found.</p> <code>get_float_custom_property</code> <p>Gets a custom property of float type.</p> <code>get_int_custom_property</code> <p>Get a custom property of int type.</p> <code>get_json_value_custom_property</code> <p>Get a custom property of JSON type.</p> <code>get_proto_custom_property</code> <p>Get a custom property of proto type.</p> <code>get_string_custom_property</code> <p>Get a custom property of string type.</p> <code>set_bool_custom_property</code> <p>Sets a custom property of bool type.</p> <code>set_float_custom_property</code> <p>Sets a custom property of float type.</p> <code>set_int_custom_property</code> <p>Set a custom property of int type.</p> <code>set_json_value_custom_property</code> <p>Sets a custom property of JSON type.</p> <code>set_mlmd_artifact</code> <p>Replace the MLMD artifact object on this artifact.</p> <code>set_mlmd_artifact_type</code> <p>Set entire ArtifactType in this object.</p> <code>set_proto_custom_property</code> <p>Sets a custom property of proto type.</p> <code>set_string_custom_property</code> <p>Set a custom property of string type.</p> <code>to_json_dict</code> <p>Convert from an object to a JSON serializable dictionary.</p> ATTRIBUTE DESCRIPTION <code>artifact_type</code> <p>Type of the underlying mlmd artifact.</p> <p> </p> <code>external_id</code> <p>external id of the underlying artifact.</p> <p> TYPE: <code>str</code> </p> <code>id</code> <p>Id of the underlying mlmd artifact.</p> <p> TYPE: <code>int</code> </p> <code>is_external</code> <p>Returns true if the artifact is external.</p> <p> TYPE: <code>bool</code> </p> <code>mlmd_artifact</code> <p>Underlying mlmd artifact.</p> <p> </p> <code>name</code> <p>Name of the underlying mlmd artifact.</p> <p> TYPE: <code>str</code> </p> <code>pipeline_name</code> <p>Name of the pipeline that produce the artifact.</p> <p> TYPE: <code>str</code> </p> <code>producer_component</code> <p>Producer component of the artifact.</p> <p> TYPE: <code>str</code> </p> <code>state</code> <p>State of the underlying mlmd artifact.</p> <p> TYPE: <code>str</code> </p> <code>type</code> <p>Type of the artifact.</p> <p> </p> <code>type_id</code> <p>Type id of the underlying mlmd artifact.</p> <p> TYPE: <code>int</code> </p> <code>type_name</code> <p>Type name of the underlying mlmd artifact.</p> <p> </p> <code>uri</code> <p>Artifact URI.</p> <p> TYPE: <code>str</code> </p> Source code in <code>tfx/types/standard_artifacts.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    \"\"\"Construct TFX first-party component artifact.\"\"\"\n    # TODO(b/176795331): Refactor directory structure to make it clearer that\n    # TFX-specific artifacts require the full \"tfx\" package be installed.\n    #\n    # Do not allow usage of TFX-specific artifact if only the core pipeline\n    # SDK package is installed.\n    try:\n        import setuptools # pytype: disable=import-error  # noqa: F401\n\n        # Test import only when setuptools is available.\n        try:\n            # `extensions` is not included in ml_pipelines_sdk and doesn't have any\n            # transitive import.\n            import tfx.extensions as _  # type: ignore  # noqa: F401 # pylint: disable=g-import-not-at-top\n        except ModuleNotFoundError as err:\n            # The following condition detects exactly whether only the DSL package\n            # is installed, and is bypassed when tests run in Bazel.\n            raise RuntimeError(\n                'The \"tfx\" and all dependent packages need to be '\n                \"installed to use this functionality.\"\n            ) from err\n    except ModuleNotFoundError:\n        pass\n\n    super().__init__(*args, **kwargs)\n</code></pre> Attributes\u00b6 <code></code> artifact_type <code>property</code> \u00b6 <pre><code>artifact_type\n</code></pre> <p>Type of the underlying mlmd artifact.</p> <code></code> external_id <code>property</code> \u00b6 <pre><code>external_id: str\n</code></pre> <p>external id of the underlying artifact.</p> <code></code> id <code>property</code> <code>writable</code> \u00b6 <pre><code>id: int\n</code></pre> <p>Id of the underlying mlmd artifact.</p> <code></code> is_external <code>property</code> <code>writable</code> \u00b6 <pre><code>is_external: bool\n</code></pre> <p>Returns true if the artifact is external.</p> <code></code> mlmd_artifact <code>property</code> \u00b6 <pre><code>mlmd_artifact\n</code></pre> <p>Underlying mlmd artifact.</p> <code></code> name <code>property</code> <code>writable</code> \u00b6 <pre><code>name: str\n</code></pre> <p>Name of the underlying mlmd artifact.</p> <code></code> pipeline_name <code>property</code> <code>writable</code> \u00b6 <pre><code>pipeline_name: str\n</code></pre> <p>Name of the pipeline that produce the artifact.</p> <code></code> producer_component <code>property</code> <code>writable</code> \u00b6 <pre><code>producer_component: str\n</code></pre> <p>Producer component of the artifact.</p> <code></code> state <code>property</code> <code>writable</code> \u00b6 <pre><code>state: str\n</code></pre> <p>State of the underlying mlmd artifact.</p> <code></code> type <code>property</code> \u00b6 <pre><code>type\n</code></pre> <p>Type of the artifact.</p> <code></code> type_id <code>property</code> <code>writable</code> \u00b6 <pre><code>type_id: int\n</code></pre> <p>Type id of the underlying mlmd artifact.</p> <code></code> type_name <code>property</code> \u00b6 <pre><code>type_name\n</code></pre> <p>Type name of the underlying mlmd artifact.</p> <code></code> uri <code>property</code> <code>writable</code> \u00b6 <pre><code>uri: str\n</code></pre> <p>Artifact URI.</p> Functions\u00b6 <code></code> copy_from \u00b6 <pre><code>copy_from(other: Artifact)\n</code></pre> <p>Set uri, properties and custom properties from a given Artifact.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef copy_from(self, other: 'Artifact'):\n  \"\"\"Set uri, properties and custom properties from a given Artifact.\"\"\"\n  assert self.type is other.type, (\n      'Unable to set properties from an artifact of different type: {} vs {}'\n      .format(self.type_name, other.type_name))\n  self.uri = other.uri\n  if other.artifact_type.HasField('id'):\n    self.type_id = other.artifact_type.id\n\n  self._artifact.properties.clear()\n  self._artifact.properties.MergeFrom(other._artifact.properties)  # pylint: disable=protected-access\n  self._artifact.custom_properties.clear()\n  self._artifact.custom_properties.MergeFrom(\n      other._artifact.custom_properties)  # pylint: disable=protected-access\n  self._cached_modifiable_properties = copy.deepcopy(\n      other._cached_modifiable_properties)  # pylint: disable=protected-access\n  self._cached_modifiable_custom_properties = copy.deepcopy(\n      other._cached_modifiable_custom_properties)  # pylint: disable=protected-access\n</code></pre> <code></code> from_json_dict <code>classmethod</code> \u00b6 <pre><code>from_json_dict(dict_data: Dict[str, Any]) -&gt; Any\n</code></pre> <p>Convert from dictionary data to an object.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@classmethod\n@doc_controls.do_not_doc_inheritable\ndef from_json_dict(cls, dict_data: Dict[str, Any]) -&gt; Any:\n  module_name = dict_data['__artifact_class_module__']\n  class_name = dict_data['__artifact_class_name__']\n  artifact = metadata_store_pb2.Artifact()\n  artifact_type = metadata_store_pb2.ArtifactType()\n  json_format.Parse(json.dumps(dict_data['artifact']), artifact)\n  json_format.Parse(json.dumps(dict_data['artifact_type']), artifact_type)\n\n  # First, try to resolve the specific class used for the artifact; if this\n  # is not possible, use a generic artifact.Artifact object.\n  result = None\n  try:\n    artifact_cls = getattr(importlib.import_module(module_name), class_name)\n    # If the artifact type is the base Artifact class, do not construct the\n    # object here since that constructor requires the mlmd_artifact_type\n    # argument.\n    if artifact_cls != Artifact:\n      result = artifact_cls()\n  except (AttributeError, ImportError, ValueError):\n    logging.warning((\n        'Could not load artifact class %s.%s; using fallback deserialization '\n        'for the relevant artifact. Please make sure that any artifact '\n        'classes can be imported within your container or environment.'),\n                    module_name, class_name)\n  if not result:\n    result = Artifact(mlmd_artifact_type=artifact_type)\n  result.set_mlmd_artifact_type(artifact_type)\n  result.set_mlmd_artifact(artifact)\n  return result\n</code></pre> <code></code> get_bool_custom_property \u00b6 <pre><code>get_bool_custom_property(key: str) -&gt; bool\n</code></pre> <p>Get a custom property of bool type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_bool_custom_property(self, key: str) -&gt; bool:\n  \"\"\"Get a custom property of bool type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return False\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, bool):\n    return json_value\n  return self._artifact.custom_properties[key].bool_value\n</code></pre> <code></code> get_custom_property \u00b6 <pre><code>get_custom_property(key: str) -&gt; Optional[Union[int, float, str, bool, JsonValueType]]\n</code></pre> <p>Gets a custom property with key. Return None if not found.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_custom_property(\n    self, key: str\n) -&gt; Optional[Union[int, float, str, bool, JsonValueType]]:\n  \"\"\"Gets a custom property with key. Return None if not found.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return None\n\n  json_value = self.get_json_value_custom_property(key)\n  if json_value:\n    return json_value\n\n  mlmd_value = self._artifact.custom_properties[key]\n  if mlmd_value.HasField('int_value'):\n    return mlmd_value.int_value\n  elif mlmd_value.HasField('double_value'):\n    return mlmd_value.double_value\n  elif mlmd_value.HasField('string_value'):\n    return mlmd_value.string_value\n  elif mlmd_value.HasField('bool_value'):\n    return mlmd_value.bool_value\n  return None\n</code></pre> <code></code> get_float_custom_property \u00b6 <pre><code>get_float_custom_property(key: str) -&gt; float\n</code></pre> <p>Gets a custom property of float type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_float_custom_property(self, key: str) -&gt; float:\n  \"\"\"Gets a custom property of float type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return 0.0\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, float):\n    return json_value\n  return self._artifact.custom_properties[key].double_value\n</code></pre> <code></code> get_int_custom_property \u00b6 <pre><code>get_int_custom_property(key: str) -&gt; int\n</code></pre> <p>Get a custom property of int type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_int_custom_property(self, key: str) -&gt; int:\n  \"\"\"Get a custom property of int type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return 0\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, float):\n    return int(json_value)\n  return self._artifact.custom_properties[key].int_value\n</code></pre> <code></code> get_json_value_custom_property \u00b6 <pre><code>get_json_value_custom_property(key: str) -&gt; JsonValueType\n</code></pre> <p>Get a custom property of JSON type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef get_json_value_custom_property(self, key: str) -&gt; JsonValueType:\n  \"\"\"Get a custom property of JSON type.\"\"\"\n  if key in self._cached_modifiable_custom_properties:\n    return self._cached_modifiable_custom_properties[key]\n  if (key not in self._artifact.custom_properties or\n      not self._artifact.custom_properties[key].HasField('struct_value')):\n    return None\n  value = _decode_struct_value(\n      self._artifact.custom_properties[key].struct_value)\n  # We must cache the decoded lists or dictionaries returned here so that\n  # if their recursive contents are modified, the Metadata proto message\n  # can be updated to reflect this.\n  if isinstance(value, (dict, list)):\n    self._cached_modifiable_custom_properties[key] = value\n  return value\n</code></pre> <code></code> get_proto_custom_property \u00b6 <pre><code>get_proto_custom_property(key: str) -&gt; Optional[Message]\n</code></pre> <p>Get a custom property of proto type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef get_proto_custom_property(self, key: str) -&gt; Optional[message.Message]:\n  \"\"\"Get a custom property of proto type.\"\"\"\n  if key in self._cached_modifiable_custom_properties:\n    return self._cached_modifiable_custom_properties[key]\n  if (key not in self._artifact.custom_properties or\n      not self._artifact.custom_properties[key].HasField('proto_value')):\n    return None\n  value = proto_utils.unpack_proto_any(\n      self._artifact.custom_properties[key].proto_value)\n  # We must cache the protobuf message here so that if its contents are\n  # modified, the Metadata proto message can be updated to reflect this.\n  if isinstance(value, message.Message):\n    self._cached_modifiable_custom_properties[key] = value\n  return value\n</code></pre> <code></code> get_string_custom_property \u00b6 <pre><code>get_string_custom_property(key: str) -&gt; str\n</code></pre> <p>Get a custom property of string type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_string_custom_property(self, key: str) -&gt; str:\n  \"\"\"Get a custom property of string type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return ''\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, str):\n    return json_value\n  return self._artifact.custom_properties[key].string_value\n</code></pre> <code></code> set_bool_custom_property \u00b6 <pre><code>set_bool_custom_property(key: str, value: bool)\n</code></pre> <p>Sets a custom property of bool type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_bool_custom_property(self, key: str, value: bool):\n  \"\"\"Sets a custom property of bool type.\"\"\"\n  self._artifact.custom_properties[key].bool_value = value\n</code></pre> <code></code> set_float_custom_property \u00b6 <pre><code>set_float_custom_property(key: str, value: float)\n</code></pre> <p>Sets a custom property of float type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_float_custom_property(self, key: str, value: float):\n  \"\"\"Sets a custom property of float type.\"\"\"\n  self._artifact.custom_properties[key].double_value = builtins.float(value)\n</code></pre> <code></code> set_int_custom_property \u00b6 <pre><code>set_int_custom_property(key: str, value: int)\n</code></pre> <p>Set a custom property of int type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_int_custom_property(self, key: str, value: int):\n  \"\"\"Set a custom property of int type.\"\"\"\n  self._artifact.custom_properties[key].int_value = builtins.int(value)\n</code></pre> <code></code> set_json_value_custom_property \u00b6 <pre><code>set_json_value_custom_property(key: str, value: JsonValueType)\n</code></pre> <p>Sets a custom property of JSON type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_json_value_custom_property(self, key: str, value: JsonValueType):\n  \"\"\"Sets a custom property of JSON type.\"\"\"\n  self._cached_modifiable_custom_properties[key] = value\n</code></pre> <code></code> set_mlmd_artifact \u00b6 <pre><code>set_mlmd_artifact(artifact: Artifact)\n</code></pre> <p>Replace the MLMD artifact object on this artifact.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_mlmd_artifact(self, artifact: metadata_store_pb2.Artifact):\n  \"\"\"Replace the MLMD artifact object on this artifact.\"\"\"\n  if not isinstance(artifact, metadata_store_pb2.Artifact):\n    raise ValueError(\n        ('Expected instance of metadata_store_pb2.Artifact, got %s '\n         'instead.') % (artifact,))\n  self._artifact = artifact\n  self._cached_modifiable_properties = {}\n  self._cached_modifiable_custom_properties = {}\n</code></pre> <code></code> set_mlmd_artifact_type \u00b6 <pre><code>set_mlmd_artifact_type(artifact_type: ArtifactType)\n</code></pre> <p>Set entire ArtifactType in this object.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_mlmd_artifact_type(self,\n                           artifact_type: metadata_store_pb2.ArtifactType):\n  \"\"\"Set entire ArtifactType in this object.\"\"\"\n  if not isinstance(artifact_type, metadata_store_pb2.ArtifactType):\n    raise ValueError(\n        ('Expected instance of metadata_store_pb2.ArtifactType, got %s '\n         'instead.') % (artifact_type,))\n  self._artifact_type = artifact_type\n  self._artifact.type_id = artifact_type.id\n</code></pre> <code></code> set_proto_custom_property \u00b6 <pre><code>set_proto_custom_property(key: str, value: Message)\n</code></pre> <p>Sets a custom property of proto type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_proto_custom_property(self, key: str, value: message.Message):\n  \"\"\"Sets a custom property of proto type.\"\"\"\n  self._cached_modifiable_custom_properties[key] = value\n</code></pre> <code></code> set_string_custom_property \u00b6 <pre><code>set_string_custom_property(key: str, value: str)\n</code></pre> <p>Set a custom property of string type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_string_custom_property(self, key: str, value: str):\n  \"\"\"Set a custom property of string type.\"\"\"\n  self._artifact.custom_properties[key].string_value = value\n</code></pre> <code></code> to_json_dict \u00b6 <pre><code>to_json_dict() -&gt; Dict[str, Any]\n</code></pre> <p>Convert from an object to a JSON serializable dictionary.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef to_json_dict(self) -&gt; Dict[str, Any]:\n  return {\n      'artifact':\n          json.loads(\n              json_format.MessageToJson(\n                  message=self.mlmd_artifact,\n                  preserving_proto_field_name=True)),\n      'artifact_type':\n          json.loads(\n              json_format.MessageToJson(\n                  message=self._artifact_type,\n                  preserving_proto_field_name=True)),\n      '__artifact_class_module__':\n          self.__class__.__module__,\n      '__artifact_class_name__':\n          self.__class__.__name__,\n  }\n</code></pre>"},{"location":"api/v1/types/#tfx.v1.types.standard_artifacts.Schema","title":"Schema","text":"<pre><code>Schema(*args, **kwargs)\n</code></pre> <p>               Bases: <code>_TfxArtifact</code></p> <p>Artifact that contains the schema of the data.</p> <p>Schema artifact is used to store the schema of the data. The schema is a proto that describes the data, including the type of each feature, the range of values for each feature, and other properties. The schema is usually generated by the SchemaGen component, which uses the statistics of the data to infer the schema. The schema can be used by other components in the pipeline to validate the data and to generate models.</p> <ul> <li>File structure:</li> <li><code>{uri}/</code><ul> <li><code>schema.pbtxt</code>: Text-proto format serialization of   tensorflow_metadata.proto.v0.schema.Schema   proto message.</li> </ul> </li> </ul> <p>Construct TFX first-party component artifact.</p> METHOD DESCRIPTION <code>copy_from</code> <p>Set uri, properties and custom properties from a given Artifact.</p> <code>from_json_dict</code> <p>Convert from dictionary data to an object.</p> <code>get_bool_custom_property</code> <p>Get a custom property of bool type.</p> <code>get_custom_property</code> <p>Gets a custom property with key. Return None if not found.</p> <code>get_float_custom_property</code> <p>Gets a custom property of float type.</p> <code>get_int_custom_property</code> <p>Get a custom property of int type.</p> <code>get_json_value_custom_property</code> <p>Get a custom property of JSON type.</p> <code>get_proto_custom_property</code> <p>Get a custom property of proto type.</p> <code>get_string_custom_property</code> <p>Get a custom property of string type.</p> <code>set_bool_custom_property</code> <p>Sets a custom property of bool type.</p> <code>set_float_custom_property</code> <p>Sets a custom property of float type.</p> <code>set_int_custom_property</code> <p>Set a custom property of int type.</p> <code>set_json_value_custom_property</code> <p>Sets a custom property of JSON type.</p> <code>set_mlmd_artifact</code> <p>Replace the MLMD artifact object on this artifact.</p> <code>set_mlmd_artifact_type</code> <p>Set entire ArtifactType in this object.</p> <code>set_proto_custom_property</code> <p>Sets a custom property of proto type.</p> <code>set_string_custom_property</code> <p>Set a custom property of string type.</p> <code>to_json_dict</code> <p>Convert from an object to a JSON serializable dictionary.</p> ATTRIBUTE DESCRIPTION <code>artifact_type</code> <p>Type of the underlying mlmd artifact.</p> <p> </p> <code>external_id</code> <p>external id of the underlying artifact.</p> <p> TYPE: <code>str</code> </p> <code>id</code> <p>Id of the underlying mlmd artifact.</p> <p> TYPE: <code>int</code> </p> <code>is_external</code> <p>Returns true if the artifact is external.</p> <p> TYPE: <code>bool</code> </p> <code>mlmd_artifact</code> <p>Underlying mlmd artifact.</p> <p> </p> <code>name</code> <p>Name of the underlying mlmd artifact.</p> <p> TYPE: <code>str</code> </p> <code>pipeline_name</code> <p>Name of the pipeline that produce the artifact.</p> <p> TYPE: <code>str</code> </p> <code>producer_component</code> <p>Producer component of the artifact.</p> <p> TYPE: <code>str</code> </p> <code>state</code> <p>State of the underlying mlmd artifact.</p> <p> TYPE: <code>str</code> </p> <code>type</code> <p>Type of the artifact.</p> <p> </p> <code>type_id</code> <p>Type id of the underlying mlmd artifact.</p> <p> TYPE: <code>int</code> </p> <code>type_name</code> <p>Type name of the underlying mlmd artifact.</p> <p> </p> <code>uri</code> <p>Artifact URI.</p> <p> TYPE: <code>str</code> </p> Source code in <code>tfx/types/standard_artifacts.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    \"\"\"Construct TFX first-party component artifact.\"\"\"\n    # TODO(b/176795331): Refactor directory structure to make it clearer that\n    # TFX-specific artifacts require the full \"tfx\" package be installed.\n    #\n    # Do not allow usage of TFX-specific artifact if only the core pipeline\n    # SDK package is installed.\n    try:\n        import setuptools # pytype: disable=import-error  # noqa: F401\n\n        # Test import only when setuptools is available.\n        try:\n            # `extensions` is not included in ml_pipelines_sdk and doesn't have any\n            # transitive import.\n            import tfx.extensions as _  # type: ignore  # noqa: F401 # pylint: disable=g-import-not-at-top\n        except ModuleNotFoundError as err:\n            # The following condition detects exactly whether only the DSL package\n            # is installed, and is bypassed when tests run in Bazel.\n            raise RuntimeError(\n                'The \"tfx\" and all dependent packages need to be '\n                \"installed to use this functionality.\"\n            ) from err\n    except ModuleNotFoundError:\n        pass\n\n    super().__init__(*args, **kwargs)\n</code></pre> Attributes\u00b6 <code></code> artifact_type <code>property</code> \u00b6 <pre><code>artifact_type\n</code></pre> <p>Type of the underlying mlmd artifact.</p> <code></code> external_id <code>property</code> \u00b6 <pre><code>external_id: str\n</code></pre> <p>external id of the underlying artifact.</p> <code></code> id <code>property</code> <code>writable</code> \u00b6 <pre><code>id: int\n</code></pre> <p>Id of the underlying mlmd artifact.</p> <code></code> is_external <code>property</code> <code>writable</code> \u00b6 <pre><code>is_external: bool\n</code></pre> <p>Returns true if the artifact is external.</p> <code></code> mlmd_artifact <code>property</code> \u00b6 <pre><code>mlmd_artifact\n</code></pre> <p>Underlying mlmd artifact.</p> <code></code> name <code>property</code> <code>writable</code> \u00b6 <pre><code>name: str\n</code></pre> <p>Name of the underlying mlmd artifact.</p> <code></code> pipeline_name <code>property</code> <code>writable</code> \u00b6 <pre><code>pipeline_name: str\n</code></pre> <p>Name of the pipeline that produce the artifact.</p> <code></code> producer_component <code>property</code> <code>writable</code> \u00b6 <pre><code>producer_component: str\n</code></pre> <p>Producer component of the artifact.</p> <code></code> state <code>property</code> <code>writable</code> \u00b6 <pre><code>state: str\n</code></pre> <p>State of the underlying mlmd artifact.</p> <code></code> type <code>property</code> \u00b6 <pre><code>type\n</code></pre> <p>Type of the artifact.</p> <code></code> type_id <code>property</code> <code>writable</code> \u00b6 <pre><code>type_id: int\n</code></pre> <p>Type id of the underlying mlmd artifact.</p> <code></code> type_name <code>property</code> \u00b6 <pre><code>type_name\n</code></pre> <p>Type name of the underlying mlmd artifact.</p> <code></code> uri <code>property</code> <code>writable</code> \u00b6 <pre><code>uri: str\n</code></pre> <p>Artifact URI.</p> Functions\u00b6 <code></code> copy_from \u00b6 <pre><code>copy_from(other: Artifact)\n</code></pre> <p>Set uri, properties and custom properties from a given Artifact.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef copy_from(self, other: 'Artifact'):\n  \"\"\"Set uri, properties and custom properties from a given Artifact.\"\"\"\n  assert self.type is other.type, (\n      'Unable to set properties from an artifact of different type: {} vs {}'\n      .format(self.type_name, other.type_name))\n  self.uri = other.uri\n  if other.artifact_type.HasField('id'):\n    self.type_id = other.artifact_type.id\n\n  self._artifact.properties.clear()\n  self._artifact.properties.MergeFrom(other._artifact.properties)  # pylint: disable=protected-access\n  self._artifact.custom_properties.clear()\n  self._artifact.custom_properties.MergeFrom(\n      other._artifact.custom_properties)  # pylint: disable=protected-access\n  self._cached_modifiable_properties = copy.deepcopy(\n      other._cached_modifiable_properties)  # pylint: disable=protected-access\n  self._cached_modifiable_custom_properties = copy.deepcopy(\n      other._cached_modifiable_custom_properties)  # pylint: disable=protected-access\n</code></pre> <code></code> from_json_dict <code>classmethod</code> \u00b6 <pre><code>from_json_dict(dict_data: Dict[str, Any]) -&gt; Any\n</code></pre> <p>Convert from dictionary data to an object.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@classmethod\n@doc_controls.do_not_doc_inheritable\ndef from_json_dict(cls, dict_data: Dict[str, Any]) -&gt; Any:\n  module_name = dict_data['__artifact_class_module__']\n  class_name = dict_data['__artifact_class_name__']\n  artifact = metadata_store_pb2.Artifact()\n  artifact_type = metadata_store_pb2.ArtifactType()\n  json_format.Parse(json.dumps(dict_data['artifact']), artifact)\n  json_format.Parse(json.dumps(dict_data['artifact_type']), artifact_type)\n\n  # First, try to resolve the specific class used for the artifact; if this\n  # is not possible, use a generic artifact.Artifact object.\n  result = None\n  try:\n    artifact_cls = getattr(importlib.import_module(module_name), class_name)\n    # If the artifact type is the base Artifact class, do not construct the\n    # object here since that constructor requires the mlmd_artifact_type\n    # argument.\n    if artifact_cls != Artifact:\n      result = artifact_cls()\n  except (AttributeError, ImportError, ValueError):\n    logging.warning((\n        'Could not load artifact class %s.%s; using fallback deserialization '\n        'for the relevant artifact. Please make sure that any artifact '\n        'classes can be imported within your container or environment.'),\n                    module_name, class_name)\n  if not result:\n    result = Artifact(mlmd_artifact_type=artifact_type)\n  result.set_mlmd_artifact_type(artifact_type)\n  result.set_mlmd_artifact(artifact)\n  return result\n</code></pre> <code></code> get_bool_custom_property \u00b6 <pre><code>get_bool_custom_property(key: str) -&gt; bool\n</code></pre> <p>Get a custom property of bool type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_bool_custom_property(self, key: str) -&gt; bool:\n  \"\"\"Get a custom property of bool type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return False\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, bool):\n    return json_value\n  return self._artifact.custom_properties[key].bool_value\n</code></pre> <code></code> get_custom_property \u00b6 <pre><code>get_custom_property(key: str) -&gt; Optional[Union[int, float, str, bool, JsonValueType]]\n</code></pre> <p>Gets a custom property with key. Return None if not found.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_custom_property(\n    self, key: str\n) -&gt; Optional[Union[int, float, str, bool, JsonValueType]]:\n  \"\"\"Gets a custom property with key. Return None if not found.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return None\n\n  json_value = self.get_json_value_custom_property(key)\n  if json_value:\n    return json_value\n\n  mlmd_value = self._artifact.custom_properties[key]\n  if mlmd_value.HasField('int_value'):\n    return mlmd_value.int_value\n  elif mlmd_value.HasField('double_value'):\n    return mlmd_value.double_value\n  elif mlmd_value.HasField('string_value'):\n    return mlmd_value.string_value\n  elif mlmd_value.HasField('bool_value'):\n    return mlmd_value.bool_value\n  return None\n</code></pre> <code></code> get_float_custom_property \u00b6 <pre><code>get_float_custom_property(key: str) -&gt; float\n</code></pre> <p>Gets a custom property of float type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_float_custom_property(self, key: str) -&gt; float:\n  \"\"\"Gets a custom property of float type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return 0.0\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, float):\n    return json_value\n  return self._artifact.custom_properties[key].double_value\n</code></pre> <code></code> get_int_custom_property \u00b6 <pre><code>get_int_custom_property(key: str) -&gt; int\n</code></pre> <p>Get a custom property of int type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_int_custom_property(self, key: str) -&gt; int:\n  \"\"\"Get a custom property of int type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return 0\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, float):\n    return int(json_value)\n  return self._artifact.custom_properties[key].int_value\n</code></pre> <code></code> get_json_value_custom_property \u00b6 <pre><code>get_json_value_custom_property(key: str) -&gt; JsonValueType\n</code></pre> <p>Get a custom property of JSON type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef get_json_value_custom_property(self, key: str) -&gt; JsonValueType:\n  \"\"\"Get a custom property of JSON type.\"\"\"\n  if key in self._cached_modifiable_custom_properties:\n    return self._cached_modifiable_custom_properties[key]\n  if (key not in self._artifact.custom_properties or\n      not self._artifact.custom_properties[key].HasField('struct_value')):\n    return None\n  value = _decode_struct_value(\n      self._artifact.custom_properties[key].struct_value)\n  # We must cache the decoded lists or dictionaries returned here so that\n  # if their recursive contents are modified, the Metadata proto message\n  # can be updated to reflect this.\n  if isinstance(value, (dict, list)):\n    self._cached_modifiable_custom_properties[key] = value\n  return value\n</code></pre> <code></code> get_proto_custom_property \u00b6 <pre><code>get_proto_custom_property(key: str) -&gt; Optional[Message]\n</code></pre> <p>Get a custom property of proto type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef get_proto_custom_property(self, key: str) -&gt; Optional[message.Message]:\n  \"\"\"Get a custom property of proto type.\"\"\"\n  if key in self._cached_modifiable_custom_properties:\n    return self._cached_modifiable_custom_properties[key]\n  if (key not in self._artifact.custom_properties or\n      not self._artifact.custom_properties[key].HasField('proto_value')):\n    return None\n  value = proto_utils.unpack_proto_any(\n      self._artifact.custom_properties[key].proto_value)\n  # We must cache the protobuf message here so that if its contents are\n  # modified, the Metadata proto message can be updated to reflect this.\n  if isinstance(value, message.Message):\n    self._cached_modifiable_custom_properties[key] = value\n  return value\n</code></pre> <code></code> get_string_custom_property \u00b6 <pre><code>get_string_custom_property(key: str) -&gt; str\n</code></pre> <p>Get a custom property of string type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_string_custom_property(self, key: str) -&gt; str:\n  \"\"\"Get a custom property of string type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return ''\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, str):\n    return json_value\n  return self._artifact.custom_properties[key].string_value\n</code></pre> <code></code> set_bool_custom_property \u00b6 <pre><code>set_bool_custom_property(key: str, value: bool)\n</code></pre> <p>Sets a custom property of bool type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_bool_custom_property(self, key: str, value: bool):\n  \"\"\"Sets a custom property of bool type.\"\"\"\n  self._artifact.custom_properties[key].bool_value = value\n</code></pre> <code></code> set_float_custom_property \u00b6 <pre><code>set_float_custom_property(key: str, value: float)\n</code></pre> <p>Sets a custom property of float type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_float_custom_property(self, key: str, value: float):\n  \"\"\"Sets a custom property of float type.\"\"\"\n  self._artifact.custom_properties[key].double_value = builtins.float(value)\n</code></pre> <code></code> set_int_custom_property \u00b6 <pre><code>set_int_custom_property(key: str, value: int)\n</code></pre> <p>Set a custom property of int type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_int_custom_property(self, key: str, value: int):\n  \"\"\"Set a custom property of int type.\"\"\"\n  self._artifact.custom_properties[key].int_value = builtins.int(value)\n</code></pre> <code></code> set_json_value_custom_property \u00b6 <pre><code>set_json_value_custom_property(key: str, value: JsonValueType)\n</code></pre> <p>Sets a custom property of JSON type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_json_value_custom_property(self, key: str, value: JsonValueType):\n  \"\"\"Sets a custom property of JSON type.\"\"\"\n  self._cached_modifiable_custom_properties[key] = value\n</code></pre> <code></code> set_mlmd_artifact \u00b6 <pre><code>set_mlmd_artifact(artifact: Artifact)\n</code></pre> <p>Replace the MLMD artifact object on this artifact.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_mlmd_artifact(self, artifact: metadata_store_pb2.Artifact):\n  \"\"\"Replace the MLMD artifact object on this artifact.\"\"\"\n  if not isinstance(artifact, metadata_store_pb2.Artifact):\n    raise ValueError(\n        ('Expected instance of metadata_store_pb2.Artifact, got %s '\n         'instead.') % (artifact,))\n  self._artifact = artifact\n  self._cached_modifiable_properties = {}\n  self._cached_modifiable_custom_properties = {}\n</code></pre> <code></code> set_mlmd_artifact_type \u00b6 <pre><code>set_mlmd_artifact_type(artifact_type: ArtifactType)\n</code></pre> <p>Set entire ArtifactType in this object.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_mlmd_artifact_type(self,\n                           artifact_type: metadata_store_pb2.ArtifactType):\n  \"\"\"Set entire ArtifactType in this object.\"\"\"\n  if not isinstance(artifact_type, metadata_store_pb2.ArtifactType):\n    raise ValueError(\n        ('Expected instance of metadata_store_pb2.ArtifactType, got %s '\n         'instead.') % (artifact_type,))\n  self._artifact_type = artifact_type\n  self._artifact.type_id = artifact_type.id\n</code></pre> <code></code> set_proto_custom_property \u00b6 <pre><code>set_proto_custom_property(key: str, value: Message)\n</code></pre> <p>Sets a custom property of proto type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_proto_custom_property(self, key: str, value: message.Message):\n  \"\"\"Sets a custom property of proto type.\"\"\"\n  self._cached_modifiable_custom_properties[key] = value\n</code></pre> <code></code> set_string_custom_property \u00b6 <pre><code>set_string_custom_property(key: str, value: str)\n</code></pre> <p>Set a custom property of string type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_string_custom_property(self, key: str, value: str):\n  \"\"\"Set a custom property of string type.\"\"\"\n  self._artifact.custom_properties[key].string_value = value\n</code></pre> <code></code> to_json_dict \u00b6 <pre><code>to_json_dict() -&gt; Dict[str, Any]\n</code></pre> <p>Convert from an object to a JSON serializable dictionary.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef to_json_dict(self) -&gt; Dict[str, Any]:\n  return {\n      'artifact':\n          json.loads(\n              json_format.MessageToJson(\n                  message=self.mlmd_artifact,\n                  preserving_proto_field_name=True)),\n      'artifact_type':\n          json.loads(\n              json_format.MessageToJson(\n                  message=self._artifact_type,\n                  preserving_proto_field_name=True)),\n      '__artifact_class_module__':\n          self.__class__.__module__,\n      '__artifact_class_name__':\n          self.__class__.__name__,\n  }\n</code></pre>"},{"location":"api/v1/types/#tfx.v1.types.standard_artifacts.String","title":"String","text":"<pre><code>String(*args, **kwargs)\n</code></pre> <p>               Bases: <code>ValueArtifact</code></p> <p>String-typed artifact.</p> <p>String value artifacts are encoded using UTF-8.</p> <p>Initializes ValueArtifact.</p> METHOD DESCRIPTION <code>annotate_as</code> <p>Annotate the value artifact type with a system artifact class.</p> <code>copy_from</code> <p>Set uri, properties and custom properties from a given Artifact.</p> <code>decode</code> <p>Method decoding the file content. Implemented by subclasses.</p> <code>encode</code> <p>Method encoding the file content. Implemented by subclasses.</p> <code>from_json_dict</code> <p>Convert from dictionary data to an object.</p> <code>get_bool_custom_property</code> <p>Get a custom property of bool type.</p> <code>get_custom_property</code> <p>Gets a custom property with key. Return None if not found.</p> <code>get_float_custom_property</code> <p>Gets a custom property of float type.</p> <code>get_int_custom_property</code> <p>Get a custom property of int type.</p> <code>get_json_value_custom_property</code> <p>Get a custom property of JSON type.</p> <code>get_proto_custom_property</code> <p>Get a custom property of proto type.</p> <code>get_string_custom_property</code> <p>Get a custom property of string type.</p> <code>set_bool_custom_property</code> <p>Sets a custom property of bool type.</p> <code>set_float_custom_property</code> <p>Sets a custom property of float type.</p> <code>set_int_custom_property</code> <p>Set a custom property of int type.</p> <code>set_json_value_custom_property</code> <p>Sets a custom property of JSON type.</p> <code>set_mlmd_artifact</code> <p>Replace the MLMD artifact object on this artifact.</p> <code>set_mlmd_artifact_type</code> <p>Set entire ArtifactType in this object.</p> <code>set_proto_custom_property</code> <p>Sets a custom property of proto type.</p> <code>set_string_custom_property</code> <p>Set a custom property of string type.</p> <code>to_json_dict</code> <p>Convert from an object to a JSON serializable dictionary.</p> ATTRIBUTE DESCRIPTION <code>artifact_type</code> <p>Type of the underlying mlmd artifact.</p> <p> </p> <code>external_id</code> <p>external id of the underlying artifact.</p> <p> TYPE: <code>str</code> </p> <code>id</code> <p>Id of the underlying mlmd artifact.</p> <p> TYPE: <code>int</code> </p> <code>is_external</code> <p>Returns true if the artifact is external.</p> <p> TYPE: <code>bool</code> </p> <code>mlmd_artifact</code> <p>Underlying mlmd artifact.</p> <p> </p> <code>name</code> <p>Name of the underlying mlmd artifact.</p> <p> TYPE: <code>str</code> </p> <code>pipeline_name</code> <p>Name of the pipeline that produce the artifact.</p> <p> TYPE: <code>str</code> </p> <code>producer_component</code> <p>Producer component of the artifact.</p> <p> TYPE: <code>str</code> </p> <code>state</code> <p>State of the underlying mlmd artifact.</p> <p> TYPE: <code>str</code> </p> <code>type</code> <p>Type of the artifact.</p> <p> </p> <code>type_id</code> <p>Type id of the underlying mlmd artifact.</p> <p> TYPE: <code>int</code> </p> <code>type_name</code> <p>Type name of the underlying mlmd artifact.</p> <p> </p> <code>uri</code> <p>Artifact URI.</p> <p> TYPE: <code>str</code> </p> <code>value</code> <p>Value stored in the artifact.</p> <p> </p> Source code in <code>tfx/types/value_artifact.py</code> <pre><code>def __init__(self, *args, **kwargs):\n  \"\"\"Initializes ValueArtifact.\"\"\"\n  self._has_value = False\n  self._modified = False\n  self._value = None\n  super().__init__(*args, **kwargs)\n</code></pre> Attributes\u00b6 <code></code> artifact_type <code>property</code> \u00b6 <pre><code>artifact_type\n</code></pre> <p>Type of the underlying mlmd artifact.</p> <code></code> external_id <code>property</code> \u00b6 <pre><code>external_id: str\n</code></pre> <p>external id of the underlying artifact.</p> <code></code> id <code>property</code> <code>writable</code> \u00b6 <pre><code>id: int\n</code></pre> <p>Id of the underlying mlmd artifact.</p> <code></code> is_external <code>property</code> <code>writable</code> \u00b6 <pre><code>is_external: bool\n</code></pre> <p>Returns true if the artifact is external.</p> <code></code> mlmd_artifact <code>property</code> \u00b6 <pre><code>mlmd_artifact\n</code></pre> <p>Underlying mlmd artifact.</p> <code></code> name <code>property</code> <code>writable</code> \u00b6 <pre><code>name: str\n</code></pre> <p>Name of the underlying mlmd artifact.</p> <code></code> pipeline_name <code>property</code> <code>writable</code> \u00b6 <pre><code>pipeline_name: str\n</code></pre> <p>Name of the pipeline that produce the artifact.</p> <code></code> producer_component <code>property</code> <code>writable</code> \u00b6 <pre><code>producer_component: str\n</code></pre> <p>Producer component of the artifact.</p> <code></code> state <code>property</code> <code>writable</code> \u00b6 <pre><code>state: str\n</code></pre> <p>State of the underlying mlmd artifact.</p> <code></code> type <code>property</code> \u00b6 <pre><code>type\n</code></pre> <p>Type of the artifact.</p> <code></code> type_id <code>property</code> <code>writable</code> \u00b6 <pre><code>type_id: int\n</code></pre> <p>Type id of the underlying mlmd artifact.</p> <code></code> type_name <code>property</code> \u00b6 <pre><code>type_name\n</code></pre> <p>Type name of the underlying mlmd artifact.</p> <code></code> uri <code>property</code> <code>writable</code> \u00b6 <pre><code>uri: str\n</code></pre> <p>Artifact URI.</p> <code></code> value <code>property</code> <code>writable</code> \u00b6 <pre><code>value\n</code></pre> <p>Value stored in the artifact.</p> Functions\u00b6 <code></code> annotate_as <code>classmethod</code> \u00b6 <pre><code>annotate_as(type_annotation: Optional[Type[SystemArtifact]] = None)\n</code></pre> <p>Annotate the value artifact type with a system artifact class.</p> <p>Example usage</p> <pre><code>from tfx import v1 as tfx\n\nOutputArtifact = tfx.dsl.components.OutputArtifact\nString = tfx.types.standard_artifacts.String\nModel = tfx.dsl.standard_annotations.Model\n\n\n@tfx.dsl.components.component\ndef MyTrainer(model: OutputArtifact[String.annotate_as(Model)]): ...\n</code></pre> PARAMETER DESCRIPTION <code>type_annotation</code> <p>the standard annotations used to annotate the value artifact type. The possible values are in <code>tfx.v1.dsl.standard_annotations</code>.</p> <p> TYPE: <code>Optional[Type[SystemArtifact]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <p>A subclass of the method caller class (e.g., <code>standard_artifacts.String</code>, <code>standard_artifacts.Float</code>) with TYPE_ANNOTATION attribute set to be <code>type_annotation</code>; returns the original class if<code>type_annotation</code> is None.</p> Source code in <code>tfx/types/value_artifact.py</code> <pre><code>@classmethod\ndef annotate_as(cls, type_annotation: Optional[Type[SystemArtifact]] = None):\n  \"\"\"Annotate the value artifact type with a system artifact class.\n\n  !!! example \"Example usage\"\n\n      ```python\n      from tfx import v1 as tfx\n\n      OutputArtifact = tfx.dsl.components.OutputArtifact\n      String = tfx.types.standard_artifacts.String\n      Model = tfx.dsl.standard_annotations.Model\n\n\n      @tfx.dsl.components.component\n      def MyTrainer(model: OutputArtifact[String.annotate_as(Model)]): ...\n      ```\n\n  Args:\n    type_annotation: the standard annotations used to annotate the value\n      artifact type. The possible values are in\n      `tfx.v1.dsl.standard_annotations`.\n\n  Returns:\n    A subclass of the method caller class (e.g., [`standard_artifacts.String`][tfx.v1.types.standard_artifacts.String],\n      [`standard_artifacts.Float`][tfx.v1.types.standard_artifacts.Float]) with TYPE_ANNOTATION attribute set to be\n      `type_annotation`; returns the original class if`type_annotation` is None.\n  \"\"\"\n  if not type_annotation:\n    return cls\n  if not issubclass(type_annotation, SystemArtifact):\n    raise ValueError(\n        'type_annotation %s is not a subclass of SystemArtifact.' %\n        type_annotation)\n  type_annotation_str = str(type_annotation.__name__)\n  return type(\n      str(cls.__name__) + '_' + type_annotation_str,\n      (cls,),\n      {\n          'TYPE_NAME': str(cls.TYPE_NAME) + '_' + type_annotation_str,\n          'TYPE_ANNOTATION': type_annotation,\n          '__module__': cls.__module__,\n      },\n  )\n</code></pre> <code></code> copy_from \u00b6 <pre><code>copy_from(other: Artifact)\n</code></pre> <p>Set uri, properties and custom properties from a given Artifact.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef copy_from(self, other: 'Artifact'):\n  \"\"\"Set uri, properties and custom properties from a given Artifact.\"\"\"\n  assert self.type is other.type, (\n      'Unable to set properties from an artifact of different type: {} vs {}'\n      .format(self.type_name, other.type_name))\n  self.uri = other.uri\n  if other.artifact_type.HasField('id'):\n    self.type_id = other.artifact_type.id\n\n  self._artifact.properties.clear()\n  self._artifact.properties.MergeFrom(other._artifact.properties)  # pylint: disable=protected-access\n  self._artifact.custom_properties.clear()\n  self._artifact.custom_properties.MergeFrom(\n      other._artifact.custom_properties)  # pylint: disable=protected-access\n  self._cached_modifiable_properties = copy.deepcopy(\n      other._cached_modifiable_properties)  # pylint: disable=protected-access\n  self._cached_modifiable_custom_properties = copy.deepcopy(\n      other._cached_modifiable_custom_properties)  # pylint: disable=protected-access\n</code></pre> <code></code> decode \u00b6 <pre><code>decode(serialized_value: bytes) -&gt; str\n</code></pre> <p>Method decoding the file content. Implemented by subclasses.</p> Source code in <code>tfx/types/standard_artifacts.py</code> <pre><code>def decode(self, serialized_value: bytes) -&gt; str:\n    return serialized_value.decode(\"utf-8\")\n</code></pre> <code></code> encode \u00b6 <pre><code>encode(value: str) -&gt; bytes\n</code></pre> <p>Method encoding the file content. Implemented by subclasses.</p> Source code in <code>tfx/types/standard_artifacts.py</code> <pre><code>def encode(self, value: str) -&gt; bytes:\n    if not isinstance(value, str):\n        raise TypeError(\n            \"Expecting Text but got value %s of type %s\" % (str(value), type(value))\n        )\n    return value.encode(\"utf-8\")\n</code></pre> <code></code> from_json_dict <code>classmethod</code> \u00b6 <pre><code>from_json_dict(dict_data: Dict[str, Any]) -&gt; Any\n</code></pre> <p>Convert from dictionary data to an object.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@classmethod\n@doc_controls.do_not_doc_inheritable\ndef from_json_dict(cls, dict_data: Dict[str, Any]) -&gt; Any:\n  module_name = dict_data['__artifact_class_module__']\n  class_name = dict_data['__artifact_class_name__']\n  artifact = metadata_store_pb2.Artifact()\n  artifact_type = metadata_store_pb2.ArtifactType()\n  json_format.Parse(json.dumps(dict_data['artifact']), artifact)\n  json_format.Parse(json.dumps(dict_data['artifact_type']), artifact_type)\n\n  # First, try to resolve the specific class used for the artifact; if this\n  # is not possible, use a generic artifact.Artifact object.\n  result = None\n  try:\n    artifact_cls = getattr(importlib.import_module(module_name), class_name)\n    # If the artifact type is the base Artifact class, do not construct the\n    # object here since that constructor requires the mlmd_artifact_type\n    # argument.\n    if artifact_cls != Artifact:\n      result = artifact_cls()\n  except (AttributeError, ImportError, ValueError):\n    logging.warning((\n        'Could not load artifact class %s.%s; using fallback deserialization '\n        'for the relevant artifact. Please make sure that any artifact '\n        'classes can be imported within your container or environment.'),\n                    module_name, class_name)\n  if not result:\n    result = Artifact(mlmd_artifact_type=artifact_type)\n  result.set_mlmd_artifact_type(artifact_type)\n  result.set_mlmd_artifact(artifact)\n  return result\n</code></pre> <code></code> get_bool_custom_property \u00b6 <pre><code>get_bool_custom_property(key: str) -&gt; bool\n</code></pre> <p>Get a custom property of bool type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_bool_custom_property(self, key: str) -&gt; bool:\n  \"\"\"Get a custom property of bool type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return False\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, bool):\n    return json_value\n  return self._artifact.custom_properties[key].bool_value\n</code></pre> <code></code> get_custom_property \u00b6 <pre><code>get_custom_property(key: str) -&gt; Optional[Union[int, float, str, bool, JsonValueType]]\n</code></pre> <p>Gets a custom property with key. Return None if not found.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_custom_property(\n    self, key: str\n) -&gt; Optional[Union[int, float, str, bool, JsonValueType]]:\n  \"\"\"Gets a custom property with key. Return None if not found.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return None\n\n  json_value = self.get_json_value_custom_property(key)\n  if json_value:\n    return json_value\n\n  mlmd_value = self._artifact.custom_properties[key]\n  if mlmd_value.HasField('int_value'):\n    return mlmd_value.int_value\n  elif mlmd_value.HasField('double_value'):\n    return mlmd_value.double_value\n  elif mlmd_value.HasField('string_value'):\n    return mlmd_value.string_value\n  elif mlmd_value.HasField('bool_value'):\n    return mlmd_value.bool_value\n  return None\n</code></pre> <code></code> get_float_custom_property \u00b6 <pre><code>get_float_custom_property(key: str) -&gt; float\n</code></pre> <p>Gets a custom property of float type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_float_custom_property(self, key: str) -&gt; float:\n  \"\"\"Gets a custom property of float type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return 0.0\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, float):\n    return json_value\n  return self._artifact.custom_properties[key].double_value\n</code></pre> <code></code> get_int_custom_property \u00b6 <pre><code>get_int_custom_property(key: str) -&gt; int\n</code></pre> <p>Get a custom property of int type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_int_custom_property(self, key: str) -&gt; int:\n  \"\"\"Get a custom property of int type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return 0\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, float):\n    return int(json_value)\n  return self._artifact.custom_properties[key].int_value\n</code></pre> <code></code> get_json_value_custom_property \u00b6 <pre><code>get_json_value_custom_property(key: str) -&gt; JsonValueType\n</code></pre> <p>Get a custom property of JSON type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef get_json_value_custom_property(self, key: str) -&gt; JsonValueType:\n  \"\"\"Get a custom property of JSON type.\"\"\"\n  if key in self._cached_modifiable_custom_properties:\n    return self._cached_modifiable_custom_properties[key]\n  if (key not in self._artifact.custom_properties or\n      not self._artifact.custom_properties[key].HasField('struct_value')):\n    return None\n  value = _decode_struct_value(\n      self._artifact.custom_properties[key].struct_value)\n  # We must cache the decoded lists or dictionaries returned here so that\n  # if their recursive contents are modified, the Metadata proto message\n  # can be updated to reflect this.\n  if isinstance(value, (dict, list)):\n    self._cached_modifiable_custom_properties[key] = value\n  return value\n</code></pre> <code></code> get_proto_custom_property \u00b6 <pre><code>get_proto_custom_property(key: str) -&gt; Optional[Message]\n</code></pre> <p>Get a custom property of proto type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef get_proto_custom_property(self, key: str) -&gt; Optional[message.Message]:\n  \"\"\"Get a custom property of proto type.\"\"\"\n  if key in self._cached_modifiable_custom_properties:\n    return self._cached_modifiable_custom_properties[key]\n  if (key not in self._artifact.custom_properties or\n      not self._artifact.custom_properties[key].HasField('proto_value')):\n    return None\n  value = proto_utils.unpack_proto_any(\n      self._artifact.custom_properties[key].proto_value)\n  # We must cache the protobuf message here so that if its contents are\n  # modified, the Metadata proto message can be updated to reflect this.\n  if isinstance(value, message.Message):\n    self._cached_modifiable_custom_properties[key] = value\n  return value\n</code></pre> <code></code> get_string_custom_property \u00b6 <pre><code>get_string_custom_property(key: str) -&gt; str\n</code></pre> <p>Get a custom property of string type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_string_custom_property(self, key: str) -&gt; str:\n  \"\"\"Get a custom property of string type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return ''\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, str):\n    return json_value\n  return self._artifact.custom_properties[key].string_value\n</code></pre> <code></code> set_bool_custom_property \u00b6 <pre><code>set_bool_custom_property(key: str, value: bool)\n</code></pre> <p>Sets a custom property of bool type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_bool_custom_property(self, key: str, value: bool):\n  \"\"\"Sets a custom property of bool type.\"\"\"\n  self._artifact.custom_properties[key].bool_value = value\n</code></pre> <code></code> set_float_custom_property \u00b6 <pre><code>set_float_custom_property(key: str, value: float)\n</code></pre> <p>Sets a custom property of float type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_float_custom_property(self, key: str, value: float):\n  \"\"\"Sets a custom property of float type.\"\"\"\n  self._artifact.custom_properties[key].double_value = builtins.float(value)\n</code></pre> <code></code> set_int_custom_property \u00b6 <pre><code>set_int_custom_property(key: str, value: int)\n</code></pre> <p>Set a custom property of int type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_int_custom_property(self, key: str, value: int):\n  \"\"\"Set a custom property of int type.\"\"\"\n  self._artifact.custom_properties[key].int_value = builtins.int(value)\n</code></pre> <code></code> set_json_value_custom_property \u00b6 <pre><code>set_json_value_custom_property(key: str, value: JsonValueType)\n</code></pre> <p>Sets a custom property of JSON type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_json_value_custom_property(self, key: str, value: JsonValueType):\n  \"\"\"Sets a custom property of JSON type.\"\"\"\n  self._cached_modifiable_custom_properties[key] = value\n</code></pre> <code></code> set_mlmd_artifact \u00b6 <pre><code>set_mlmd_artifact(artifact: Artifact)\n</code></pre> <p>Replace the MLMD artifact object on this artifact.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_mlmd_artifact(self, artifact: metadata_store_pb2.Artifact):\n  \"\"\"Replace the MLMD artifact object on this artifact.\"\"\"\n  if not isinstance(artifact, metadata_store_pb2.Artifact):\n    raise ValueError(\n        ('Expected instance of metadata_store_pb2.Artifact, got %s '\n         'instead.') % (artifact,))\n  self._artifact = artifact\n  self._cached_modifiable_properties = {}\n  self._cached_modifiable_custom_properties = {}\n</code></pre> <code></code> set_mlmd_artifact_type \u00b6 <pre><code>set_mlmd_artifact_type(artifact_type: ArtifactType)\n</code></pre> <p>Set entire ArtifactType in this object.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_mlmd_artifact_type(self,\n                           artifact_type: metadata_store_pb2.ArtifactType):\n  \"\"\"Set entire ArtifactType in this object.\"\"\"\n  if not isinstance(artifact_type, metadata_store_pb2.ArtifactType):\n    raise ValueError(\n        ('Expected instance of metadata_store_pb2.ArtifactType, got %s '\n         'instead.') % (artifact_type,))\n  self._artifact_type = artifact_type\n  self._artifact.type_id = artifact_type.id\n</code></pre> <code></code> set_proto_custom_property \u00b6 <pre><code>set_proto_custom_property(key: str, value: Message)\n</code></pre> <p>Sets a custom property of proto type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_proto_custom_property(self, key: str, value: message.Message):\n  \"\"\"Sets a custom property of proto type.\"\"\"\n  self._cached_modifiable_custom_properties[key] = value\n</code></pre> <code></code> set_string_custom_property \u00b6 <pre><code>set_string_custom_property(key: str, value: str)\n</code></pre> <p>Set a custom property of string type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_string_custom_property(self, key: str, value: str):\n  \"\"\"Set a custom property of string type.\"\"\"\n  self._artifact.custom_properties[key].string_value = value\n</code></pre> <code></code> to_json_dict \u00b6 <pre><code>to_json_dict() -&gt; Dict[str, Any]\n</code></pre> <p>Convert from an object to a JSON serializable dictionary.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef to_json_dict(self) -&gt; Dict[str, Any]:\n  return {\n      'artifact':\n          json.loads(\n              json_format.MessageToJson(\n                  message=self.mlmd_artifact,\n                  preserving_proto_field_name=True)),\n      'artifact_type':\n          json.loads(\n              json_format.MessageToJson(\n                  message=self._artifact_type,\n                  preserving_proto_field_name=True)),\n      '__artifact_class_module__':\n          self.__class__.__module__,\n      '__artifact_class_name__':\n          self.__class__.__name__,\n  }\n</code></pre>"},{"location":"api/v1/types/#tfx.v1.types.standard_artifacts.TransformCache","title":"TransformCache","text":"<pre><code>TransformCache(*args, **kwargs)\n</code></pre> <p>               Bases: <code>_TfxArtifact</code></p> <p>TFX first-party component artifact definition.</p> <p>Construct TFX first-party component artifact.</p> METHOD DESCRIPTION <code>copy_from</code> <p>Set uri, properties and custom properties from a given Artifact.</p> <code>from_json_dict</code> <p>Convert from dictionary data to an object.</p> <code>get_bool_custom_property</code> <p>Get a custom property of bool type.</p> <code>get_custom_property</code> <p>Gets a custom property with key. Return None if not found.</p> <code>get_float_custom_property</code> <p>Gets a custom property of float type.</p> <code>get_int_custom_property</code> <p>Get a custom property of int type.</p> <code>get_json_value_custom_property</code> <p>Get a custom property of JSON type.</p> <code>get_proto_custom_property</code> <p>Get a custom property of proto type.</p> <code>get_string_custom_property</code> <p>Get a custom property of string type.</p> <code>set_bool_custom_property</code> <p>Sets a custom property of bool type.</p> <code>set_float_custom_property</code> <p>Sets a custom property of float type.</p> <code>set_int_custom_property</code> <p>Set a custom property of int type.</p> <code>set_json_value_custom_property</code> <p>Sets a custom property of JSON type.</p> <code>set_mlmd_artifact</code> <p>Replace the MLMD artifact object on this artifact.</p> <code>set_mlmd_artifact_type</code> <p>Set entire ArtifactType in this object.</p> <code>set_proto_custom_property</code> <p>Sets a custom property of proto type.</p> <code>set_string_custom_property</code> <p>Set a custom property of string type.</p> <code>to_json_dict</code> <p>Convert from an object to a JSON serializable dictionary.</p> ATTRIBUTE DESCRIPTION <code>artifact_type</code> <p>Type of the underlying mlmd artifact.</p> <p> </p> <code>external_id</code> <p>external id of the underlying artifact.</p> <p> TYPE: <code>str</code> </p> <code>id</code> <p>Id of the underlying mlmd artifact.</p> <p> TYPE: <code>int</code> </p> <code>is_external</code> <p>Returns true if the artifact is external.</p> <p> TYPE: <code>bool</code> </p> <code>mlmd_artifact</code> <p>Underlying mlmd artifact.</p> <p> </p> <code>name</code> <p>Name of the underlying mlmd artifact.</p> <p> TYPE: <code>str</code> </p> <code>pipeline_name</code> <p>Name of the pipeline that produce the artifact.</p> <p> TYPE: <code>str</code> </p> <code>producer_component</code> <p>Producer component of the artifact.</p> <p> TYPE: <code>str</code> </p> <code>state</code> <p>State of the underlying mlmd artifact.</p> <p> TYPE: <code>str</code> </p> <code>type</code> <p>Type of the artifact.</p> <p> </p> <code>type_id</code> <p>Type id of the underlying mlmd artifact.</p> <p> TYPE: <code>int</code> </p> <code>type_name</code> <p>Type name of the underlying mlmd artifact.</p> <p> </p> <code>uri</code> <p>Artifact URI.</p> <p> TYPE: <code>str</code> </p> Source code in <code>tfx/types/standard_artifacts.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    \"\"\"Construct TFX first-party component artifact.\"\"\"\n    # TODO(b/176795331): Refactor directory structure to make it clearer that\n    # TFX-specific artifacts require the full \"tfx\" package be installed.\n    #\n    # Do not allow usage of TFX-specific artifact if only the core pipeline\n    # SDK package is installed.\n    try:\n        import setuptools # pytype: disable=import-error  # noqa: F401\n\n        # Test import only when setuptools is available.\n        try:\n            # `extensions` is not included in ml_pipelines_sdk and doesn't have any\n            # transitive import.\n            import tfx.extensions as _  # type: ignore  # noqa: F401 # pylint: disable=g-import-not-at-top\n        except ModuleNotFoundError as err:\n            # The following condition detects exactly whether only the DSL package\n            # is installed, and is bypassed when tests run in Bazel.\n            raise RuntimeError(\n                'The \"tfx\" and all dependent packages need to be '\n                \"installed to use this functionality.\"\n            ) from err\n    except ModuleNotFoundError:\n        pass\n\n    super().__init__(*args, **kwargs)\n</code></pre> Attributes\u00b6 <code></code> artifact_type <code>property</code> \u00b6 <pre><code>artifact_type\n</code></pre> <p>Type of the underlying mlmd artifact.</p> <code></code> external_id <code>property</code> \u00b6 <pre><code>external_id: str\n</code></pre> <p>external id of the underlying artifact.</p> <code></code> id <code>property</code> <code>writable</code> \u00b6 <pre><code>id: int\n</code></pre> <p>Id of the underlying mlmd artifact.</p> <code></code> is_external <code>property</code> <code>writable</code> \u00b6 <pre><code>is_external: bool\n</code></pre> <p>Returns true if the artifact is external.</p> <code></code> mlmd_artifact <code>property</code> \u00b6 <pre><code>mlmd_artifact\n</code></pre> <p>Underlying mlmd artifact.</p> <code></code> name <code>property</code> <code>writable</code> \u00b6 <pre><code>name: str\n</code></pre> <p>Name of the underlying mlmd artifact.</p> <code></code> pipeline_name <code>property</code> <code>writable</code> \u00b6 <pre><code>pipeline_name: str\n</code></pre> <p>Name of the pipeline that produce the artifact.</p> <code></code> producer_component <code>property</code> <code>writable</code> \u00b6 <pre><code>producer_component: str\n</code></pre> <p>Producer component of the artifact.</p> <code></code> state <code>property</code> <code>writable</code> \u00b6 <pre><code>state: str\n</code></pre> <p>State of the underlying mlmd artifact.</p> <code></code> type <code>property</code> \u00b6 <pre><code>type\n</code></pre> <p>Type of the artifact.</p> <code></code> type_id <code>property</code> <code>writable</code> \u00b6 <pre><code>type_id: int\n</code></pre> <p>Type id of the underlying mlmd artifact.</p> <code></code> type_name <code>property</code> \u00b6 <pre><code>type_name\n</code></pre> <p>Type name of the underlying mlmd artifact.</p> <code></code> uri <code>property</code> <code>writable</code> \u00b6 <pre><code>uri: str\n</code></pre> <p>Artifact URI.</p> Functions\u00b6 <code></code> copy_from \u00b6 <pre><code>copy_from(other: Artifact)\n</code></pre> <p>Set uri, properties and custom properties from a given Artifact.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef copy_from(self, other: 'Artifact'):\n  \"\"\"Set uri, properties and custom properties from a given Artifact.\"\"\"\n  assert self.type is other.type, (\n      'Unable to set properties from an artifact of different type: {} vs {}'\n      .format(self.type_name, other.type_name))\n  self.uri = other.uri\n  if other.artifact_type.HasField('id'):\n    self.type_id = other.artifact_type.id\n\n  self._artifact.properties.clear()\n  self._artifact.properties.MergeFrom(other._artifact.properties)  # pylint: disable=protected-access\n  self._artifact.custom_properties.clear()\n  self._artifact.custom_properties.MergeFrom(\n      other._artifact.custom_properties)  # pylint: disable=protected-access\n  self._cached_modifiable_properties = copy.deepcopy(\n      other._cached_modifiable_properties)  # pylint: disable=protected-access\n  self._cached_modifiable_custom_properties = copy.deepcopy(\n      other._cached_modifiable_custom_properties)  # pylint: disable=protected-access\n</code></pre> <code></code> from_json_dict <code>classmethod</code> \u00b6 <pre><code>from_json_dict(dict_data: Dict[str, Any]) -&gt; Any\n</code></pre> <p>Convert from dictionary data to an object.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@classmethod\n@doc_controls.do_not_doc_inheritable\ndef from_json_dict(cls, dict_data: Dict[str, Any]) -&gt; Any:\n  module_name = dict_data['__artifact_class_module__']\n  class_name = dict_data['__artifact_class_name__']\n  artifact = metadata_store_pb2.Artifact()\n  artifact_type = metadata_store_pb2.ArtifactType()\n  json_format.Parse(json.dumps(dict_data['artifact']), artifact)\n  json_format.Parse(json.dumps(dict_data['artifact_type']), artifact_type)\n\n  # First, try to resolve the specific class used for the artifact; if this\n  # is not possible, use a generic artifact.Artifact object.\n  result = None\n  try:\n    artifact_cls = getattr(importlib.import_module(module_name), class_name)\n    # If the artifact type is the base Artifact class, do not construct the\n    # object here since that constructor requires the mlmd_artifact_type\n    # argument.\n    if artifact_cls != Artifact:\n      result = artifact_cls()\n  except (AttributeError, ImportError, ValueError):\n    logging.warning((\n        'Could not load artifact class %s.%s; using fallback deserialization '\n        'for the relevant artifact. Please make sure that any artifact '\n        'classes can be imported within your container or environment.'),\n                    module_name, class_name)\n  if not result:\n    result = Artifact(mlmd_artifact_type=artifact_type)\n  result.set_mlmd_artifact_type(artifact_type)\n  result.set_mlmd_artifact(artifact)\n  return result\n</code></pre> <code></code> get_bool_custom_property \u00b6 <pre><code>get_bool_custom_property(key: str) -&gt; bool\n</code></pre> <p>Get a custom property of bool type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_bool_custom_property(self, key: str) -&gt; bool:\n  \"\"\"Get a custom property of bool type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return False\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, bool):\n    return json_value\n  return self._artifact.custom_properties[key].bool_value\n</code></pre> <code></code> get_custom_property \u00b6 <pre><code>get_custom_property(key: str) -&gt; Optional[Union[int, float, str, bool, JsonValueType]]\n</code></pre> <p>Gets a custom property with key. Return None if not found.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_custom_property(\n    self, key: str\n) -&gt; Optional[Union[int, float, str, bool, JsonValueType]]:\n  \"\"\"Gets a custom property with key. Return None if not found.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return None\n\n  json_value = self.get_json_value_custom_property(key)\n  if json_value:\n    return json_value\n\n  mlmd_value = self._artifact.custom_properties[key]\n  if mlmd_value.HasField('int_value'):\n    return mlmd_value.int_value\n  elif mlmd_value.HasField('double_value'):\n    return mlmd_value.double_value\n  elif mlmd_value.HasField('string_value'):\n    return mlmd_value.string_value\n  elif mlmd_value.HasField('bool_value'):\n    return mlmd_value.bool_value\n  return None\n</code></pre> <code></code> get_float_custom_property \u00b6 <pre><code>get_float_custom_property(key: str) -&gt; float\n</code></pre> <p>Gets a custom property of float type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_float_custom_property(self, key: str) -&gt; float:\n  \"\"\"Gets a custom property of float type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return 0.0\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, float):\n    return json_value\n  return self._artifact.custom_properties[key].double_value\n</code></pre> <code></code> get_int_custom_property \u00b6 <pre><code>get_int_custom_property(key: str) -&gt; int\n</code></pre> <p>Get a custom property of int type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_int_custom_property(self, key: str) -&gt; int:\n  \"\"\"Get a custom property of int type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return 0\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, float):\n    return int(json_value)\n  return self._artifact.custom_properties[key].int_value\n</code></pre> <code></code> get_json_value_custom_property \u00b6 <pre><code>get_json_value_custom_property(key: str) -&gt; JsonValueType\n</code></pre> <p>Get a custom property of JSON type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef get_json_value_custom_property(self, key: str) -&gt; JsonValueType:\n  \"\"\"Get a custom property of JSON type.\"\"\"\n  if key in self._cached_modifiable_custom_properties:\n    return self._cached_modifiable_custom_properties[key]\n  if (key not in self._artifact.custom_properties or\n      not self._artifact.custom_properties[key].HasField('struct_value')):\n    return None\n  value = _decode_struct_value(\n      self._artifact.custom_properties[key].struct_value)\n  # We must cache the decoded lists or dictionaries returned here so that\n  # if their recursive contents are modified, the Metadata proto message\n  # can be updated to reflect this.\n  if isinstance(value, (dict, list)):\n    self._cached_modifiable_custom_properties[key] = value\n  return value\n</code></pre> <code></code> get_proto_custom_property \u00b6 <pre><code>get_proto_custom_property(key: str) -&gt; Optional[Message]\n</code></pre> <p>Get a custom property of proto type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef get_proto_custom_property(self, key: str) -&gt; Optional[message.Message]:\n  \"\"\"Get a custom property of proto type.\"\"\"\n  if key in self._cached_modifiable_custom_properties:\n    return self._cached_modifiable_custom_properties[key]\n  if (key not in self._artifact.custom_properties or\n      not self._artifact.custom_properties[key].HasField('proto_value')):\n    return None\n  value = proto_utils.unpack_proto_any(\n      self._artifact.custom_properties[key].proto_value)\n  # We must cache the protobuf message here so that if its contents are\n  # modified, the Metadata proto message can be updated to reflect this.\n  if isinstance(value, message.Message):\n    self._cached_modifiable_custom_properties[key] = value\n  return value\n</code></pre> <code></code> get_string_custom_property \u00b6 <pre><code>get_string_custom_property(key: str) -&gt; str\n</code></pre> <p>Get a custom property of string type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_string_custom_property(self, key: str) -&gt; str:\n  \"\"\"Get a custom property of string type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return ''\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, str):\n    return json_value\n  return self._artifact.custom_properties[key].string_value\n</code></pre> <code></code> set_bool_custom_property \u00b6 <pre><code>set_bool_custom_property(key: str, value: bool)\n</code></pre> <p>Sets a custom property of bool type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_bool_custom_property(self, key: str, value: bool):\n  \"\"\"Sets a custom property of bool type.\"\"\"\n  self._artifact.custom_properties[key].bool_value = value\n</code></pre> <code></code> set_float_custom_property \u00b6 <pre><code>set_float_custom_property(key: str, value: float)\n</code></pre> <p>Sets a custom property of float type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_float_custom_property(self, key: str, value: float):\n  \"\"\"Sets a custom property of float type.\"\"\"\n  self._artifact.custom_properties[key].double_value = builtins.float(value)\n</code></pre> <code></code> set_int_custom_property \u00b6 <pre><code>set_int_custom_property(key: str, value: int)\n</code></pre> <p>Set a custom property of int type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_int_custom_property(self, key: str, value: int):\n  \"\"\"Set a custom property of int type.\"\"\"\n  self._artifact.custom_properties[key].int_value = builtins.int(value)\n</code></pre> <code></code> set_json_value_custom_property \u00b6 <pre><code>set_json_value_custom_property(key: str, value: JsonValueType)\n</code></pre> <p>Sets a custom property of JSON type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_json_value_custom_property(self, key: str, value: JsonValueType):\n  \"\"\"Sets a custom property of JSON type.\"\"\"\n  self._cached_modifiable_custom_properties[key] = value\n</code></pre> <code></code> set_mlmd_artifact \u00b6 <pre><code>set_mlmd_artifact(artifact: Artifact)\n</code></pre> <p>Replace the MLMD artifact object on this artifact.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_mlmd_artifact(self, artifact: metadata_store_pb2.Artifact):\n  \"\"\"Replace the MLMD artifact object on this artifact.\"\"\"\n  if not isinstance(artifact, metadata_store_pb2.Artifact):\n    raise ValueError(\n        ('Expected instance of metadata_store_pb2.Artifact, got %s '\n         'instead.') % (artifact,))\n  self._artifact = artifact\n  self._cached_modifiable_properties = {}\n  self._cached_modifiable_custom_properties = {}\n</code></pre> <code></code> set_mlmd_artifact_type \u00b6 <pre><code>set_mlmd_artifact_type(artifact_type: ArtifactType)\n</code></pre> <p>Set entire ArtifactType in this object.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_mlmd_artifact_type(self,\n                           artifact_type: metadata_store_pb2.ArtifactType):\n  \"\"\"Set entire ArtifactType in this object.\"\"\"\n  if not isinstance(artifact_type, metadata_store_pb2.ArtifactType):\n    raise ValueError(\n        ('Expected instance of metadata_store_pb2.ArtifactType, got %s '\n         'instead.') % (artifact_type,))\n  self._artifact_type = artifact_type\n  self._artifact.type_id = artifact_type.id\n</code></pre> <code></code> set_proto_custom_property \u00b6 <pre><code>set_proto_custom_property(key: str, value: Message)\n</code></pre> <p>Sets a custom property of proto type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_proto_custom_property(self, key: str, value: message.Message):\n  \"\"\"Sets a custom property of proto type.\"\"\"\n  self._cached_modifiable_custom_properties[key] = value\n</code></pre> <code></code> set_string_custom_property \u00b6 <pre><code>set_string_custom_property(key: str, value: str)\n</code></pre> <p>Set a custom property of string type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_string_custom_property(self, key: str, value: str):\n  \"\"\"Set a custom property of string type.\"\"\"\n  self._artifact.custom_properties[key].string_value = value\n</code></pre> <code></code> to_json_dict \u00b6 <pre><code>to_json_dict() -&gt; Dict[str, Any]\n</code></pre> <p>Convert from an object to a JSON serializable dictionary.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef to_json_dict(self) -&gt; Dict[str, Any]:\n  return {\n      'artifact':\n          json.loads(\n              json_format.MessageToJson(\n                  message=self.mlmd_artifact,\n                  preserving_proto_field_name=True)),\n      'artifact_type':\n          json.loads(\n              json_format.MessageToJson(\n                  message=self._artifact_type,\n                  preserving_proto_field_name=True)),\n      '__artifact_class_module__':\n          self.__class__.__module__,\n      '__artifact_class_name__':\n          self.__class__.__name__,\n  }\n</code></pre>"},{"location":"api/v1/types/#tfx.v1.types.standard_artifacts.TransformGraph","title":"TransformGraph","text":"<pre><code>TransformGraph(*args, **kwargs)\n</code></pre> <p>               Bases: <code>_TfxArtifact</code></p> <p>TFX first-party component artifact definition.</p> <p>Construct TFX first-party component artifact.</p> METHOD DESCRIPTION <code>copy_from</code> <p>Set uri, properties and custom properties from a given Artifact.</p> <code>from_json_dict</code> <p>Convert from dictionary data to an object.</p> <code>get_bool_custom_property</code> <p>Get a custom property of bool type.</p> <code>get_custom_property</code> <p>Gets a custom property with key. Return None if not found.</p> <code>get_float_custom_property</code> <p>Gets a custom property of float type.</p> <code>get_int_custom_property</code> <p>Get a custom property of int type.</p> <code>get_json_value_custom_property</code> <p>Get a custom property of JSON type.</p> <code>get_proto_custom_property</code> <p>Get a custom property of proto type.</p> <code>get_string_custom_property</code> <p>Get a custom property of string type.</p> <code>set_bool_custom_property</code> <p>Sets a custom property of bool type.</p> <code>set_float_custom_property</code> <p>Sets a custom property of float type.</p> <code>set_int_custom_property</code> <p>Set a custom property of int type.</p> <code>set_json_value_custom_property</code> <p>Sets a custom property of JSON type.</p> <code>set_mlmd_artifact</code> <p>Replace the MLMD artifact object on this artifact.</p> <code>set_mlmd_artifact_type</code> <p>Set entire ArtifactType in this object.</p> <code>set_proto_custom_property</code> <p>Sets a custom property of proto type.</p> <code>set_string_custom_property</code> <p>Set a custom property of string type.</p> <code>to_json_dict</code> <p>Convert from an object to a JSON serializable dictionary.</p> ATTRIBUTE DESCRIPTION <code>artifact_type</code> <p>Type of the underlying mlmd artifact.</p> <p> </p> <code>external_id</code> <p>external id of the underlying artifact.</p> <p> TYPE: <code>str</code> </p> <code>id</code> <p>Id of the underlying mlmd artifact.</p> <p> TYPE: <code>int</code> </p> <code>is_external</code> <p>Returns true if the artifact is external.</p> <p> TYPE: <code>bool</code> </p> <code>mlmd_artifact</code> <p>Underlying mlmd artifact.</p> <p> </p> <code>name</code> <p>Name of the underlying mlmd artifact.</p> <p> TYPE: <code>str</code> </p> <code>pipeline_name</code> <p>Name of the pipeline that produce the artifact.</p> <p> TYPE: <code>str</code> </p> <code>producer_component</code> <p>Producer component of the artifact.</p> <p> TYPE: <code>str</code> </p> <code>state</code> <p>State of the underlying mlmd artifact.</p> <p> TYPE: <code>str</code> </p> <code>type</code> <p>Type of the artifact.</p> <p> </p> <code>type_id</code> <p>Type id of the underlying mlmd artifact.</p> <p> TYPE: <code>int</code> </p> <code>type_name</code> <p>Type name of the underlying mlmd artifact.</p> <p> </p> <code>uri</code> <p>Artifact URI.</p> <p> TYPE: <code>str</code> </p> Source code in <code>tfx/types/standard_artifacts.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    \"\"\"Construct TFX first-party component artifact.\"\"\"\n    # TODO(b/176795331): Refactor directory structure to make it clearer that\n    # TFX-specific artifacts require the full \"tfx\" package be installed.\n    #\n    # Do not allow usage of TFX-specific artifact if only the core pipeline\n    # SDK package is installed.\n    try:\n        import setuptools # pytype: disable=import-error  # noqa: F401\n\n        # Test import only when setuptools is available.\n        try:\n            # `extensions` is not included in ml_pipelines_sdk and doesn't have any\n            # transitive import.\n            import tfx.extensions as _  # type: ignore  # noqa: F401 # pylint: disable=g-import-not-at-top\n        except ModuleNotFoundError as err:\n            # The following condition detects exactly whether only the DSL package\n            # is installed, and is bypassed when tests run in Bazel.\n            raise RuntimeError(\n                'The \"tfx\" and all dependent packages need to be '\n                \"installed to use this functionality.\"\n            ) from err\n    except ModuleNotFoundError:\n        pass\n\n    super().__init__(*args, **kwargs)\n</code></pre> Attributes\u00b6 <code></code> artifact_type <code>property</code> \u00b6 <pre><code>artifact_type\n</code></pre> <p>Type of the underlying mlmd artifact.</p> <code></code> external_id <code>property</code> \u00b6 <pre><code>external_id: str\n</code></pre> <p>external id of the underlying artifact.</p> <code></code> id <code>property</code> <code>writable</code> \u00b6 <pre><code>id: int\n</code></pre> <p>Id of the underlying mlmd artifact.</p> <code></code> is_external <code>property</code> <code>writable</code> \u00b6 <pre><code>is_external: bool\n</code></pre> <p>Returns true if the artifact is external.</p> <code></code> mlmd_artifact <code>property</code> \u00b6 <pre><code>mlmd_artifact\n</code></pre> <p>Underlying mlmd artifact.</p> <code></code> name <code>property</code> <code>writable</code> \u00b6 <pre><code>name: str\n</code></pre> <p>Name of the underlying mlmd artifact.</p> <code></code> pipeline_name <code>property</code> <code>writable</code> \u00b6 <pre><code>pipeline_name: str\n</code></pre> <p>Name of the pipeline that produce the artifact.</p> <code></code> producer_component <code>property</code> <code>writable</code> \u00b6 <pre><code>producer_component: str\n</code></pre> <p>Producer component of the artifact.</p> <code></code> state <code>property</code> <code>writable</code> \u00b6 <pre><code>state: str\n</code></pre> <p>State of the underlying mlmd artifact.</p> <code></code> type <code>property</code> \u00b6 <pre><code>type\n</code></pre> <p>Type of the artifact.</p> <code></code> type_id <code>property</code> <code>writable</code> \u00b6 <pre><code>type_id: int\n</code></pre> <p>Type id of the underlying mlmd artifact.</p> <code></code> type_name <code>property</code> \u00b6 <pre><code>type_name\n</code></pre> <p>Type name of the underlying mlmd artifact.</p> <code></code> uri <code>property</code> <code>writable</code> \u00b6 <pre><code>uri: str\n</code></pre> <p>Artifact URI.</p> Functions\u00b6 <code></code> copy_from \u00b6 <pre><code>copy_from(other: Artifact)\n</code></pre> <p>Set uri, properties and custom properties from a given Artifact.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef copy_from(self, other: 'Artifact'):\n  \"\"\"Set uri, properties and custom properties from a given Artifact.\"\"\"\n  assert self.type is other.type, (\n      'Unable to set properties from an artifact of different type: {} vs {}'\n      .format(self.type_name, other.type_name))\n  self.uri = other.uri\n  if other.artifact_type.HasField('id'):\n    self.type_id = other.artifact_type.id\n\n  self._artifact.properties.clear()\n  self._artifact.properties.MergeFrom(other._artifact.properties)  # pylint: disable=protected-access\n  self._artifact.custom_properties.clear()\n  self._artifact.custom_properties.MergeFrom(\n      other._artifact.custom_properties)  # pylint: disable=protected-access\n  self._cached_modifiable_properties = copy.deepcopy(\n      other._cached_modifiable_properties)  # pylint: disable=protected-access\n  self._cached_modifiable_custom_properties = copy.deepcopy(\n      other._cached_modifiable_custom_properties)  # pylint: disable=protected-access\n</code></pre> <code></code> from_json_dict <code>classmethod</code> \u00b6 <pre><code>from_json_dict(dict_data: Dict[str, Any]) -&gt; Any\n</code></pre> <p>Convert from dictionary data to an object.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@classmethod\n@doc_controls.do_not_doc_inheritable\ndef from_json_dict(cls, dict_data: Dict[str, Any]) -&gt; Any:\n  module_name = dict_data['__artifact_class_module__']\n  class_name = dict_data['__artifact_class_name__']\n  artifact = metadata_store_pb2.Artifact()\n  artifact_type = metadata_store_pb2.ArtifactType()\n  json_format.Parse(json.dumps(dict_data['artifact']), artifact)\n  json_format.Parse(json.dumps(dict_data['artifact_type']), artifact_type)\n\n  # First, try to resolve the specific class used for the artifact; if this\n  # is not possible, use a generic artifact.Artifact object.\n  result = None\n  try:\n    artifact_cls = getattr(importlib.import_module(module_name), class_name)\n    # If the artifact type is the base Artifact class, do not construct the\n    # object here since that constructor requires the mlmd_artifact_type\n    # argument.\n    if artifact_cls != Artifact:\n      result = artifact_cls()\n  except (AttributeError, ImportError, ValueError):\n    logging.warning((\n        'Could not load artifact class %s.%s; using fallback deserialization '\n        'for the relevant artifact. Please make sure that any artifact '\n        'classes can be imported within your container or environment.'),\n                    module_name, class_name)\n  if not result:\n    result = Artifact(mlmd_artifact_type=artifact_type)\n  result.set_mlmd_artifact_type(artifact_type)\n  result.set_mlmd_artifact(artifact)\n  return result\n</code></pre> <code></code> get_bool_custom_property \u00b6 <pre><code>get_bool_custom_property(key: str) -&gt; bool\n</code></pre> <p>Get a custom property of bool type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_bool_custom_property(self, key: str) -&gt; bool:\n  \"\"\"Get a custom property of bool type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return False\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, bool):\n    return json_value\n  return self._artifact.custom_properties[key].bool_value\n</code></pre> <code></code> get_custom_property \u00b6 <pre><code>get_custom_property(key: str) -&gt; Optional[Union[int, float, str, bool, JsonValueType]]\n</code></pre> <p>Gets a custom property with key. Return None if not found.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_custom_property(\n    self, key: str\n) -&gt; Optional[Union[int, float, str, bool, JsonValueType]]:\n  \"\"\"Gets a custom property with key. Return None if not found.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return None\n\n  json_value = self.get_json_value_custom_property(key)\n  if json_value:\n    return json_value\n\n  mlmd_value = self._artifact.custom_properties[key]\n  if mlmd_value.HasField('int_value'):\n    return mlmd_value.int_value\n  elif mlmd_value.HasField('double_value'):\n    return mlmd_value.double_value\n  elif mlmd_value.HasField('string_value'):\n    return mlmd_value.string_value\n  elif mlmd_value.HasField('bool_value'):\n    return mlmd_value.bool_value\n  return None\n</code></pre> <code></code> get_float_custom_property \u00b6 <pre><code>get_float_custom_property(key: str) -&gt; float\n</code></pre> <p>Gets a custom property of float type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_float_custom_property(self, key: str) -&gt; float:\n  \"\"\"Gets a custom property of float type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return 0.0\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, float):\n    return json_value\n  return self._artifact.custom_properties[key].double_value\n</code></pre> <code></code> get_int_custom_property \u00b6 <pre><code>get_int_custom_property(key: str) -&gt; int\n</code></pre> <p>Get a custom property of int type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_int_custom_property(self, key: str) -&gt; int:\n  \"\"\"Get a custom property of int type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return 0\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, float):\n    return int(json_value)\n  return self._artifact.custom_properties[key].int_value\n</code></pre> <code></code> get_json_value_custom_property \u00b6 <pre><code>get_json_value_custom_property(key: str) -&gt; JsonValueType\n</code></pre> <p>Get a custom property of JSON type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef get_json_value_custom_property(self, key: str) -&gt; JsonValueType:\n  \"\"\"Get a custom property of JSON type.\"\"\"\n  if key in self._cached_modifiable_custom_properties:\n    return self._cached_modifiable_custom_properties[key]\n  if (key not in self._artifact.custom_properties or\n      not self._artifact.custom_properties[key].HasField('struct_value')):\n    return None\n  value = _decode_struct_value(\n      self._artifact.custom_properties[key].struct_value)\n  # We must cache the decoded lists or dictionaries returned here so that\n  # if their recursive contents are modified, the Metadata proto message\n  # can be updated to reflect this.\n  if isinstance(value, (dict, list)):\n    self._cached_modifiable_custom_properties[key] = value\n  return value\n</code></pre> <code></code> get_proto_custom_property \u00b6 <pre><code>get_proto_custom_property(key: str) -&gt; Optional[Message]\n</code></pre> <p>Get a custom property of proto type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef get_proto_custom_property(self, key: str) -&gt; Optional[message.Message]:\n  \"\"\"Get a custom property of proto type.\"\"\"\n  if key in self._cached_modifiable_custom_properties:\n    return self._cached_modifiable_custom_properties[key]\n  if (key not in self._artifact.custom_properties or\n      not self._artifact.custom_properties[key].HasField('proto_value')):\n    return None\n  value = proto_utils.unpack_proto_any(\n      self._artifact.custom_properties[key].proto_value)\n  # We must cache the protobuf message here so that if its contents are\n  # modified, the Metadata proto message can be updated to reflect this.\n  if isinstance(value, message.Message):\n    self._cached_modifiable_custom_properties[key] = value\n  return value\n</code></pre> <code></code> get_string_custom_property \u00b6 <pre><code>get_string_custom_property(key: str) -&gt; str\n</code></pre> <p>Get a custom property of string type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_string_custom_property(self, key: str) -&gt; str:\n  \"\"\"Get a custom property of string type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return ''\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, str):\n    return json_value\n  return self._artifact.custom_properties[key].string_value\n</code></pre> <code></code> set_bool_custom_property \u00b6 <pre><code>set_bool_custom_property(key: str, value: bool)\n</code></pre> <p>Sets a custom property of bool type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_bool_custom_property(self, key: str, value: bool):\n  \"\"\"Sets a custom property of bool type.\"\"\"\n  self._artifact.custom_properties[key].bool_value = value\n</code></pre> <code></code> set_float_custom_property \u00b6 <pre><code>set_float_custom_property(key: str, value: float)\n</code></pre> <p>Sets a custom property of float type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_float_custom_property(self, key: str, value: float):\n  \"\"\"Sets a custom property of float type.\"\"\"\n  self._artifact.custom_properties[key].double_value = builtins.float(value)\n</code></pre> <code></code> set_int_custom_property \u00b6 <pre><code>set_int_custom_property(key: str, value: int)\n</code></pre> <p>Set a custom property of int type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_int_custom_property(self, key: str, value: int):\n  \"\"\"Set a custom property of int type.\"\"\"\n  self._artifact.custom_properties[key].int_value = builtins.int(value)\n</code></pre> <code></code> set_json_value_custom_property \u00b6 <pre><code>set_json_value_custom_property(key: str, value: JsonValueType)\n</code></pre> <p>Sets a custom property of JSON type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_json_value_custom_property(self, key: str, value: JsonValueType):\n  \"\"\"Sets a custom property of JSON type.\"\"\"\n  self._cached_modifiable_custom_properties[key] = value\n</code></pre> <code></code> set_mlmd_artifact \u00b6 <pre><code>set_mlmd_artifact(artifact: Artifact)\n</code></pre> <p>Replace the MLMD artifact object on this artifact.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_mlmd_artifact(self, artifact: metadata_store_pb2.Artifact):\n  \"\"\"Replace the MLMD artifact object on this artifact.\"\"\"\n  if not isinstance(artifact, metadata_store_pb2.Artifact):\n    raise ValueError(\n        ('Expected instance of metadata_store_pb2.Artifact, got %s '\n         'instead.') % (artifact,))\n  self._artifact = artifact\n  self._cached_modifiable_properties = {}\n  self._cached_modifiable_custom_properties = {}\n</code></pre> <code></code> set_mlmd_artifact_type \u00b6 <pre><code>set_mlmd_artifact_type(artifact_type: ArtifactType)\n</code></pre> <p>Set entire ArtifactType in this object.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_mlmd_artifact_type(self,\n                           artifact_type: metadata_store_pb2.ArtifactType):\n  \"\"\"Set entire ArtifactType in this object.\"\"\"\n  if not isinstance(artifact_type, metadata_store_pb2.ArtifactType):\n    raise ValueError(\n        ('Expected instance of metadata_store_pb2.ArtifactType, got %s '\n         'instead.') % (artifact_type,))\n  self._artifact_type = artifact_type\n  self._artifact.type_id = artifact_type.id\n</code></pre> <code></code> set_proto_custom_property \u00b6 <pre><code>set_proto_custom_property(key: str, value: Message)\n</code></pre> <p>Sets a custom property of proto type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_proto_custom_property(self, key: str, value: message.Message):\n  \"\"\"Sets a custom property of proto type.\"\"\"\n  self._cached_modifiable_custom_properties[key] = value\n</code></pre> <code></code> set_string_custom_property \u00b6 <pre><code>set_string_custom_property(key: str, value: str)\n</code></pre> <p>Set a custom property of string type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_string_custom_property(self, key: str, value: str):\n  \"\"\"Set a custom property of string type.\"\"\"\n  self._artifact.custom_properties[key].string_value = value\n</code></pre> <code></code> to_json_dict \u00b6 <pre><code>to_json_dict() -&gt; Dict[str, Any]\n</code></pre> <p>Convert from an object to a JSON serializable dictionary.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef to_json_dict(self) -&gt; Dict[str, Any]:\n  return {\n      'artifact':\n          json.loads(\n              json_format.MessageToJson(\n                  message=self.mlmd_artifact,\n                  preserving_proto_field_name=True)),\n      'artifact_type':\n          json.loads(\n              json_format.MessageToJson(\n                  message=self._artifact_type,\n                  preserving_proto_field_name=True)),\n      '__artifact_class_module__':\n          self.__class__.__module__,\n      '__artifact_class_name__':\n          self.__class__.__name__,\n  }\n</code></pre>"},{"location":"api/v1/types/#tfx.v1.types.standard_artifacts.TunerResults","title":"TunerResults","text":"<pre><code>TunerResults(*args, **kwargs)\n</code></pre> <p>               Bases: <code>_TfxArtifact</code></p> <p>TFX first-party component artifact definition.</p> <p>Construct TFX first-party component artifact.</p> METHOD DESCRIPTION <code>copy_from</code> <p>Set uri, properties and custom properties from a given Artifact.</p> <code>from_json_dict</code> <p>Convert from dictionary data to an object.</p> <code>get_bool_custom_property</code> <p>Get a custom property of bool type.</p> <code>get_custom_property</code> <p>Gets a custom property with key. Return None if not found.</p> <code>get_float_custom_property</code> <p>Gets a custom property of float type.</p> <code>get_int_custom_property</code> <p>Get a custom property of int type.</p> <code>get_json_value_custom_property</code> <p>Get a custom property of JSON type.</p> <code>get_proto_custom_property</code> <p>Get a custom property of proto type.</p> <code>get_string_custom_property</code> <p>Get a custom property of string type.</p> <code>set_bool_custom_property</code> <p>Sets a custom property of bool type.</p> <code>set_float_custom_property</code> <p>Sets a custom property of float type.</p> <code>set_int_custom_property</code> <p>Set a custom property of int type.</p> <code>set_json_value_custom_property</code> <p>Sets a custom property of JSON type.</p> <code>set_mlmd_artifact</code> <p>Replace the MLMD artifact object on this artifact.</p> <code>set_mlmd_artifact_type</code> <p>Set entire ArtifactType in this object.</p> <code>set_proto_custom_property</code> <p>Sets a custom property of proto type.</p> <code>set_string_custom_property</code> <p>Set a custom property of string type.</p> <code>to_json_dict</code> <p>Convert from an object to a JSON serializable dictionary.</p> ATTRIBUTE DESCRIPTION <code>artifact_type</code> <p>Type of the underlying mlmd artifact.</p> <p> </p> <code>external_id</code> <p>external id of the underlying artifact.</p> <p> TYPE: <code>str</code> </p> <code>id</code> <p>Id of the underlying mlmd artifact.</p> <p> TYPE: <code>int</code> </p> <code>is_external</code> <p>Returns true if the artifact is external.</p> <p> TYPE: <code>bool</code> </p> <code>mlmd_artifact</code> <p>Underlying mlmd artifact.</p> <p> </p> <code>name</code> <p>Name of the underlying mlmd artifact.</p> <p> TYPE: <code>str</code> </p> <code>pipeline_name</code> <p>Name of the pipeline that produce the artifact.</p> <p> TYPE: <code>str</code> </p> <code>producer_component</code> <p>Producer component of the artifact.</p> <p> TYPE: <code>str</code> </p> <code>state</code> <p>State of the underlying mlmd artifact.</p> <p> TYPE: <code>str</code> </p> <code>type</code> <p>Type of the artifact.</p> <p> </p> <code>type_id</code> <p>Type id of the underlying mlmd artifact.</p> <p> TYPE: <code>int</code> </p> <code>type_name</code> <p>Type name of the underlying mlmd artifact.</p> <p> </p> <code>uri</code> <p>Artifact URI.</p> <p> TYPE: <code>str</code> </p> Source code in <code>tfx/types/standard_artifacts.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    \"\"\"Construct TFX first-party component artifact.\"\"\"\n    # TODO(b/176795331): Refactor directory structure to make it clearer that\n    # TFX-specific artifacts require the full \"tfx\" package be installed.\n    #\n    # Do not allow usage of TFX-specific artifact if only the core pipeline\n    # SDK package is installed.\n    try:\n        import setuptools # pytype: disable=import-error  # noqa: F401\n\n        # Test import only when setuptools is available.\n        try:\n            # `extensions` is not included in ml_pipelines_sdk and doesn't have any\n            # transitive import.\n            import tfx.extensions as _  # type: ignore  # noqa: F401 # pylint: disable=g-import-not-at-top\n        except ModuleNotFoundError as err:\n            # The following condition detects exactly whether only the DSL package\n            # is installed, and is bypassed when tests run in Bazel.\n            raise RuntimeError(\n                'The \"tfx\" and all dependent packages need to be '\n                \"installed to use this functionality.\"\n            ) from err\n    except ModuleNotFoundError:\n        pass\n\n    super().__init__(*args, **kwargs)\n</code></pre> Attributes\u00b6 <code></code> artifact_type <code>property</code> \u00b6 <pre><code>artifact_type\n</code></pre> <p>Type of the underlying mlmd artifact.</p> <code></code> external_id <code>property</code> \u00b6 <pre><code>external_id: str\n</code></pre> <p>external id of the underlying artifact.</p> <code></code> id <code>property</code> <code>writable</code> \u00b6 <pre><code>id: int\n</code></pre> <p>Id of the underlying mlmd artifact.</p> <code></code> is_external <code>property</code> <code>writable</code> \u00b6 <pre><code>is_external: bool\n</code></pre> <p>Returns true if the artifact is external.</p> <code></code> mlmd_artifact <code>property</code> \u00b6 <pre><code>mlmd_artifact\n</code></pre> <p>Underlying mlmd artifact.</p> <code></code> name <code>property</code> <code>writable</code> \u00b6 <pre><code>name: str\n</code></pre> <p>Name of the underlying mlmd artifact.</p> <code></code> pipeline_name <code>property</code> <code>writable</code> \u00b6 <pre><code>pipeline_name: str\n</code></pre> <p>Name of the pipeline that produce the artifact.</p> <code></code> producer_component <code>property</code> <code>writable</code> \u00b6 <pre><code>producer_component: str\n</code></pre> <p>Producer component of the artifact.</p> <code></code> state <code>property</code> <code>writable</code> \u00b6 <pre><code>state: str\n</code></pre> <p>State of the underlying mlmd artifact.</p> <code></code> type <code>property</code> \u00b6 <pre><code>type\n</code></pre> <p>Type of the artifact.</p> <code></code> type_id <code>property</code> <code>writable</code> \u00b6 <pre><code>type_id: int\n</code></pre> <p>Type id of the underlying mlmd artifact.</p> <code></code> type_name <code>property</code> \u00b6 <pre><code>type_name\n</code></pre> <p>Type name of the underlying mlmd artifact.</p> <code></code> uri <code>property</code> <code>writable</code> \u00b6 <pre><code>uri: str\n</code></pre> <p>Artifact URI.</p> Functions\u00b6 <code></code> copy_from \u00b6 <pre><code>copy_from(other: Artifact)\n</code></pre> <p>Set uri, properties and custom properties from a given Artifact.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef copy_from(self, other: 'Artifact'):\n  \"\"\"Set uri, properties and custom properties from a given Artifact.\"\"\"\n  assert self.type is other.type, (\n      'Unable to set properties from an artifact of different type: {} vs {}'\n      .format(self.type_name, other.type_name))\n  self.uri = other.uri\n  if other.artifact_type.HasField('id'):\n    self.type_id = other.artifact_type.id\n\n  self._artifact.properties.clear()\n  self._artifact.properties.MergeFrom(other._artifact.properties)  # pylint: disable=protected-access\n  self._artifact.custom_properties.clear()\n  self._artifact.custom_properties.MergeFrom(\n      other._artifact.custom_properties)  # pylint: disable=protected-access\n  self._cached_modifiable_properties = copy.deepcopy(\n      other._cached_modifiable_properties)  # pylint: disable=protected-access\n  self._cached_modifiable_custom_properties = copy.deepcopy(\n      other._cached_modifiable_custom_properties)  # pylint: disable=protected-access\n</code></pre> <code></code> from_json_dict <code>classmethod</code> \u00b6 <pre><code>from_json_dict(dict_data: Dict[str, Any]) -&gt; Any\n</code></pre> <p>Convert from dictionary data to an object.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@classmethod\n@doc_controls.do_not_doc_inheritable\ndef from_json_dict(cls, dict_data: Dict[str, Any]) -&gt; Any:\n  module_name = dict_data['__artifact_class_module__']\n  class_name = dict_data['__artifact_class_name__']\n  artifact = metadata_store_pb2.Artifact()\n  artifact_type = metadata_store_pb2.ArtifactType()\n  json_format.Parse(json.dumps(dict_data['artifact']), artifact)\n  json_format.Parse(json.dumps(dict_data['artifact_type']), artifact_type)\n\n  # First, try to resolve the specific class used for the artifact; if this\n  # is not possible, use a generic artifact.Artifact object.\n  result = None\n  try:\n    artifact_cls = getattr(importlib.import_module(module_name), class_name)\n    # If the artifact type is the base Artifact class, do not construct the\n    # object here since that constructor requires the mlmd_artifact_type\n    # argument.\n    if artifact_cls != Artifact:\n      result = artifact_cls()\n  except (AttributeError, ImportError, ValueError):\n    logging.warning((\n        'Could not load artifact class %s.%s; using fallback deserialization '\n        'for the relevant artifact. Please make sure that any artifact '\n        'classes can be imported within your container or environment.'),\n                    module_name, class_name)\n  if not result:\n    result = Artifact(mlmd_artifact_type=artifact_type)\n  result.set_mlmd_artifact_type(artifact_type)\n  result.set_mlmd_artifact(artifact)\n  return result\n</code></pre> <code></code> get_bool_custom_property \u00b6 <pre><code>get_bool_custom_property(key: str) -&gt; bool\n</code></pre> <p>Get a custom property of bool type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_bool_custom_property(self, key: str) -&gt; bool:\n  \"\"\"Get a custom property of bool type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return False\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, bool):\n    return json_value\n  return self._artifact.custom_properties[key].bool_value\n</code></pre> <code></code> get_custom_property \u00b6 <pre><code>get_custom_property(key: str) -&gt; Optional[Union[int, float, str, bool, JsonValueType]]\n</code></pre> <p>Gets a custom property with key. Return None if not found.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_custom_property(\n    self, key: str\n) -&gt; Optional[Union[int, float, str, bool, JsonValueType]]:\n  \"\"\"Gets a custom property with key. Return None if not found.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return None\n\n  json_value = self.get_json_value_custom_property(key)\n  if json_value:\n    return json_value\n\n  mlmd_value = self._artifact.custom_properties[key]\n  if mlmd_value.HasField('int_value'):\n    return mlmd_value.int_value\n  elif mlmd_value.HasField('double_value'):\n    return mlmd_value.double_value\n  elif mlmd_value.HasField('string_value'):\n    return mlmd_value.string_value\n  elif mlmd_value.HasField('bool_value'):\n    return mlmd_value.bool_value\n  return None\n</code></pre> <code></code> get_float_custom_property \u00b6 <pre><code>get_float_custom_property(key: str) -&gt; float\n</code></pre> <p>Gets a custom property of float type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_float_custom_property(self, key: str) -&gt; float:\n  \"\"\"Gets a custom property of float type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return 0.0\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, float):\n    return json_value\n  return self._artifact.custom_properties[key].double_value\n</code></pre> <code></code> get_int_custom_property \u00b6 <pre><code>get_int_custom_property(key: str) -&gt; int\n</code></pre> <p>Get a custom property of int type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_int_custom_property(self, key: str) -&gt; int:\n  \"\"\"Get a custom property of int type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return 0\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, float):\n    return int(json_value)\n  return self._artifact.custom_properties[key].int_value\n</code></pre> <code></code> get_json_value_custom_property \u00b6 <pre><code>get_json_value_custom_property(key: str) -&gt; JsonValueType\n</code></pre> <p>Get a custom property of JSON type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef get_json_value_custom_property(self, key: str) -&gt; JsonValueType:\n  \"\"\"Get a custom property of JSON type.\"\"\"\n  if key in self._cached_modifiable_custom_properties:\n    return self._cached_modifiable_custom_properties[key]\n  if (key not in self._artifact.custom_properties or\n      not self._artifact.custom_properties[key].HasField('struct_value')):\n    return None\n  value = _decode_struct_value(\n      self._artifact.custom_properties[key].struct_value)\n  # We must cache the decoded lists or dictionaries returned here so that\n  # if their recursive contents are modified, the Metadata proto message\n  # can be updated to reflect this.\n  if isinstance(value, (dict, list)):\n    self._cached_modifiable_custom_properties[key] = value\n  return value\n</code></pre> <code></code> get_proto_custom_property \u00b6 <pre><code>get_proto_custom_property(key: str) -&gt; Optional[Message]\n</code></pre> <p>Get a custom property of proto type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef get_proto_custom_property(self, key: str) -&gt; Optional[message.Message]:\n  \"\"\"Get a custom property of proto type.\"\"\"\n  if key in self._cached_modifiable_custom_properties:\n    return self._cached_modifiable_custom_properties[key]\n  if (key not in self._artifact.custom_properties or\n      not self._artifact.custom_properties[key].HasField('proto_value')):\n    return None\n  value = proto_utils.unpack_proto_any(\n      self._artifact.custom_properties[key].proto_value)\n  # We must cache the protobuf message here so that if its contents are\n  # modified, the Metadata proto message can be updated to reflect this.\n  if isinstance(value, message.Message):\n    self._cached_modifiable_custom_properties[key] = value\n  return value\n</code></pre> <code></code> get_string_custom_property \u00b6 <pre><code>get_string_custom_property(key: str) -&gt; str\n</code></pre> <p>Get a custom property of string type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef get_string_custom_property(self, key: str) -&gt; str:\n  \"\"\"Get a custom property of string type.\"\"\"\n  if key not in self._artifact.custom_properties:\n    return ''\n  json_value = self.get_json_value_custom_property(key)\n  if isinstance(json_value, str):\n    return json_value\n  return self._artifact.custom_properties[key].string_value\n</code></pre> <code></code> set_bool_custom_property \u00b6 <pre><code>set_bool_custom_property(key: str, value: bool)\n</code></pre> <p>Sets a custom property of bool type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_bool_custom_property(self, key: str, value: bool):\n  \"\"\"Sets a custom property of bool type.\"\"\"\n  self._artifact.custom_properties[key].bool_value = value\n</code></pre> <code></code> set_float_custom_property \u00b6 <pre><code>set_float_custom_property(key: str, value: float)\n</code></pre> <p>Sets a custom property of float type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_float_custom_property(self, key: str, value: float):\n  \"\"\"Sets a custom property of float type.\"\"\"\n  self._artifact.custom_properties[key].double_value = builtins.float(value)\n</code></pre> <code></code> set_int_custom_property \u00b6 <pre><code>set_int_custom_property(key: str, value: int)\n</code></pre> <p>Set a custom property of int type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_int_custom_property(self, key: str, value: int):\n  \"\"\"Set a custom property of int type.\"\"\"\n  self._artifact.custom_properties[key].int_value = builtins.int(value)\n</code></pre> <code></code> set_json_value_custom_property \u00b6 <pre><code>set_json_value_custom_property(key: str, value: JsonValueType)\n</code></pre> <p>Sets a custom property of JSON type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_json_value_custom_property(self, key: str, value: JsonValueType):\n  \"\"\"Sets a custom property of JSON type.\"\"\"\n  self._cached_modifiable_custom_properties[key] = value\n</code></pre> <code></code> set_mlmd_artifact \u00b6 <pre><code>set_mlmd_artifact(artifact: Artifact)\n</code></pre> <p>Replace the MLMD artifact object on this artifact.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_mlmd_artifact(self, artifact: metadata_store_pb2.Artifact):\n  \"\"\"Replace the MLMD artifact object on this artifact.\"\"\"\n  if not isinstance(artifact, metadata_store_pb2.Artifact):\n    raise ValueError(\n        ('Expected instance of metadata_store_pb2.Artifact, got %s '\n         'instead.') % (artifact,))\n  self._artifact = artifact\n  self._cached_modifiable_properties = {}\n  self._cached_modifiable_custom_properties = {}\n</code></pre> <code></code> set_mlmd_artifact_type \u00b6 <pre><code>set_mlmd_artifact_type(artifact_type: ArtifactType)\n</code></pre> <p>Set entire ArtifactType in this object.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_mlmd_artifact_type(self,\n                           artifact_type: metadata_store_pb2.ArtifactType):\n  \"\"\"Set entire ArtifactType in this object.\"\"\"\n  if not isinstance(artifact_type, metadata_store_pb2.ArtifactType):\n    raise ValueError(\n        ('Expected instance of metadata_store_pb2.ArtifactType, got %s '\n         'instead.') % (artifact_type,))\n  self._artifact_type = artifact_type\n  self._artifact.type_id = artifact_type.id\n</code></pre> <code></code> set_proto_custom_property \u00b6 <pre><code>set_proto_custom_property(key: str, value: Message)\n</code></pre> <p>Sets a custom property of proto type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef set_proto_custom_property(self, key: str, value: message.Message):\n  \"\"\"Sets a custom property of proto type.\"\"\"\n  self._cached_modifiable_custom_properties[key] = value\n</code></pre> <code></code> set_string_custom_property \u00b6 <pre><code>set_string_custom_property(key: str, value: str)\n</code></pre> <p>Set a custom property of string type.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_in_subclasses\ndef set_string_custom_property(self, key: str, value: str):\n  \"\"\"Set a custom property of string type.\"\"\"\n  self._artifact.custom_properties[key].string_value = value\n</code></pre> <code></code> to_json_dict \u00b6 <pre><code>to_json_dict() -&gt; Dict[str, Any]\n</code></pre> <p>Convert from an object to a JSON serializable dictionary.</p> Source code in <code>tfx/types/artifact.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef to_json_dict(self) -&gt; Dict[str, Any]:\n  return {\n      'artifact':\n          json.loads(\n              json_format.MessageToJson(\n                  message=self.mlmd_artifact,\n                  preserving_proto_field_name=True)),\n      'artifact_type':\n          json.loads(\n              json_format.MessageToJson(\n                  message=self._artifact_type,\n                  preserving_proto_field_name=True)),\n      '__artifact_class_module__':\n          self.__class__.__module__,\n      '__artifact_class_name__':\n          self.__class__.__name__,\n  }\n</code></pre>"},{"location":"api/v1/utils/","title":"Utils","text":""},{"location":"api/v1/utils/#tfx.v1.utils","title":"tfx.v1.utils","text":"<p>TFX utils module.</p> FUNCTION DESCRIPTION <code>parse_pbtxt_file</code> <p>Parses a protobuf message from a text file and return message itself.</p> ATTRIBUTE DESCRIPTION <code>JsonableType</code> <p> </p>"},{"location":"api/v1/utils/#tfx.v1.utils-attributes","title":"Attributes","text":""},{"location":"api/v1/utils/#tfx.v1.utils.JsonableType","title":"JsonableType  <code>module-attribute</code>","text":"<pre><code>JsonableType = JsonableValue\n</code></pre>"},{"location":"api/v1/utils/#tfx.v1.utils-functions","title":"Functions","text":""},{"location":"api/v1/utils/#tfx.v1.utils.parse_pbtxt_file","title":"parse_pbtxt_file","text":"<pre><code>parse_pbtxt_file(file_name: str, message: ProtoMessage) -&gt; ProtoMessage\n</code></pre> <p>Parses a protobuf message from a text file and return message itself.</p> Source code in <code>tfx/utils/io_utils.py</code> <pre><code>def parse_pbtxt_file(file_name: str, message: ProtoMessage) -&gt; ProtoMessage:\n  \"\"\"Parses a protobuf message from a text file and return message itself.\"\"\"\n  contents = fileio.open(file_name).read()\n  text_format.Parse(contents, message)\n  return message\n</code></pre>"},{"location":"guide/","title":"The TFX User Guide","text":""},{"location":"guide/#introduction","title":"Introduction","text":"<p>TFX is a Google-production-scale machine learning (ML) platform based on TensorFlow. It provides a configuration framework and shared libraries to integrate common components needed to define, launch, and monitor your machine learning system.</p>"},{"location":"guide/#tfx-10","title":"TFX 1.0","text":"<p>We are happy to announce the availability of the TFX 1.0.0. This is the initial post-beta release of TFX, which provides stable public APIs and artifacts. You can be assured that your future TFX pipelines will keep working after an upgrade within the compatibility scope defined in this RFC.</p>"},{"location":"guide/#installation","title":"Installation","text":"<pre><code>pip install tfx\n</code></pre> <p>Note</p> <p>See the TensorFlow Serving, TensorFlow JS, and/or TensorFlow Lite documentation for installing those optional components.</p> <p>Note</p> <p>This installs Apache Beam with the DirectRunner.  You can also separately install runners that perform distributed computation, such as Apache Flink or Apache Spark.</p>"},{"location":"guide/#nightly-packages","title":"Nightly Packages","text":"<p>TFX also hosts nightly packages at https://pypi-nightly.tensorflow.org on Google Cloud. To install the latest nightly package, please use the following command:</p> <pre><code>pip install --extra-index-url https://pypi-nightly.tensorflow.org/simple --pre tfx\n</code></pre> <p>This will install the nightly packages for the major dependencies of TFX such as TensorFlow Model Analysis (TFMA), TensorFlow Data Validation (TFDV), TensorFlow Transform (TFT), TFX Basic Shared Libraries (TFX-BSL), ML Metadata (MLMD).</p> <p>Note</p> <p>These nightly packages are unstable and breakages are likely to happen. The fix could often take a week or more depending on the complexity involved.</p>"},{"location":"guide/#about-tfx","title":"About TFX","text":"<p>TFX is a platform for building and managing ML workflows in a production environment. TFX provides the following:</p> <ul> <li> <p>A toolkit for building ML pipelines. TFX pipelines let you orchestrate your     ML workflow on several platforms, such as: Apache Airflow, Apache Beam, and     Kubeflow Pipelines.</p> <p>Learn more about TFX pipelines.</p> </li> <li> <p>A set of standard components that you can use as a part of a pipeline, or as     a part of your ML training script. TFX standard components provide proven     functionality to help you get started building an ML process easily.</p> <p>Learn more about TFX standard components.</p> </li> <li> <p>Libraries which provide the base functionality for many of the standard     components. You can use the TFX libraries to add this functionality to your     own custom components, or use them separately.</p> <p>Learn more about the TFX libraries.</p> </li> </ul> <p>TFX is a Google-production-scale machine learning toolkit based on TensorFlow. It provides a configuration framework and shared libraries to integrate common components needed to define, launch, and monitor your machine learning system.</p>"},{"location":"guide/#tfx-standard-components","title":"TFX Standard Components","text":"<p>A TFX pipeline is a sequence of components that implement an ML pipeline which is specifically designed for scalable, high-performance machine learning tasks. That includes modeling, training, serving inference, and managing deployments to online, native mobile, and JavaScript targets.</p> <p>A TFX pipeline typically includes the following components:</p> <ul> <li> <p>ExampleGen is the initial input component of a pipeline     that ingests and optionally splits the input dataset.</p> </li> <li> <p>StatisticsGen calculates statistics for the dataset.</p> </li> <li> <p>SchemaGen examines the statistics and creates a data     schema.</p> </li> <li> <p>ExampleValidator looks for anomalies and missing values     in the dataset.</p> </li> <li> <p>Transform performs feature engineering on the dataset.</p> </li> <li> <p>Trainer trains the model.</p> </li> <li> <p>Tuner tunes the hyperparameters of the model.</p> </li> <li> <p>Evaluator performs deep analysis of the training results     and helps you validate your exported models, ensuring that they are \"good     enough\" to be pushed to production.</p> </li> <li> <p>InfraValidator checks the model is actually     servable from the infrastructure, and prevents bad model from being pushed.</p> </li> <li> <p>Pusher deploys the model on a serving infrastructure.</p> </li> <li> <p>BulkInferrer performs batch processing on a model     with unlabelled inference requests.</p> </li> </ul> <p>This diagram illustrates the flow of data between these components:</p> <p></p>"},{"location":"guide/#tfx-libraries","title":"TFX Libraries","text":"<p>TFX includes both libraries and pipeline components.  This diagram illustrates the relationships between TFX libraries and pipeline components:</p> <p></p> <p>TFX provides several Python packages that are the libraries which are used to create pipeline components.  You'll use these libraries to create the components of your pipelines so that your code can focus on the unique aspects of your pipeline.</p> <p>TFX libraries include:</p> <ul> <li> <p>TensorFlow Data Validation (TFDV) is a library for analyzing     and validating machine learning data. It is designed to be highly scalable     and to work well with TensorFlow and TFX. TFDV includes:</p> <ul> <li>Scalable calculation of summary statistics of training and test data.</li> <li>Integration with a viewer for data distributions and statistics, as well     as faceted comparison of pairs of datasets (Facets).</li> <li>Automated data-schema generation to describe expectations about data     like required values, ranges, and vocabularies.</li> <li>A schema viewer to help you inspect the schema.</li> <li>Anomaly detection to identify anomalies, such as missing features,     out-of- range values, or wrong feature types, to name a few.</li> <li>An anomalies viewer so that you can see what features have anomalies and     learn more in order to correct them.</li> </ul> </li> <li> <p>TensorFlow Transform (TFT) is a library for preprocessing data     with TensorFlow. TensorFlow Transform is useful for data that requires a     full- pass, such as:</p> <ul> <li>Normalize an input value by mean and standard deviation.</li> <li>Convert strings to integers by generating a vocabulary over all input     values.</li> <li>Convert floats to integers by assigning them to buckets based on the     observed data distribution.</li> </ul> </li> <li> <p>TensorFlow is used for training models with TFX. It ingests     training data and modeling code and creates a SavedModel result. It also     integrates a feature engineering pipeline created by TensorFlow Transform     for preprocessing input data.</p> <p>KerasTuner is used for tuning hyperparameters for model.</p> <p>Note</p> <p>TFX supports TensorFlow 1.15 and, with some exceptions, 2.x. For details, see Designing TensorFlow Modeling Code For TFX.</p> </li> <li> <p>TensorFlow Model Analysis (TFMA) is a library for evaluating     TensorFlow models. It is used along with TensorFlow to create an     EvalSavedModel, which becomes the basis for its analysis. It allows users to     evaluate their models on large amounts of data in a distributed manner,     using the same metrics defined in their trainer. These metrics can be     computed over different slices of data and visualized in Jupyter notebooks.</p> </li> <li> <p>TensorFlow Metadata (TFMD)     provides standard representations for metadata that are useful when training     machine learning models with TensorFlow. The metadata may be produced by     hand or automatically during input data analysis, and may be consumed for     data validation, exploration, and transformation. The metadata serialization     formats include:</p> <ul> <li>A schema describing tabular data (e.g., tf.Examples).</li> <li>A collection of summary statistics over such datasets.</li> </ul> </li> <li> <p>ML Metadata (MLMD) is a library for recording and retrieving     metadata associated with ML developer and data scientist workflows. Most     often the metadata uses TFMD representations. MLMD manages persistence using     SQL-Lite,     MySQL, and other similar data stores.</p> </li> </ul>"},{"location":"guide/#supporting-technologies","title":"Supporting Technologies","text":""},{"location":"guide/#required","title":"Required","text":"<ul> <li>Apache Beam is an open source, unified model for defining both batch and streaming data-parallel processing pipelines. TFX uses Apache Beam to implement data-parallel pipelines.  The pipeline is then executed by one of Beam's supported distributed processing back-ends, which include Apache Flink, Apache Spark, Google Cloud Dataflow, and others.</li> </ul>"},{"location":"guide/#optional","title":"Optional","text":"<p>Orchestrators such as Apache Airflow and Kubeflow make configuring, operating, monitoring, and maintaining an ML pipeline easier.</p> <ul> <li> <p>Apache Airflow is a platform to     programmatically author, schedule and monitor workflows. TFX uses Airflow to     author workflows as directed acyclic graphs (DAGs) of tasks. The Airflow     scheduler executes tasks on an array of workers while following the     specified dependencies. Rich command line utilities make performing complex     surgeries on DAGs a snap. The rich user interface makes it easy to visualize     pipelines running in production, monitor progress, and troubleshoot issues     when needed. When workflows are defined as code, they become more     maintainable, versionable, testable, and collaborative.</p> </li> <li> <p>Kubeflow is dedicated to making deployments     of machine learning (ML) workflows on Kubernetes simple, portable and     scalable. Kubeflow's goal is not to recreate other services, but to provide     a straightforward way to deploy best-of-breed open-source systems for ML to     diverse infrastructures.     Kubeflow Pipelines     enable composition and execution of reproducible workflows on Kubeflow,     integrated with experimentation and notebook based experiences. Kubeflow     Pipelines services on Kubernetes include the hosted Metadata store,     container based orchestration engine, notebook server, and UI to help users     develop, run, and manage complex ML pipelines at scale. The Kubeflow     Pipelines SDK allows for creation and sharing of components and composition     of pipelines programmatically.</p> </li> </ul>"},{"location":"guide/#portability-and-interoperability","title":"Portability and Interoperability","text":"<p>TFX is designed to be portable to multiple environments and orchestration frameworks, including Apache Airflow, Apache Beam and Kubeflow . It is also portable to different computing platforms, including on-premise, and cloud platforms such as the Google Cloud Platform (GCP). In particular, TFX interoperates with several managed GCP services, such as Cloud AI Platform for Training and Prediction, and Cloud Dataflow for distributed data processing for several other aspects of the ML lifecycle.</p> <p>Note</p> <p>The current revision of this user guide primarily discusses deployment on a bare-metal system using Apache Airflow for orchestration.</p>"},{"location":"guide/#model-vs-savedmodel","title":"Model vs. SavedModel","text":""},{"location":"guide/#model","title":"Model","text":"<p>A model is the output of the training process. It is the serialized record of the weights that have been learned during the training process. These weights can be subsequently used to compute predictions for new input examples. For TFX and TensorFlow, 'model' refers to the checkpoints containing the weights learned up to that point.</p> <p>Note that 'model' might also refer to the definition of the TensorFlow computation graph (i.e. a Python file) that expresses how a prediction will be computed. The two senses may be used interchangeably based on context.</p>"},{"location":"guide/#savedmodel","title":"SavedModel","text":"<ul> <li>What is a SavedModel: a universal, language-neutral, hermetic, recoverable serialization of a TensorFlow model.</li> <li>Why is it important: It enables higher-level systems to produce, transform, and consume TensorFlow models using a single abstraction.</li> </ul> <p>SavedModel is the recommended serialization format for serving a TensorFlow model in production, or exporting a trained model for a native mobile or JavaScript application. For example, to turn a model into a REST service for making predictions, you can serialize the model as a SavedModel and serve it using TensorFlow Serving. See Serving a TensorFlow Model for more information.</p>"},{"location":"guide/#schema","title":"Schema","text":"<p>Some TFX components use a description of your input data called a schema. The schema is an instance of schema.proto. Schemas are a type of protocol buffer, more generally known as a \"protobuf\".  The schema can specify data types for feature values, whether a feature has to be present in all examples, allowed value ranges, and other properties.  One of the benefits of using TensorFlow Data Validation (TFDV) is that it will automatically generate a schema by inferring types, categories, and ranges from the training data.</p> <p>Here's an excerpt from a schema protobuf:</p> <pre><code>...\nfeature {\n  name: \"age\"\n  value_count {\n    min: 1\n    max: 1\n  }\n  type: FLOAT\n  presence {\n    min_fraction: 1\n    min_count: 1\n  }\n}\nfeature {\n  name: \"capital-gain\"\n  value_count {\n    min: 1\n    max: 1\n  }\n  type: FLOAT\n  presence {\n    min_fraction: 1\n    min_count: 1\n  }\n}\n...\n</code></pre> <p>The following components use the schema:</p> <ul> <li>TensorFlow Data Validation</li> <li>TensorFlow Transform</li> </ul> <p>In a typical TFX pipeline TensorFlow Data Validation generates a schema, which is consumed by the other components.</p> <p>Note</p> <p>The auto-generated schema is best-effort and only tries to infer basic properties of the data. It is expected that developers review and modify it as needed.</p>"},{"location":"guide/#developing-with-tfx","title":"Developing with TFX","text":"<p>TFX provides a powerful platform for every phase of a machine learning project, from research, experimentation, and development on your local machine, through deployment. In order to avoid code duplication and eliminate the potential for training/serving skew it is strongly recommended to implement your TFX pipeline for both model training and deployment of trained models, and use Transform components which leverage the TensorFlow Transform library for both training and inference. By doing so you will use the same preprocessing and analysis code consistently, and avoid differences between data used for training and data fed to your trained models in production, as well as benefitting from writing that code once.</p>"},{"location":"guide/#data-exploration-visualization-and-cleaning","title":"Data Exploration, Visualization, and Cleaning","text":"<p>TFX pipelines typically begin with an ExampleGen component, which accepts input data and formats it as tf.Examples.  Often this is done after the data has been split into training and evaluation datasets so that there are actually two copies of ExampleGen components, one each for training and evaluation. This is typically followed by a StatisticsGen component and a SchemaGen component, which will examine your data and infer a data schema and statistics.  The schema and statistics will be consumed by an ExampleValidator component, which will look for anomalies, missing values, and incorrect data types in your data.  All of these components leverage the capabilities of the TensorFlow Data Validation library.</p> <p>TensorFlow Data Validation (TFDV) is a valuable tool when doing initial exploration, visualization, and cleaning of your dataset.  TFDV examines your data and infers the data types, categories, and ranges, and then automatically helps identify anomalies and missing values.  It also provides visualization tools that can help you examine and understand your dataset. After your pipeline completes you can read metadata from MLMD and use the visualization tools of TFDV in a Jupyter notebook to analyze your data.</p> <p>Following your initial model training and deployment, TFDV can be used to monitor new data from inference requests to your deployed models, and look for anomalies and/or drift.  This is especially useful for time series data that changes over time as a result of trend or seasonality, and can help inform when there are data problems or when models need to be retrained on new data.</p>"},{"location":"guide/#data-visualization","title":"Data Visualization","text":"<p>After you have completed your first run of your data through the section of your pipeline that uses TFDV (typically StatisticsGen, SchemaGen, and ExampleValidator) you can visualize the results in a Jupyter style notebook.  For additional runs you can compare these results as you make adjustments, until your data is optimal for your model and application.</p> <p>You will first query ML Metadata (MLMD) to locate the results of these executions of these components, and then use the visualization support API in TFDV to create the visualizations in your notebook. This includes tfdv.load_statistics() and tfdv.visualize_statistics() Using this visualization you can better understand the characteristics of your dataset, and if necessary modify as required.</p>"},{"location":"guide/#developing-and-training-models","title":"Developing and Training Models","text":"<p>A typical TFX pipeline will include a Transform component, which will perform feature engineering by leveraging the capabilities of the TensorFlow Transform (TFT) library. A Transform component consumes the schema created by a SchemaGen component, and applies data transformations to create, combine, and transform the features that will be used to train your model. Cleanup of missing values and conversion of types should also be done in the Transform component if there is ever a possibility that these will also be present in data sent for inference requests. There are some important considerations when designing TensorFlow code for training in TFX.</p> <p></p> <p>The result of a Transform component is a SavedModel which will be imported and used in your modeling code in TensorFlow, during a Trainer component.  This SavedModel includes all of the data engineering transformations that were created in the Transform component, so that the identical transforms are performed using the exact same code during both training and inference.  Using the modeling code, including the SavedModel from the Transform component, you can consume your training and evaluation data and train your model.</p> <p>An optional Tuner component can be added before Trainer to tune the hyperparameters (e.g., number of layers) for the model. With the given model and hyperparameters' search space, tuning algorithm will find the best hyperparameters based on the objective.</p>"},{"location":"guide/#analyzing-and-understanding-model-performance","title":"Analyzing and Understanding Model Performance","text":"<p>Following initial model development and training it's important to analyze and really understand your model's performance.  A typical TFX pipeline will include an Evaluator component, which leverages the capabilities of the TensorFlow Model Analysis (TFMA) library, which provides a power toolset for this phase of development.  An Evaluator component consumes the model that you exported above, and allows you to specify a list of <code>tfma.SlicingSpec</code> that you can use when visualizing and analyzing your model's performance. Each <code>SlicingSpec</code> defines a slice of your training data that you want to examine, such as particular categories for categorical features, or particular ranges for numerical features.</p> <p>For example, this would be important for trying to understand your model's performance for different segments of your customers, which could be segmented by annual purchases, geographical data, age group, or gender.  This can be especially important for datasets with long tails, where the performance of a dominant group may mask unacceptable performance for important, yet smaller groups.  For example, your model may perform well for average employees but fail miserably for executive staff, and it might be important to you to know that.</p>"},{"location":"guide/#model-analysis-and-visualization","title":"Model Analysis and Visualization","text":"<p>After you have completed your first run of your data through training your model and running the Evaluator component (which leverages TFMA) on the training results, you can visualize the results in a Jupyter style notebook.  For additional runs you can compare these results as you make adjustments, until your results are optimal for your model and application.</p> <p>You will first query ML Metadata (MLMD) to locate the results of these executions of these components, and then use the visualization support API in TFMA to create the visualizations in your notebook.  This includes tfma.load_eval_results and tfma.view.render_slicing_metrics Using this visualization you can better understand the characteristics of your model, and if necessary modify as required.</p>"},{"location":"guide/#validating-model-performance","title":"Validating Model Performance","text":"<p>As part of analyzing a model's performance you might want to validate the performance against a baseline (such as the currently serving model). Model validation is performed by passing both a candidate and baseline model to the Evaluator component. The Evaluator computes metrics (e.g. AUC, loss) for both the candidate and baseline along with a corresponding set of diff metrics. Thresholds may then be applied and used to gate pushing your models to production.</p>"},{"location":"guide/#validating-that-a-model-can-be-served","title":"Validating That A Model Can Be Served","text":"<p>Before deploying the trained model, you might want to validate whether the model is really servable in the serving infrastructure. This is especially important in production environments to ensure that the newly published model does not prevent the system from serving predictions. The InfraValidator component will make a canary deployment of your model in a sandboxed environment, and optionally send real requests to check that your model works correctly.</p>"},{"location":"guide/#deployment-targets","title":"Deployment Targets","text":"<p>Once you have developed and trained a model that you're happy with, it's now time to deploy it to one or more deployment target(s) where it will receive inference requests.  TFX supports deployment to three classes of deployment targets.  Trained models which have been exported as SavedModels can be deployed to any or all of these deployment targets.</p> <p></p>"},{"location":"guide/#inference-tensorflow-serving","title":"Inference: TensorFlow Serving","text":"<p>TensorFlow Serving (TFS) is a flexible, high-performance serving system for machine learning models, designed for production environments. It consumes a SavedModel and will accept inference requests over either REST or gRPC interfaces. It runs as a set of processes on one or more network servers, using one of several advanced architectures to handle synchronization and distributed computation. See the TFS documentation for more information on developing and deploying TFS solutions.</p> <p>In a typical pipeline, a SavedModel which has been trained in a Trainer component would first be infra-validated in an InfraValidator component. InfraValidator launches a canary TFS model server to actually serve the SavedModel. If validation has passed, a Pusher component will finally deploy the SavedModel to your TFS infrastructure. This includes handling multiple versions and model updates.</p>"},{"location":"guide/#inference-in-native-mobile-and-iot-applications-tensorflow-lite","title":"Inference in Native Mobile and IoT Applications: TensorFlow Lite","text":"<p>TensorFlow Lite is a suite of tools which is dedicated to help developers use their trained TensorFlow Models in native mobile and IoT applications.  It consumes the same SavedModels as TensorFlow Serving, and applies optimizations such as quantization and pruning to optimize the size and performance of the resulting models for the challenges of running on mobile and IoT devices.  See the TensorFlow Lite documentation for more information on using TensorFlow Lite.</p>"},{"location":"guide/#inference-in-javascript-tensorflow-js","title":"Inference in JavaScript: TensorFlow JS","text":"<p>TensorFlow JS is a JavaScript library for training and deploying ML models in the browser and on Node.js.  It consumes the same SavedModels as TensorFlow Serving and TensorFlow Lite, and converts them to the TensorFlow.js Web format.  See the TensorFlow JS documentation for more details on using TensorFlow JS.</p>"},{"location":"guide/#creating-a-tfx-pipeline-with-airflow","title":"Creating a TFX Pipeline With Airflow","text":"<p>Check airflow workshop for details</p>"},{"location":"guide/#creating-a-tfx-pipeline-with-kubeflow","title":"Creating a TFX Pipeline With Kubeflow","text":""},{"location":"guide/#setup","title":"Setup","text":"<p>Kubeflow requires a Kubernetes cluster to run the pipelines at scale. See the Kubeflow deployment guideline that guide through the options for deploying the Kubeflow cluster.</p>"},{"location":"guide/#configure-and-run-tfx-pipeline","title":"Configure and run TFX pipeline","text":"<p>Please follow the TFX on Cloud AI Platform Pipeline tutorial to run the TFX example pipeline on Kubeflow. TFX components have been containerized to compose the Kubeflow pipeline and the sample illustrates the ability to configure the pipeline to read large public dataset and execute training and data processing steps at scale in the cloud.</p>"},{"location":"guide/#command-line-interface-for-pipeline-actions","title":"Command line interface for pipeline actions","text":"<p>TFX provides a unified CLI which helps the perform full range of pipeline actions such as create, update, run, list, and delete pipelines on various orchestrators including Apache Airflow, Apache Beam, and Kubeflow. For details, please follow these instructions.</p>"},{"location":"guide/addons/","title":"Community-developed components, examples, and tools for TFX","text":"<p>Developers helping developers. TFX-Addons is a collection of community projects to build new components, examples, libraries, and tools for TFX. The projects are organized under the auspices of the special interest group, SIG TFX-Addons.</p> <p>Join the community and share your work with the world!</p> <p>TFX-Addons is available on PyPI for all OS. To install the latest version, run:</p> <pre><code>pip install tfx-addons\n</code></pre> <p>You can then use TFX-Addons like this:</p> <pre><code>from tfx import v1 as tfx\nimport tfx_addons as tfxa\n\n# Then you can easily load projects tfxa.{project_name}. For example:\ntfxa.feast_examplegen.FeastExampleGen(...)\n</code></pre> <ul> <li> <p>Feast ExampleGen Component</p> <p>An ExampleGen component for ingesting datasets from a Feast Feature Store.</p> <p> Feast ExampleGen</p> </li> <li> <p>Feature Selection Component</p> <p>Perform feature selection using various algorithms with this TFX component.</p> <p> Feature Selection</p> </li> <li> <p>Firebase Publisher Component</p> <p>A TFX component to publish/update ML models to Firebase ML.</p> <p> Firebase Publisher</p> </li> <li> <p>Hugging Face Pusher Component</p> <p>Hugging Face Model Hub. Optionally pushes the application to the Hugging Face Spaces Hub.</p> <p> Hugging Face Pusher</p> </li> <li> <p>Message Exit Handler Component</p> <p>Handle the completion or failure of a pipeline by notifying users, including any error messages.</p> <p> Message Exit Handler</p> </li> <li> <p>MLMD Client Library</p> <p>Client library to inspect content in ML Metadata populated by TFX pipelines.</p> <p> MLMD Client</p> </li> <li> <p>Model Card Generator</p> <p>The ModelCardGenerator takes dataset statistics, model evaluation, and a pushed model to automatically populate parts of a model card.</p> <p> Model Card Generator</p> </li> <li> <p>Pandas Transform Component</p> <p>Use Pandas dataframes instead of the standard Transform component for your feature engineering. Processing is distributed using Apache Beam for scalability.</p> <p> Pandas Transform</p> </li> <li> <p>Sampling Component</p> <p>A TFX component to sample data from examples, using probabilistic estimation.</p> <p> Sampling</p> </li> <li> <p>Schema Curation Component</p> <p>Apply user code to a schema produced by the SchemaGen component, and curate it based on domain knowledge.</p> <p> Schema Curation</p> </li> <li> <p>XGBoost Evaluator Component</p> <p>Evaluate XGBoost models by extending the standard Evaluator component.</p> <p> XGBoost Evaluator</p> </li> </ul>"},{"location":"guide/airflow/","title":"Orchestrating TFX Pipelines","text":""},{"location":"guide/airflow/#apache-airflow","title":"Apache Airflow","text":"<p>Apache Airflow is a platform to programmatically author, schedule and monitor workflows. TFX uses Airflow to author workflows as directed acyclic graphs (DAGs) of tasks. The Airflow scheduler executes tasks on an array of workers while following the specified dependencies. Rich command line utilities make performing complex surgeries on DAGs a snap. The rich user interface makes it easy to visualize pipelines running in production, monitor progress, and troubleshoot issues when needed. When workflows are defined as code, they become more maintainable, versionable, testable, and collaborative.</p> <p>See the Apache Airflow for details on installing and using Apache Airflow.</p>"},{"location":"guide/beam/","title":"Apache Beam and TFX","text":"<p>Apache Beam provides a framework for running batch and streaming data processing jobs that run on a variety of execution engines. Several of the TFX libraries use Beam for running tasks, which enables a high degree of scalability across compute clusters.  Beam includes support for a variety of execution engines or \"runners\", including a direct runner which runs on a single compute node and is very useful for development, testing, or small deployments.  Beam provides an abstraction layer which enables TFX to run on any supported runner without code modifications.  TFX uses the Beam Python API, so it is limited to the runners that are supported by the Python API.</p>"},{"location":"guide/beam/#deployment-and-scalability","title":"Deployment and Scalability","text":"<p>As workload requirements increase Beam can scale to very large deployments across large compute clusters. This is limited only by the scalability of the underlying runner.  Runners in large deployments will typically be deployed to a container orchestration system such as Kubernetes or Apache Mesos for automating application deployment, scaling, and management.</p> <p>See the Apache Beam documentation for more information on Apache Beam.</p> <p>For Google Cloud users, Dataflow is the recommended runner, which provides a serverless and cost-effective platform through autoscaling of resources, dynamic work rebalancing, deep integration with other Google Cloud services, built-in security, and monitoring.</p>"},{"location":"guide/beam/#custom-python-code-and-dependencies","title":"Custom Python Code and Dependencies","text":"<p>One notable complexity of using Beam in a TFX pipeline is handling custom code and/or the dependencies needed from additional Python modules. Here are some examples of when this might be an issue:</p> <ul> <li>preprocessing_fn needs to refer to the user's own Python module</li> <li>a custom extractor for the Evaluator component</li> <li>custom modules which are sub-classed from a TFX component</li> </ul> <p>TFX relies on Beam's support for Managing Python Pipeline Dependencies to handle Python dependencies. Currently there are two ways to manage this:</p> <ol> <li>Providing Python Code and Dependencies as Source Package</li> <li>[Dataflow only] Using a Container Image as Worker</li> </ol> <p>These are discussed next.</p>"},{"location":"guide/beam/#providing-python-code-and-dependencies-as-a-source-package","title":"Providing Python Code and Dependencies as a Source Package","text":"<p>This is recommended for users who:</p> <ol> <li>Are familiar with Python packaging and</li> <li>Only use Python source code (i.e., no C modules or shared libraries).</li> </ol> <p>Please follow one of the paths in Managing Python Pipeline Dependencies to provide this using one of the following beam_pipeline_args:</p> <ul> <li><code>--setup_file</code></li> <li><code>--extra_package</code></li> <li><code>--requirements_file</code></li> </ul> <p>Notice: In any of above cases, please make sure that the same version of <code>tfx</code> is listed as a dependency.</p>"},{"location":"guide/beam/#dataflow-only-using-a-container-image-for-a-worker","title":"[Dataflow only] Using a Container Image for a Worker","text":"<p>TFX 0.26.0 and above has experimental support for using custom container image for Dataflow workers.</p> <p>In order to use this, you have to:</p> <ul> <li>Build a Docker image which has both <code>tfx</code> and the users' custom code and     dependencies pre-installed.<ul> <li>For users who (1) use <code>tfx&gt;=0.26</code> and (2) uses python 3.7 to develop their pipelines,     the easiest way to do this is extending the corresponding version of the official     <code>tensorflow/tfx</code> image:</li> </ul> </li> </ul> <pre><code># You can use a build-arg to dynamically pass in the\n# version of TFX being used to your Dockerfile.\n\nARG TFX_VERSION\nFROM tensorflow/tfx:${TFX_VERSION}\n# COPY your code and dependencies in\n</code></pre> <ul> <li>Push the image built to a container image registry which is accessible by     the project used by Dataflow.<ul> <li>Google Cloud users can consider using     Cloud Build     which nicely automates above steps.</li> </ul> </li> <li>Provide following <code>beam_pipeline_args</code>:</li> </ul> <pre><code>beam_pipeline_args.extend([\n    '--runner=DataflowRunner',\n    '--project={project-id}',\n    '--worker_harness_container_image={image-ref}',\n    '--experiments=use_runner_v2',\n])\n</code></pre> <p>TODO(b/171733562): Remove use_runner_v2 once it is default for Dataflow.</p> <p>TODO(b/179738639): Create documentation for how to test custom container locally after https://issues.apache.org/jira/browse/BEAM-5440.</p>"},{"location":"guide/beam/#beam-pipeline-arguments","title":"Beam Pipeline Arguments","text":"<p>Several TFX components rely on Beam for distributed data processing. They are configured with <code>beam_pipeline_args</code>, which is specified during during pipeline creation:</p> <pre><code>my_pipeline = Pipeline(\n    ...,\n    beam_pipeline_args=[...])\n</code></pre> <p>TFX 0.30 and above adds an interface, <code>with_beam_pipeline_args</code>, for extending the pipeline level beam args per component:</p> <pre><code>example_gen = CsvExampleGen(input_base=data_root).with_beam_pipeline_args([...])\n</code></pre>"},{"location":"guide/build_local_pipeline/","title":"Building a TFX Pipeline Locally","text":"<p>TFX makes it easier to orchestrate your machine learning (ML) workflow as a pipeline, in order to:</p> <ul> <li>Automate your ML process, which lets you regularly retrain, evaluate, and     deploy your model.</li> <li>Create ML pipelines which include deep analysis of model performance and     validation of newly trained models to ensure performance and reliability.</li> <li>Monitor training data for anomalies and eliminate training-serving skew</li> <li>Increase the velocity of experimentation by running a pipeline with     different sets of hyperparameters.</li> </ul> <p>A typical pipeline development process begins on a local machine, with data analysis and component setup, before being deployed into production. This guide describes two ways to build a pipeline locally.</p> <ul> <li>Customize a TFX pipeline template to fit the needs of your ML workflow. TFX     pipeline templates are prebuilt workflows that demonstrate best practices     using the TFX standard components.</li> <li>Build a pipeline using TFX. In this use case, you define a pipeline without     starting from a template.</li> </ul> <p>As you are developing your pipeline, you can run it with <code>LocalDagRunner</code>. Then, once the pipeline components have been well defined and tested, you would use a production-grade orchestrator such as Kubeflow or Airflow.</p>"},{"location":"guide/build_local_pipeline/#before-you-begin","title":"Before you begin","text":"<p>TFX is a Python package, so you will need to set up a Python development environment, such as a virtual environment or a Docker container. Then:</p> <pre><code>pip install tfx\n</code></pre> <p>If you are new to TFX pipelines, learn more about the core concepts for TFX pipelines before continuing.</p>"},{"location":"guide/build_local_pipeline/#build-a-pipeline-using-a-template","title":"Build a pipeline using a template","text":"<p>TFX Pipeline Templates make it easier to get started with pipeline development by providing a prebuilt set of pipeline definitions that you can customize for your use case.</p> <p>The following sections describe how to create a copy of a template and customize it to meet your needs.</p>"},{"location":"guide/build_local_pipeline/#create-a-copy-of-the-pipeline-template","title":"Create a copy of the pipeline template","text":"<ol> <li> <p>See list of the available TFX pipeline templates:</p> <pre><code>tfx template list\n</code></pre> </li> <li> <p>Select a template from the list</p> <pre><code>tfx template copy --model=template --pipeline_name=pipeline-name \\\n--destination_path=destination-path\n</code></pre> <p>Replace the following:</p> <ul> <li><code>template</code>: The name of the template you want to copy.</li> <li><code>pipeline-name</code>: The name of the pipeline to create.</li> <li><code>destination-path</code>: The path to copy the template into.</li> </ul> <p>Learn more about the <code>tfx template copy</code> command.</p> </li> <li> <p>A copy of the pipeline template has been created at the path you specified.</p> </li> </ol> <p>Note: The rest of this guide assumes you selected the <code>penguin</code> template.</p>"},{"location":"guide/build_local_pipeline/#explore-the-pipeline-template","title":"Explore the pipeline template","text":"<p>This section provides an overview of the scaffolding created by a template.</p> <ol> <li> <p>Explore the directories and files that were copied to your pipeline's root     directory</p> <ul> <li>A pipeline directory with<ul> <li><code>pipeline.py</code> - defines the pipeline, and lists which components are     being used</li> <li><code>configs.py</code> - hold configuration details such as where the data is     coming from or which orchestrator is being used</li> </ul> </li> <li>A data directory<ul> <li>This typically contains a <code>data.csv</code> file, which is the default     source for <code>ExampleGen</code>. You can change the data source in     <code>configs.py</code>.</li> </ul> </li> <li> <p>A models directory with preprocessing code and model implementations</p> </li> <li> <p>The template copies DAG runners for local environment and Kubeflow.</p> </li> <li> <p>Some templates also include Python Notebooks so that you can explore     your data and artifacts with Machine Learning MetaData.</p> </li> </ul> </li> <li> <p>Run the following commands in your pipeline directory:</p> <pre><code>tfx pipeline create --pipeline_path local_runner.py\n</code></pre> <pre><code>tfx run create --pipeline_name &lt;var&gt;pipeline_name&lt;/var&gt;\n</code></pre> <p>The command creates a pipeline run using <code>LocalDagRunner</code>, which adds the following directories to your pipeline:</p> <ul> <li>A tfx_metadata directory which contains the ML Metadata store used     locally.</li> <li>A tfx_pipeline_output directory which contains the pipeline's file     outputs.</li> </ul> <p>Note: <code>LocalDagRunner</code> is one several orchestrators which are supported in TFX. It is specially suitable for running pipelines locally for faster iterations, possibly with smaller datasets. <code>LocalDagRunner</code> may not be suitable for production use since it runs on a single machine, which is more vulnerable to work being lost if the system becomes unavailable. TFX also supports orchestrators such as Apache Beam, Apache Airflow, and Kubeflow Pipeline. If you're using TFX with a different orchestrator, use the appropriate DAG runner for that orchestrator.</p> <p>Note: As of this writing, <code>LocalDagRunner</code> is used in the <code>penguin</code> template, while the <code>taxi</code> template uses Apache Beam. The config files for the <code>taxi</code> template are set up to use Beam, and the CLI command is the same.</p> </li> <li> <p>Open your pipeline's <code>pipeline/configs.py</code> file and review the contents.     This script defines the configuration options used by the pipeline and the     component functions. This is where you would specify things like the     location of the datasource or the number of training steps in a run.</p> </li> <li> <p>Open your pipeline's <code>pipeline/pipeline.py</code> file and review the contents.     This script creates the TFX pipeline. Initially, the pipeline contains only     an <code>ExampleGen</code> component.</p> <ul> <li>Follow the instructions in the TODO comments in <code>pipeline.py</code> to add     more steps to the pipeline.</li> </ul> </li> <li> <p>Open <code>local_runner.py</code> file and review the contents. This script creates a     pipeline run and specifies the run's parameters, such as the <code>data_path</code>     and <code>preprocessing_fn</code>.</p> </li> <li> <p>You have reviewed the scaffolding created by the template and created a     pipeline run using <code>LocalDagRunner</code>. Next, customize the template to fit     your requirements.</p> </li> </ol>"},{"location":"guide/build_local_pipeline/#customize-your-pipeline","title":"Customize your pipeline","text":"<p>This section provides an overview of how to get started customizing your template.</p> <ol> <li> <p>Design your pipeline. The scaffolding that a template provides helps you     implement a pipeline for tabular data using the TFX standard components. If     you are moving an existing ML workflow into a pipeline, you may need to     revise your code to make full use of     TFX standard components. You may also need     to create custom components that     implement features which are unique to your workflow or that are not yet     supported by TFX standard components.</p> </li> <li> <p>Once you have designed your pipeline, iteratively customize the pipeline     using the following process. Start from the component that ingests data into     your pipeline, which is usually the <code>ExampleGen</code> component.</p> <ol> <li> <p>Customize the pipeline or a component to fit your use case. These     customizations may include changes like:</p> <ul> <li>Changing pipeline parameters.</li> <li>Adding components to the pipeline or removing them.</li> <li>Replacing the data input source. This data source can either be a     file or queries into services such as BigQuery.</li> <li>Changing a component's configuration in the pipeline.</li> <li>Changing a component's customization function.</li> </ul> </li> <li> <p>Run the component locally using the <code>local_runner.py</code> script, or another     appropriate DAG runner if you are using a different orchestrator. If the     script fails, debug the failure and retry running the script.</p> </li> <li> <p>Once this customization is working, move on to the next customization.</p> </li> </ol> </li> <li> <p>Working iteratively, you can customize each step in the template workflow to     meet your needs.</p> </li> </ol>"},{"location":"guide/build_local_pipeline/#build-a-custom-pipeline","title":"Build a custom pipeline","text":"<p>Use the following instructions to learn more about building a custom pipeline without using a template.</p> <ol> <li> <p>Design your pipeline. The TFX standard components provide proven     functionality to help you implement a complete ML workflow. If you are     moving an existing ML workflow into a pipeline, you may need to revise your     code to make full use of TFX standard components. You may also need to     create custom components that implement     features such as data augmentation.</p> <ul> <li>Learn more about     standard TFX components.</li> <li>Learn more about custom components.</li> </ul> </li> <li> <p>Create a script file to define your pipeline using the following example.     This guide refers to this file as <code>my_pipeline.py</code>.</p> <pre><code>import os\nfrom typing import Optional, Text, List\nfrom absl import logging\nfrom ml_metadata.proto import metadata_store_pb2\nimport tfx.v1 as tfx\n\nPIPELINE_NAME = 'my_pipeline'\nPIPELINE_ROOT = os.path.join('.', 'my_pipeline_output')\nMETADATA_PATH = os.path.join('.', 'tfx_metadata', PIPELINE_NAME, 'metadata.db')\nENABLE_CACHE = True\n\ndef create_pipeline(\n  pipeline_name: Text,\n  pipeline_root:Text,\n  enable_cache: bool,\n  metadata_connection_config: Optional[\n    metadata_store_pb2.ConnectionConfig] = None,\n  beam_pipeline_args: Optional[List[Text]] = None\n):\n  components = []\n\n  return tfx.dsl.Pipeline(\n        pipeline_name=pipeline_name,\n        pipeline_root=pipeline_root,\n        components=components,\n        enable_cache=enable_cache,\n        metadata_connection_config=metadata_connection_config,\n        beam_pipeline_args=beam_pipeline_args, &lt;!-- needed? --&gt;\n    )\n\ndef run_pipeline():\n  my_pipeline = create_pipeline(\n      pipeline_name=PIPELINE_NAME,\n      pipeline_root=PIPELINE_ROOT,\n      enable_cache=ENABLE_CACHE,\n      metadata_connection_config=tfx.orchestration.metadata.sqlite_metadata_connection_config(METADATA_PATH)\n      )\n\n  tfx.orchestration.LocalDagRunner().run(my_pipeline)\n\nif __name__ == '__main__':\n  logging.set_verbosity(logging.INFO)\n  run_pipeline()\n</code></pre> <p>In the coming steps, you define your pipeline in <code>create_pipeline</code> and run your pipeline locally using the local runner.</p> <p>Iteratively build your pipeline using the following process.</p> <ol> <li> <p>Customize the pipeline or a component to fit your use case. These     customizations may include changes like:</p> <ul> <li>Changing pipeline parameters.</li> <li>Adding components to the pipeline or removing them.</li> <li>Replacing a data input file.</li> <li>Changing a component's configuration in the pipeline.</li> <li>Changing a component's customization function.</li> </ul> </li> <li> <p>Run the component locally using the local runner or by running the     script directly. If the script fails, debug the failure and retry     running the script.</p> </li> <li> <p>Once this customization is working, move on to the next customization.</p> </li> </ol> <p>Start from the first node in your pipeline's workflow, typically the first node ingests data into your pipeline.</p> </li> <li> <p>Add the first node in your workflow to your pipeline. In this example, the     pipeline uses the <code>ExampleGen</code> standard component to load a CSV from a     directory at <code>./data</code>.</p> <pre><code>from tfx.components import CsvExampleGen\n\nDATA_PATH = os.path.join('.', 'data')\n\ndef create_pipeline(\n  pipeline_name: Text,\n  pipeline_root:Text,\n  data_path: Text,\n  enable_cache: bool,\n  metadata_connection_config: Optional[\n    metadata_store_pb2.ConnectionConfig] = None,\n  beam_pipeline_args: Optional[List[Text]] = None\n):\n  components = []\n\n  example_gen = tfx.components.CsvExampleGen(input_base=data_path)\n  components.append(example_gen)\n\n  return tfx.dsl.Pipeline(\n        pipeline_name=pipeline_name,\n        pipeline_root=pipeline_root,\n        components=components,\n        enable_cache=enable_cache,\n        metadata_connection_config=metadata_connection_config,\n        beam_pipeline_args=beam_pipeline_args, &lt;!-- needed? --&gt;\n    )\n\ndef run_pipeline():\n  my_pipeline = create_pipeline(\n    pipeline_name=PIPELINE_NAME,\n    pipeline_root=PIPELINE_ROOT,\n    data_path=DATA_PATH,\n    enable_cache=ENABLE_CACHE,\n    metadata_connection_config=tfx.orchestration.metadata.sqlite_metadata_connection_config(METADATA_PATH)\n    )\n\n  tfx.orchestration.LocalDagRunner().run(my_pipeline)\n</code></pre> <p><code>CsvExampleGen</code> creates serialized example records using the data in the CSV at the specified data path. By setting the <code>CsvExampleGen</code> component's <code>input_base</code> parameter with the data root.</p> </li> <li> <p>Create a <code>data</code> directory in the same directory as <code>my_pipeline.py</code>. Add a     small CSV file to the <code>data</code> directory.</p> </li> <li> <p>Use the following command to run your <code>my_pipeline.py</code> script.</p> <pre><code>python my_pipeline.py\n</code></pre> <p>The result should be something like the following:</p> <pre><code>INFO:absl:Component CsvExampleGen depends on [].\nINFO:absl:Component CsvExampleGen is scheduled.\nINFO:absl:Component CsvExampleGen is running.\nINFO:absl:Running driver for CsvExampleGen\nINFO:absl:MetadataStore with DB connection initialized\nINFO:absl:Running executor for CsvExampleGen\nINFO:absl:Generating examples.\nINFO:absl:Using 1 process(es) for Local pipeline execution.\nINFO:absl:Processing input csv data ./data/* to TFExample.\nWARNING:root:Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\nINFO:absl:Examples generated.\nINFO:absl:Running publisher for CsvExampleGen\nINFO:absl:MetadataStore with DB connection initialized\nINFO:absl:Component CsvExampleGen is finished.\n</code></pre> </li> <li> <p>Continue to iteratively add components to your pipeline.</p> </li> </ol>"},{"location":"guide/build_tfx_pipeline/","title":"Building TFX pipelines","text":"<p>Note</p> <p>For a conceptual view of TFX Pipelines, see Understanding TFX Pipelines.</p> <p>Note</p> <p>Want to build your first pipeline before you dive into the details? Get started building a pipeline using a template.</p>"},{"location":"guide/build_tfx_pipeline/#using-the-pipeline-class","title":"Using the <code>Pipeline</code> class","text":"<p>TFX pipelines are defined using the <code>Pipeline</code> class. The following example demonstrates how to use the <code>Pipeline</code> class.</p> <pre><code>pipeline.Pipeline(\n    pipeline_name=pipeline-name,\n    pipeline_root=pipeline-root,\n    components=components,\n    enable_cache=enable-cache,\n    metadata_connection_config=metadata-connection-config,\n)\n</code></pre> <p>Replace the following:</p> <ul> <li> <p><code>pipeline-name</code>: The name of this pipeline. The pipeline name must     be unique.</p> <p>TFX uses the pipeline name when querying ML Metadata for component input artifacts. Reusing a pipeline name may result in unexpected behaviors.</p> </li> <li> <p><code>pipeline-root</code>: The root path of this pipeline's outputs. The root     path must be the full path to a directory that your orchestrator has read     and write access to. At runtime, TFX uses the pipeline root to generate     output paths for component artifacts. This directory can be local, or on a     supported distributed file system, such as Google Cloud Storage or HDFS.</p> </li> <li> <p><code>components</code>: A list of component instances that make up this     pipeline's workflow.</p> </li> <li> <p><code>enable-cache</code>: (Optional.) A boolean value that indicates if this     pipeline uses caching to speed up pipeline execution.</p> </li> <li> <p><code>metadata-connection-config</code>: (Optional.) A connection     configuration for ML Metadata.</p> </li> </ul>"},{"location":"guide/build_tfx_pipeline/#defining-the-component-execution-graph","title":"Defining the component execution graph","text":"<p>Component instances produce artifacts as outputs and typically depend on artifacts produced by upstream component instances as inputs. The execution sequence for component instances is determined by creating a directed acyclic graph (DAG) of the artifact dependencies.</p> <p>For instance, the <code>ExampleGen</code> standard component can ingest data from a CSV file and output serialized example records. The <code>StatisticsGen</code> standard component accepts these example records as input and produces dataset statistics. In this example, the instance of <code>StatisticsGen</code> must follow <code>ExampleGen</code> because <code>SchemaGen</code> depends on the output of <code>ExampleGen</code>.</p>"},{"location":"guide/build_tfx_pipeline/#task-based-dependencies","title":"Task-based dependencies","text":"<p>Note</p> <p>Using task-based dependencies is typically not recommended. Defining the execution graph with artifact dependencies lets you take advantage of the automatic artifact lineage tracking and caching features of TFX.</p> <p>You can also define task-based dependencies using your component's <code>add_upstream_node</code> and <code>add_downstream_node</code> methods. <code>add_upstream_node</code> lets you specify that the current component must be executed after the specified component. <code>add_downstream_node</code> lets you specify that the current component must be executed before the specified component.</p>"},{"location":"guide/build_tfx_pipeline/#pipeline-templates","title":"Pipeline templates","text":"<p>The easiest way to get a pipeline set up quickly, and to see how all the pieces fit together, is to use a template. Using templates is covered in Building a TFX Pipeline Locally.</p>"},{"location":"guide/build_tfx_pipeline/#caching","title":"Caching","text":"<p>TFX pipeline caching lets your pipeline skip over components that have been executed with the same set of inputs in a previous pipeline run. If caching is enabled, the pipeline attempts to match the signature of each component, the component and set of inputs, to one of this pipeline's previous component executions. If there is a match, the pipeline uses the component outputs from the previous run. If there is not a match, the component is executed.</p> <p>Do not use caching if your pipeline uses non-deterministic components. For example, if you create a component to create a random number for your pipeline, enabling the cache causes this component to execute once. In this example, subsequent runs use the first run's random number instead of generating a random number.</p>"},{"location":"guide/bulkinferrer/","title":"The BulkInferrer TFX Pipeline Component","text":"<p>The BulkInferrer TFX component performs batch inference on unlabeled data. The generated InferenceResult(<code>tensorflow_serving.apis.prediction_log_pb2.PredictionLog</code>) contains the original features and the prediction results.</p> <p>BulkInferrer consumes:</p> <ul> <li>A trained model in     SavedModel format.</li> <li>Unlabelled tf.Examples that contain features.</li> <li>(Optional) Validation result from     Evaluator component.</li> </ul> <p>BulkInferrer emits:</p> <ul> <li>InferenceResult</li> </ul>"},{"location":"guide/bulkinferrer/#using-the-bulkinferrer-component","title":"Using the BulkInferrer Component","text":"<p>A BulkInferrer TFX component is used to perform batch inference on unlabeled tf.Examples. It is typically deployed after an Evaluator component to perform inference with a validated model, or after a Trainer component to directly perform inference on exported model.</p> <p>It currently performs in-memory model inference and remote inference. Remote inference requires the model to be hosted on Cloud AI Platform.</p> <p>Typical code looks like this:</p> <pre><code>bulk_inferrer = BulkInferrer(\n    examples=examples_gen.outputs['examples'],\n    model=trainer.outputs['model'],\n    model_blessing=evaluator.outputs['blessing'],\n    data_spec=bulk_inferrer_pb2.DataSpec(),\n    model_spec=bulk_inferrer_pb2.ModelSpec()\n)\n</code></pre> <p>More details are available in the BulkInferrer API reference.</p>"},{"location":"guide/cli/","title":"Using the TFX Command-line Interface","text":"<p>The TFX command-line interface (CLI) performs a full range of pipeline actions using pipeline orchestrators, such as Kubeflow Pipelines, Vertex Pipelines. Local orchestrator can be also used for faster development or debugging. Apache Beam and Apache airflow is supported as experimental features. For example, you can use the CLI to:</p> <ul> <li>Create, update, and delete pipelines.</li> <li>Run a pipeline and monitor the run on various orchestrators.</li> <li>List pipelines and pipeline runs.</li> </ul> <p>Note</p> <p>The TFX CLI doesn't currently provide compatibility guarantees. The CLI interface might change as new versions are released.</p>"},{"location":"guide/cli/#about-the-tfx-cli","title":"About the TFX CLI","text":"<p>The TFX CLI is installed as a part of the TFX package. All CLI commands follow the structure below:</p> <pre><code>tfx &lt;command-group&gt; &lt;command&gt; &lt;flags&gt;\n</code></pre> <p>The following command-group options are currently supported:</p> <ul> <li><code>tfx pipeline</code> - Create and manage TFX pipelines.</li> <li><code>tfx run</code> - Create and manage runs of TFX pipelines on various     orchestration platforms.</li> <li><code>tfx template</code> - Experimental commands for     listing and copying TFX pipeline templates.</li> </ul> <p>Each command group provides a set of commands. Follow the instructions in the pipeline commands, run commands, and template commands sections to learn more about using these commands.</p> <p>Warning</p> <p>Currently not all commands are supported in every orchestrator. Such commands explicitly mention the engines supported.</p> <p>Flags let you pass arguments into CLI commands. Words in flags are separated with either a hyphen (<code>-</code>) or an underscore (<code>_</code>). For example, the pipeline name flag can be specified as either <code>--pipeline-name</code> or <code>--pipeline_name</code>. This document specifies flags with underscores for brevity. Learn more about flags used in the TFX CLI.</p>"},{"location":"guide/cli/#tfx-pipeline","title":"tfx pipeline","text":"<p>The structure for commands in the <code>tfx pipeline</code> command group is as follows:</p> <pre><code>tfx pipeline command required-flags [optional-flags]\n</code></pre> <p>Use the following sections to learn more about the commands in the <code>tfx pipeline</code> command group.</p>"},{"location":"guide/cli/#create","title":"create","text":"<p>Creates a new pipeline in the given orchestrator.</p> <p>Usage:</p> <pre><code>tfx pipeline create --pipeline_path=pipeline-path [--endpoint=endpoint --engine=engine \\\n--iap_client_id=iap-client-id --namespace=namespace \\\n--build_image --build_base_image=build-base-image]\n</code></pre> --pipeline_path=<code>pipeline-path</code> The path to the pipeline configuration file. --endpoint=<code>endpoint</code> <p>(Optional.) Endpoint of the Kubeflow Pipelines API service. The endpoint of your Kubeflow Pipelines API service is the same as URL of the Kubeflow Pipelines dashboard. Your endpoint value should be something like:</p> <pre><code>https://host-name/pipeline\n</code></pre> <p>If you do not know the endpoint for your Kubeflow Pipelines cluster, contact you cluster administrator.</p> <p>If the <code>--endpoint</code> is not specified, the in-cluster service DNS name is used as the default value. This name works only if the CLI command executes in a pod on the Kubeflow Pipelines cluster, such as a Kubeflow Jupyter notebooks instance.</p> --engine=<code>engine</code> <p>(Optional.) The orchestrator to be used for the pipeline. The value of engine must match on of the following values:</p> <ul> <li>kubeflow: sets engine to Kubeflow</li> <li>local: sets engine to local orchestrator</li> <li>vertex: sets engine to Vertex Pipelines</li> <li>airflow: (experimental) sets engine to Apache Airflow</li> <li>beam: (experimental) sets engine to Apache Beam</li> </ul> <p>If the engine is not set, the engine is auto-detected based on the environment.</p> <p>Important Note</p> <p>The orchestrator required by the DagRunner in the pipeline config file must match the selected or autodetected engine. Engine auto-detection is based on user environment. If Apache Airflow and Kubeflow Pipelines are not installed, then the local orchestrator is used by default.</p> --iap_client_id=<code>iap-client-id</code> (Optional.) Client ID for IAP protected endpoint when using Kubeflow Pipelines. --namespace=<code>namespace</code> (Optional.) Kubernetes namespace to connect to the Kubeflow Pipelines API. If the namespace is not specified, the value defaults to <code>kubeflow</code>. --build_image <p>(Optional.) When the <code>engine</code> is kubeflow or vertex, TFX creates a container image for your pipeline if specified. <code>Dockerfile</code> in the current directory will be used, and TFX will automatically generate one if not exists.</p> <p>The built image will be pushed to the remote registry which is specified in <code>KubeflowDagRunnerConfig</code> or <code>KubeflowV2DagRunnerConfig</code>.</p> --build_base_image=<code>build-base-image</code> <p>(Optional.) When the <code>engine</code> is kubeflow, TFX creates a container image for your pipeline. The build base image specifies the base container image to use when building the pipeline container image.</p>"},{"location":"guide/cli/#examples","title":"Examples","text":"<p>Kubeflow:</p> <pre><code>tfx pipeline create --engine=kubeflow --pipeline_path=pipeline-path \\\n--iap_client_id=iap-client-id --namespace=namespace --endpoint=endpoint \\\n--build_image\n</code></pre> <p>Local:</p> <pre><code>tfx pipeline create --engine=local --pipeline_path=pipeline-path\n</code></pre> <p>Vertex:</p> <pre><code>tfx pipeline create --engine=vertex --pipeline_path=pipeline-path \\\n--build_image\n</code></pre> <p>To autodetect engine from user environment, simply avoid using the engine flag like the example below. For more details, check the flags section.</p> <pre><code>tfx pipeline create --pipeline_path=pipeline-path\n</code></pre>"},{"location":"guide/cli/#update","title":"update","text":"<p>Updates an existing pipeline in the given orchestrator.</p> <p>Usage:</p> <pre><code>tfx pipeline update --pipeline_path=pipeline-path [--endpoint=endpoint --engine=engine \\\n--iap_client_id=iap-client-id --namespace=namespace --build_image]\n</code></pre> --pipeline_path=<code>pipeline-path</code> The path to the pipeline configuration file. --endpoint=<code>endpoint</code> <p>(Optional.) Endpoint of the Kubeflow Pipelines API service. The endpoint of your Kubeflow Pipelines API service is the same as URL of the Kubeflow Pipelines dashboard. Your endpoint value should be something like:</p> <pre><code>https://host-name/pipeline\n</code></pre> <p>If you do not know the endpoint for your Kubeflow Pipelines cluster, contact you cluster administrator.</p> <p>If the <code>--endpoint</code> is not specified, the in-cluster service DNS name is used as the default value. This name works only if the CLI command executes in a pod on the Kubeflow Pipelines cluster, such as a Kubeflow Jupyter notebooks instance.</p> --engine=<code>engine</code> <p>(Optional.) The orchestrator to be used for the pipeline. The value of engine must match on of the following values:</p> <ul> <li>kubeflow: sets engine to Kubeflow</li> <li>local: sets engine to local orchestrator</li> <li>vertex: sets engine to Vertex Pipelines</li> <li>airflow: (experimental) sets engine to Apache Airflow</li> <li>beam: (experimental) sets engine to Apache Beam</li> </ul> <p>If the engine is not set, the engine is auto-detected based on the environment.</p> <p>Important Note</p> <p>The orchestrator required by the DagRunner in the pipeline config file must match the selected or autodetected engine. Engine auto-detection is based on user environment. If Apache Airflow and Kubeflow Pipelines are not installed, then the local orchestrator is used by default.</p> --iap_client_id=<code>iap-client-id</code> (Optional.) Client ID for IAP protected endpoint. --namespace=<code>namespace</code> (Optional.) Kubernetes namespace to connect to the Kubeflow Pipelines API. If the namespace is not specified, the value defaults to <code>kubeflow</code>. --build_image <p>(Optional.) When the <code>engine</code> is kubeflow or vertex, TFX creates a container image for your pipeline if specified. <code>Dockerfile</code> in the current directory will be used.</p> <p>The built image will be pushed to the remote registry which is specified in <code>KubeflowDagRunnerConfig</code> or <code>KubeflowV2DagRunnerConfig</code>.</p>"},{"location":"guide/cli/#examples_1","title":"Examples","text":"<p>Kubeflow:</p> <pre><code>tfx pipeline update --engine=kubeflow --pipeline_path=pipeline-path \\\n--iap_client_id=iap-client-id --namespace=namespace --endpoint=endpoint \\\n--build_image\n</code></pre> <p>Local:</p> <pre><code>tfx pipeline update --engine=local --pipeline_path=pipeline-path\n</code></pre> <p>Vertex:</p> <pre><code>tfx pipeline update --engine=vertex --pipeline_path=pipeline-path \\\n--build_image\n</code></pre>"},{"location":"guide/cli/#compile","title":"compile","text":"<p>Compiles the pipeline config file to create a workflow file in Kubeflow and performs the following checks while compiling:</p> <ol> <li>Checks if the pipeline path is valid.</li> <li>Checks if the pipeline details are extracted successfully from the pipeline     config file.</li> <li>Checks if the DagRunner in the pipeline config matches the engine.</li> <li>Checks if the workflow file is created successfully in the package path     provided (only for Kubeflow).</li> </ol> <p>Recommended to use before creating or updating a pipeline.</p> <p>Usage:</p> <pre><code>tfx pipeline compile --pipeline_path=pipeline-path [--engine=engine]\n</code></pre> --pipeline_path=<code>pipeline-path</code> The path to the pipeline configuration file. --engine=<code>engine</code> <p>(Optional.) The orchestrator to be used for the pipeline. The value of engine must match on of the following values:</p> <ul> <li>kubeflow: sets engine to Kubeflow</li> <li>local: sets engine to local orchestrator</li> <li>vertex: sets engine to Vertex Pipelines</li> <li>airflow: (experimental) sets engine to Apache Airflow</li> <li>beam: (experimental) sets engine to Apache Beam</li> </ul> <p>If the engine is not set, the engine is auto-detected based on the environment.</p> <p>Important Note</p> <p>The orchestrator required by the DagRunner in the pipeline config file must match the selected or autodetected engine. Engine auto-detection is based on user environment. If Apache Airflow and Kubeflow Pipelines are not installed, then the local orchestrator is used by default.</p>"},{"location":"guide/cli/#examples_2","title":"Examples","text":"<p>Kubeflow:</p> <pre><code>tfx pipeline compile --engine=kubeflow --pipeline_path=pipeline-path\n</code></pre> <p>Local:</p> <pre><code>tfx pipeline compile --engine=local --pipeline_path=pipeline-path\n</code></pre> <p>Vertex:</p> <pre><code>tfx pipeline compile --engine=vertex --pipeline_path=pipeline-path\n</code></pre>"},{"location":"guide/cli/#delete","title":"delete","text":"<p>Deletes a pipeline from the given orchestrator.</p> <p>Usage:</p> <pre><code>tfx pipeline delete --pipeline_path=pipeline-path [--endpoint=endpoint --engine=engine \\\n--iap_client_id=iap-client-id --namespace=namespace]\n</code></pre> --pipeline_path=<code>pipeline-path</code> The path to the pipeline configuration file. --endpoint=<code>endpoint</code> <p>(Optional.) Endpoint of the Kubeflow Pipelines API service. The endpoint of your Kubeflow Pipelines API service is the same as URL of the Kubeflow Pipelines dashboard. Your endpoint value should be something like:</p> <pre><code>https://host-name/pipeline\n</code></pre> <p>If you do not know the endpoint for your Kubeflow Pipelines cluster, contact you cluster administrator.</p> <p>If the <code>--endpoint</code> is not specified, the in-cluster service DNS name is used as the default value. This name works only if the CLI command executes in a pod on the Kubeflow Pipelines cluster, such as a Kubeflow Jupyter notebooks instance.</p> --engine=<code>engine</code> <p>(Optional.) The orchestrator to be used for the pipeline. The value of engine must match on of the following values:</p> <ul> <li>kubeflow: sets engine to Kubeflow</li> <li>local: sets engine to local orchestrator</li> <li>vertex: sets engine to Vertex Pipelines</li> <li>airflow: (experimental) sets engine to Apache Airflow</li> <li>beam: (experimental) sets engine to Apache Beam</li> </ul> <p>If the engine is not set, the engine is auto-detected based on the environment.</p> <p>Important Note</p> <p>The orchestrator required by the DagRunner in the pipeline config file must match the selected or autodetected engine. Engine auto-detection is based on user environment. If Apache Airflow and Kubeflow Pipelines are not installed, then the local orchestrator is used by default.</p> --iap_client_id=<code>iap-client-id</code> (Optional.) Client ID for IAP protected endpoint. --namespace=<code>namespace</code> (Optional.) Kubernetes namespace to connect to the Kubeflow Pipelines API. If the namespace is not specified, the value defaults to <code>kubeflow</code>."},{"location":"guide/cli/#examples_3","title":"Examples","text":"<p>Kubeflow:</p> <pre><code>tfx pipeline delete --engine=kubeflow --pipeline_name=pipeline-name \\\n--iap_client_id=iap-client-id --namespace=namespace --endpoint=endpoint\n</code></pre> <p>Local:</p> <pre><code>tfx pipeline delete --engine=local --pipeline_name=pipeline-name\n</code></pre> <p>Vertex:</p> <pre><code>tfx pipeline delete --engine=vertex --pipeline_name=pipeline-name\n</code></pre>"},{"location":"guide/cli/#list","title":"list","text":"<p>Lists all the pipelines in the given orchestrator.</p> <p>Usage:</p> <pre><code>tfx pipeline list [--endpoint=endpoint --engine=engine \\\n--iap_client_id=iap-client-id --namespace=namespace]\n</code></pre> --endpoint=<code>endpoint</code> <p>(Optional.) Endpoint of the Kubeflow Pipelines API service. The endpoint of your Kubeflow Pipelines API service is the same as URL of the Kubeflow Pipelines dashboard. Your endpoint value should be something like:</p> <pre><code>https://host-name/pipeline\n</code></pre> <p>If you do not know the endpoint for your Kubeflow Pipelines cluster, contact you cluster administrator.</p> <p>If the <code>--endpoint</code> is not specified, the in-cluster service DNS name is used as the default value. This name works only if the CLI command executes in a pod on the Kubeflow Pipelines cluster, such as a Kubeflow Jupyter notebooks instance.</p> --engine=<code>engine</code> <p>(Optional.) The orchestrator to be used for the pipeline. The value of engine must match on of the following values:</p> <ul> <li>kubeflow: sets engine to Kubeflow</li> <li>local: sets engine to local orchestrator</li> <li>vertex: sets engine to Vertex Pipelines</li> <li>airflow: (experimental) sets engine to Apache Airflow</li> <li>beam: (experimental) sets engine to Apache Beam</li> </ul> <p>If the engine is not set, the engine is auto-detected based on the environment.</p> <p>Important Note</p> <p>The orchestrator required by the DagRunner in the pipeline config file must match the selected or autodetected engine. Engine auto-detection is based on user environment. If Apache Airflow and Kubeflow Pipelines are not installed, then the local orchestrator is used by default.</p> --iap_client_id=<code>iap-client-id</code> (Optional.) Client ID for IAP protected endpoint. --namespace=<code>namespace</code> (Optional.) Kubernetes namespace to connect to the Kubeflow Pipelines API. If the namespace is not specified, the value defaults to <code>kubeflow</code>."},{"location":"guide/cli/#examples_4","title":"Examples","text":"<p>Kubeflow:</p> <pre><code>tfx pipeline list --engine=kubeflow --iap_client_id=iap-client-id \\\n--namespace=namespace --endpoint=endpoint\n</code></pre> <p>Local:</p> <pre><code>tfx pipeline list --engine=local\n</code></pre> <p>Vertex:</p> <pre><code>tfx pipeline list --engine=vertex\n</code></pre>"},{"location":"guide/cli/#tfx-run","title":"tfx run","text":"<p>The structure for commands in the <code>tfx run</code> command group is as follows:</p> <pre><code>tfx run command required-flags [optional-flags]\n</code></pre> <p>Use the following sections to learn more about the commands in the <code>tfx run</code> command group.</p>"},{"location":"guide/cli/#create_1","title":"create","text":"<p>Creates a new run instance for a pipeline in the orchestrator. For Kubeflow, the most recent pipeline version of the pipeline in the cluster is used.</p> <p>Usage:</p> <pre><code>tfx run create --pipeline_name=pipeline-name [--endpoint=endpoint \\\n--engine=engine --iap_client_id=iap-client-id --namespace=namespace]\n</code></pre> --pipeline_name=<code>pipeline-name</code> The name of the pipeline. --endpoint=<code>endpoint</code> <p>(Optional.) Endpoint of the Kubeflow Pipelines API service. The endpoint of your Kubeflow Pipelines API service is the same as URL of the Kubeflow Pipelines dashboard. Your endpoint value should be something like:</p> <pre><code>https://host-name/pipeline\n</code></pre> <p>If you do not know the endpoint for your Kubeflow Pipelines cluster, contact you cluster administrator.</p> <p>If the <code>--endpoint</code> is not specified, the in-cluster service DNS name is used as the default value. This name works only if the CLI command executes in a pod on the Kubeflow Pipelines cluster, such as a Kubeflow Jupyter notebooks instance.</p> --engine=<code>engine</code> <p>(Optional.) The orchestrator to be used for the pipeline. The value of engine must match on of the following values:</p> <ul> <li>kubeflow: sets engine to Kubeflow</li> <li>local: sets engine to local orchestrator</li> <li>vertex: sets engine to Vertex Pipelines</li> <li>airflow: (experimental) sets engine to Apache Airflow</li> <li>beam: (experimental) sets engine to Apache Beam</li> </ul> <p>If the engine is not set, the engine is auto-detected based on the environment.</p> <p>Important Note</p> <p>The orchestrator required by the DagRunner in the pipeline config file must match the selected or autodetected engine. Engine auto-detection is based on user environment. If Apache Airflow and Kubeflow Pipelines are not installed, then the local orchestrator is used by default.</p> --runtime_parameter=<code>parameter-name</code>=<code>parameter-value</code> (Optional.) Sets a runtime parameter value. Can be set multiple times to set values of multiple variables. Only applicable to <code>airflow</code>, <code>kubeflow</code> and <code>vertex</code> engine. --iap_client_id=<code>iap-client-id</code> (Optional.) Client ID for IAP protected endpoint. --namespace=<code>namespace</code> (Optional.) Kubernetes namespace to connect to the Kubeflow Pipelines API. If the namespace is not specified, the value defaults to <code>kubeflow</code>. --project=<code>GCP-project-id</code> (Required for Vertex.) GCP project id for the vertex pipeline. --region=<code>GCP-region</code> (Required for Vertex.) GCP region name like us-central1. See [Vertex documentation](https://cloud.google.com/vertex-ai/docs/general/locations) for available regions."},{"location":"guide/cli/#examples_5","title":"Examples","text":"<p>Kubeflow:</p> <pre><code>tfx run create --engine=kubeflow --pipeline_name=pipeline-name --iap_client_id=iap-client-id \\\n--namespace=namespace --endpoint=endpoint\n</code></pre> <p>Local:</p> <pre><code>tfx run create --engine=local --pipeline_name=pipeline-name\n</code></pre> <p>Vertex:</p> <pre><code>tfx run create --engine=vertex --pipeline_name=pipeline-name \\\n  --runtime_parameter=var_name=var_value \\\n  --project=gcp-project-id --region=gcp-region\n</code></pre>"},{"location":"guide/cli/#terminate","title":"terminate","text":"<p>Stops a run of a given pipeline.</p> <p>Important Note</p> <p>Currently supported only in Kubeflow.</p> <p>Usage:</p> <pre><code>tfx run terminate --run_id=run-id [--endpoint=endpoint --engine=engine \\\n--iap_client_id=iap-client-id --namespace=namespace]\n</code></pre> --run_id=<code>run-id</code> Unique identifier for a pipeline run. --endpoint=<code>endpoint</code> <p>(Optional.) Endpoint of the Kubeflow Pipelines API service. The endpoint of your Kubeflow Pipelines API service is the same as URL of the Kubeflow Pipelines dashboard. Your endpoint value should be something like:</p> <pre><code>https://host-name/pipeline\n</code></pre> <p>If you do not know the endpoint for your Kubeflow Pipelines cluster, contact you cluster administrator.</p> <p>If the <code>--endpoint</code> is not specified, the in-cluster service DNS name is used as the default value. This name works only if the CLI command executes in a pod on the Kubeflow Pipelines cluster, such as a Kubeflow Jupyter notebooks instance.</p> --engine=<code>engine</code> <p>(Optional.) The orchestrator to be used for the pipeline. The value of engine must match on of the following values:</p> <ul> <li>kubeflow: sets engine to Kubeflow</li> </ul> <p>If the engine is not set, the engine is auto-detected based on the environment.</p> <p>Important Note</p> <p>The orchestrator required by the DagRunner in the pipeline config file must match the selected or autodetected engine. Engine auto-detection is based on user environment. If Apache Airflow and Kubeflow Pipelines are not installed, then the local orchestrator is used by default.</p> --iap_client_id=<code>iap-client-id</code> (Optional.) Client ID for IAP protected endpoint. --namespace=<code>namespace</code> (Optional.) Kubernetes namespace to connect to the Kubeflow Pipelines API. If the namespace is not specified, the value defaults to <code>kubeflow</code>."},{"location":"guide/cli/#examples_6","title":"Examples","text":"<p>Kubeflow:</p> <pre><code>tfx run delete --engine=kubeflow --run_id=run-id --iap_client_id=iap-client-id \\\n--namespace=namespace --endpoint=endpoint\n</code></pre>"},{"location":"guide/cli/#list_1","title":"list","text":"<p>Lists all runs of a pipeline.</p> <p>Important Note</p> <p>Currently not supported in Local and Apache Beam.</p> <p>Usage:</p> <pre><code>tfx run list --pipeline_name=pipeline-name [--endpoint=endpoint \\\n--engine=engine --iap_client_id=iap-client-id --namespace=namespace]\n</code></pre> --pipeline_name=<code>pipeline-name</code> The name of the pipeline. --endpoint=<code>endpoint</code> <p>(Optional.) Endpoint of the Kubeflow Pipelines API service. The endpoint of your Kubeflow Pipelines API service is the same as URL of the Kubeflow Pipelines dashboard. Your endpoint value should be something like:</p> <pre><code>https://host-name/pipeline\n</code></pre> <p>If you do not know the endpoint for your Kubeflow Pipelines cluster, contact you cluster administrator.</p> <p>If the <code>--endpoint</code> is not specified, the in-cluster service DNS name is used as the default value. This name works only if the CLI command executes in a pod on the Kubeflow Pipelines cluster, such as a Kubeflow Jupyter notebooks instance.</p> --engine=<code>engine</code> <p>(Optional.) The orchestrator to be used for the pipeline. The value of engine must match on of the following values:</p> <ul> <li>kubeflow: sets engine to Kubeflow</li> <li>airflow: (experimental) sets engine to Apache Airflow</li> </ul> <p>If the engine is not set, the engine is auto-detected based on the environment.</p> <p>Important Note</p> <p>The orchestrator required by the DagRunner in the pipeline config file must match the selected or autodetected engine. Engine auto-detection is based on user environment. If Apache Airflow and Kubeflow Pipelines are not installed, then the local orchestrator is used by default.</p> --iap_client_id=<code>iap-client-id</code> (Optional.) Client ID for IAP protected endpoint. --namespace=<code>namespace</code> (Optional.) Kubernetes namespace to connect to the Kubeflow Pipelines API. If the namespace is not specified, the value defaults to <code>kubeflow</code>."},{"location":"guide/cli/#examples_7","title":"Examples","text":"<p>Kubeflow:</p> <pre><code>tfx run list --engine=kubeflow --pipeline_name=pipeline-name --iap_client_id=iap-client-id \\\n--namespace=namespace --endpoint=endpoint\n</code></pre>"},{"location":"guide/cli/#status","title":"status","text":"<p>Returns the current status of a run.</p> <p>Important Note</p> <p>Currently not supported in Local and Apache Beam.</p> <p>Usage:</p> <pre><code>tfx run status --pipeline_name=pipeline-name --run_id=run-id [--endpoint=endpoint \\\n--engine=engine --iap_client_id=iap-client-id --namespace=namespace]\n</code></pre> --pipeline_name=<code>pipeline-name</code> The name of the pipeline. --run_id=<code>run-id</code> Unique identifier for a pipeline run. --endpoint=<code>endpoint</code> <p>(Optional.) Endpoint of the Kubeflow Pipelines API service. The endpoint of your Kubeflow Pipelines API service is the same as URL of the Kubeflow Pipelines dashboard. Your endpoint value should be something like:</p> <pre><code>https://host-name/pipeline\n</code></pre> <p>If you do not know the endpoint for your Kubeflow Pipelines cluster, contact you cluster administrator.</p> <p>If the <code>--endpoint</code> is not specified, the in-cluster service DNS name is used as the default value. This name works only if the CLI command executes in a pod on the Kubeflow Pipelines cluster, such as a Kubeflow Jupyter notebooks instance.</p> --engine=<code>engine</code> <p>(Optional.) The orchestrator to be used for the pipeline. The value of engine must match on of the following values:</p> <ul> <li>kubeflow: sets engine to Kubeflow</li> <li>airflow: (experimental) sets engine to Apache Airflow</li> </ul> <p>If the engine is not set, the engine is auto-detected based on the environment.</p> <p>Important Note</p> <p>The orchestrator required by the DagRunner in the pipeline config file must match the selected or autodetected engine. Engine auto-detection is based on user environment. If Apache Airflow and Kubeflow Pipelines are not installed, then the local orchestrator is used by default.</p> --iap_client_id=<code>iap-client-id</code> (Optional.) Client ID for IAP protected endpoint. --namespace=<code>namespace</code> (Optional.) Kubernetes namespace to connect to the Kubeflow Pipelines API. If the namespace is not specified, the value defaults to <code>kubeflow</code>."},{"location":"guide/cli/#examples_8","title":"Examples","text":"<p>Kubeflow:</p> <pre><code>tfx run status --engine=kubeflow --run_id=run-id --pipeline_name=pipeline-name \\\n--iap_client_id=iap-client-id --namespace=namespace --endpoint=endpoint\n</code></pre>"},{"location":"guide/cli/#delete_1","title":"delete","text":"<p>Deletes a run of a given pipeline.</p> <p>Note</p> <p>Currently supported only in Kubeflow</p> <p>Usage:</p> <pre><code>tfx run delete --run_id=run-id [--engine=engine --iap_client_id=iap-client-id \\\n--namespace=namespace --endpoint=endpoint]\n</code></pre> --run_id=<code>run-id</code> Unique identifier for a pipeline run. --endpoint=<code>endpoint</code> <p>(Optional.) Endpoint of the Kubeflow Pipelines API service. The endpoint of your Kubeflow Pipelines API service is the same as URL of the Kubeflow Pipelines dashboard. Your endpoint value should be something like:</p> <pre><code>https://host-name/pipeline\n</code></pre> <p>If you do not know the endpoint for your Kubeflow Pipelines cluster, contact you cluster administrator.</p> <p>If the <code>--endpoint</code> is not specified, the in-cluster service DNS name is used as the default value. This name works only if the CLI command executes in a pod on the Kubeflow Pipelines cluster, such as a Kubeflow Jupyter notebooks instance.</p> --engine=<code>engine</code> <p>(Optional.) The orchestrator to be used for the pipeline. The value of engine must match on of the following values:</p> <ul> <li>kubeflow: sets engine to Kubeflow</li> </ul> <p>If the engine is not set, the engine is auto-detected based on the environment.</p> <p>Important Note</p> <p>The orchestrator required by the DagRunner in the pipeline config file must match the selected or autodetected engine. Engine auto-detection is based on user environment. If Apache Airflow and Kubeflow Pipelines are not installed, then the local orchestrator is used by default.</p> --iap_client_id=<code>iap-client-id</code> (Optional.) Client ID for IAP protected endpoint. --namespace=<code>namespace</code> (Optional.) Kubernetes namespace to connect to the Kubeflow Pipelines API. If the namespace is not specified, the value defaults to <code>kubeflow</code>."},{"location":"guide/cli/#examples_9","title":"Examples","text":"<p>Kubeflow:</p> <pre><code>tfx run delete --engine=kubeflow --run_id=run-id --iap_client_id=iap-client-id \\\n--namespace=namespace --endpoint=endpoint\n</code></pre>"},{"location":"guide/cli/#tfx-template-experimental","title":"tfx template [Experimental]","text":"<p>The structure for commands in the <code>tfx template</code> command group is as follows:</p> <pre><code>tfx template command required-flags [optional-flags]\n</code></pre> <p>Use the following sections to learn more about the commands in the <code>tfx template</code> command group. Template is an experimental feature and subject to change at any time.</p>"},{"location":"guide/cli/#list_2","title":"list","text":"<p>List available TFX pipeline templates.</p> <p>Usage:</p> <pre><code>tfx template list\n</code></pre>"},{"location":"guide/cli/#copy","title":"copy","text":"<p>Copy a template to the destination directory.</p> <p>Usage:</p> <pre><code>tfx template copy --model=model --pipeline_name=pipeline-name \\\n--destination_path=destination-path\n</code></pre> --model=<code>model</code> The name of the model built by the pipeline template. --pipeline_name=<code>pipeline-name</code> The name of the pipeline. --destination_path=<code>destination-path</code> The path to copy the template to."},{"location":"guide/cli/#understanding-tfx-cli-flags","title":"Understanding TFX CLI Flags","text":""},{"location":"guide/cli/#common-flags","title":"Common flags","text":"--engine=<code>engine</code> <p>The orchestrator to be used for the pipeline. The value of engine must match on of the following values:</p> <ul> <li>kubeflow: sets engine to Kubeflow</li> <li>local: sets engine to local orchestrator</li> <li>vertex: sets engine to Vertex Pipelines</li> <li>airflow: (experimental) sets engine to Apache Airflow</li> <li>beam: (experimental) sets engine to Apache Beam</li> </ul> <p>If the engine is not set, the engine is auto-detected based on the environment.</p> <p>Important Note</p> <p>The orchestrator required by the DagRunner in the pipeline config file must match the selected or autodetected engine. Engine auto-detection is based on user environment. If Apache Airflow and Kubeflow Pipelines are not installed, then the local orchestrator is used by default.</p> --pipeline_name=<code>pipeline-name</code> The name of the pipeline. --pipeline_path=<code>pipeline-path</code> The path to the pipeline configuration file. --run_id=<code>run-id</code> Unique identifier for a pipeline run."},{"location":"guide/cli/#kubeflow-specific-flags","title":"Kubeflow specific flags","text":"--endpoint=<code>endpoint</code> <p>Endpoint of the Kubeflow Pipelines API service. The endpoint of your Kubeflow Pipelines API service is the same as URL of the Kubeflow Pipelines dashboard. Your endpoint value should be something like:</p> <pre><code>https://host-name/pipeline\n</code></pre> <p>If you do not know the endpoint for your Kubeflow Pipelines cluster, contact you cluster administrator.</p> <p>If the <code>--endpoint</code> is not specified, the in-cluster service DNS name is used as the default value. This name works only if the CLI command executes in a pod on the Kubeflow Pipelines cluster, such as a Kubeflow Jupyter notebooks instance.</p> --iap_client_id=<code>iap-client-id</code> Client ID for IAP protected endpoint. --namespace=<code>namespace</code> Kubernetes namespace to connect to the Kubeflow Pipelines API. If the namespace is not specified, the value defaults to <code>kubeflow</code>."},{"location":"guide/cli/#generated-files-by-tfx-cli","title":"Generated files by TFX CLI","text":"<p>When pipelines are created and run, several files are generated for pipeline management.</p> <ul> <li>${HOME}/tfx/local, beam, airflow, vertex<ul> <li>Pipeline metadata read from the configuration is stored under     <code>${HOME}/tfx/${ORCHESTRATION_ENGINE}/${PIPELINE_NAME}</code>. This location     can be customized by setting environment varaible like <code>AIRFLOW_HOME</code> or     <code>KUBEFLOW_HOME</code>. This behavior might be changed in future releases. This     directory is used to store pipeline information including pipeline ids     in the Kubeflow Pipelines cluster which is needed to create runs or     update pipelines.</li> <li>Before TFX 0.25, these files were located under     <code>${HOME}/${ORCHESTRATION_ENGINE}</code>. In TFX 0.25, files in the old     location will be moved to the new location automatically for smooth     migration.</li> <li>From TFX 0.27, kubeflow doesn't create these metadata files in local     filesystem. However, see below for other files that kubeflow creates.</li> </ul> </li> <li>(Kubeflow only) Dockerfile and a container image<ul> <li>Kubeflow Pipelines requires two kinds of input for a pipeline. These     files are generated by TFX in the current directory.</li> <li>One is a container image which will be used to run components in the     pipeline. This container image is built when a pipeline for Kubeflow     Pipelines is created or updated with <code>--build-image</code> flag. TFX CLI will     generate <code>Dockerfile</code> if not exists, and will build and push a container     image to the registry specified in KubeflowDagRunnerConfig.</li> </ul> </li> </ul>"},{"location":"guide/container_component/","title":"Building Container-based components","text":"<p>Container-based components provide the flexibility to integrate code written in any language into your pipeline, so long as you can execute that code in a Docker container.</p> <p>If you are new to TFX pipelines, learn more about the core concepts of TFX pipelines.</p>"},{"location":"guide/container_component/#creating-a-container-based-component","title":"Creating a Container-based Component","text":"<p>Container-based components are backed by containerized command-line programs. If you already have a container image, you can use TFX to create a component from it by using the <code>create_container_component</code> function to declare inputs and outputs. Function parameters:</p> <ul> <li>name: The name of the component.</li> <li>inputs: A dictionary that maps input names to types. outputs: A     dictionary that maps output names to types parameters: A dictionary that     maps parameter names to types.</li> <li>image: Container image name, and optionally image tag.</li> <li>command: Container entrypoint command line. Not executed within a shell.     The command line can use placeholder objects that are replaced at     compilation time with the input, output, or parameter. The placeholder     objects can be imported from     <code>tfx.dsl.component.experimental.placeholders</code>.     Note that Jinja templates are not supported.</li> </ul> <p>Return value: a Component class inheriting from base_component.BaseComponent which can be instantiated and used inside the pipeline.</p>"},{"location":"guide/container_component/#placeholders","title":"Placeholders","text":"<p>For a component that has inputs or outputs, the <code>command</code> often needs to have placeholders that are replaced with actual data at runtime. Several placeholders are provided for this purpose:</p> <ul> <li> <p><code>InputValuePlaceholder</code>: A placeholder for the value of the input artifact.     At runtime, this placeholder is replaced with the string representation of     the artifact's value.</p> </li> <li> <p><code>InputUriPlaceholder</code>: A placeholder for the URI of the input artifact     argument. At runtime, this placeholder is replaced with the URI of the input     artifact's data.</p> </li> <li> <p><code>OutputUriPlaceholder</code>: A placeholder for the URI of the output artifact     argument. At runtime, this placeholder is replaced with the URI where the     component should store the output artifact's data.</p> </li> </ul> <p>Learn more about TFX component command-line placeholders.</p>"},{"location":"guide/container_component/#example-container-based-component","title":"Example Container-based Component","text":"<p>The following is an example of a non-python component that downloads, transforms, and uploads the data:</p> <pre><code>import tfx.v1 as tfx\n\ngrep_component = tfx.dsl.components.create_container_component(\n    name='FilterWithGrep',\n    inputs={\n        'text': tfx.standard_artifacts.ExternalArtifact,\n    },\n    outputs={\n        'filtered_text': tfx.standard_artifacts.ExternalArtifact,\n    },\n    parameters={\n        'pattern': str,\n    },\n    # The component code uses gsutil to upload the data to Google Cloud Storage, so the\n    # container image needs to have gsutil installed and configured.\n    image='google/cloud-sdk:278.0.0',\n    command=[\n        'sh', '-exc',\n        '''\n          pattern=\"$1\"\n          text_uri=\"$3\"/data  # Adding suffix, because currently the URI are \"directories\". This will be fixed soon.\n          text_path=$(mktemp)\n          filtered_text_uri=\"$5\"/data  # Adding suffix, because currently the URI are \"directories\". This will be fixed soon.\n          filtered_text_path=$(mktemp)\n\n          # Getting data into the container\n          gsutil cp \"$text_uri\" \"$text_path\"\n\n          # Running the main code\n          grep \"$pattern\" \"$text_path\" &gt;\"$filtered_text_path\"\n\n          # Getting data out of the container\n          gsutil cp \"$filtered_text_path\" \"$filtered_text_uri\"\n        ''',\n        '--pattern', tfx.dsl.placeholders.InputValuePlaceholder('pattern'),\n        '--text', tfx.dsl.placeholders.InputUriPlaceholder('text'),\n        '--filtered-text', tfx.dsl.placeholders.OutputUriPlaceholder('filtered_text'),\n    ],\n)\n</code></pre>"},{"location":"guide/custom_component/","title":"Building Fully Custom Components","text":"<p>This guide describes how to use the TFX API to build a fully custom component. Fully custom components let you build components by defining the component specification, executor, and component interface classes. This approach lets you reuse and extend a standard component to fit your needs.</p> <p>If you are new to TFX pipelines, learn more about the core concepts of TFX pipelines.</p>"},{"location":"guide/custom_component/#custom-executor-or-custom-component","title":"Custom executor or custom component","text":"<p>If only custom processing logic is needed while the inputs, outputs, and execution properties of the component are the same as an existing component, a custom executor is sufficient. A fully custom component is needed when any of the inputs, outputs, or execution properties are different from any existing TFX components.</p>"},{"location":"guide/custom_component/#how-to-create-a-custom-component","title":"How to create a custom component?","text":"<p>Developing a fully custom component requires:</p> <ul> <li>A defined set of input and output artifact specifications for the new     component. Specially, the types for the input artifacts should be consistent     with the output artifact types of the components that produce the artifacts     and the types for the output artifacts should be consistent with the input     artifact types of the components that consume the artifacts if any.</li> <li>The non-artifact execution parameters that are needed for the new component.</li> </ul>"},{"location":"guide/custom_component/#componentspec","title":"ComponentSpec","text":"<p>The <code>ComponentSpec</code> class defines the component contract by defining the input and output artifacts to a component as well as the parameters that are used for the component execution. It has three parts:</p> <ul> <li>INPUTS: A dictionary of typed parameters for the input artifacts that are     passed into the component executor. Normally input artifacts are the outputs     from upstream components and thus share the same type.</li> <li>OUTPUTS: A dictionary of typed parameters for the output artifacts which     the component produces.</li> <li>PARAMETERS: A dictionary of additional     ExecutionParameter     items that will be passed into the component executor. These are     non-artifact parameters that we want to define flexibly in the pipeline DSL     and pass into execution.</li> </ul> <p>Here is an example of the ComponentSpec:</p> <pre><code>class HelloComponentSpec(types.ComponentSpec):\n  \"\"\"ComponentSpec for Custom TFX Hello World Component.\"\"\"\n\n  PARAMETERS = {\n      # These are parameters that will be passed in the call to\n      # create an instance of this component.\n      'name': ExecutionParameter(type=Text),\n  }\n  INPUTS = {\n      # This will be a dictionary with input artifacts, including URIs\n      'input_data': ChannelParameter(type=standard_artifacts.Examples),\n  }\n  OUTPUTS = {\n      # This will be a dictionary which this component will populate\n      'output_data': ChannelParameter(type=standard_artifacts.Examples),\n  }\n</code></pre>"},{"location":"guide/custom_component/#executor","title":"Executor","text":"<p>Next, write the executor code for the new component. Basically, a new subclass of <code>base_executor.BaseExecutor</code> needs to be created with its <code>Do</code> function overriden. In the <code>Do</code> function, the arguments <code>input_dict</code>, <code>output_dict</code> and <code>exec_properties</code> that are passed in map to <code>INPUTS</code>, <code>OUTPUTS</code> and <code>PARAMETERS</code> that are defined in ComponentSpec respectively. For <code>exec_properties</code>, the value can be fetched directly through a dictionary lookup. For artifacts in <code>input_dict</code> and <code>output_dict</code>, there are convenient functions available in artifact_utils class that can be used to fetch artifact instance or artifact uri.</p> <pre><code>class Executor(base_executor.BaseExecutor):\n  \"\"\"Executor for HelloComponent.\"\"\"\n\n  def Do(self, input_dict: Dict[Text, List[types.Artifact]],\n         output_dict: Dict[Text, List[types.Artifact]],\n         exec_properties: Dict[Text, Any]) -&gt; None:\n    ...\n\n    split_to_instance = {}\n    for artifact in input_dict['input_data']:\n      for split in json.loads(artifact.split_names):\n        uri = artifact_utils.get_split_uri([artifact], split)\n        split_to_instance[split] = uri\n\n    for split, instance in split_to_instance.items():\n      input_dir = instance\n      output_dir = artifact_utils.get_split_uri(\n          output_dict['output_data'], split)\n      for filename in tf.io.gfile.listdir(input_dir):\n        input_uri = os.path.join(input_dir, filename)\n        output_uri = os.path.join(output_dir, filename)\n        io_utils.copy_file(src=input_uri, dst=output_uri, overwrite=True)\n</code></pre>"},{"location":"guide/custom_component/#unit-testing-a-custom-executor","title":"Unit testing a custom executor","text":"<p>Unit tests for the custom executor can be created similar to this one.</p>"},{"location":"guide/custom_component/#component-interface","title":"Component interface","text":"<p>Now that the most complex part is complete, the next step is to assemble these pieces into a component interface, to enable the component to be used in a pipeline. There are several steps:</p> <ul> <li>Make the component interface a subclass of <code>base_component.BaseComponent</code></li> <li>Assign a class variable <code>SPEC_CLASS</code> with the <code>ComponentSpec</code> class that was     defined earlier</li> <li>Assign a class variable <code>EXECUTOR_SPEC</code> with the Executor class that was     defined earlier</li> <li>Define the <code>__init__()</code> constructor function by using the arguments to the     function to construct an instance of the ComponentSpec class and invoke the     super function with that value, along with an optional name</li> </ul> <p>When an instance of the component is created, type checking logic in the <code>base_component.BaseComponent</code> class will be invoked to ensure that the arguments which were passed in are compatible with the type info defined in the <code>ComponentSpec</code> class.</p> <pre><code>from tfx.types import standard_artifacts\nfrom hello_component import executor\n\nclass HelloComponent(base_component.BaseComponent):\n  \"\"\"Custom TFX Hello World Component.\"\"\"\n\n  SPEC_CLASS = HelloComponentSpec\n  EXECUTOR_SPEC = executor_spec.ExecutorClassSpec(executor.Executor)\n\n  def __init__(self,\n               input_data: types.Channel = None,\n               output_data: types.Channel = None,\n               name: Optional[Text] = None):\n    if not output_data:\n      examples_artifact = standard_artifacts.Examples()\n      examples_artifact.split_names = input_data.get()[0].split_names\n      output_data = channel_utils.as_channel([examples_artifact])\n\n    spec = HelloComponentSpec(input_data=input_data,\n                              output_data=output_data, name=name)\n    super(HelloComponent, self).__init__(spec=spec)\n</code></pre>"},{"location":"guide/custom_component/#assemble-into-a-tfx-pipeline","title":"Assemble into a TFX pipeline","text":"<p>The last step is to plug the new custom component into a TFX pipeline. Besides adding an instance of the new component, the following are also needed:</p> <ul> <li>Properly wire the upstream and downstream components of the new component to     it. This is done by referencing the outputs of the upstream component in the     new component and referencing the outputs of the new component in downstream     components</li> <li>Add the new component instance to the components list when constructing the     pipeline.</li> </ul> <p>The example below highlights the aforementioned changes. Full example can be found in the TFX GitHub repo.</p> <pre><code>def _create_pipeline():\n  ...\n  example_gen = CsvExampleGen(input_base=examples)\n  hello = component.HelloComponent(\n      input_data=example_gen.outputs['examples'], name='HelloWorld')\n  statistics_gen = StatisticsGen(examples=hello.outputs['output_data'])\n  ...\n  return pipeline.Pipeline(\n      ...\n      components=[example_gen, hello, statistics_gen, ...],\n      ...\n  )\n</code></pre>"},{"location":"guide/custom_component/#deploy-a-fully-custom-component","title":"Deploy a fully custom component","text":"<p>Beside code changes, all the newly added parts (<code>ComponentSpec</code>, <code>Executor</code>, component interface) need to be accessible in pipeline running environment in order to run the pipeline properly.</p>"},{"location":"guide/custom_function_component/","title":"Custom Python function components","text":"<p>Python function-based component definition makes it easier for you to create TFX custom components, by saving you the effort of defining a component specification class, executor class, and component interface class. In this component definition style, you write a function that is annotated with type hints. The type hints describe the input artifacts, output artifacts, and parameters of your component.</p> <p>Writing your custom component in this style is very straightforward, as in the following example.</p> <pre><code>class MyOutput(TypedDict):\n  accuracy: float\n\n@component\ndef MyValidationComponent(\n    model: InputArtifact[Model],\n    blessing: OutputArtifact[Model],\n    accuracy_threshold: Parameter[int] = 10,\n) -&gt; MyOutput:\n  '''My simple custom model validation component.'''\n\n  accuracy = evaluate_model(model)\n  if accuracy &gt;= accuracy_threshold:\n    write_output_blessing(blessing)\n\n  return {\n    'accuracy': accuracy\n  }\n</code></pre> <p>Under the hood, this defines a custom component that is a subclass of <code>BaseComponent</code> and its Spec and Executor classes.</p> <p>Note</p> <p>The feature (BaseBeamComponent based component by annotating a function with <code>@component(use_beam=True)</code>) described below is experimental and there is no public backwards compatibility guarantees.</p> <p>If you want to define a subclass of <code>BaseBeamComponent</code> such that you could use a beam pipeline with TFX-pipeline-wise shared configuration, i.e., <code>beam_pipeline_args</code> when compiling the pipeline (Chicago Taxi Pipeline Example) you could set <code>use_beam=True</code> in the decorator and add another <code>BeamComponentParameter</code> with default value <code>None</code> in your function as the following example:</p> <pre><code>@component(use_beam=True)\ndef MyDataProcessor(\n    examples: InputArtifact[Example],\n    processed_examples: OutputArtifact[Example],\n    beam_pipeline: BeamComponentParameter[beam.Pipeline] = None,\n    ) -&gt; None:\n  '''My simple custom model validation component.'''\n\n  with beam_pipeline as p:\n    # data pipeline definition with beam_pipeline begins\n    ...\n    # data pipeline definition with beam_pipeline ends\n</code></pre> <p>If you are new to TFX pipelines, learn more about the core concepts of TFX pipelines.</p>"},{"location":"guide/custom_function_component/#inputs-outputs-and-parameters","title":"Inputs, outputs, and parameters","text":"<p>In TFX, inputs and outputs are tracked as Artifact objects which describe the location of and metadata properties associated with the underlying data; this information is stored in ML Metadata. Artifacts can describe complex data types or simple data types, such as: int, float, bytes, or unicode strings.</p> <p>A parameter is an argument (int, float, bytes, or unicode string) to a component known at pipeline construction time. Parameters are useful for specifying arguments and hyperparameters like training iteration count, dropout rate, and other configuration to your component. Parameters are stored as properties of component executions when tracked in ML Metadata.</p> <p>Note</p> <p>Currently, output simple data type values cannot be used as parameters since they are not known at execution time. Similarly, input simple data type values currently cannot take concrete values known at pipeline construction time. We may remove this restriction in a future release of TFX.</p>"},{"location":"guide/custom_function_component/#definition","title":"Definition","text":"<p>To create a custom component, write a function that implements your custom logic and decorate it with the <code>@component</code> decorator from the <code>tfx.dsl.component.experimental.decorators</code> module. To define your component\u2019s input and output schema, annotate your function\u2019s arguments and return value using annotations from the <code>tfx.dsl.component.experimental.annotations</code> module:</p> <ul> <li> <p>For each artifact input, apply the <code>InputArtifact[ArtifactType]</code> type     hint annotation. Replace <code>ArtifactType</code> with the artifact\u2019s type, which is a     subclass of <code>tfx.types.Artifact</code>. These inputs can be optional arguments.</p> </li> <li> <p>For each output artifact, apply the <code>OutputArtifact[ArtifactType]</code> type     hint annotation. Replace <code>ArtifactType</code> with the artifact\u2019s type, which is a     subclass of <code>tfx.types.Artifact</code>. Component output artifacts should be     passed as input arguments of the function, so that your component can write     outputs to a system-managed location and set appropriate artifact metadata     properties. This argument can be optional or this argument can be defined     with a default value.</p> </li> <li> <p>For each parameter, use the type hint annotation <code>Parameter[T]</code>. Replace     <code>T</code> with the type of the parameter. We currently only support primitive     python types: <code>bool</code>, <code>int</code>, <code>float</code>, <code>str</code>, or <code>bytes</code>.</p> </li> <li> <p>For beam pipeline, use the type hint annotation     <code>BeamComponentParameter[beam.Pipeline]</code>. Set the default value to be <code>None</code>.     The value <code>None</code> will be replaced by an instantiated beam pipeline created     by <code>_make_beam_pipeline()</code> of     <code>BaseBeamExecutor</code></p> </li> <li> <p>For each simple data type input (<code>int</code>, <code>float</code>, <code>str</code> or <code>bytes</code>) not     known at pipeline construction time, use the type hint <code>T</code>. Note that in the     TFX 0.22 release, concrete values cannot be passed at pipeline construction     time for this type of input (use the <code>Parameter</code> annotation instead, as     described in the previous section). This argument can be optional or this     argument can be defined with a default value. If your component has simple     data type outputs (<code>int</code>, <code>float</code>, <code>str</code> or <code>bytes</code>), you can return these     outputs by using a <code>TypedDict</code> as a return type annotation, and returning an     appropriate dict object.</p> </li> </ul> <p>In the body of your function, input and output artifacts are passed as <code>tfx.types.Artifact</code> objects; you can inspect its <code>.uri</code> to get its system-managed location and read/set any properties. Input parameters and simple data type inputs are passed as objects of the specified type. Simple data type outputs should be returned as a dictionary, where the keys are the appropriate output names and the values are the desired return values.</p> <p>The completed function component can look like this:</p> <pre><code>from typing import TypedDict\nimport tfx.v1 as tfx\nfrom tfx.dsl.component.experimental.decorators import component\n\nclass MyOutput(TypedDict):\n  loss: float\n  accuracy: float\n\n@component\ndef MyTrainerComponent(\n    training_data: tfx.dsl.components.InputArtifact[tfx.types.standard_artifacts.Examples],\n    model: tfx.dsl.components.OutputArtifact[tfx.types.standard_artifacts.Model],\n    dropout_hyperparameter: float,\n    num_iterations: tfx.dsl.components.Parameter[int] = 10\n) -&gt; MyOutput:\n  '''My simple trainer component.'''\n\n  records = read_examples(training_data.uri)\n  model_obj = train_model(records, num_iterations, dropout_hyperparameter)\n  model_obj.write_to(model.uri)\n\n  return {\n    'loss': model_obj.loss,\n    'accuracy': model_obj.accuracy\n  }\n\n# Example usage in a pipeline graph definition:\n# ...\ntrainer = MyTrainerComponent(\n    examples=example_gen.outputs['examples'],\n    dropout_hyperparameter=other_component.outputs['dropout'],\n    num_iterations=1000)\npusher = Pusher(model=trainer.outputs['model'])\n# ...\n</code></pre> <p>The preceding example defines <code>MyTrainerComponent</code> as a Python function-based custom component. This component consumes an <code>examples</code> artifact as its input, and produces a <code>model</code> artifact as its output. The component uses the <code>artifact_instance.uri</code> to read or write the artifact at its system-managed location. The component takes a <code>num_iterations</code> input parameter and a <code>dropout_hyperparameter</code> simple data type value, and the component outputs <code>loss</code> and <code>accuracy</code> metrics as simple data type output values. The output <code>model</code> artifact is then used by the <code>Pusher</code> component.</p>"},{"location":"guide/custom_orchestrator/","title":"Orchestrating TFX Pipelines","text":""},{"location":"guide/custom_orchestrator/#custom-orchestrator","title":"Custom Orchestrator","text":"<p>TFX is designed to be portable to multiple environments and orchestration frameworks. Developers can create custom orchestrators or add additional orchestrators in addition to the default orchestrators that are supported by TFX, namely Local, Vertex AI, Airflow and Kubeflow.</p> <p>All orchestrators must inherit from TfxRunner. TFX orchestrators take the logical pipeline object, which contains pipeline args, components, and DAG, and are responsible for scheduling components of the TFX pipeline based on the dependencies defined by the DAG.</p> <p>For example, let's look at how to create a custom orchestrator with BaseComponentLauncher. BaseComponentLauncher already handles driver, executor, and publisher of a single component. The new orchestrator just needs to schedule ComponentLaunchers based on the DAG. A simple orchestrator is provided as the LocalDagRunner, which runs the components one by one in DAG's topological order.</p> <p>This orchestrator can be used in the Python DSL:</p> <pre><code>def _create_pipeline(...) -&gt; dsl.Pipeline:\n  ...\n  return dsl.Pipeline(...)\n\nif __name__ == '__main__':\n  orchestration.LocalDagRunner().run(_create_pipeline(...))\n</code></pre> <p>To run above Python DSL file (assuming it is named dsl.py), simply do the following:</p> <pre><code>python dsl.py\n</code></pre>"},{"location":"guide/evaluator/","title":"The Evaluator TFX Pipeline Component","text":"<p>The Evaluator TFX pipeline component performs deep analysis on the training results for your models, to help you understand how your model performs on subsets of your data. The Evaluator also helps you validate your exported models, ensuring that they are \"good enough\" to be pushed to production.</p> <p>When validation is enabled, the Evaluator compares new models against a baseline (such as the currently serving model) to determine if they're \"good enough\" relative to the baseline. It does so by evaluating both models on an eval dataset and computing their performance on metrics (e.g. AUC, loss). If the new model's metrics meet developer-specified criteria relative to the baseline model (e.g. AUC is not lower), the model is \"blessed\" (marked as good), indicating to the Pusher that it is ok to push the model to production.</p> <ul> <li>Consumes:<ul> <li>An eval split from     Examples</li> <li>A trained model from Trainer</li> <li>A previously blessed model (if validation to be performed)</li> </ul> </li> <li>Emits:<ul> <li>Analysis results to ML Metadata</li> <li>Validation results to ML Metadata (if validation to be     performed)</li> </ul> </li> </ul>"},{"location":"guide/evaluator/#evaluator-and-tensorflow-model-analysis","title":"Evaluator and TensorFlow Model Analysis","text":"<p>Evaluator leverages the TensorFlow Model Analysis library to perform the analysis, which in turn use Apache Beam for scalable processing.</p>"},{"location":"guide/evaluator/#using-the-evaluator-component","title":"Using the Evaluator Component","text":"<p>An Evaluator pipeline component is typically very easy to deploy and requires little customization, since most of the work is done by the Evaluator TFX component.</p> <p>To setup the evaluator the following information is needed:</p> <ul> <li>Metrics to configure (only required if additional metrics are being added     outside of those saved with the model). See     Tensorflow Model Analysis Metrics     for more information.</li> <li>Slices to configure (if no slices are given then an \"overall\" slice will be     added by default). See     Tensorflow Model Analysis Setup     for more information.</li> </ul> <p>If validation is to be included, the following additional information is needed:</p> <ul> <li>Which model to compare against (latest blessed, etc).</li> <li>Model validations (thresholds) to verify. See     Tensorflow Model Analysis Model Validations     for more information.</li> </ul> <p>When enabled, validation will be performed against all of the metrics and slices that were defined.</p> <p>Typical code looks like this:</p> <pre><code>import tensorflow_model_analysis as tfma\n...\n\n# For TFMA evaluation\n\neval_config = tfma.EvalConfig(\n    model_specs=[\n        # This assumes a serving model with signature 'serving_default'. If\n        # using a TFLite model, then you must set model_type='tf_lite'.\n        tfma.ModelSpec(label_key='&lt;label_key&gt;')\n    ],\n    metrics_specs=[\n        tfma.MetricsSpec(\n            # The metrics added here are in addition to those saved with the\n            # model (assuming either a keras model or EvalSavedModel is used).\n            # Any metrics added into the saved model (for example using\n            # model.compile(..., metrics=[...]), etc) will be computed\n            # automatically.\n            metrics=[\n                tfma.MetricConfig(class_name='ExampleCount'),\n                tfma.MetricConfig(\n                    class_name='BinaryAccuracy',\n                    threshold=tfma.MetricThreshold(\n                        value_threshold=tfma.GenericValueThreshold(\n                            lower_bound={'value': 0.5}),\n                        change_threshold=tfma.GenericChangeThreshold(\n                            direction=tfma.MetricDirection.HIGHER_IS_BETTER,\n                            absolute={'value': -1e-10})))\n            ]\n        )\n    ],\n    slicing_specs=[\n        # An empty slice spec means the overall slice, i.e. the whole dataset.\n        tfma.SlicingSpec(),\n        # Data can be sliced along a feature column. In this case, data is\n        # sliced along feature column trip_start_hour.\n        tfma.SlicingSpec(feature_keys=['trip_start_hour'])\n    ])\n\n# The following component is experimental and may change in the future. This is\n# required to specify the latest blessed model will be used as the baseline.\nmodel_resolver = Resolver(\n      strategy_class=dsl.experimental.LatestBlessedModelStrategy,\n      model=Channel(type=Model),\n      model_blessing=Channel(type=ModelBlessing)\n).with_id('latest_blessed_model_resolver')\n\nmodel_analyzer = Evaluator(\n      examples=examples_gen.outputs['examples'],\n      model=trainer.outputs['model'],\n      baseline_model=model_resolver.outputs['model'],\n      # Change threshold will be ignored if there is no baseline (first run).\n      eval_config=eval_config)\n</code></pre> <p>The evaluator produces an EvalResult (and optionally a ValidationResult if validation was used) that can be loaded using TFMA. The following is an example of how to load the results into a Jupyter notebook:</p> <pre><code>import tensorflow_model_analysis as tfma\n\noutput_path = evaluator.outputs['evaluation'].get()[0].uri\n\n# Load the evaluation results.\neval_result = tfma.load_eval_result(output_path)\n\n# Visualize the metrics and plots using tfma.view.render_slicing_metrics,\n# tfma.view.render_plot, etc.\ntfma.view.render_slicing_metrics(tfma_result)\n...\n\n# Load the validation results\nvalidation_result = tfma.load_validation_result(output_path)\nif not validation_result.validation_ok:\n  ...\n</code></pre> <p>More details are available in the Evaluator API reference.</p>"},{"location":"guide/examplegen/","title":"The ExampleGen TFX Pipeline Component","text":"<p>The ExampleGen TFX Pipeline component ingests data into TFX pipelines. It consumes external files/services to generate Examples which will be read by other TFX components. It also provides consistent and configurable partition, and shuffles the dataset for ML best practice.</p> <ul> <li>Consumes: Data from external data sources such as CSV, <code>TFRecord</code>, Avro,     Parquet and BigQuery.</li> <li>Emits: <code>tf.Example</code> records, <code>tf.SequenceExample</code> records, or proto format,     depending on the payload format.</li> </ul>"},{"location":"guide/examplegen/#examplegen-and-other-components","title":"ExampleGen and Other Components","text":"<p>ExampleGen provides data to components that make use of the TensorFlow Data Validation library, such as SchemaGen, StatisticsGen, and Example Validator. It also provides data to Transform, which makes use of the TensorFlow Transform library, and ultimately to deployment targets during inference.</p>"},{"location":"guide/examplegen/#data-sources-and-formats","title":"Data Sources and Formats","text":"<p>Currently a standard installation of TFX includes full ExampleGen components for these data sources and formats:</p> <ul> <li>CSV</li> <li>tf.Record</li> <li>BigQuery</li> </ul> <p>Custom executors are also available which enable the development of ExampleGen components for these data sources and formats:</p> <ul> <li>Avro</li> <li>Parquet</li> </ul> <p>See the usage examples in the source code and this discussion for more information on how to use and develop custom executors.</p> <p>Note</p> <p>In most case it's better to inherit from <code>base_example_gen_executor</code> instead of <code>base_executor</code>. So following the Avro or Parquet example in the Executor source code may be advisable.</p> <p>In addition, these data sources and formats are available as custom component examples:</p> <ul> <li>Presto</li> </ul>"},{"location":"guide/examplegen/#ingesting-data-formats-which-are-supported-by-apache-beam","title":"Ingesting data formats which are supported by Apache Beam","text":"<p>Apache Beam supports ingesting data from a broad range of data sources and formats, (see below).  These capabilities can be used to create custom ExampleGen components for TFX, which is demonstrated by some existing ExampleGen components (see below).</p>"},{"location":"guide/examplegen/#how-to-use-an-examplegen-component","title":"How to use an ExampleGen Component","text":"<p>For supported data sources (currently, CSV files, TFRecord files with <code>tf.Example</code>, <code>tf.SequenceExample</code> and proto format, and results of BigQuery queries) the ExampleGen pipeline component can be used directly in deploy and requires little customization. For example:</p> <pre><code>example_gen = CsvExampleGen(input_base='data_root')\n</code></pre> <p>or like below for importing external TFRecord with <code>tf.Example</code> directly:</p> <pre><code>example_gen = ImportExampleGen(input_base=path_to_tfrecord_dir)\n</code></pre>"},{"location":"guide/examplegen/#span-version-and-split","title":"Span, Version and Split","text":"<p>A Span is a grouping of training examples. If your data is persisted on a filesystem, each Span may be stored in a separate directory. The semantics of a Span are not hardcoded into TFX; a Span may correspond to a day of data, an hour of data, or any other grouping that is meaningful to your task.</p> <p>Each Span can hold multiple Versions of data. To give an example, if you remove some examples from a Span to clean up poor quality data, this could result in a new Version of that Span. By default, TFX components operate on the latest Version within a Span.</p> <p>Each Version within a Span can further be subdivided into multiple Splits. The most common use-case for splitting a Span is to split it into training and eval data.</p> <p></p>"},{"location":"guide/examplegen/#custom-inputoutput-split","title":"Custom input/output split","text":"<p>Note</p> <p>This feature is only available after TFX 0.14.</p> <p>To customize the train/eval split ratio which ExampleGen will output, set the <code>output_config</code> for ExampleGen component. For example:</p> <pre><code># Input has a single split 'input_dir/*'.\n# Output 2 splits: train:eval=3:1.\noutput = proto.Output(\n             split_config=example_gen_pb2.SplitConfig(splits=[\n                 proto.SplitConfig.Split(name='train', hash_buckets=3),\n                 proto.SplitConfig.Split(name='eval', hash_buckets=1)\n             ]))\nexample_gen = CsvExampleGen(input_base=input_dir, output_config=output)\n</code></pre> <p>Notice how the <code>hash_buckets</code> were set in this example.</p> <p>For an input source which has already been split, set the <code>input_config</code> for ExampleGen component:</p> <pre><code># Input train split is 'input_dir/train/*', eval split is 'input_dir/eval/*'.\n# Output splits are generated one-to-one mapping from input splits.\ninput = proto.Input(splits=[\n                example_gen_pb2.Input.Split(name='train', pattern='train/*'),\n                example_gen_pb2.Input.Split(name='eval', pattern='eval/*')\n            ])\nexample_gen = CsvExampleGen(input_base=input_dir, input_config=input)\n</code></pre> <p>For file based example gen (e.g. CsvExampleGen and ImportExampleGen), <code>pattern</code> is a glob relative file pattern that maps to input files with root directory given by input base path. For query-based example gen (e.g. BigQueryExampleGen, PrestoExampleGen), <code>pattern</code> is a SQL query.</p> <p>By default, the entire input base dir is treated as a single input split, and the train and eval output split is generated with a 2:1 ratio.</p> <p>Please refer to proto/example_gen.proto for ExampleGen's input and output split configuration. And refer to downstream components guide for utilizing the custom splits downstream.</p>"},{"location":"guide/examplegen/#splitting-method","title":"Splitting Method","text":"<p>When using <code>hash_buckets</code> splitting method, instead of the entire record, one can use a feature for partitioning the examples. If a feature is present, ExampleGen will use a fingerprint of that feature as the partition key.</p> <p>This feature can be used to maintain a stable split w.r.t. certain properties of examples: for example, a user will always be put in the same split if \"user_id\" were selected as the partition feature name.</p> <p>The interpretation of what a \"feature\" means and how to match a \"feature\" with the specified name depends on the ExampleGen implementation and the type of the examples.</p> <p>For ready-made ExampleGen implementations:</p> <ul> <li>If it generates tf.Example, then a \"feature\" means an entry in     tf.Example.features.feature.</li> <li>If it generates tf.SequenceExample, then a \"feature\" means an entry in     tf.SequenceExample.context.feature.</li> <li>Only int64 and bytes features are supported.</li> </ul> <p>In the following cases, ExampleGen throws runtime errors:</p> <ul> <li>Specified feature name does not exist in the example.</li> <li>Empty feature: <code>tf.train.Feature()</code>.</li> <li>Non supported feature types, e.g., float features.</li> </ul> <p>To output the train/eval split based on a feature in the examples, set the <code>output_config</code> for ExampleGen component. For example:</p> <pre><code># Input has a single split 'input_dir/*'.\n# Output 2 splits based on 'user_id' features: train:eval=3:1.\noutput = proto.Output(\n             split_config=proto.SplitConfig(splits=[\n                 proto.SplitConfig.Split(name='train', hash_buckets=3),\n                 proto.SplitConfig.Split(name='eval', hash_buckets=1)\n             ],\n             partition_feature_name='user_id'))\nexample_gen = CsvExampleGen(input_base=input_dir, output_config=output)\n</code></pre> <p>Notice how the <code>partition_feature_name</code> was set in this example.</p>"},{"location":"guide/examplegen/#span","title":"Span","text":"<p>Note</p> <p>This feature is only available after TFX 0.15.</p> <p>Span can be retrieved by using '{SPAN}' spec in the input glob pattern:</p> <ul> <li>This spec matches digits and maps the data into the relevant SPAN numbers.     For example, 'data_{SPAN}-*.tfrecord' will collect files like     'data_12-a.tfrecord', 'data_12-b.tfrecord'.</li> <li>Optionally, this spec can be specified with the width of the integers when     mapped. For example, 'data_{SPAN:2}.file' maps to files like 'data_02.file'     and 'data_27.file' (as inputs for Span-2 and Span-27 respectively), but does     not map to 'data_1.file' nor 'data_123.file'.</li> <li>When SPAN spec is missing, it's assumed to be always Span '0'.</li> <li>If SPAN is specified, pipeline will process the latest span, and store the     span number in metadata.</li> </ul> <p>For example, let's assume there are input data:</p> <ul> <li>'/tmp/span-1/train/data'</li> <li>'/tmp/span-1/eval/data'</li> <li>'/tmp/span-2/train/data'</li> <li>'/tmp/span-2/eval/data'</li> </ul> <p>and the input config is shown as below:</p> <pre><code>splits {\n  name: 'train'\n  pattern: 'span-{SPAN}/train/*'\n}\nsplits {\n  name: 'eval'\n  pattern: 'span-{SPAN}/eval/*'\n}\n</code></pre> <p>when triggering the pipeline, it will process:</p> <ul> <li>'/tmp/span-2/train/data' as train split</li> <li>'/tmp/span-2/eval/data' as eval split</li> </ul> <p>with span number as '2'. If later on '/tmp/span-3/...' are ready, simply trigger the pipeline again and it will pick up span '3' for processing. Below shows the code example for using span spec:</p> <pre><code>input = proto.Input(splits=[\n                proto.Input.Split(name='train',\n                                            pattern='span-{SPAN}/train/*'),\n                proto.Input.Split(name='eval',\n                                            pattern='span-{SPAN}/eval/*')\n            ])\nexample_gen = CsvExampleGen(input_base='/tmp', input_config=input)\n</code></pre> <p>Retrieving a certain span can be done with RangeConfig, which is detailed below.</p>"},{"location":"guide/examplegen/#date","title":"Date","text":"<p>Note</p> <p>This feature is only available after TFX 0.24.0.</p> <p>If your data source is organized on filesystem by date, TFX supports mapping dates directly to span numbers. There are three specs to represent mapping from dates to spans: {YYYY}, {MM} and {DD}:</p> <ul> <li>The three specs should be altogether present in the     input glob pattern     if any is specified:</li> <li>Either {SPAN} spec or this set of date specs can be specified exclusively.</li> <li>A calendar date with the year from YYYY, the month from MM, and the day of     the month from DD is calculated, then the span number is calculated as as     the number of days since unix epoch (i.e. 1970-01-01). For example,     'log-{YYYY}{MM}{DD}.data' matches to a file 'log-19700101.data' and consumes     it as input for Span-0, and 'log-20170101.data' as input for Span-17167.</li> <li>If this set of date specs is specified, pipeline will process the latest     latest date, and store the corresponding span number in metadata.</li> </ul> <p>For example, let's assume there are input data organized by calendar date:</p> <ul> <li>'/tmp/1970-01-02/train/data'</li> <li>'/tmp/1970-01-02/eval/data'</li> <li>'/tmp/1970-01-03/train/data'</li> <li>'/tmp/1970-01-03/eval/data'</li> </ul> <p>and the input config is shown as below:</p> <pre><code>splits {\n  name: 'train'\n  pattern: '{YYYY}-{MM}-{DD}/train/*'\n}\nsplits {\n  name: 'eval'\n  pattern: '{YYYY}-{MM}-{DD}/eval/*'\n}\n</code></pre> <p>when triggering the pipeline, it will process:</p> <ul> <li>'/tmp/1970-01-03/train/data' as train split</li> <li>'/tmp/1970-01-03/eval/data' as eval split</li> </ul> <p>with span number as '2'. If later on '/tmp/1970-01-04/...' are ready, simply trigger the pipeline again and it will pick up span '3' for processing. Below shows the code example for using date spec:</p> <pre><code>input = proto.Input(splits=[\n                proto.Input.Split(name='train',\n                                            pattern='{YYYY}-{MM}-{DD}/train/*'),\n                proto.Input.Split(name='eval',\n                                            pattern='{YYYY}-{MM}-{DD}/eval/*')\n            ])\nexample_gen = CsvExampleGen(input_base='/tmp', input_config=input)\n</code></pre>"},{"location":"guide/examplegen/#version","title":"Version","text":"<p>Note</p> <p>This feature is only available after TFX 0.24.0.</p> <p>Version can be retrieved by using '{VERSION}' spec in the input glob pattern:</p> <ul> <li>This spec matches digits and maps the data to the relevant VERSION numbers     under the SPAN. Note that the Version spec can be used combination with     either Span or Date spec.</li> <li>This spec can also be optionally specified with the width in the same way as     SPAN spec. e.g. 'span-{SPAN}/version-{VERSION:4}/data-*'.</li> <li>When VERSION spec is missing, version is set to be None.</li> <li>If SPAN and VERSION are both specified, pipeline will process the latest     version for the latest span, and store the version number in metadata.</li> <li>If VERSION is specified, but not SPAN (or date spec), an error will be     thrown.</li> </ul> <p>For example, let's assume there are input data:</p> <ul> <li>'/tmp/span-1/ver-1/train/data'</li> <li>'/tmp/span-1/ver-1/eval/data'</li> <li>'/tmp/span-2/ver-1/train/data'</li> <li>'/tmp/span-2/ver-1/eval/data'</li> <li>'/tmp/span-2/ver-2/train/data'</li> <li>'/tmp/span-2/ver-2/eval/data'</li> </ul> <p>and the input config is shown as below:</p> <pre><code>splits {\n  name: 'train'\n  pattern: 'span-{SPAN}/ver-{VERSION}/train/*'\n}\nsplits {\n  name: 'eval'\n  pattern: 'span-{SPAN}/ver-{VERSION}/eval/*'\n}\n</code></pre> <p>when triggering the pipeline, it will process:</p> <ul> <li>'/tmp/span-2/ver-2/train/data' as train split</li> <li>'/tmp/span-2/ver-2/eval/data' as eval split</li> </ul> <p>with span number as '2' and version number as '2'. If later on '/tmp/span-2/ver-3/...' are ready, simply trigger the pipeline again and it will pick up span '2' and version '3' for processing. Below shows the code example for using version spec:</p> <pre><code>input = proto.Input(splits=[\n                proto.Input.Split(name='train',\n                                            pattern='span-{SPAN}/ver-{VERSION}/train/*'),\n                proto.Input.Split(name='eval',\n                                            pattern='span-{SPAN}/ver-{VERSION}/eval/*')\n            ])\nexample_gen = CsvExampleGen(input_base='/tmp', input_config=input)\n</code></pre>"},{"location":"guide/examplegen/#range-config","title":"Range Config","text":"<p>Note</p> <p>This feature is only available after TFX 0.24.0.</p> <p>TFX supports retrieval and processing of a specific span in file-based ExampleGen using range config, an abstract config used to describe ranges for different TFX entities. To retrieve a specific span, set the <code>range_config</code> for a file-based ExampleGen component. For example, let's assume there are input data:</p> <ul> <li>'/tmp/span-01/train/data'</li> <li>'/tmp/span-01/eval/data'</li> <li>'/tmp/span-02/train/data'</li> <li>'/tmp/span-02/eval/data'</li> </ul> <p>To specifically retrieve and process data with span '1', we specify a range config in addition to the input config. Note that ExampleGen only supports single-span static ranges (to specify processing of specific individual spans). Thus, for StaticRange, start_span_number must equal end_span_number. Using the provided span, and the span width information (if provided) for zero-padding, ExampleGen will replace the SPAN spec in the provided split patterns with the desired span number. An example of usage is shown below:</p> <pre><code># In cases where files have zero-padding, the width modifier in SPAN spec is\n# required so TFX can correctly substitute spec with zero-padded span number.\ninput = proto.Input(splits=[\n                proto.Input.Split(name='train',\n                                            pattern='span-{SPAN:2}/train/*'),\n                proto.Input.Split(name='eval',\n                                            pattern='span-{SPAN:2}/eval/*')\n            ])\n# Specify the span number to be processed here using StaticRange.\nrange = proto.RangeConfig(\n                static_range=proto.StaticRange(\n                        start_span_number=1, end_span_number=1)\n            )\n\n# After substitution, the train and eval split patterns will be\n# 'input_dir/span-01/train/*' and 'input_dir/span-01/eval/*', respectively.\nexample_gen = CsvExampleGen(input_base=input_dir, input_config=input,\n                            range_config=range)\n</code></pre> <p>Range config can also be used to process specific dates, if the date spec is used instead of SPAN spec. For example, let's assume there are input data organized by calendar date:</p> <ul> <li>'/tmp/1970-01-02/train/data'</li> <li>'/tmp/1970-01-02/eval/data'</li> <li>'/tmp/1970-01-03/train/data'</li> <li>'/tmp/1970-01-03/eval/data'</li> </ul> <p>To specifically retrieve and process data on January 2nd, 1970, we do the following:</p> <pre><code>from  tfx.components.example_gen import utils\n\ninput = proto.Input(splits=[\n                proto.Input.Split(name='train',\n                                            pattern='{YYYY}-{MM}-{DD}/train/*'),\n                proto.Input.Split(name='eval',\n                                            pattern='{YYYY}-{MM}-{DD}/eval/*')\n            ])\n# Specify date to be converted to span number to be processed using StaticRange.\nspan = utils.date_to_span_number(1970, 1, 2)\nrange = proto.RangeConfig(\n                static_range=range_config_pb2.StaticRange(\n                        start_span_number=span, end_span_number=span)\n            )\n\n# After substitution, the train and eval split patterns will be\n# 'input_dir/1970-01-02/train/*' and 'input_dir/1970-01-02/eval/*',\n# respectively.\nexample_gen = CsvExampleGen(input_base=input_dir, input_config=input,\n                            range_config=range)\n</code></pre>"},{"location":"guide/examplegen/#custom-examplegen","title":"Custom ExampleGen","text":"<p>If the currently available ExampleGen components don't fit your needs, you can create a custom ExampleGen, which will enable you to read from different data sources or in different data formats.</p>"},{"location":"guide/examplegen/#file-based-examplegen-customization-experimental","title":"File-Based ExampleGen Customization (Experimental)","text":"<p>First, extend BaseExampleGenExecutor with a custom Beam PTransform, which provides the conversion from your train/eval input split to TF examples. For example, the CsvExampleGen executor provides the conversion from an input CSV split to TF examples.</p> <p>Then, create a component with above executor, as done in CsvExampleGen component. Alternatively, pass a custom executor into the standard ExampleGen component as shown below.</p> <pre><code>from tfx.components.base import executor_spec\nfrom tfx.components.example_gen.csv_example_gen import executor\n\nexample_gen = FileBasedExampleGen(\n    input_base=os.path.join(base_dir, 'data/simple'),\n    custom_executor_spec=executor_spec.ExecutorClassSpec(executor.Executor))\n</code></pre> <p>Now, we also support reading Avro and Parquet files using this method.</p>"},{"location":"guide/examplegen/#additional-data-formats","title":"Additional Data Formats","text":"<p>Apache Beam supports reading a number of additional data formats. through Beam I/O Transforms. You can create custom ExampleGen components by leveraging the Beam I/O Transforms using a pattern similar to the Avro example</p> <p></p><pre><code>  return (pipeline\n          | 'ReadFromAvro' &gt;&gt; beam.io.ReadFromAvro(avro_pattern)\n          | 'ToTFExample' &gt;&gt; beam.Map(utils.dict_to_example))\n</code></pre> As of this writing the currently supported formats and data sources for the Beam Python SDK include: <ul> <li>Amazon S3</li> <li>Apache Avro</li> <li>Apache Hadoop</li> <li>Apache Kafka</li> <li>Apache Parquet</li> <li>Google Cloud BigQuery</li> <li>Google Cloud BigTable</li> <li>Google Cloud Datastore</li> <li>Google Cloud Pub/Sub</li> <li>Google Cloud Storage (GCS)</li> <li>MongoDB</li> </ul> <p>Check the Beam docs for the latest list.</p>"},{"location":"guide/examplegen/#query-based-examplegen-customization-experimental","title":"Query-Based ExampleGen Customization (Experimental)","text":"<p>First, extend BaseExampleGenExecutor with a custom Beam PTransform, which reads from the external data source. Then, create a simple component by extending QueryBasedExampleGen.</p> <p>This may or may not require additional connection configurations. For example, the BigQuery executor reads using a default beam.io connector, which abstracts the connection configuration details. The Presto executor, requires a custom Beam PTransform and a custom connection configuration protobuf as input.</p> <p>If a connection configuration is required for a custom ExampleGen component, create a new protobuf and pass it in through custom_config, which is now an optional execution parameter. Below is an example of how to use a configured component.</p> <pre><code>from tfx.examples.custom_components.presto_example_gen.proto import presto_config_pb2\nfrom tfx.examples.custom_components.presto_example_gen.presto_component.component import PrestoExampleGen\n\npresto_config = presto_config_pb2.PrestoConnConfig(host='localhost', port=8080)\nexample_gen = PrestoExampleGen(presto_config, query='SELECT * FROM chicago_taxi_trips')\n</code></pre>"},{"location":"guide/examplegen/#examplegen-downstream-components","title":"ExampleGen Downstream Components","text":"<p>Custom split configuration is supported for downstream components.</p>"},{"location":"guide/examplegen/#statisticsgen","title":"StatisticsGen","text":"<p>Default behavior is to perform stats generation for all splits.</p> <p>To exclude any splits, set the <code>exclude_splits</code> for StatisticsGen component. For example:</p> <pre><code># Exclude the 'eval' split.\nstatistics_gen = StatisticsGen(\n             examples=example_gen.outputs['examples'],\n             exclude_splits=['eval'])\n</code></pre>"},{"location":"guide/examplegen/#schemagen","title":"SchemaGen","text":"<p>Default behavior is to generate a schema based on all splits.</p> <p>To exclude any splits, set the <code>exclude_splits</code> for SchemaGen component. For example:</p> <pre><code># Exclude the 'eval' split.\nschema_gen = SchemaGen(\n             statistics=statistics_gen.outputs['statistics'],\n             exclude_splits=['eval'])\n</code></pre>"},{"location":"guide/examplegen/#examplevalidator","title":"ExampleValidator","text":"<p>Default behavior is to validate the statistics of all splits on input examples against a schema.</p> <p>To exclude any splits, set the <code>exclude_splits</code> for ExampleValidator component. For example:</p> <pre><code># Exclude the 'eval' split.\nexample_validator = ExampleValidator(\n             statistics=statistics_gen.outputs['statistics'],\n             schema=schema_gen.outputs['schema'],\n             exclude_splits=['eval'])\n</code></pre>"},{"location":"guide/examplegen/#transform","title":"Transform","text":"<p>Default behavior is analyze and produce the metadata from the 'train' split and transform all splits.</p> <p>To specify the analyze splits and transform splits, set the <code>splits_config</code> for Transform component. For example:</p> <pre><code># Analyze the 'train' split and transform all splits.\ntransform = Transform(\n      examples=example_gen.outputs['examples'],\n      schema=schema_gen.outputs['schema'],\n      module_file=_taxi_module_file,\n      splits_config=proto.SplitsConfig(analyze=['train'],\n                                               transform=['train', 'eval']))\n</code></pre>"},{"location":"guide/examplegen/#trainer-and-tuner","title":"Trainer and Tuner","text":"<p>Default behavior is train on the 'train' split and evaluate on the 'eval' split.</p> <p>To specify the train splits and evaluate splits, set the <code>train_args</code> and <code>eval_args</code> for Trainer component. For example:</p> <pre><code># Train on the 'train' split and evaluate on the 'eval' split.\nTrainer = Trainer(\n      module_file=_taxi_module_file,\n      examples=transform.outputs['transformed_examples'],\n      schema=schema_gen.outputs['schema'],\n      transform_graph=transform.outputs['transform_graph'],\n      train_args=proto.TrainArgs(splits=['train'], num_steps=10000),\n      eval_args=proto.EvalArgs(splits=['eval'], num_steps=5000))\n</code></pre>"},{"location":"guide/examplegen/#evaluator","title":"Evaluator","text":"<p>Default behavior is provide metrics computed on the 'eval' split.</p> <p>To compute evaluation statistics on custom splits, set the <code>example_splits</code> for Evaluator component. For example:</p> <pre><code># Compute metrics on the 'eval1' split and the 'eval2' split.\nevaluator = Evaluator(\n      examples=example_gen.outputs['examples'],\n      model=trainer.outputs['model'],\n      example_splits=['eval1', 'eval2'])\n</code></pre> <p>More details are available in the CsvExampleGen API reference, FileBasedExampleGen API implementation, and ImportExampleGen API reference.</p>"},{"location":"guide/exampleval/","title":"The ExampleValidator TFX Pipeline Component","text":"<p>The ExampleValidator pipeline component identifies anomalies in training and serving data. It can detect different classes of anomalies in the data. For example it can:</p> <ol> <li>perform validity checks by comparing data statistics against a schema that     codifies expectations of the user.</li> <li>detect training-serving skew by comparing training and serving     data.</li> <li>detect data drift by looking at a series of data.</li> <li>perform custom validations using a SQL-based configuration.</li> </ol> <p>The ExampleValidator pipeline component identifies any anomalies in the example data by comparing data statistics computed by the StatisticsGen pipeline component against a schema. The inferred schema codifies properties which the input data is expected to satisfy, and can be modified by the developer.</p> <ul> <li>Consumes: A schema from a SchemaGen component, and statistics from a StatisticsGen component.</li> <li>Emits: Validation results</li> </ul>"},{"location":"guide/exampleval/#examplevalidator-and-tensorflow-data-validation","title":"ExampleValidator and TensorFlow Data Validation","text":"<p>ExampleValidator makes extensive use of TensorFlow Data Validation for validating your input data.</p>"},{"location":"guide/exampleval/#using-the-examplevalidator-component","title":"Using the ExampleValidator Component","text":"<p>An ExampleValidator pipeline component is typically very easy to deploy and requires little customization. Typical code looks like this:</p> <pre><code>validate_stats = ExampleValidator(\n      statistics=statistics_gen.outputs['statistics'],\n      schema=schema_gen.outputs['schema']\n      )\n</code></pre> <p>More details are available in the ExampleValidator API reference.</p>"},{"location":"guide/fairness_indicators/","title":"Fairness Indicators","text":"<p>Fairness Indicators is designed to support teams in evaluating and improving models for fairness concerns in partnership with the broader Tensorflow toolkit. The tool is currently actively used internally by many of our products, and is now available in BETA to try for your own use cases.</p> <p></p>"},{"location":"guide/fairness_indicators/#what-is-fairness-indicators","title":"What is Fairness Indicators?","text":"<p>Fairness Indicators is a library that enables easy computation of commonly-identified fairness metrics for binary and multiclass classifiers. Many existing tools for evaluating fairness concerns don\u2019t work well on large scale datasets and models. At Google, it is important for us to have tools that can work on billion-user systems. Fairness Indicators will allow you to evaluate across any size of use case.</p> <p>In particular, Fairness Indicators includes the ability to:</p> <ul> <li>Evaluate the distribution of datasets</li> <li>Evaluate model performance, sliced across defined groups of users<ul> <li>Feel confident about your results with confidence intervals and evals at     multiple thresholds</li> </ul> </li> <li>Dive deep into individual slices to explore root causes and opportunities     for improvement</li> </ul> <p>The pip package download includes:</p> <ul> <li>Tensorflow Data Validation (TFDV)</li> <li>Tensorflow Model Analysis (TFMA)<ul> <li>Fairness Indicators</li> </ul> </li> <li>The What-If Tool (WIT)</li> </ul>"},{"location":"guide/fairness_indicators/#using-fairness-indicators-with-tensorflow-models","title":"Using Fairness Indicators with Tensorflow Models","text":""},{"location":"guide/fairness_indicators/#data","title":"Data","text":"<p>To run Fairness Indicators with TFMA, make sure the evaluation dataset is labelled for the features you would like to slice by. If you don't have the exact slice features for your fairness concerns, you may explore trying to find an evaluation set that does, or considering proxy features within your feature set that may highlight outcome disparities. For additional guidance, see here.</p>"},{"location":"guide/fairness_indicators/#configuring-slices","title":"Configuring Slices","text":"<p>Next, define the slices you would like to evaluate on:</p> <pre><code>slice_spec = [\n  tfma.slicer.SingleSliceSpec(columns=[\u2018fur color\u2019])\n]\n</code></pre> <p>If you want to evaluate intersectional slices (for example, both fur color and height), you can set the following:</p> <pre><code>slice_spec = [\n  tfma.slicer.SingleSliceSpec(columns=[\u2018fur_color\u2019, \u2018height\u2019])\n]`\n</code></pre>"},{"location":"guide/fairness_indicators/#compute-fairness-metrics","title":"Compute Fairness Metrics","text":"<p>Add a Fairness Indicators callback to the <code>metrics_callback</code> list. In the callback, you can define a list of thresholds that the model will be evaluated at.</p> <pre><code>from tensorflow_model_analysis.addons.fairness.post_export_metrics import fairness_indicators\n\n# Build the fairness metrics. Besides the thresholds, you also can config the example_weight_key, labels_key here. For more details, please check the api.\nmetrics_callbacks = \\\n    [tfma.post_export_metrics.fairness_indicators(thresholds=[0.1, 0.3,\n     0.5, 0.7, 0.9])]\n\neval_shared_model = tfma.default_eval_shared_model(\n    eval_saved_model_path=tfma_export_dir,\n    add_metrics_callbacks=metrics_callbacks)\n</code></pre> <p>Before running the config, determine whether or not you want to enable computation of confidence intervals. Confidence intervals are computed using Poisson bootstrapping and require recomputation over 20 samples.</p> <pre><code>compute_confidence_intervals = True\n</code></pre> <p>Run the TFMA evaluation pipeline:</p> <pre><code>validate_dataset = tf.data.TFRecordDataset(filenames=[validate_tf_file])\n\n# Run the fairness evaluation.\nwith beam.Pipeline() as pipeline:\n  _ = (\n      pipeline\n      | beam.Create([v.numpy() for v in validate_dataset])\n      | 'ExtractEvaluateAndWriteResults' &gt;&gt;\n       tfma.ExtractEvaluateAndWriteResults(\n                 eval_shared_model=eval_shared_model,\n                 slice_spec=slice_spec,\n                 compute_confidence_intervals=compute_confidence_intervals,\n                 output_path=tfma_eval_result_path)\n  )\neval_result = tfma.load_eval_result(output_path=tfma_eval_result_path)\n</code></pre>"},{"location":"guide/fairness_indicators/#render-fairness-indicators","title":"Render Fairness Indicators","text":"<pre><code>from tensorflow_model_analysis.addons.fairness.view import widget_view\n\nwidget_view.render_fairness_indicator(eval_result=eval_result)\n</code></pre> <p>Tips for using Fairness Indicators:</p> <ul> <li>Select metrics to display by checking the boxes on the left hand side.     Individual graphs for each of the metrics will appear in the widget, in     order.</li> <li>Change the baseline slice, the first bar on the graph, using the     dropdown selector. Deltas will be calculated with this baseline value.</li> <li>Select thresholds using the dropdown selector. You can view multiple     thresholds on the same graph. Selected thresholds will be bolded, and you     can click a bolded threshold to un-select it.</li> <li>Hover over a bar to see metrics for that slice.</li> <li>Identify disparities with the baseline using the \"Diff w. baseline\"     column, which identifies the percentage difference between the current slice     and the baseline.</li> <li>Explore the data points of a slice in depth using the     What-If Tool. See     here for an example.</li> </ul>"},{"location":"guide/fairness_indicators/#rendering-fairness-indicators-for-multiple-models","title":"Rendering Fairness Indicators for Multiple Models","text":"<p>Fairness Indicators can also be used to compare models. Instead of passing in a single eval_result, pass in a multi_eval_results object, which is a dictionary mapping two model names to eval_result objects.</p> <pre><code>from tensorflow_model_analysis.addons.fairness.view import widget_view\n\neval_result1 = tfma.load_eval_result(...)\neval_result2 = tfma.load_eval_result(...)\nmulti_eval_results = {\"MyFirstModel\": eval_result1, \"MySecondModel\": eval_result2}\n\nwidget_view.render_fairness_indicator(multi_eval_results=multi_eval_results)\n</code></pre> <p></p> <p>Model comparison can be used alongside threshold comparison. For example, you can compare two models at two sets of thresholds to find the optimal combination for your fairness metrics.</p>"},{"location":"guide/fairness_indicators/#using-fairness-indicators-with-non-tensorflow-models","title":"Using Fairness Indicators with non-TensorFlow Models","text":"<p>To better support clients that have different models and workflows, we have developed an evaluation library which is agnostic to the model being evaluated.</p> <p>Anyone who wants to evaluate their machine learning system can use this, especially if you have non-TensorFlow based models. Using the Apache Beam Python SDK, you can create a standalone TFMA evaluation binary and then run it to analyze your model.</p>"},{"location":"guide/fairness_indicators/#data_1","title":"Data","text":"<p>This step is to provide the dataset you want the evaluations to run on. It should be in tf.Example proto format having labels, predictions and other features you might want to slice on.</p> <pre><code>tf.Example {\n    features {\n        feature {\n          key: \"fur_color\" value { bytes_list { value: \"gray\" } }\n        }\n        feature {\n          key: \"height\" value { bytes_list { value: \"tall\" } }\n        }\n        feature {\n          key: \"prediction\" value { float_list { value: 0.9 } }\n        }\n        feature {\n          key: \"label\" value { float_list { value: 1.0 } }\n        }\n    }\n}\n</code></pre>"},{"location":"guide/fairness_indicators/#model","title":"Model","text":"<p>Instead of specifying a model, you an create a model agnostic eval config and extractor to parse and provide the data TFMA needs to compute metrics. ModelAgnosticConfig spec defines the features, predictions, and labels to be used from the input examples.</p> <p>For this, create a feature map with keys representing all the features including label and prediction keys and values representing the data type of the feature.</p> <pre><code>feature_map[label_key] = tf.FixedLenFeature([], tf.float32, default_value=[0])\n</code></pre> <p>Create a model agnostic config using label keys, prediction keys and the feature map.</p> <pre><code>model_agnostic_config = model_agnostic_predict.ModelAgnosticConfig(\n    label_keys=list(ground_truth_labels),\n    prediction_keys=list(predition_labels),\n    feature_spec=feature_map)\n</code></pre>"},{"location":"guide/fairness_indicators/#set-up-model-agnostic-extractor","title":"Set up Model Agnostic Extractor","text":"<p>Extractor is used to extract the features, labels and predictions from the input using model agnostic config. And if you want to slice your data, you also need to define the slice key spec, containing information about the columns you want to slice on.</p> <pre><code>model_agnostic_extractors = [\n    model_agnostic_extractor.ModelAgnosticExtractor(\n        model_agnostic_config=model_agnostic_config, desired_batch_size=3),\n    slice_key_extractor.SliceKeyExtractor([\n        slicer.SingleSliceSpec(),\n        slicer.SingleSliceSpec(columns=[\u2018height\u2019]),\n    ])\n]\n</code></pre>"},{"location":"guide/fairness_indicators/#compute-fairness-metrics_1","title":"Compute Fairness Metrics","text":"<p>As part of EvalSharedModel, you can provide all the metrics on which you want your model to be evaluated. Metrics are provided in the form of metrics callbacks like the ones defined in post_export_metrics or fairness_indicators.</p> <pre><code>metrics_callbacks.append(\n    post_export_metrics.fairness_indicators(\n        thresholds=[0.5, 0.9],\n        target_prediction_keys=[prediction_key],\n        labels_key=label_key))\n</code></pre> <p>It also takes in a <code>construct_fn</code> which is used to create a tensorflow graph to perform the evaluation.</p> <pre><code>eval_shared_model = types.EvalSharedModel(\n    add_metrics_callbacks=metrics_callbacks,\n    construct_fn=model_agnostic_evaluate_graph.make_construct_fn(\n        add_metrics_callbacks=metrics_callbacks,\n        fpl_feed_config=model_agnostic_extractor\n        .ModelAgnosticGetFPLFeedConfig(model_agnostic_config)))\n</code></pre> <p>Once everything is set up, use one of <code>ExtractEvaluate</code> or <code>ExtractEvaluateAndWriteResults</code> functions provided by model_eval_lib to evaluate the model.</p> <pre><code>_ = (\n    examples |\n    'ExtractEvaluateAndWriteResults' &gt;&gt;\n        model_eval_lib.ExtractEvaluateAndWriteResults(\n        eval_shared_model=eval_shared_model,\n        output_path=output_path,\n        extractors=model_agnostic_extractors))\n\neval_result = tensorflow_model_analysis.load_eval_result(output_path=tfma_eval_result_path)\n</code></pre> <p>Finally, render Fairness Indicators using the instructions from the \"Render Fairness Indicators\" section above.</p>"},{"location":"guide/fairness_indicators/#more-examples","title":"More Examples","text":"<p>The Fairness Indicators examples directory contains several examples:</p> <ul> <li>Fairness_Indicators_Example_Colab.ipynb     gives an overview of Fairness Indicators in     TensorFlow Model Analysis and     how to use it with a real dataset. This notebook also goes over     TensorFlow Data Validation     and What-If Tool, two tools for     analyzing TensorFlow models that are packaged with Fairness Indicators.</li> <li>Fairness_Indicators_on_TF_Hub.ipynb     demonstrates how to use Fairness Indicators to compare models trained on     different text embeddings.     This notebook uses text embeddings from     TensorFlow Hub, TensorFlow's library to     publish, discover, and reuse model components.</li> <li>Fairness_Indicators_TensorBoard_Plugin_Example_Colab.ipynb     demonstrates how to visualize Fairness Indicators in TensorBoard.</li> </ul>"},{"location":"guide/infra_validator/","title":"The InfraValidator TFX Pipeline Component","text":"<p>InfraValidator is a TFX component that is used as an early warning layer before pushing a model into production. The name \"infra\" validator came from the fact that it is validating the model in the actual model serving \"infrastructure\". If Evaluator is to guarantee the performance of the model, InfraValidator is to guarantee the model is mechanically fine and prevents bad models from being pushed.</p>"},{"location":"guide/infra_validator/#how-does-it-work","title":"How does it work?","text":"<p>InfraValidator takes the model, launches a sand-boxed model server with the model, and sees if it can be successfully loaded and optionally queried. The infra validation result will be generated in the <code>blessing</code> output in the same way as Evaluator does.</p> <p>InfraValidator focuses on the compatibility between the model server binary (e.g. TensorFlow Serving) and the model to deploy. Despite the name \"infra\" validator, it is the user's responsibility to configure the environment correctly, and infra validator only interacts with the model server in the user-configured environment to see if it works fine. Configuring this environment correctly will ensure that infra validation passing or failing will be indicative of whether the model would be servable in the production serving environment. This implies some of, but is not limited to, the following:</p> <ol> <li>InfraValidator is using the same model server binary as will be used in     production. This is the minimal level to which the infra validation     environment must converge.</li> <li>InfraValidator is using the same resources (e.g. allocation quantity and     type of CPU, memory, and accelerators) as will be used in production.</li> <li>InfraValidator is using the same model server configuration as will be used     in production.</li> </ol> <p>Depending on the situation, users can choose to what degree InfraValidator should be identical to the production environment. Technically, a model can be infra validated in a local Docker environment and then served in a completely different environment (e.g. Kubernetes cluster) without a problem. However, InfraValidator will not have checked for this divergence.</p>"},{"location":"guide/infra_validator/#operation-mode","title":"Operation mode","text":"<p>Depending on the configuration, infra validation is done in one of the following modes:</p> <ul> <li><code>LOAD_ONLY</code> mode: checking whether the model was successfully loaded in the     serving infrastructure or not. OR</li> <li><code>LOAD_AND_QUERY</code> mode: <code>LOAD_ONLY</code> mode plus sending some sample requests to     check if model is capable of serving inferences. InfraValidator does not     care the prediction was correct or not. Only whether the request was     successful or not matters.</li> </ul>"},{"location":"guide/infra_validator/#how-do-i-use-it","title":"How do I use it?","text":"<p>Usually InfraValidator is defined next to an Evaluator component, and its output is fed to a Pusher. If InfraValidator fails, the model will not be pushed.</p> <pre><code>evaluator = Evaluator(\n    model=trainer.outputs['model'],\n    examples=example_gen.outputs['examples'],\n    baseline_model=model_resolver.outputs['model'],\n    eval_config=tfx.proto.EvalConfig(...)\n)\n\ninfra_validator = InfraValidator(\n    model=trainer.outputs['model'],\n    serving_spec=tfx.proto.ServingSpec(...)\n)\n\npusher = Pusher(\n    model=trainer.outputs['model'],\n    model_blessing=evaluator.outputs['blessing'],\n    infra_blessing=infra_validator.outputs['blessing'],\n    push_destination=tfx.proto.PushDestination(...)\n)\n</code></pre>"},{"location":"guide/infra_validator/#configuring-an-infravalidator-component","title":"Configuring an InfraValidator component.","text":"<p>There are three kinds of protos to configure InfraValidator.</p>"},{"location":"guide/infra_validator/#servingspec","title":"<code>ServingSpec</code>","text":"<p><code>ServingSpec</code> is the most crucial configuration for the InfraValidator. It defines:</p> <ul> <li>what type of model server to run</li> <li>where to run it</li> </ul> <p>For model server types (called serving binary) we support</p> <ul> <li>TensorFlow Serving</li> </ul> <p>Note</p> <p>InfraValidator allows specifying multiple versions of the same model server type in order to upgrade the model server version without affecting model compatibility. For example, user can test <code>tensorflow/serving</code> image with both <code>2.1.0</code> and <code>latest</code> versions, to ensure the model will be compatible with the latest <code>tensorflow/serving</code> version as well.</p> <p>Following serving platforms are currently supported:</p> <ul> <li>Local Docker (Docker should be installed in advance)</li> <li>Kubernetes (limited support for KubeflowDagRunner only)</li> </ul> <p>The choice for serving binary and serving platform are made by specifying a <code>oneof</code> block of the <code>ServingSpec</code>. For example to use TensorFlow Serving binary running on the Kubernetes cluster, <code>tensorflow_serving</code> and <code>kubernetes</code> field should be set.</p> <pre><code>infra_validator=InfraValidator(\n    model=trainer.outputs['model'],\n    serving_spec=tfx.proto.ServingSpec(\n        tensorflow_serving=tfx.proto.TensorFlowServing(\n            tags=['latest']\n        ),\n        kubernetes=tfx.proto.KubernetesConfig()\n    )\n)\n</code></pre> <p>To further configure <code>ServingSpec</code>, please check out the protobuf definition.</p>"},{"location":"guide/infra_validator/#validationspec","title":"<code>ValidationSpec</code>","text":"<p>Optional configuration to adjust the infra validation criteria or workflow.</p> <pre><code>infra_validator=InfraValidator(\n    model=trainer.outputs['model'],\n    serving_spec=tfx.proto.ServingSpec(...),\n    validation_spec=tfx.proto.ValidationSpec(\n        # How much time to wait for model to load before automatically making\n        # validation fail.\n        max_loading_time_seconds=60,\n        # How many times to retry if infra validation fails.\n        num_tries=3\n    )\n)\n</code></pre> <p>All ValidationSpec fields have a sound default value. Check more detail from the protobuf definition.</p>"},{"location":"guide/infra_validator/#requestspec","title":"<code>RequestSpec</code>","text":"<p>Optional configuration to specify how to build sample requests when running infra validation in <code>LOAD_AND_QUERY</code> mode. In order to use <code>LOAD_AND_QUERY</code> mode, it is required to specify both <code>request_spec</code> execution properties as well as <code>examples</code> input channel in the component definition.</p> <pre><code>infra_validator = InfraValidator(\n    model=trainer.outputs['model'],\n    # This is the source for the data that will be used to build a request.\n    examples=example_gen.outputs['examples'],\n    serving_spec=tfx.proto.ServingSpec(\n        # Depending on what kind of model server you're using, RequestSpec\n        # should specify the compatible one.\n        tensorflow_serving=tfx.proto.TensorFlowServing(tags=['latest']),\n        local_docker=tfx.proto.LocalDockerConfig(),\n    ),\n    request_spec=tfx.proto.RequestSpec(\n        # InfraValidator will look at how \"classification\" signature is defined\n        # in the model, and automatically convert some samples from `examples`\n        # artifact to prediction RPC requests.\n        tensorflow_serving=tfx.proto.TensorFlowServingRequestSpec(\n            signature_names=['classification']\n        ),\n        num_examples=10  # How many requests to make.\n    )\n)\n</code></pre>"},{"location":"guide/infra_validator/#producing-a-savedmodel-with-warmup","title":"Producing a SavedModel with warmup","text":"<p>(From version 0.30.0)</p> <p>Since InfraValidator validates model with real requests, it can easily reuse these validation requests as warmup requests of a SavedModel. InfraValidator provides an option (<code>RequestSpec.make_warmup</code>) to export a SavedModel with warmup.</p> <pre><code>infra_validator = InfraValidator(\n    ...,\n    request_spec=tfx.proto.RequestSpec(..., make_warmup=True)\n)\n</code></pre> <p>Then the output <code>InfraBlessing</code> artifact will contain a SavedModel with warmup, and can also be pushed by the Pusher, just like <code>Model</code> artifact.</p>"},{"location":"guide/infra_validator/#limitations","title":"Limitations","text":"<p>Current InfraValidator is not complete yet, and has some limitations.</p> <ul> <li>Only TensorFlow SavedModel model format can be     validated.</li> <li>When running TFX on Kubernetes, the pipeline should be executed by     <code>KubeflowDagRunner</code> inside Kubeflow Pipelines. The model server will be     launched in the same Kubernetes cluster and the namespace that Kubeflow is     using.</li> <li>InfraValidator is primarily focused on deployments to     TensorFlow Serving, and while still useful it is less accurate     for deployments to TensorFlow Lite and TensorFlow.js, or     other inference frameworks.</li> <li> <p>There's a limited support on <code>LOAD_AND_QUERY</code> mode for the     Predict     method signature (which is the only exportable method in TensorFlow 2).     InfraValidator requires the Predict signature to consume a serialized     <code>tf.Example</code> as the only input.</p> <pre><code>@tf.function\ndef parse_and_run(serialized_example):\n  features = tf.io.parse_example(serialized_example, FEATURES)\n  return model(features)\n\nmodel.save('path/to/save', signatures={\n  # This exports \"Predict\" method signature under name \"serving_default\".\n  'serving_default': parse_and_run.get_concrete_function(\n      tf.TensorSpec(shape=[None], dtype=tf.string, name='examples'))\n})\n</code></pre> <ul> <li>Check out an     Penguin example     sample code to see how this signature interacts with other components in     TFX.</li> </ul> </li> </ul>"},{"location":"guide/keras/","title":"TensorFlow 2.x in TFX","text":"<p>TensorFlow 2.0 was released in 2019, with tight integration of Keras, eager execution by default, and Pythonic function execution, among other new features and improvements.</p> <p>This guide provides a comprehensive technical overview of TF 2.x in TFX.</p>"},{"location":"guide/keras/#which-version-to-use","title":"Which version to use?","text":"<p>TFX is compatible with TensorFlow 2.x, and the high-level APIs that existed in TensorFlow 1.x (particularly Estimators) continue to work.</p>"},{"location":"guide/keras/#start-new-projects-in-tensorflow-2x","title":"Start new projects in TensorFlow 2.x","text":"<p>Since TensorFlow 2.x retains the high-level capabilities of TensorFlow 1.x, there is no advantage to using the older version on new projects, even if you don't plan to use the new features.</p> <p>Therefore, if you are starting a new TFX project, we recommend that you use TensorFlow 2.x. You may want to update your code later as full support for Keras and other new features become available, and the scope of changes will be much more limited if you start with TensorFlow 2.x, rather than trying to upgrade from TensorFlow 1.x in the future.</p>"},{"location":"guide/keras/#converting-existing-projects-to-tensorflow-2x","title":"Converting existing projects to TensorFlow 2.x","text":"<p>Code written for TensorFlow 1.x is largely compatible with TensorFlow 2.x and will continue to work in TFX.</p> <p>However, if you'd like to take advantage of improvements and new features as they become available in TF 2.x, you can follow the instructions for migrating to TF 2.x.</p>"},{"location":"guide/keras/#estimator","title":"Estimator","text":"<p>The Estimator API has been fully dropped since TensorFlow 2.16, we decided to discontinue the support for it.</p>"},{"location":"guide/keras/#native-keras-ie-keras-without-estimator","title":"Native Keras (i.e. Keras without Estimator)","text":"<p>Note</p> <p>Full support for all features in Keras is in progress, in most cases, Keras in TFX will work as expected. It does not yet work with Sparse Features for FeatureColumns.</p>"},{"location":"guide/keras/#examples-and-colab","title":"Examples and Colab","text":"<p>Here are several examples with native Keras:</p> <ul> <li>Penguin     (module file):     'Hello world' end-to-end example.</li> <li>MNIST     (module file):     Image end-to-end example.</li> <li>Taxi     (module file):     end-to-end example with advanced Transform usage.</li> </ul> <p>We also have a per-component Keras Colab.</p>"},{"location":"guide/keras/#tfx-components","title":"TFX Components","text":"<p>The following sections explain how related TFX components support native Keras.</p>"},{"location":"guide/keras/#transform","title":"Transform","text":"<p>Transform currently has experimental support for Keras models.</p> <p>The Transform component itself can be used for native Keras without change. The <code>preprocessing_fn</code> definition remains the same, using TensorFlow and tf.Transform ops.</p> <p>The serving function and eval function are changed for native Keras. Details will be discussed in the following Trainer and Evaluator sections.</p> <p>Note</p> <p>Transformations within the <code>preprocessing_fn</code> cannot be applied to the label feature for training or eval.</p>"},{"location":"guide/keras/#trainer","title":"Trainer","text":""},{"location":"guide/keras/#keras-module-file-with-transform","title":"Keras Module file with Transform","text":"<p>The training module file must contains a <code>run_fn</code> which will be called by the <code>GenericExecutor</code>, a typical Keras <code>run_fn</code> would look like this:</p> <pre><code>def run_fn(fn_args: TrainerFnArgs):\n  \"\"\"Train the model based on given args.\n\n  Args:\n    fn_args: Holds args used to train the model as name/value pairs.\n  \"\"\"\n  tf_transform_output = tft.TFTransformOutput(fn_args.transform_output)\n\n  # Train and eval files contains transformed examples.\n  # _input_fn read dataset based on transformed schema from tft.\n  train_dataset = _input_fn(fn_args.train_files, fn_args.data_accessor,\n                            tf_transform_output.transformed_metadata.schema)\n  eval_dataset = _input_fn(fn_args.eval_files, fn_args.data_accessor,\n                           tf_transform_output.transformed_metadata.schema)\n\n  model = _build_keras_model()\n\n  model.fit(\n      train_dataset,\n      steps_per_epoch=fn_args.train_steps,\n      validation_data=eval_dataset,\n      validation_steps=fn_args.eval_steps)\n\n  signatures = {\n      'serving_default':\n          _get_serve_tf_examples_fn(model,\n                                    tf_transform_output).get_concrete_function(\n                                        tf.TensorSpec(\n                                            shape=[None],\n                                            dtype=tf.string,\n                                            name='examples')),\n  }\n  model.save(fn_args.serving_model_dir, save_format='tf', signatures=signatures)\n</code></pre> <p>In the <code>run_fn</code> above, a serving signature is needed when exporting the trained model so that model can take raw examples for prediction. A typical serving function would look like this:</p> <pre><code>def _get_serve_tf_examples_fn(model, tf_transform_output):\n  \"\"\"Returns a function that parses a serialized tf.Example.\"\"\"\n\n  # the layer is added as an attribute to the model in order to make sure that\n  # the model assets are handled correctly when exporting.\n  model.tft_layer = tf_transform_output.transform_features_layer()\n\n  @tf.function\n  def serve_tf_examples_fn(serialized_tf_examples):\n    \"\"\"Returns the output to be used in the serving signature.\"\"\"\n    feature_spec = tf_transform_output.raw_feature_spec()\n    feature_spec.pop(_LABEL_KEY)\n    parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)\n\n    transformed_features = model.tft_layer(parsed_features)\n\n    return model(transformed_features)\n\n  return serve_tf_examples_fn\n</code></pre> <p>In above serving function, tf.Transform transformations need to be applied to the raw data for inference, using the <code>tft.TransformFeaturesLayer</code> layer. The previous <code>_serving_input_receiver_fn</code> which was required for Estimators will no longer be needed with Keras.</p>"},{"location":"guide/keras/#keras-module-file-without-transform","title":"Keras Module file without Transform","text":"<p>This is similar to the module file shown above, but without the transformations:</p> <pre><code>def _get_serve_tf_examples_fn(model, schema):\n\n  @tf.function\n  def serve_tf_examples_fn(serialized_tf_examples):\n    feature_spec = _get_raw_feature_spec(schema)\n    feature_spec.pop(_LABEL_KEY)\n    parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)\n    return model(parsed_features)\n\n  return serve_tf_examples_fn\n\n\ndef run_fn(fn_args: TrainerFnArgs):\n  schema = io_utils.parse_pbtxt_file(fn_args.schema_file, schema_pb2.Schema())\n\n  # Train and eval files contains raw examples.\n  # _input_fn reads the dataset based on raw data schema.\n  train_dataset = _input_fn(fn_args.train_files, fn_args.data_accessor, schema)\n  eval_dataset = _input_fn(fn_args.eval_files, fn_args.data_accessor, schema)\n\n  model = _build_keras_model()\n\n  model.fit(\n      train_dataset,\n      steps_per_epoch=fn_args.train_steps,\n      validation_data=eval_dataset,\n      validation_steps=fn_args.eval_steps)\n\n  signatures = {\n      'serving_default':\n          _get_serve_tf_examples_fn(model, schema).get_concrete_function(\n              tf.TensorSpec(shape=[None], dtype=tf.string, name='examples')),\n  }\n  model.save(fn_args.serving_model_dir, save_format='tf', signatures=signatures)\n</code></pre>"},{"location":"guide/keras/#tfdistributestrategy","title":"tf.distribute.Strategy","text":"<p>At this time TFX only supports single worker strategies (e.g., MirroredStrategy, OneDeviceStrategy).</p> <p>To use a distribution strategy, create an appropriate tf.distribute.Strategy and move the creation and compiling of the Keras model inside a strategy scope.</p> <p>For example, replace above <code>model = _build_keras_model()</code> with:</p> <pre><code>  mirrored_strategy = tf.distribute.MirroredStrategy()\n  with mirrored_strategy.scope():\n    model = _build_keras_model()\n\n  # Rest of the code can be unchanged.\n  model.fit(...)\n</code></pre> <p>To verify the device (CPU/GPU) used by <code>MirroredStrategy</code>, enable info level tensorflow logging:</p> <pre><code>import logging\nlogging.getLogger(\"tensorflow\").setLevel(logging.INFO)\n</code></pre> <p>and you should be able to see <code>Using MirroredStrategy with devices (...)</code> in the log.</p> <p>Note</p> <p>The environment variable <code>TF_FORCE_GPU_ALLOW_GROWTH=true</code> might be needed for a GPU out of memory issue. For details, please refer to tensorflow GPU guide.</p>"},{"location":"guide/keras/#evaluator","title":"Evaluator","text":"<p>In TFMA v0.2x, ModelValidator and Evaluator have been combined into a single new Evaluator component. The new Evaluator component can perform both single model evaluation and also validate the current model compared with previous models. With this change, the Pusher component now consumes a blessing result from Evaluator instead of ModelValidator.</p> <p>See Evaluator for more information.</p>"},{"location":"guide/kubeflow/","title":"Orchestrating TFX Pipelines","text":""},{"location":"guide/kubeflow/#kubeflow-pipelines","title":"Kubeflow Pipelines","text":"<p>Kubeflow is an open source ML platform dedicated to making deployments of machine learning (ML) workflows on Kubernetes simple, portable and scalable. Kubeflow Pipelines is part of the Kubeflow platform that enables composition and execution of reproducible workflows on Kubeflow, integrated with experimentation and notebook based experiences. Kubeflow Pipelines services on Kubernetes include the hosted Metadata store, container based orchestration engine, notebook server, and UI to help users develop, run, and manage complex ML pipelines at scale. The Kubeflow Pipelines SDK allows for creation and sharing of components and composition and of pipelines programmatically.</p> <p>See the TFX example on Kubeflow Pipelines for details on running TFX at scale on Google cloud.</p>"},{"location":"guide/local_orchestrator/","title":"Orchestrating TFX Pipelines","text":""},{"location":"guide/local_orchestrator/#local-orchestrator","title":"Local Orchestrator","text":"<p>Local orchestrator is a simple orchestrator that is included in the TFX Python package. It runs pipelines in the local environment in a single process. It provides fast iterations for development and debugging, but it is not suitable for large production workloads. Please use Vertex Pipelines or Kubeflow Pipelines for production use cases.</p> <p>Try the TFX tutorials running in Colab to learn how to use the local orchestrator.</p>"},{"location":"guide/mlmd/","title":"ML Metadata","text":"<p>ML Metadata (MLMD) is a library for recording and retrieving metadata associated with ML developer and data scientist workflows. MLMD is an integral part of TensorFlow Extended (TFX), but is designed so that it can be used independently.</p> <p>Every run of a production ML pipeline generates metadata containing information about the various pipeline components, their executions (e.g. training runs), and resulting artifacts (e.g. trained models). In the event of unexpected pipeline behavior or errors, this metadata can be leveraged to analyze the lineage of pipeline components and debug issues. Think of this metadata as the equivalent of logging in software development.</p> <p>MLMD helps you understand and analyze all the interconnected parts of your ML pipeline instead of analyzing them in isolation and can help you answer questions about your ML pipeline such as:</p> <ul> <li>Which dataset did the model train on?</li> <li>What were the hyperparameters used to train the model?</li> <li>Which pipeline run created the model?</li> <li>Which training run led to this model?</li> <li>Which version of TensorFlow created this model?</li> <li>When was the failed model pushed?</li> </ul>"},{"location":"guide/mlmd/#metadata-store","title":"Metadata store","text":"<p>MLMD registers the following types of metadata in a database called the Metadata Store.</p> <ol> <li>Metadata about the artifacts generated through the components/steps of your     ML pipelines</li> <li>Metadata about the executions of these components/steps</li> <li>Metadata about pipelines and associated lineage information</li> </ol> <p>The Metadata Store provides APIs to record and retrieve metadata to and from the storage backend. The storage backend is pluggable and can be extended. MLMD provides reference implementations for SQLite (which supports in-memory and disk) and MySQL out of the box.</p> <p>This graphic shows a high-level overview of the various components that are part of MLMD.</p> <p></p>"},{"location":"guide/mlmd/#metadata-storage-backends-and-store-connection-configuration","title":"Metadata storage backends and store connection configuration","text":"<p>The <code>MetadataStore</code> object receives a connection configuration that corresponds to the storage backend used.</p> <ul> <li>Fake Database provides an in-memory DB (using SQLite) for fast     experimentation and local runs. The database is deleted when the store     object is destroyed.</li> </ul> <pre><code>import ml_metadata as mlmd\nfrom ml_metadata.metadata_store import metadata_store\nfrom ml_metadata.proto import metadata_store_pb2\n\nconnection_config = metadata_store_pb2.ConnectionConfig()\nconnection_config.fake_database.SetInParent() # Sets an empty fake database proto.\nstore = metadata_store.MetadataStore(connection_config)\n</code></pre> <ul> <li>SQLite reads and writes files from disk.</li> </ul> <pre><code>connection_config = metadata_store_pb2.ConnectionConfig()\nconnection_config.sqlite.filename_uri = '...'\nconnection_config.sqlite.connection_mode = 3 # READWRITE_OPENCREATE\nstore = metadata_store.MetadataStore(connection_config)\n</code></pre> <ul> <li>MySQL connects to a MySQL server.</li> </ul> <pre><code>connection_config = metadata_store_pb2.ConnectionConfig()\nconnection_config.mysql.host = '...'\nconnection_config.mysql.port = '...'\nconnection_config.mysql.database = '...'\nconnection_config.mysql.user = '...'\nconnection_config.mysql.password = '...'\nstore = metadata_store.MetadataStore(connection_config)\n</code></pre> <p>Similarly, when using a MySQL instance with Google CloudSQL (quickstart, connect-overview), one could also use SSL option if applicable.</p> <pre><code>connection_config.mysql.ssl_options.key = '...'\nconnection_config.mysql.ssl_options.cert = '...'\nconnection_config.mysql.ssl_options.ca = '...'\nconnection_config.mysql.ssl_options.capath = '...'\nconnection_config.mysql.ssl_options.cipher = '...'\nconnection_config.mysql.ssl_options.verify_server_cert = '...'\nstore = metadata_store.MetadataStore(connection_config)\n</code></pre> <ul> <li>PostgreSQL connects to a PostgreSQL server.</li> </ul> <pre><code>connection_config = metadata_store_pb2.ConnectionConfig()\nconnection_config.postgresql.host = '...'\nconnection_config.postgresql.port = '...'\nconnection_config.postgresql.user = '...'\nconnection_config.postgresql.password = '...'\nconnection_config.postgresql.dbname = '...'\nstore = metadata_store.MetadataStore(connection_config)\n</code></pre> <p>Similarly, when using a PostgreSQL instance with Google CloudSQL (quickstart, connect-overview), one could also use SSL option if applicable.</p> <pre><code>connection_config.postgresql.ssloption.sslmode = '...' # disable, allow, verify-ca, verify-full, etc.\nconnection_config.postgresql.ssloption.sslcert = '...'\nconnection_config.postgresql.ssloption.sslkey = '...'\nconnection_config.postgresql.ssloption.sslpassword = '...'\nconnection_config.postgresql.ssloption.sslrootcert = '...'\nstore = metadata_store.MetadataStore(connection_config)\n</code></pre>"},{"location":"guide/mlmd/#data-model","title":"Data model","text":"<p>The Metadata Store uses the following data model to record and retrieve metadata from the storage backend.</p> <ul> <li><code>ArtifactType</code> describes an artifact's type and its properties that are     stored in the metadata store. You can register these types on-the-fly with     the metadata store in code, or you can load them in the store from a     serialized format. Once you register a type, its definition is available     throughout the lifetime of the store.</li> <li>An <code>Artifact</code> describes a specific instance of an <code>ArtifactType</code>, and its     properties that are written to the metadata store.</li> <li>An <code>ExecutionType</code> describes a type of component or step in a workflow, and     its runtime parameters.</li> <li>An <code>Execution</code> is a record of a component run or a step in an ML workflow     and the runtime parameters. An execution can be thought of as an instance of     an <code>ExecutionType</code>. Executions are recorded when you run an ML pipeline or     step.</li> <li>An <code>Event</code> is a record of the relationship between artifacts and executions.     When an execution happens, events record every artifact that was used by the     execution, and every artifact that was produced. These records allow for     lineage tracking throughout a workflow. By looking at all events, MLMD knows     what executions happened and what artifacts were created as a result. MLMD     can then recurse back from any artifact to all of its upstream inputs.</li> <li>A <code>ContextType</code> describes a type of conceptual group of artifacts and     executions in a workflow, and its structural properties. For example:     projects, pipeline runs, experiments, owners etc.</li> <li>A <code>Context</code> is an instance of a <code>ContextType</code>. It captures the shared     information within the group. For example: project name, changelist commit     id, experiment annotations etc. It has a user-defined unique name within its     <code>ContextType</code>.</li> <li>An <code>Attribution</code> is a record of the relationship between artifacts and     contexts.</li> <li>An <code>Association</code> is a record of the relationship between executions and     contexts.</li> </ul>"},{"location":"guide/mlmd/#mlmd-functionality","title":"MLMD Functionality","text":"<p>Tracking the inputs and outputs of all components/steps in an ML workflow and their lineage allows ML platforms to enable several important features. The following list provides a non-exhaustive overview of some of the major benefits.</p> <ul> <li>List all Artifacts of a specific type. Example: all Models that have     been trained.</li> <li>Load two Artifacts of the same type for comparison. Example: compare     results from two experiments.</li> <li>Show a DAG of all related executions and their input and output artifacts     of a context. Example: visualize the workflow of an experiment for     debugging and discovery.</li> <li>Recurse back through all events to see how an artifact was created.     Examples: see what data went into a model; enforce data retention plans.</li> <li>Identify all artifacts that were created using a given artifact.     Examples: see all Models trained from a specific dataset; mark models based     upon bad data.</li> <li>Determine if an execution has been run on the same inputs before.     Example: determine whether a component/step has already completed the same     work and the previous output can just be reused.</li> <li>Record and query context of workflow runs. Examples: track the owner and     changelist used for a workflow run; group the lineage by experiments; manage     artifacts by projects.</li> <li>Declarative nodes filtering capabilities on properties and 1-hop     neighborhood nodes. Examples: look for artifacts of a type and under some     pipeline context; return typed artifacts where a given property\u2019s value is     within a range; find previous executions in a context with the same inputs.</li> </ul> <p>See the MLMD tutorial for an example that shows you how to use the MLMD API and the metadata store to retrieve lineage information.</p>"},{"location":"guide/mlmd/#integrate-ml-metadata-into-your-ml-workflows","title":"Integrate ML Metadata into your ML Workflows","text":"<p>If you are a platform developer interested in integrating MLMD into your system, use the example workflow below to use the low-level MLMD APIs to track the execution of a training task. You can also use higher-level Python APIs in notebook environments to record experiment metadata.</p> <p></p> <p>1) Register artifact types</p> <pre><code># Create ArtifactTypes, e.g., Data and Model\ndata_type = metadata_store_pb2.ArtifactType()\ndata_type.name = \"DataSet\"\ndata_type.properties[\"day\"] = metadata_store_pb2.INT\ndata_type.properties[\"split\"] = metadata_store_pb2.STRING\ndata_type_id = store.put_artifact_type(data_type)\n\nmodel_type = metadata_store_pb2.ArtifactType()\nmodel_type.name = \"SavedModel\"\nmodel_type.properties[\"version\"] = metadata_store_pb2.INT\nmodel_type.properties[\"name\"] = metadata_store_pb2.STRING\nmodel_type_id = store.put_artifact_type(model_type)\n\n# Query all registered Artifact types.\nartifact_types = store.get_artifact_types()\n</code></pre> <p>2) Register execution types for all steps in the ML workflow</p> <pre><code># Create an ExecutionType, e.g., Trainer\ntrainer_type = metadata_store_pb2.ExecutionType()\ntrainer_type.name = \"Trainer\"\ntrainer_type.properties[\"state\"] = metadata_store_pb2.STRING\ntrainer_type_id = store.put_execution_type(trainer_type)\n\n# Query a registered Execution type with the returned id\n[registered_type] = store.get_execution_types_by_id([trainer_type_id])\n</code></pre> <p>3) Create an artifact of DataSet ArtifactType</p> <pre><code># Create an input artifact of type DataSet\ndata_artifact = metadata_store_pb2.Artifact()\ndata_artifact.uri = 'path/to/data'\ndata_artifact.properties[\"day\"].int_value = 1\ndata_artifact.properties[\"split\"].string_value = 'train'\ndata_artifact.type_id = data_type_id\n[data_artifact_id] = store.put_artifacts([data_artifact])\n\n# Query all registered Artifacts\nartifacts = store.get_artifacts()\n\n# Plus, there are many ways to query the same Artifact\n[stored_data_artifact] = store.get_artifacts_by_id([data_artifact_id])\nartifacts_with_uri = store.get_artifacts_by_uri(data_artifact.uri)\nartifacts_with_conditions = store.get_artifacts(\n      list_options=mlmd.ListOptions(\n          filter_query='uri LIKE \"%/data\" AND properties.day.int_value &gt; 0'))\n</code></pre> <p>4) Create an execution of the Trainer run</p> <pre><code># Register the Execution of a Trainer run\ntrainer_run = metadata_store_pb2.Execution()\ntrainer_run.type_id = trainer_type_id\ntrainer_run.properties[\"state\"].string_value = \"RUNNING\"\n[run_id] = store.put_executions([trainer_run])\n\n# Query all registered Execution\nexecutions = store.get_executions_by_id([run_id])\n# Similarly, the same execution can be queried with conditions.\nexecutions_with_conditions = store.get_executions(\n    list_options = mlmd.ListOptions(\n        filter_query='type = \"Trainer\" AND properties.state.string_value IS NOT NULL'))\n</code></pre> <p>5) Define the input event and read data</p> <pre><code># Define the input event\ninput_event = metadata_store_pb2.Event()\ninput_event.artifact_id = data_artifact_id\ninput_event.execution_id = run_id\ninput_event.type = metadata_store_pb2.Event.DECLARED_INPUT\n\n# Record the input event in the metadata store\nstore.put_events([input_event])\n</code></pre> <p>6) Declare the output artifact</p> <pre><code># Declare the output artifact of type SavedModel\nmodel_artifact = metadata_store_pb2.Artifact()\nmodel_artifact.uri = 'path/to/model/file'\nmodel_artifact.properties[\"version\"].int_value = 1\nmodel_artifact.properties[\"name\"].string_value = 'MNIST-v1'\nmodel_artifact.type_id = model_type_id\n[model_artifact_id] = store.put_artifacts([model_artifact])\n</code></pre> <p>7) Record the output event</p> <pre><code># Declare the output event\noutput_event = metadata_store_pb2.Event()\noutput_event.artifact_id = model_artifact_id\noutput_event.execution_id = run_id\noutput_event.type = metadata_store_pb2.Event.DECLARED_OUTPUT\n\n# Submit output event to the Metadata Store\nstore.put_events([output_event])\n</code></pre> <p>8) Mark the execution as completed</p> <pre><code>trainer_run.id = run_id\ntrainer_run.properties[\"state\"].string_value = \"COMPLETED\"\nstore.put_executions([trainer_run])\n</code></pre> <p>9) Group artifacts and executions under a context using attributions and assertions artifacts</p> <pre><code># Create a ContextType, e.g., Experiment with a note property\nexperiment_type = metadata_store_pb2.ContextType()\nexperiment_type.name = \"Experiment\"\nexperiment_type.properties[\"note\"] = metadata_store_pb2.STRING\nexperiment_type_id = store.put_context_type(experiment_type)\n\n# Group the model and the trainer run to an experiment.\nmy_experiment = metadata_store_pb2.Context()\nmy_experiment.type_id = experiment_type_id\n# Give the experiment a name\nmy_experiment.name = \"exp1\"\nmy_experiment.properties[\"note\"].string_value = \"My first experiment.\"\n[experiment_id] = store.put_contexts([my_experiment])\n\nattribution = metadata_store_pb2.Attribution()\nattribution.artifact_id = model_artifact_id\nattribution.context_id = experiment_id\n\nassociation = metadata_store_pb2.Association()\nassociation.execution_id = run_id\nassociation.context_id = experiment_id\n\nstore.put_attributions_and_associations([attribution], [association])\n\n# Query the Artifacts and Executions that are linked to the Context.\nexperiment_artifacts = store.get_artifacts_by_context(experiment_id)\nexperiment_executions = store.get_executions_by_context(experiment_id)\n\n# You can also use neighborhood queries to fetch these artifacts and executions\n# with conditions.\nexperiment_artifacts_with_conditions = store.get_artifacts(\n    list_options = mlmd.ListOptions(\n        filter_query=('contexts_a.type = \"Experiment\" AND contexts_a.name = \"exp1\"')))\nexperiment_executions_with_conditions = store.get_executions(\n    list_options = mlmd.ListOptions(\n        filter_query=('contexts_a.id = {}'.format(experiment_id))))\n</code></pre>"},{"location":"guide/mlmd/#use-mlmd-with-a-remote-grpc-server","title":"Use MLMD with a remote gRPC server","text":"<p>You can use MLMD with remote gRPC servers as shown below:</p> <ul> <li>Start a server</li> </ul> <pre><code>bazel run -c opt --define grpc_no_ares=true  //ml_metadata/metadata_store:metadata_store_server\n</code></pre> <p>By default, the server uses a fake in-memory db per request and does not persist the metadata across calls. It can also be configured with a MLMD <code>MetadataStoreServerConfig</code> to use SQLite files or MySQL instances. The config can be stored in a text protobuf file and passed to the binary with <code>--metadata_store_server_config_file=path_to_the_config_file</code>.</p> <p>An example <code>MetadataStoreServerConfig</code> file in text protobuf format:</p> <pre><code>connection_config {\n  sqlite {\n    filename_uri: '/tmp/test_db'\n    connection_mode: READWRITE_OPENCREATE\n  }\n}\n</code></pre> <ul> <li>Create the client stub and use it in Python</li> </ul> <pre><code>from grpc import insecure_channel\nfrom ml_metadata.proto import metadata_store_pb2\nfrom ml_metadata.proto import metadata_store_service_pb2\nfrom ml_metadata.proto import metadata_store_service_pb2_grpc\n\nchannel = insecure_channel('localhost:8080')\nstub = metadata_store_service_pb2_grpc.MetadataStoreServiceStub(channel)\n</code></pre> <ul> <li>Use MLMD with RPC calls</li> </ul> <pre><code># Create ArtifactTypes, e.g., Data and Model\ndata_type = metadata_store_pb2.ArtifactType()\ndata_type.name = \"DataSet\"\ndata_type.properties[\"day\"] = metadata_store_pb2.INT\ndata_type.properties[\"split\"] = metadata_store_pb2.STRING\n\nrequest = metadata_store_service_pb2.PutArtifactTypeRequest()\nrequest.all_fields_match = True\nrequest.artifact_type.CopyFrom(data_type)\nstub.PutArtifactType(request)\n\nmodel_type = metadata_store_pb2.ArtifactType()\nmodel_type.name = \"SavedModel\"\nmodel_type.properties[\"version\"] = metadata_store_pb2.INT\nmodel_type.properties[\"name\"] = metadata_store_pb2.STRING\n\nrequest.artifact_type.CopyFrom(model_type)\nstub.PutArtifactType(request)\n</code></pre>"},{"location":"guide/mlmd/#resources","title":"Resources","text":"<p>The MLMD library has a high-level API that you can readily use with your ML pipelines. See the MLMD API documentation for more details.</p> <p>Check out MLMD Declarative Nodes Filtering to learn how to use MLMD declarative nodes filtering capabilities on properties and 1-hop neighborhood nodes.</p> <p>Also check out the MLMD tutorial to learn how to use MLMD to trace the lineage of your pipeline components.</p> <p>MLMD provides utilities to handle schema and data migrations across releases. See the MLMD Guide for more details.</p>"},{"location":"guide/modelval/","title":"The ModelValidator TFX Pipeline Component (Deprecated)","text":"<p>The ModelValidator was used to check if a model was good enough to be used in production. We still think that validation is useful, but since the model Evaluator has already computed all the metrics you want to validate against we decided to fuse the two so you don't have to duplicate the computations.</p> <p>While we have deprecated the ModelValidator and don't recommend it's use, if you need to maintain an existing ModelValidator component an example configuration is as follows:</p> <pre><code>import tfx\nimport tensorflow_model_analysis as tfma\nfrom tfx.components.model_validator.component import ModelValidator\n\n...\n\nmodel_validator = ModelValidator(\n      examples=example_gen.outputs['output_data'],\n      model=trainer.outputs['model'])\n</code></pre> <p>For those that would like to migrate the configuration to the Evaluator, a similar configuration for the Evaluator would look as follows:</p> <pre><code>from tfx import components\nimport tensorflow_model_analysis as tfma\n\n...\n\neval_config = tfma.EvalConfig(\n    model_specs=[\n        # This assumes a serving model with signature 'serving_default'.\n        tfma.ModelSpec(label_key='&lt;label_key&gt;')\n    ],\n    metrics_specs=[\n        tfma.MetricsSpec(\n            # The metrics added here are in addition to those saved with the\n            # model (assuming either a keras model or EvalSavedModel is used).\n            # Any metrics added into the saved model (for example using\n            # model.compile(..., metrics=[...]), etc) will be computed\n            # automatically.\n            metrics=[\n                tfma.MetricConfig(class_name='ExampleCount'),\n                tfma.MetricConfig(\n                    class_name='BinaryAccuracy',\n                    threshold=tfma.MetricThreshold(\n                        value_threshold=tfma.GenericValueThreshold(\n                            lower_bound={'value': 0.5}),\n                        change_threshold=tfma.GenericChangeThreshold(\n                            direction=tfma.MetricDirection.HIGHER_IS_BETTER,\n                            absolute={'value': -1e-10})))\n            ]\n        )\n    ],\n    slicing_specs=[\n        # An empty slice spec means the overall slice, i.e. the whole dataset.\n        tfma.SlicingSpec(),\n        # Data can be sliced along a feature column. In this case, data is\n        # sliced along feature column trip_start_hour.\n        tfma.SlicingSpec(feature_keys=['trip_start_hour'])\n    ])\n\nmodel_resolver = Resolver(\n      strategy_class=latest_blessed_model_resolver.LatestBlessedModelResolver,\n      model=Channel(type=Model),\n      model_blessing=Channel(type=ModelBlessing)\n).with_id('latest_blessed_model_resolver')\n\nmodel_analyzer = components.Evaluator(\n      examples=examples_gen.outputs['examples'],\n      model=trainer.outputs['model'],\n      baseline_model=model_resolver.outputs['model'],\n      # Change threshold will be ignored if there is no baseline (first run).\n      eval_config=eval_config)\n</code></pre>"},{"location":"guide/non_tf/","title":"Using Other ML Frameworks in TFX","text":"<p>TFX as a platform is framework neutral, and can be used with other ML frameworks, e.g., JAX, scikit-learn.</p> <p>For model developers, this means they do not need to rewrite their model code implemented in another ML framework, but can instead reuse the bulk of the training code as-is in TFX, and benefit from other capabilities TFX and the rest of the TensorFlow Ecosystem offers.</p> <p>The TFX pipeline SDK and most modules in TFX, e.g., pipeline orchestrator, don't have any direct dependency on TensorFlow, but there are some aspects which are oriented towards TensorFlow, such as data formats. With some consideration of the needs of a particular modeling framework, a TFX pipeline can be used to train models in any other Python-based ML framework. This includes Scikit-learn, XGBoost, and PyTorch, among others. Some of the considerations for using the standard TFX components with other frameworks include:</p> <ul> <li>ExampleGen outputs     tf.train.Example     in TFRecord files. It's a generic representation for training data, and     downstream components use     TFXIO     to read it as Arrow/RecordBatch in memory, which can be further converted to     <code>tf.dataset</code>, <code>Tensors</code> or other formats. Payload/File formats other than     tf.train.Example/TFRecord are being considered, but for TFXIO users it     should be a blackbox.</li> <li>Transform can be used to generate transformed training examples no     matter what framework is used for training, but if the model format is not     <code>saved_model</code>, users won't be able to embed the transform graph into the     model. In that case, model prediction needs to take transformed features     instead of raw features, and users can run transform as a preprocessing     step before calling the model prediction when serving.</li> <li>Trainer supports     GenericTraining     so users can train their models using any ML framework.</li> <li>Evaluator by default only supports <code>saved_model</code>, but users can provide     a UDF that generates predictions for model evaluation.</li> </ul> <p>Training a model in a non-Python-based framework will require isolating a custom training component in a Docker container, as part of a pipeline which is running in a containerized environment such as Kubernetes.</p>"},{"location":"guide/non_tf/#jax","title":"JAX","text":"<p>JAX is Autograd and XLA, brought together for high-performance machine learning research. Flax is a neural network library and ecosystem for JAX, designed for flexibility.</p> <p>With jax2tf, we are able to convert trained JAX/Flax models into <code>saved_model</code> format, which can be used seamlessly in TFX with generic training and model evaluation. For details, check this example.</p>"},{"location":"guide/non_tf/#scikit-learn","title":"scikit-learn","text":"<p>Scikit-learn is a machine learning library for the Python programming language. We have an e2e example with customized training and evaluation in TFX-Addons.</p>"},{"location":"guide/pusher/","title":"The Pusher TFX Pipeline Component","text":"<p>The Pusher component is used to push a validated model to a deployment target during model training or re-training. Before the deployment, Pusher relies on one or more blessings from other validation components to decide whether to push the model or not.</p> <ul> <li>Evaluator blesses the model if the new trained model is \"good     enough\" to be pushed to production.</li> <li>(Optional but recommended) InfraValidator blesses the     model if the model is mechanically servable in a production environment.</li> </ul> <p>A Pusher component consumes a trained model in SavedModel format, and produces the same SavedModel, along with versioning metadata.</p>"},{"location":"guide/pusher/#using-the-pusher-component","title":"Using the Pusher Component","text":"<p>A Pusher pipeline component is typically very easy to deploy and requires little customization, since all of the work is done by the Pusher TFX component. Typical code looks like this:</p> <pre><code>pusher = Pusher(\n  model=trainer.outputs['model'],\n  model_blessing=evaluator.outputs['blessing'],\n  infra_blessing=infra_validator.outputs['blessing'],\n  push_destination=tfx.proto.PushDestination(\n    filesystem=tfx.proto.PushDestination.Filesystem(\n        base_directory=serving_model_dir)\n  )\n)\n</code></pre>"},{"location":"guide/pusher/#pushing-a-model-produced-from-infravalidator","title":"Pushing a model produced from InfraValidator.","text":"<p>(From version 0.30.0)</p> <p>InfraValidator can also produce <code>InfraBlessing</code> artifact containing a model with warmup, and Pusher can push it just like a <code>Model</code> artifact.</p> <pre><code>infra_validator = InfraValidator(\n    ...,\n    # make_warmup=True will produce a model with warmup requests in its\n    # 'blessing' output.\n    request_spec=tfx.proto.RequestSpec(..., make_warmup=True)\n)\n\npusher = Pusher(\n    # Push model from 'infra_blessing' input.\n    infra_blessing=infra_validator.outputs['blessing'],\n    push_destination=tfx.proto.PushDestination(...)\n)\n</code></pre> <p>More details are available in the Pusher API reference.</p>"},{"location":"guide/schemagen/","title":"The SchemaGen TFX Pipeline Component","text":"<p>Some TFX components use a description of your input data called a schema. The schema is an instance of schema.proto. It can specify data types for feature values, whether a feature has to be present in all examples, allowed value ranges, and other properties.  A SchemaGen pipeline component will automatically generate a schema by inferring types, categories, and ranges from the training data.</p> <ul> <li>Consumes: statistics from a StatisticsGen component</li> <li>Emits: Data schema proto</li> </ul> <p>Here's an excerpt from a schema proto:</p> <pre><code>...\nfeature {\n  name: \"age\"\n  value_count {\n    min: 1\n    max: 1\n  }\n  type: FLOAT\n  presence {\n    min_fraction: 1\n    min_count: 1\n  }\n}\nfeature {\n  name: \"capital-gain\"\n  value_count {\n    min: 1\n    max: 1\n  }\n  type: FLOAT\n  presence {\n    min_fraction: 1\n    min_count: 1\n  }\n}\n...\n</code></pre> <p>The following TFX libraries use the schema:</p> <ul> <li>TensorFlow Data Validation</li> <li>TensorFlow Transform</li> <li>TensorFlow Model Analysis</li> </ul> <p>In a typical TFX pipeline SchemaGen generates a schema, which is consumed by the other pipeline components. However, the auto-generated schema is best-effort and only tries to infer basic properties of the data. It is expected that developers review and modify it as needed.</p> <p>The modified schema can be brought back into the pipeline using ImportSchemaGen component. The SchemaGen component for the initial schema generation can be removed and all downstream components can use the output of ImportSchemaGen. It is also recommended to add ExampleValidator using the imported schema to examine the training data continuously.</p>"},{"location":"guide/schemagen/#schemagen-and-tensorflow-data-validation","title":"SchemaGen and TensorFlow Data Validation","text":"<p>SchemaGen makes extensive use of TensorFlow Data Validation for inferring a schema.</p>"},{"location":"guide/schemagen/#using-the-schemagen-component","title":"Using the SchemaGen Component","text":""},{"location":"guide/schemagen/#for-the-initial-schema-generation","title":"For the initial schema generation","text":"<p>A SchemaGen pipeline component is typically very easy to deploy and requires little customization. Typical code looks like this:</p> <pre><code>schema_gen = tfx.components.SchemaGen(\n    statistics=stats_gen.outputs['statistics'])\n</code></pre> <p>More details are available in the SchemaGen API reference.</p>"},{"location":"guide/schemagen/#for-the-reviewed-schema-import","title":"For the reviewed schema import","text":"<p>Add ImportSchemaGen component to the pipeline to bring the reviewed schema definition into the pipeline.</p> <pre><code>schema_gen = tfx.components.ImportSchemaGen(\n    schema_file='/some/path/schema.pbtxt')\n</code></pre> <p>The <code>schema_file</code> should be a full path to the text protobuf file.</p> <p>More details are available in the ImportSchemaGen API reference.</p>"},{"location":"guide/serving/","title":"Serving Models","text":""},{"location":"guide/serving/#introduction","title":"Introduction","text":"<p>TensorFlow Serving is a flexible, high-performance serving system for machine learning models, designed for production environments. TensorFlow Serving makes it easy to deploy new algorithms and experiments, while keeping the same server architecture and APIs. TensorFlow Serving provides out-of-the-box integration with TensorFlow models, but can be easily extended to serve other types of models and data.</p> <p>Detailed developer documentation on TensorFlow Serving is available:</p> <ul> <li>Architecture Overview</li> <li>Server API</li> <li>REST Client API</li> </ul>"},{"location":"guide/solutions/","title":"TFX Cloud Solutions","text":"<p>Looking for insights into how TFX can be applied to build a solution that meets your needs? These in-depth articles and guides may help!</p> <p>Note</p> <p>These articles discuss complete solutions in which TFX is a key part, but not the only part. This is nearly always the case for real-world deployments. So implementing these solutions yourself will require more than just TFX. The main goal is to give you some insight into how others have implemented solutions that may meet requirements that are similar to yours, and not to serve as a cookbook or list of approved applications of TFX.</p>"},{"location":"guide/solutions/#architecture-of-a-machine-learning-system-for-near-real-time-item-matching","title":"Architecture of a machine learning system for near real-time item matching","text":"<p>Use this document to learn about the architecture of a machine learning (ML) solution that learns and serves item embeddings. Embeddings can help you understand what items your customers consider to be similar, which enables you to offer real-time \"similar item\" suggestions in your application. This solution shows you how to identify similar songs in a dataset, and then use this information to make song recommendations. Read more</p>"},{"location":"guide/solutions/#data-preprocessing-for-machine-learning-options-and-recommendations","title":"Data preprocessing for machine learning: options and recommendations","text":"<p>This two-part article explores the topic of data engineering and feature engineering for machine learning (ML). This first part discusses best practices of preprocessing data in a machine learning pipeline on Google Cloud. The article focuses on using TensorFlow and the open source TensorFlow Transform (tf.Transform) library to prepare data, train the model, and serve the model for prediction. This part highlights the challenges of preprocessing data for machine learning, and illustrates the options and scenarios for performing data transformation on Google Cloud effectively. Part 1 Part 2</p>"},{"location":"guide/solutions/#architecture-for-mlops-using-tfx-kubeflow-pipelines-and-cloud-build","title":"Architecture for MLOps using TFX, Kubeflow Pipelines, and Cloud Build","text":"<p>This document describes the overall architecture of a machine learning (ML) system using TensorFlow Extended (TFX) libraries. It also discusses how to set up a continuous integration (CI), continuous delivery (CD), and continuous training (CT) for the ML system using Cloud Build and Kubeflow Pipelines. Read more</p>"},{"location":"guide/solutions/#mlops-continuous-delivery-and-automation-pipelines-in-machine-learning","title":"MLOps: Continuous delivery and automation pipelines in machine learning","text":"<p>This document discusses techniques for implementing and automating continuous integration (CI), continuous delivery (CD), and continuous training (CT) for machine learning (ML) systems. Data science and ML are becoming core capabilities for solving complex real-world problems, transforming industries, and delivering value in all domains. Read more</p>"},{"location":"guide/solutions/#setting-up-an-mlops-environment-on-google-cloud","title":"Setting up an MLOps environment on Google Cloud","text":"<p>This reference guide outlines the architecture of a machine learning operations (MLOps) environment on Google Cloud. The guide accompanies hands-on labs in GitHub that walk you through the process of provisioning and configuring the environment described here. Virtually all industries are adopting machine learning (ML) at a rapidly accelerating pace. A key challenge for getting value from ML is to create ways to deploy and operate ML systems effectively. This guide is intended for machine learning (ML) and DevOps engineers. Read more</p>"},{"location":"guide/solutions/#key-requirements-for-an-mlops-foundation","title":"Key requirements for an MLOps foundation","text":"<p>AI-driven organizations are using data and machine learning to solve their hardest problems and are reaping the rewards.</p> <p>\u201cCompanies that fully absorb AI in their value-producing workflows by 2025 will dominate the 2030 world economy with +120% cash flow growth,\u201d according to McKinsey Global Institute.</p> <p>But it\u2019s not easy right now. Machine learning (ML) systems have a special capacity for creating technical debt if not managed well. Read more</p>"},{"location":"guide/solutions/#how-to-create-and-deploy-a-model-card-in-the-cloud-with-scikit-learn","title":"How to create and deploy a model card in the cloud with Scikit-Learn","text":"<p>Machine learning models are now being used to accomplish many challenging tasks. With their vast potential, ML models also raise questions about their usage, construction, and limitations. Documenting the answers to these questions helps to bring clarity and shared understanding. To help advance these goals, Google has introduced model cards. Read more</p>"},{"location":"guide/solutions/#analyzing-and-validating-data-at-scale-for-machine-learning-with-tensorflow-data-validation","title":"Analyzing and validating data at scale for machine learning with TensorFlow Data Validation","text":"<p>This document discusses how to use the TensorFlow Data Validation (TFDV) library for data exploration and descriptive analytics during experimentation. Data scientists and machine learning (ML) engineers can use TFDV in a production ML system to validate data that's used in a continuous training (CT) pipeline, and to detect skews and outliers in data received for prediction serving. It includes hands-on labs. Read more</p>"},{"location":"guide/statsgen/","title":"The StatisticsGen TFX Pipeline Component","text":"<p>The StatisticsGen TFX pipeline component generates features statistics over both training and serving data, which can be used by other pipeline components. StatisticsGen uses Beam to scale to large datasets.</p> <ul> <li>Consumes: datasets created by an ExampleGen pipeline component.</li> <li>Emits: Dataset statistics.</li> </ul>"},{"location":"guide/statsgen/#statisticsgen-and-tensorflow-data-validation","title":"StatisticsGen and TensorFlow Data Validation","text":"<p>StatisticsGen makes extensive use of TensorFlow Data Validation for generating statistics from your dataset.</p>"},{"location":"guide/statsgen/#using-the-statsgen-component","title":"Using the StatsGen Component","text":"<p>A StatisticsGen pipeline component is typically very easy to deploy and requires little customization. Typical code looks like this:</p> <pre><code>compute_eval_stats = StatisticsGen(\n      examples=example_gen.outputs['examples'],\n      name='compute-eval-stats'\n      )\n</code></pre>"},{"location":"guide/statsgen/#using-the-statsgen-component-with-a-schema","title":"Using the StatsGen Component With a Schema","text":"<p>For the first run of a pipeline, the output of StatisticsGen will be used to infer a schema. However, on subsequent runs you may have a manually curated schema that contains additional information about your data set. By providing this schema to StatisticsGen, TFDV can provide more useful statistics based on declared properties of your data set.</p> <p>In this setting, you will invoke StatisticsGen with a curated schema that has been imported by an ImporterNode like this:</p> <pre><code>user_schema_importer = Importer(\n    source_uri=user_schema_dir, # directory containing only schema text proto\n    artifact_type=standard_artifacts.Schema).with_id('schema_importer')\n\ncompute_eval_stats = StatisticsGen(\n      examples=example_gen.outputs['examples'],\n      schema=user_schema_importer.outputs['result'],\n      name='compute-eval-stats'\n      )\n</code></pre>"},{"location":"guide/statsgen/#creating-a-curated-schema","title":"Creating a Curated Schema","text":"<p><code>Schema</code> in TFX is an instance of the TensorFlow Metadata <code>Schema</code> proto. This can be composed in text format from scratch. However, it is easier to use the inferred schema produced by <code>SchemaGen</code> as a starting point. Once the <code>SchemaGen</code> component has executed, the schema will be located under the pipeline root in the following path:</p> <pre><code>&lt;pipeline_root&gt;/SchemaGen/schema/&lt;artifact_id&gt;/schema.pbtxt\n</code></pre> <p>Where <code>&lt;artifact_id&gt;</code> represents a unique ID for this version of the schema in MLMD. This schema proto can then be modified to communicate information about the dataset which cannot be reliably inferred, which will make the output of <code>StatisticsGen</code> more useful and the validation performed in the <code>ExampleValidator</code> component more stringent.</p> <p>More details are available in the StatisticsGen API reference.</p>"},{"location":"guide/tfdv/","title":"TensorFlow Data Validation: Checking and analyzing your data","text":"<p>Once your data is in a TFX pipeline, you can use TFX components to analyze and transform it. You can use these tools even before you train a model.</p> <p>There are many reasons to analyze and transform your data:</p> <ul> <li>To find problems in your data. Common problems include:<ul> <li>Missing data, such as features with empty values.</li> <li>Labels treated as features, so that your model gets to peek at the right     answer during training.</li> <li>Features with values outside the range you expect.</li> <li>Data anomalies.</li> <li>Transfer learned model has preprocessing that does not match the     training data.</li> </ul> </li> <li>To engineer more effective feature sets. For example, you can identify:<ul> <li>Especially informative features.</li> <li>Redundant features.</li> <li>Features that vary so widely in scale that they may slow learning.</li> <li>Features with little or no unique predictive information.</li> </ul> </li> </ul> <p>TFX tools can both help find data bugs, and help with feature engineering.</p>"},{"location":"guide/tfdv/#tensorflow-data-validation","title":"TensorFlow Data Validation","text":"<ul> <li>Overview</li> <li>Schema Based Example Validation</li> <li>Training-Serving Skew Detection</li> <li>Drift Detection</li> </ul>"},{"location":"guide/tfdv/#overview","title":"Overview","text":"<p>TensorFlow Data Validation identifies anomalies in training and serving data, and can automatically create a schema by examining the data. The component can be configured to detect different classes of anomalies in the data. It can</p> <ol> <li>Perform validity checks by comparing data statistics against a schema that     codifies expectations of the user.</li> <li>Detect training-serving skew by comparing examples in training and serving     data.</li> <li>Detect data drift by looking at a series of data.</li> </ol> <p>We document each of these functionalities independently:</p> <ul> <li>Schema Based Example Validation</li> <li>Training-Serving Skew Detection</li> <li>Drift Detection</li> </ul>"},{"location":"guide/tfdv/#schema-based-example-validation","title":"Schema Based Example Validation","text":"<p>TensorFlow Data Validation identifies any anomalies in the input data by comparing data statistics against a schema. The schema codifies properties which the input data is expected to satisfy, such as data types or categorical values, and can be modified or replaced by the user.</p> <p>Tensorflow Data Validation is typically invoked multiple times within the context of the TFX pipeline: (i) for every split obtained from ExampleGen, (ii) for all pre-transformed data used by Transform and (iii) for all post-transform data generated by Transform. When invoked in the context of Transform (ii-iii), statistics options and schema-based constraints can be set by defining the <code>stats_options_updater_fn</code>. This is particilarly useful when validating unstructured data (e.g. text features). See the user code for an example.</p>"},{"location":"guide/tfdv/#advanced-schema-features","title":"Advanced Schema Features","text":"<p>This section covers more advanced schema configuration that can help with special setups.</p>"},{"location":"guide/tfdv/#sparse-features","title":"Sparse Features","text":"<p>Encoding sparse features in Examples usually introduces multiple Features that are expected to have the same valency for all Examples. For example the sparse feature:</p> <pre><code>\nWeightedCategories = [('CategoryA', 0.3), ('CategoryX', 0.7)]\n</code></pre> <p>would be encoded using separate Features for index and value:</p> <pre><code>\nWeightedCategoriesIndex = ['CategoryA', 'CategoryX']\nWeightedCategoriesValue = [0.3, 0.7]\n</code></pre> <p>with the restriction that the valency of the index and value feature should match for all Examples. This restriction can be made explicit in the schema by defining a sparse_feature:</p> <pre><code>\nsparse_feature {\n  name: 'WeightedCategories'\n  index_feature { name: 'WeightedCategoriesIndex' }\n  value_feature { name: 'WeightedCategoriesValue' }\n}\n</code></pre> <p>The sparse feature definition requires one or more index and one value feature which refer to features that exist in the schema. Explicitly defining sparse features enables TFDV to check that the valencies of all referred features match.</p> <p>Some use cases introduce similar valency restrictions between Features, but do not necessarily encode a sparse feature. Using sparse feature should unblock you, but is not ideal.</p>"},{"location":"guide/tfdv/#schema-environments","title":"Schema Environments","text":"<p>By default validations assume that all Examples in a pipeline adhere to a single schema. In some cases introducing slight schema variations is necessary, for instance features used as labels are required during training (and should be validated), but are missing during serving. Environments can be used to express such requirements, in particular <code>default_environment()</code>, <code>in_environment()</code>, <code>not_in_environment()</code>.</p> <p>For example, assume a feature named 'LABEL' is required for training, but is expected to be missing from serving. This can be expressed by:</p> <ul> <li>Define two distinct environments in the schema: [\"SERVING\", \"TRAINING\"] and     associate 'LABEL' only with environment \"TRAINING\".</li> <li>Associate the training data with environment \"TRAINING\" and the     serving data with environment \"SERVING\".</li> </ul>"},{"location":"guide/tfdv/#schema-generation","title":"Schema Generation","text":"<p>The input data schema is specified as an instance of the TensorFlow Schema.</p> <p>Instead of constructing a schema manually from scratch, a developer can rely on TensorFlow Data Validation's automatic schema construction. Specifically, TensorFlow Data Validation automatically constructs an initial schema based on statistics computed over training data available in the pipeline. Users can simply review this autogenerated schema, modify it as needed, check it into a version control system, and push it explicitly into the pipeline for further validation.</p> <p>TFDV includes <code>infer_schema()</code> to generate a schema automatically.  For example:</p> <pre><code>schema = tfdv.infer_schema(statistics=train_stats)\ntfdv.display_schema(schema=schema)\n</code></pre> <p>This triggers an automatic schema generation based on the following rules:</p> <ul> <li> <p>If a schema has already been auto-generated then it is used as is.</p> </li> <li> <p>Otherwise, TensorFlow Data Validation examines the available data statistics and computes a suitable schema for the data.</p> </li> </ul> <p>Note</p> <p>The auto-generated schema is best-effort and only tries to infer basic properties of the data. It is expected that users review and modify it as needed.</p>"},{"location":"guide/tfdv/#training-serving-skew-detection","title":"Training-Serving Skew Detection","text":""},{"location":"guide/tfdv/#overview_1","title":"Overview","text":"<p>TensorFlow Data Validation can detect distribution skew between training and serving data. Distribution skew occurs when the distribution of feature values for training data is significantly different from serving data. One of the key causes for distribution skew is using either a completely different corpus for training data generation to overcome lack of initial data in the desired corpus. Another reason is a faulty sampling mechanism that only chooses a subsample of the serving data to train on.</p>"},{"location":"guide/tfdv/#example-scenario","title":"Example Scenario","text":"<p>Note</p> <p>For instance, in order to compensate for an underrepresented slice of data, if a biased sampling is used without upweighting the downsampled examples appropriately, the distribution of feature values between training and serving data gets artificially skewed.</p> <p>See the TensorFlow Data Validation Get Started Guide for information about configuring training-serving skew detection.</p>"},{"location":"guide/tfdv/#drift-detection","title":"Drift Detection","text":"<p>Drift detection is supported between consecutive spans of data (i.e., between span N and span N+1), such as between different days of training data. We express drift in terms of L-infinity distance for categorical features and approximate Jensen-Shannon divergence for numeric features. You can set the threshold distance so that you receive warnings when the drift is higher than is acceptable. Setting the correct distance is typically an iterative process requiring domain knowledge and experimentation.</p> <p>See the TensorFlow Data Validation Get Started Guide for information about configuring drift detection.</p>"},{"location":"guide/tfdv/#using-visualizations-to-check-your-data","title":"Using Visualizations to Check Your Data","text":"<p>TensorFlow Data Validation provides tools for visualizing the distribution of feature values. By examining these distributions in a Jupyter notebook using Facets you can catch common problems with data.</p> <p></p>"},{"location":"guide/tfdv/#identifying-suspicious-distributions","title":"Identifying Suspicious Distributions","text":"<p>You can identify common bugs in your data by using a Facets Overview display to look for suspicious distributions of feature values.</p>"},{"location":"guide/tfdv/#unbalanced-data","title":"Unbalanced Data","text":"<p>An unbalanced feature is a feature for which one value predominates. Unbalanced features can occur naturally, but if a feature always has the same value you may have a data bug. To detect unbalanced features in a Facets Overview, choose \"Non-uniformity\" from the \"Sort by\" dropdown.</p> <p>The most unbalanced features will be listed at the top of each feature-type list. For example, the following screenshot shows one feature that is all zeros, and a second that is highly unbalanced, at the top of the \"Numeric Features\" list:</p> <p></p>"},{"location":"guide/tfdv/#uniformly-distributed-data","title":"Uniformly Distributed Data","text":"<p>A uniformly distributed feature is one for which all possible values appear with close to the same frequency. As with unbalanced data, this distribution can occur naturally, but can also be produced by data bugs.</p> <p>To detect uniformly distributed features in a Facets Overview, choose \"Non- uniformity\" from the \"Sort by\" dropdown and check the \"Reverse order\" checkbox:</p> <p></p> <p>String data is represented using bar charts if there are 20 or fewer unique values, and as a cumulative distribution graph if there are more than 20 unique values. So for string data, uniform distributions can appear as either flat bar graphs like the one above or straight lines like the one below:</p> <p></p>"},{"location":"guide/tfdv/#bugs-that-can-produce-uniformly-distributed-data","title":"Bugs That Can Produce Uniformly Distributed Data","text":"<p>Here are some common bugs that can produce uniformly distributed data:</p> <ul> <li> <p>Using strings to represent non-string data types such as dates. For example,     you will have many unique values for a datetime feature with representations     like \"2017-03-01-11-45-03\". Unique values will be distributed uniformly.</p> </li> <li> <p>Including indices like \"row number\" as features. Here again you have many     unique values.</p> </li> </ul>"},{"location":"guide/tfdv/#missing-data","title":"Missing Data","text":"<p>To check whether a feature is missing values entirely:</p> <ol> <li>Choose \"Amount missing/zero\" from the \"Sort by\" drop-down.</li> <li>Check the \"Reverse order\" checkbox.</li> <li>Look at the \"missing\" column to see the percentage of instances with missing     values for a feature.</li> </ol> <p>A data bug can also cause incomplete feature values. For example you may expect a feature's value list to always have three elements and discover that sometimes it only has one. To check for incomplete values or other cases where feature value lists don't have the expected number of elements:</p> <ol> <li> <p>Choose \"Value list length\" from the \"Chart to show\" drop-down menu on the     right.</p> </li> <li> <p>Look at the chart to the right of each feature row. The chart shows the     range of value list lengths for the feature. For example, the highlighted     row in the screenshot below shows a feature that has some zero-length value     lists:</p> </li> </ol> <p></p>"},{"location":"guide/tfdv/#large-differences-in-scale-between-features","title":"Large Differences in Scale Between Features","text":"<p>If your features vary widely in scale, then the model may have difficulties learning. For example, if some features vary from 0 to 1 and others vary from 0 to 1,000,000,000, you have a big difference in scale. Compare the \"max\" and \"min\" columns across features to find widely varying scales.</p> <p>Consider normalizing feature values to reduce these wide variations.</p>"},{"location":"guide/tfdv/#labels-with-invalid-labels","title":"Labels with Invalid Labels","text":"<p>TensorFlow's Estimators have restrictions on the type of data they accept as labels. For example, binary classifiers typically only work with {0, 1} labels.</p> <p>Review the label values in the Facets Overview and make sure they conform to the requirements of Estimators.</p>"},{"location":"guide/tfma/","title":"Improving Model Quality With TensorFlow Model Analysis","text":""},{"location":"guide/tfma/#introduction","title":"Introduction","text":"<p>As you tweak your model during development, you need to check whether your changes are improving your model. Just checking accuracy may not be enough. For example, if you have a classifier for a problem in which 95% of your instances are positive, you may be able to improve accuracy by simply always predicting positive, but you won't have a very robust classifier.</p>"},{"location":"guide/tfma/#overview","title":"Overview","text":"<p>The goal of TensorFlow Model Analysis is to provide a mechanism for model evaluation in TFX. TensorFlow Model Analysis allows you to perform model evaluations in the TFX pipeline, and view resultant metrics and plots in a Jupyter notebook. Specifically, it can provide:</p> <ul> <li>Metrics computed on entire training and holdout     dataset, as well as next-day evaluations</li> <li>Tracking metrics over time</li> <li>Model quality performance on different feature slices</li> <li>Model validation for ensuring that     model's maintain consistent performance</li> </ul>"},{"location":"guide/tfma/#next-steps","title":"Next Steps","text":"<p>Try our TFMA tutorial.</p> <p>Check out our github page for details on the supported metrics and plots and associated notebook visualizations.</p> <p>See the installation and getting started guides for information and examples on how to get set up in a standalone pipeline. Recall that TFMA is also used within the Evaluator component in TFX, so these resources will be useful for getting started in TFX as well.</p>"},{"location":"guide/tft/","title":"Transform library for non-TFX users","text":"<p>Transform is available as a standalone library.</p> <ul> <li>Getting Started with TensorFlow Transform</li> <li>TensorFlow Transform API Reference</li> </ul> <p>The <code>tft</code> module documentation is the only module that is relevant to TFX users. The <code>tft_beam</code> module is relevant only when using Transform as a standalone library. Typically, a TFX user constructs a <code>preprocessing_fn</code>, and the rest of the Transform library calls are made by the Transform component.</p> <p>You can also use the Apache Beam <code>MLTransform</code> class to preprocess data for training and inference. The <code>MLTransform</code> class wraps multiple TFX data processing transforms in one class. For more information, see Preprocess data with MLTransform in the Apache Beam documentation.</p>"},{"location":"guide/tft_bestpractices/","title":"Tft bestpractices","text":""},{"location":"guide/tft_bestpractices/#data-preprocessing-for-ml-options-and-recommendations","title":"Data preprocessing for ML: options and recommendations","text":"<p>This document is the first in a two-part series that explores the topic of data engineering and feature engineering for machine learning (ML), with a focus on supervised learning tasks. This first part discusses the best practices for preprocessing data in an ML pipeline on Google Cloud. The document focuses on using TensorFlow and the open source TensorFlow Transform (<code>tf.Transform</code>) library to prepare data, train the model, and serve the model for prediction. This document highlights the challenges of preprocessing data for ML, and it describes the options and scenarios for performing data transformation on Google Cloud effectively.</p> <p>This document assumes that you're familiar with BigQuery, Dataflow, Vertex AI, and the TensorFlow Keras API.</p> <p>The second document, Data preprocessing for ML with Google Cloud, provides a step-by-step tutorial for how to implement a <code>tf.Transform</code> pipeline.</p>"},{"location":"guide/tft_bestpractices/#introduction","title":"Introduction","text":"<p>ML helps you automatically find complex and potentially useful patterns in data. These patterns are condensed in an ML model that can then be used on new data points\u2014a process called making predictions or performing inference.</p> <p>Building an ML model is a multistep process. Each step presents its own technical and conceptual challenges. This two-part series focuses on supervised learning tasks and  the process of selecting, transforming, and augmenting the source data to create powerful predictive signals to the target variable. These operations combine domain knowledge with data science techniques. The operations are the essence of feature engineering.</p> <p>The size of training datasets for real-world ML models can easily be equal to or greater than one terabyte (TB). Therefore, you need large-scale data processing frameworks in order to process these datasets efficiently and distributedly. When you use an ML model to make predictions, you have to apply the same transformations that you used for the training data on the new data points. By applying the same transformations, you present the live dataset to the ML model the way that the model expects.</p> <p>This document discusses these challenges for different levels of granularity of feature engineering operations: instance-level, full-pass, and time-window aggregations. This document also describes the options and scenarios to perform data transformation for ML on Google Cloud.</p> <p>This document also provides an overview of TensorFlow Transform (<code>tf.Transform</code>), a library for TensorFlow that lets you define both instance-level and full-pass data transformation through data preprocessing pipelines. These pipelines are executed with Apache Beam, and they create artifacts that let you apply the same transformations during prediction as when the model is served.</p>"},{"location":"guide/tft_bestpractices/#preprocessing-data-for-ml","title":"Preprocessing data for ML","text":"<p>This section introduces data preprocessing operations and stages of data readiness. It also discusses the types of the preprocessing operations and their granularity.</p>"},{"location":"guide/tft_bestpractices/#data-engineering-compared-to-feature-engineering","title":"Data engineering compared to feature engineering","text":"<p>Preprocessing the data for ML involves both data engineering and feature engineering. Data engineering is the process of converting raw data into prepared data. Feature engineering then tunes the prepared data to create the features that are expected by the ML model. These terms have the following meanings:</p> Raw data (or just data) The data in its source form, without   any prior preparation for ML. In this context, the data might be in its raw   form (in a data lake) or in a transformed form (in a data warehouse).   Transformed data that's in a data warehouse might have been converted from   its original raw form to be used for analytics. However, in this context,   raw data means that the data hasn't been prepared specifically for your   ML task. Data is also considered raw data if it's sent from streaming   systems that eventually call ML models for predictions. Prepared data The dataset in the form ready for your ML task: data sources have been parsed, joined, and put into a tabular form. Prepared data is aggregated and summarized to the right granularity\u2014for example, each row in the dataset represents a unique customer, and each column represents summary information for the customer, like the total spent in the last six weeks. In a prepared data table, irrelevant columns have been dropped, and invalid records have been filtered out. For supervised learning tasks, the target feature is present. Engineered features The dataset with the tuned features that are expected by the model\u2014that is, features that are created by performing certain ML-specific operations on the columns in the prepared dataset, and creating new features for your model during training and prediction, as described later in Preprocessing operations. Examples of these operations include scaling numerical columns to a value between 0 and 1, clipping values, and one-hot-encoding categorical features. <p>The following diagram, figure 1, shows the steps that are involved in preparing preprocessed data:</p> <p>Figure 1: The flow of data from raw data to prepared data to engineered features to machine learning. </p> <p>In practice, data from the same source is often at different stages of readiness. For example, a field from a table in your data warehouse might be used directly as an engineered feature. At the same time, another field in the same table might need to go through transformations before it becomes an engineered feature. Similarly, data engineering and feature engineering operations might be combined in the same data preprocessing step.</p>"},{"location":"guide/tft_bestpractices/#preprocessing-operations","title":"Preprocessing operations","text":"<p>Data preprocessing includes several operations. Each operation is designed to help ML build better predictive models. The details of these preprocessing operations are outside the scope of this document, but some operations are briefly described in this section.</p> <p>For structured data, data preprocessing operations include the following:</p> <ul> <li>Data cleansing: removing or correcting records that have corrupted     or invalid values from raw data, and removing records that are missing a     large number of columns.</li> <li>Instances selection and partitioning: selecting data points from the     input dataset to create     training, evaluation (validation), and test sets.     This process includes techniques for repeatable random sampling, minority     classes oversampling, and stratified partitioning.</li> <li>Feature tuning: improving the quality of a feature for ML, which     includes scaling and normalizing numeric values, imputing missing values,     clipping outliers, and adjusting values that have skewed distributions.</li> <li>Feature transformation: converting a numeric feature to a     categorical feature (through     bucketization),     and converting categorical features to a numeric representation (through     one-hot encoding,     learning with counts,     sparse feature embeddings, etc.). Some models work only with numeric or     categorical features, while others can handle mixed type features. Even     when models handle both types, they can benefit from different     representations (numeric and categorical) of the same feature.</li> <li>Feature extraction: reducing the number of features by creating     lower-dimension, more powerful data representations using techniques such     as     PCA,     embedding     extraction, and     hashing.</li> <li>Feature selection: selecting a subset of the input features for     training the model, and ignoring the irrelevant or redundant ones, using     filter or wrapper methods.     Feature selection can also involve simply dropping features if the features     are missing a large number of values.</li> <li>Feature construction: creating new features by using typical     techniques, such as     polynomial expansion     (by using univariate mathematical functions) or     feature crossing     (to capture feature interactions). Features can also be constructed by     using business logic from the domain of the ML use case.</li> </ul> <p>When you work with unstructured data (for example, images, audio, or text documents), deep learning replaces domain-knowledge-based feature engineering by folding it into the model architecture. A convolutional layer is an automatic feature preprocessor. Constructing the right model architecture requires some empirical knowledge of the data. In addition, some amount of preprocessing is needed, such as the following:</p> <ul> <li>For text documents:     stemming and lemmatization,     TF-IDF     calculation, and     n-gram     extraction, embedding lookup.</li> <li>For images: clipping, resizing, cropping, Gaussian blur, and canary filters.</li> <li>For all types of data (including text and images):     transfer learning,     which treats all-but-last layers of the fully trained model as a feature     engineering step.</li> </ul>"},{"location":"guide/tft_bestpractices/#preprocessing-granularity","title":"Preprocessing granularity","text":"<p>This section discusses the granularity of types of data transformations. It shows why this perspective is critical when preparing new data points for predictions using transformations that are applied on training data.</p> <p>Preprocessing and transformation operations can be categorized as follows, based on operation granularity:</p> <ul> <li> <p>Instance-level transformations during training and prediction.     These are straightforward transformations, where only values from the same     instance are needed for the transformation. For example, instance-level     transformations might include clipping the value of a feature to some     threshold, polynomially expanding another feature, multiplying two     features, or comparing two features to create a Boolean flag.</p> <p>These transformations must be applied identically during training and prediction, because the model will be trained on the transformed features, not on the raw input values. If the data isn't transformed identically, then the model behaves poorly because it is presented with data that has a distribution of values that it wasn't trained with. For more information, see the discussion of training-serving skew in the Preprocessing challenges section. -   Full-pass transformations during training, but instance-level transformations during prediction. In this scenario, transformations are stateful, because they use some precomputed statistics to perform the transformation. During training, you analyze the whole body of training data to compute quantities such as minimum, maximum, mean, and variance for transforming training data, evaluation data, and new data at prediction time.</p> <p>For example, to normalize a numeric feature for training, you compute its mean (\u03bc) and its standard deviation (\u03c3) across the whole of the training data. This computation is called a full-pass (or analyze) operation. When you serve the model for prediction, the value of a new data point is normalized to avoid training-serving skew. Therefore, \u03bc and \u03c3 values that are computed during training are used to adjust the feature value, which is the following simple instance-level operation:</p> \\[ value_{\\text{scaled}} = \\frac{value_{\\text{raw}} - \\mu}{\\sigma} \\] <p>Full-pass transformations include the following:</p> <ul> <li>MinMax scaling numerical features using min and max     computed from the training dataset.</li> <li>Standard scaling (z-score normalization) numerical features     using \u03bc and \u03c3 computed on the training dataset.</li> <li>Bucketizing numerical features using quantiles.</li> <li>Imputing missing values using the median (numerical features) or     the mode (categorical features).</li> <li>Converting strings (nominal values) to integers (indexes) by     extracting all the distinct values (vocabulary) of an input categorical     feature.</li> <li>Counting the occurrence of a term (feature value) in all the     documents (instances) to calculate for TF-IDF.</li> <li>Computing the PCA of the input features to project the data into     a lower dimensional space (with linearly dependent features).</li> </ul> <p>You should use only the training data to compute statistics like \u03bc, \u03c3, min, and max. If you add the test and evaluation data for these operations, you are leaking information from the evaluation and test data to train the model. Doing so affects the reliability of the test and evaluation results. To ensure that you apply a consistent transformation to all datasets, you use the same statistics computed from the training data to transform the test and evaluation data.</p> </li> <li> <p>Historical aggregations during training and prediction. This     involves creating business aggregations, derivations, and flags as input     signals to the prediction task\u2014for example, creating     recency, frequency, and monetary (RFM)     metrics for customers to build propensity models. These types of features     can be precomputed and stored in a feature store to be used during model     training, batch scoring, and online prediction serving. You can also     perform additional feature engineering (for example, transformation and     tuning) to these aggregations before training and prediction.</p> </li> <li> <p>Historical aggregations during training, but real-time aggregations     during prediction. This approach involves creating a feature by     summarizing real-time values over time. In this approach, the instances to     be aggregated are defined through temporal window clauses. For example, you     can use this approach if you want to train a model that estimates the taxi     trip time based on the traffic metrics for the route in the last 5 minutes,     in the last 10 minutes, in the last 30 minutes, and at other intervals. You     can also use this approach to predict the failure of an engine part based     on the moving average of temperature and vibration values computed over the     last 3 minutes. Although these aggregations can be prepared offline for     training, they are computed in real time from a data stream during serving.</p> <p>More precisely, when you prepare training data, if the aggregated value isn't in the raw data, the value is created during the data engineering phase. The raw data is usually stored in a database with a format of <code>(entity, timestamp, value)</code>. In the previous examples, <code>entity</code> is the route segment identifier for the taxi routes and the engine part identifier for the engine failure. You can use windowing operations to compute <code>(entity, time_index, aggregated_value_over_time_window)</code> and use the aggregation features as an input for your model training.</p> <p>When the model for real-time (online) prediction is served, the model expects features derived from the aggregated values as an input. Therefore, you can use a stream-processing technology like Apache Beam to compute the aggregations from the real-time data points streamed into your system. Stream-processing technology aggregates real-time data based on time windows as new data points arrive. You can also perform additional feature engineering (for example, transformation and tuning) to these aggregations before training and prediction.</p> </li> </ul>"},{"location":"guide/tft_bestpractices/#ml-pipeline-on-google-cloud","title":"ML pipeline on Google Cloud","text":"<p>This section discusses the core components of a typical end-to-end pipeline to train and serve TensorFlow ML models on Google Cloud using managed services. It also discusses where you can implement different categories of the data preprocessing operations, and common challenges that you might face when you implement such transformations. The How tf.Transform works section shows how the TensorFlow Transform library helps to address these challenges.</p>"},{"location":"guide/tft_bestpractices/#high-level-architecture","title":"High-level architecture","text":"<p>The following diagram, figure 2, shows a high-level architecture of a typical ML pipeline for training and serving TensorFlow models. The labels A, B, and C in the diagram refer to the different places in the pipeline where data preprocessing can take place. Details about these steps are provided in the following section.</p> <p>Figure 2: High-level architecture for ML training and serving on Google Cloud. </p> <p>The pipeline consists of the following steps:</p> <ol> <li>After raw data is imported, tabular data is stored in BigQuery, and other     data like images, audio, and video, is stored in Cloud Storage. The second     part of this series uses tabular data stored in BigQuery as an example.</li> <li>Data engineering (preparation) and feature engineering are executed at scale     using Dataflow. This execution produces ML-ready training, evaluation, and     test sets that are stored in Cloud Storage. Ideally, these datasets are     stored as     TFRecord files,     which is the optimized format for TensorFlow computations.</li> <li>A TensorFlow model     trainer package     is submitted to Vertex AI Training, which uses the preprocessed data from     the previous steps to train the model. The output of this step is a trained     TensorFlow SavedModel that     is exported to Cloud Storage.</li> <li>The trained TensorFlow model is deployed to Vertex AI Prediction as a     service that has a REST API so that it can be used for online predictions.     The same model can also be used for batch prediction jobs.</li> <li>After the model is deployed as a REST API, client apps and internal systems     can invoke the API by sending requests with some data points, and receiving     responses from the model with predictions.</li> <li>For orchestrating and automating this pipeline, you can use     Vertex AI Pipelines     as a scheduler to invoke the data preparation, model training, and model     deployment steps.</li> </ol> <p>You can also use Vertex AI Feature Store to store input features to make predictions. For example, you can periodically create engineered features from the latest raw data and store them in Vertex AI Feature Store. Client apps fetch the required input features from Vertex AI Feature Store and send them to the model to receive predictions.</p>"},{"location":"guide/tft_bestpractices/#where-to-do-preprocessing","title":"Where to do preprocessing","text":"<p>In figure 2, the labels A, B, and C show that data preprocessing operations can take place in BigQuery, Dataflow, or TensorFlow. The following sections describe how each of these options work.</p>"},{"location":"guide/tft_bestpractices/#option-a-bigquery","title":"Option A: BigQuery","text":"<p>Typically, logic is implemented in BigQuery for the following operations:</p> <ul> <li>Sampling: randomly selecting a subset from the data.</li> <li>Filtering: removing irrelevant or invalid instances.</li> <li>Partitioning: splitting the data to produce training, evaluation, and     test sets.</li> </ul> <p>BigQuery SQL scripts can be used as a source query for the Dataflow preprocessing pipeline, which is the data processing step in figure 2. For example, if a system is used in Canada, and the data warehouse has transactions from around the world, filtering to get Canada-only training data is best done in BigQuery. Feature engineering in BigQuery is simple and scalable, and supports implementing instance-level and historical aggregations feature transformations.</p> <p>However, we recommend that you use BigQuery for feature engineering only if you use your model for batch prediction (scoring), or if the features are precomputed in BigQuery, but stored in Vertex AI Feature Store to be used during online prediction. If you plan to deploy the model for online predictions, and if you don't have the engineered feature in an online feature store, you have to replicate the SQL preprocessing operations to transform the raw data points that other systems generate. In other words, you need to implement the logic twice: one time in SQL to preprocess training data in BigQuery, and a second time in the logic of the app that consumes the model to preprocess online data points for prediction.</p> <p>For example, if your client app is written in Java, you need to reimplement the logic in Java. This can introduce errors due to implementation discrepancies, as described in the training-serving skew section of Preprocessing challenges later in this document. It's also extra overhead to maintain two different implementations. Whenever you change the logic in SQL to preprocess the training data, you need to change the Java implementation accordingly to preprocess data at serving time.</p> <p>If you are using your model only for batch prediction (for example, using Vertex AI batch prediction), and if your data for scoring is sourced from BigQuery, you can implement these preprocessing operations as part of the BigQuery SQL script. In that case, you can use the same preprocessing SQL script to prepare both training and scoring data.</p> <p>Full-pass stateful transformations aren't suitable for implementation in BigQuery. If you use BigQuery for full-pass transformations, you need auxiliary tables to store quantities needed by stateful transformations, such as means and variances to scale numerical features. Further, implementation of full-pass transformations using SQL on BigQuery creates increased complexity in the SQL scripts, and creates intricate dependency between training and the scoring SQL scripts.</p>"},{"location":"guide/tft_bestpractices/#option-b-dataflow","title":"Option B: Dataflow","text":"<p>As shown in figure 2, you can implement computationally expensive preprocessing operations in Apache Beam, and run them at scale using Dataflow. Dataflow is a fully managed autoscaling service for batch and stream data processing. When you use Dataflow, you can also use external specialized libraries for data processing, unlike BigQuery.</p> <p>Dataflow can perform instance-level transformations, and historical and real-time aggregation feature transformations. In particular, if your ML models expect an input feature like <code>total_number_of_clicks_last_90sec</code>, Apache Beam windowing functions can compute these features based on aggregating the values of time windows of real-time (streaming) events data (for example, click events). In the earlier discussion of granularity of transformations, this was referred to as \"Historical aggregations during training, but real-time aggregations during prediction.\"</p> <p>The following diagram, figure 3, shows the role of Dataflow in processing stream data for near real-time predictions.</p> <p>Figure 3: High-level architecture using stream data for prediction in Dataflow. </p> <p>As shown in figure 3, during processing, events called data points are ingested into Pub/Sub. Dataflow consumes these data points, computes features based on aggregates over time, and then calls the deployed ML model API for predictions. Predictions are then sent to an outbound Pub/Sub queue. From Pub/Sub, predictions can be consumed by downstream systems like monitoring or control, or they can be pushed back (for example, as notifications) to the original requesting client. Predictions can also be stored in a low-latency data store like Cloud Bigtable for real-time fetching. Cloud Bigtable can also be used to accumulate and store these real-time aggregations so they can be looked up when needed for prediction.</p> <p>The same Apache Beam implementation can be used to batch-process training data that comes from an offline datastore like BigQuery and stream-process real-time data for serving online predictions.</p> <p>In other typical architectures, such as the architecture shown in figure 2, the client app directly calls the deployed model API for online predictions. In that case, if preprocessing operations are implemented in Dataflow to prepare the training data, the operations aren't applied to the prediction data that goes directly to the model. Therefore, transformations like these should be integrated in the model during serving for online predictions.</p> <p>Dataflow can be used to perform full-pass transformation, by computing the required statistics at scale. However, these statistics need to be stored somewhere to be used during prediction to transform prediction data points. By using the TensorFlow Transform (<code>tf.Transform</code>) library, you can directly embed these statistics in the model instead of storing them elsewhere. This approach is explained later in How tf.Transform works.</p>"},{"location":"guide/tft_bestpractices/#option-c-tensorflow","title":"Option C: TensorFlow","text":"<p>As shown in figure 2, you can implement data preprocessing and transformation operations in the TensorFlow model itself. As shown in the figure, the preprocessing that you implement for training the TensorFlow model becomes an integral part of the model when the model is exported and deployed for predictions. Transformations in the TensorFlow model can be accomplished in one of the following ways:</p> <ul> <li>Implementing all of the instance-level transformation logic in the     <code>input_fn</code> function and in the <code>serving_fn</code> function. The <code>input_fn</code>     function prepares a dataset using the     <code>tf.data.Dataset</code> API     for training a model. The <code>serving_fn</code> function receives and prepares the     data for predictions.</li> <li>Putting the transformation code directly in your TensorFlow     model by using     Keras preprocessing layers     or     creating custom layers.</li> </ul> <p>The transformation logic code in the <code>serving_fn</code> function defines the serving interface of your SavedModel for online prediction. If you implement the same transformations that were used for preparing training data in the transformation logic code of the <code>serving_fn</code> function, it ensures that the same transformations are applied to new prediction data points when they're served.</p> <p>However, because the TensorFlow model processes each data point independently or in a small batch, you can't calculate aggregations from all data points. As a result, full-pass transformations can't be implemented in your TensorFlow model.</p>"},{"location":"guide/tft_bestpractices/#preprocessing-challenges","title":"Preprocessing challenges","text":"<p>The following are the primary challenges of implementing data preprocessing:</p> <ul> <li> <p>Training-serving skew.     Training-serving skew     refers to a difference between effectiveness (predictive performance)     during training and during serving. This skew can be caused by a     discrepancy between how you handle data in the training and the serving     pipelines. For example, if your model is trained on a logarithmically     transformed feature, but it's presented with the raw feature during     serving, the prediction output might not be accurate.</p> <p>If the transformations become part of the model itself, it can be straightforward to handle instance-level transformations, as described earlier in Option C: TensorFlow. In that case, the model serving interface (the <code>serving_fn</code> function) expects raw data, while the model internally transforms this data before computing the output. The transformations are the same as those that were applied on the raw training and prediction data points.</p> </li> <li> <p>Full-pass transformations. You can't implement full-pass     transformations such as scaling and normalization transformations in your     TensorFlow model. In full-pass transformations, some     statistics (for example, <code>max</code> and <code>min</code> values to scale numeric features)     must be computed on the training data beforehand, as described in     Option B: Dataflow.     The values then have to be stored somewhere to be used during model serving     for prediction to transform the new raw data points as instance-level     transformations, which avoids training-serving skew. You can use the     TensorFlow Transform (<code>tf.Transform</code>) library to directly     embed the statistics in your TensorFlow model. This approach     is explained later in     How tf.Transform works.</p> </li> <li> <p>Preparing the data up front for better training efficiency.     Implementing instance-level transformations as part of the model can     degrade the efficiency of the training process. This degradation occurs     because the same transformations are repeatedly applied to the same     training data on each epoch. Imagine that you have raw training data with     1,000 features, and you apply a mix of instance-level transformations to     generate 10,000 features. If you implement these transformations as part of     your model, and if you then feed the model the raw training data, these     10,000 operations are applied N times on each instance, where N is the     number of epochs. In addition, if you're using accelerators (GPUs or TPUs),     they sit idle while the CPU performs those transformations, which isn't an     efficient use of your costly accelerators.</p> <p>Ideally, the training data is transformed before training, using the technique described under Option B: Dataflow, where the 10,000 transformation operations are applied only once on each training instance. The transformed training data is then presented to the model. No further transformations are applied, and the accelerators are busy all of the time. In addition, using Dataflow helps you to preprocess large amounts of data at scale, using a fully managed service.</p> <p>Preparing the training data up front can improve training efficiency. However, implementing the transformation logic outside of the model (the approaches described in Option A: BigQuery or Option B: Dataflow) doesn't resolve the issue of training-serving skew. Unless you store the engineered feature in the feature store to be used for both training and prediction, the transformation logic must be implemented somewhere to be applied on new data points coming for prediction, because the model interface expects transformed data. The TensorFlow Transform (<code>tf.Transform</code>) library can help you to address this issue, as described in the following section.</p> </li> </ul>"},{"location":"guide/tft_bestpractices/#how-tftransform-works","title":"How tf.Transform works","text":"<p>The <code>tf.Transform</code> library is useful for transformations that require a full pass. The output of the <code>tf.Transform</code> library is exported as a TensorFlow graph that represents the instance-level transformation logic and the statistics computed from full-pass transformations, to be used for training and serving. Using the same graph for both training and serving can prevent skew, because the same transformations are applied in both stages. In addition, the <code>tf.Transform</code> library can run at scale in a batch processing pipeline on Dataflow to prepare the training data up front and improve training efficiency.</p> <p>The following diagram, figure 4, shows how the <code>tf.Transform</code> library preprocesses and transforms data for training and prediction. The process is described in the following sections.</p> <p>Figure 4: Behavior of <code>tf.Transform</code> for preprocessing and transforming data.</p>"},{"location":"guide/tft_bestpractices/#transform-training-and-evaluation-data","title":"Transform training and evaluation data","text":"<p>You preprocess the raw training data using the transformation implemented in the <code>tf.Transform</code> Apache Beam APIs, and run it at scale on Dataflow. The preprocessing occurs in the following phases:</p> <ul> <li>Analyze phase: During the analyze phase, the required statistics     (like means, variances, and quantiles) for stateful transformations are     computed on the training data with full-pass operations. This phase     produces a set of transformation artifacts, including the <code>transform_fn</code>     graph. The <code>transform_fn</code> graph is a TensorFlow graph that     has the transformation logic as instance-level operations. It includes the     statistics computed in the analyze phase as constants.</li> <li>Transform phase: During the transform phase, the <code>transform_fn</code>     graph is applied to the raw training data, where the computed statistics     are used to process the data records (for example, to scale numerical     columns) in an instance-level fashion.</li> </ul> <p>A two-phase approach like this addresses the preprocessing challenge of performing full-pass transformations.</p> <p>When the evaluation data is preprocessed, only instance-level operations are applied, using the logic in the <code>transform_fn</code> graph and the statistics computed from the analyze phase in the training data. In other words, you don't analyze the evaluation data in a full-pass fashion to compute new statistics, such as \u03bc and \u03c3, to normalize numeric features in evaluation data. Instead, you use the computed statistics from the training data to transform the evaluation data in an instance-level fashion.</p> <p>The transformed training and evaluation data are prepared at scale using Dataflow, before they are used to train the model. This batch data-preparation process addresses the preprocessing challenge of preparing the data up front to improve training efficiency. As shown in figure 4, the model internal interface expects transformed features.</p>"},{"location":"guide/tft_bestpractices/#attach-transformations-to-the-exported-model","title":"Attach transformations to the exported model","text":"<p>As noted, the <code>transform_fn</code> graph that's produced by the <code>tf.Transform</code> pipeline is stored as an exported TensorFlow graph. The exported graph consists of the transformation logic as instance-level operations, and all of the statistics computed in the full-pass transformations as graph constants. When the trained model is exported for serving, the <code>transform_fn</code> graph is attached to the SavedModel as part of its <code>serving_fn</code> function.</p> <p>While it's serving the model for prediction, the model serving interface expects data points in the raw format (that is, before any transformations). However, the model internal interface expects the data in the transformed format.</p> <p>The <code>transform_fn</code> graph, which is now part of the model, applies all the preprocessing logic on the incoming data point. It uses the stored constants (like \u03bc and \u03c3 to normalize the numeric features) in the instance-level operation during prediction. Therefore, the <code>transform_fn</code> graph converts the raw data point into the transformed format. The transformed format is what is expected by the model internal interface in order to produce prediction, as shown in figure 4.</p> <p>This mechanism resolves the preprocessing challenge of the training-serving skew, because the same logic (implementation) that is used to transform the training and evaluation data is applied to transform the new data points during prediction serving.</p>"},{"location":"guide/tft_bestpractices/#preprocessing-options-summary","title":"Preprocessing options summary","text":"<p>The following table summarizes the data preprocessing options that this document discussed. In the table, \"N/A\" stands for \"not applicable.\"</p> <p>Data preprocessing option</p> <p>Instance-level</p> <p>(stateless transformations)</p> <p>Full-pass during training and instance-level during serving</p> <p>(stateful transformations)</p> <p>Real-time (window) aggregations during training and serving</p> <p>(streaming transformations)</p> <p>BigQuery</p> <p>(SQL)</p> <p>Batch scoring: OK\u2014the same transformation implementation is applied on data during training and batch scoring.</p> <p>Online prediction: Not recommended\u2014you can process training data, but it results in training-serving skew because you process serving data using different tools. </p> <p>Batch scoring: Not recommended.</p> <p>Online prediction: Not recommended.</p> <p>Although you can use statistics computed using BigQuery for instance-level batch/online transformations, it isn't easy because you must maintain a stats store to be populated during training and used during prediction.</p> <p>Batch scoring: N/A\u2014aggregates like these are computed based on real-time events.</p> <p>Online prediction: Not recommended\u2014you can process training data, but it results in training-serving skew because you process serving data using different tools. </p> <p>Dataflow</p> <p>(Apache Beam)</p> <p>Batch scoring: OK\u2014the same transformation implementation is applied on data during training and batch scoring.</p> <p>Online prediction: OK\u2014if data at serving time comes from Pub/Sub to be consumed by Dataflow. Otherwise, results in training-serving skew.</p> <p>Batch scoring: Not recommended.</p> <p>Online predictions: Not recommended.</p> <p>Although you can use statistics computed using Dataflow for instance-level batch/online transformations, it isn't easy because you must maintain a stats store to be populated during training and used during prediction.</p> <p>Batch scoring: N/A---aggregates like these are computed based on real-time events.</p> <p>Online prediction: OK\u2014the same Apache Beam transformation is applied on data during training (batch) and serving (stream).</p> <p>Dataflow</p> <p>(Apache Beam + TFT)</p> <p>Batch scoring: OK\u2014the same transformation implementation is applied to data during training and batch scoring.</p> <p>Online prediction: Recommended\u2014it avoids training-serving skew and prepares training data up front.</p> <p>Batch scoring: Recommended.</p> <p>Online prediction: Recommended.</p> <p>Both uses are recommended because transformation logic and computed statistics during training are stored as a TensorFlow graph that's attached to the exported model for serving.</p> <p>Batch scoring: N/A---aggregates like these are computed based on real-time events.</p> <p>Online prediction: OK\u2014the same Apache Beam transformation is applied on data during training (batch) and serving (stream).</p> <p>TensorFlow <sup>*</sup></p> <p>(<code>input_fn</code> &amp; <code>serving_fn</code>)</p> <p>Batch scoring: Not recommended.</p> <p>Online prediction: Not recommended.</p> <p>For training efficiency in both cases, it's better to prepare the training data up front.</p> <p>Batch scoring: Not Possible.</p> <p>Online prediction: Not Possible.</p> <p>Batch scoring: N/A\u2014aggregates like these are computed based on real-time events.</p> <p>Online prediction: Not Possible.</p> <p><sup>*</sup> With TensorFlow, transformations like crossing, embedding, and one-hot encoding should be performed declaratively as <code>feature_columns</code> columns.</p>"},{"location":"guide/tft_bestpractices/#whats-next","title":"What's next","text":"<ul> <li>To implement a <code>tf.Transform</code> pipeline and run it using Dataflow, read part     two of this series,     Data preprocessing for ML using TensorFlow Transform.</li> <li>Take the Coursera specialization on ML with     TensorFlow on Google Cloud.</li> <li>Learn about best practices for ML engineering in     Rules of ML.</li> <li>For more reference architectures, diagrams, and best practices, explore the     TFX     Cloud Solutions.</li> </ul>"},{"location":"guide/train/","title":"Designing TensorFlow Modeling Code For TFX","text":"<p>When designing your TensorFlow modeling code for TFX there are a few items to be aware of, including the choice of a modeling API.</p> <ul> <li>Consumes: SavedModel from Transform, and data from ExampleGen</li> <li>Emits: Trained model in SavedModel format</li> </ul> <p>Note</p> <p>TFX supports nearly all of TensorFlow 2.X, with minor exceptions. TFX also fully supports TensorFlow 1.15.</p> <ul> <li>New TFX pipelines should use TensorFlow 2.x with Keras models via the Generic Trainer.</li> <li>Full support for TensorFlow 2.X, including improved support for tf.distribute, will be added incrementally in upcoming releases.</li> <li>Previous TFX pipelines can continue to use TensorFlow 1.15. To switch them to TensorFlow 2.X, see the TensorFlow migration guide.</li> </ul> <p>To keep up to date on TFX releases, see the TFX OSS Roadmap, read the TFX blog and subscribe to the TensorFlow newsletter.</p> <p>Your model's input layer should consume from the SavedModel that was created by a Transform component, and the layers of the Transform model should be included with your model so that when you export your SavedModel and EvalSavedModel they will include the transformations that were created by the Transform component.</p>"},{"location":"guide/trainer/","title":"The Trainer TFX Pipeline Component","text":"<p>The Trainer TFX pipeline component trains a TensorFlow model.</p>"},{"location":"guide/trainer/#trainer-and-tensorflow","title":"Trainer and TensorFlow","text":"<p>Trainer makes extensive use of the Python TensorFlow API for training models.</p> <p>Note</p> <p>TFX supports TensorFlow 1.15 and 2.x.</p>"},{"location":"guide/trainer/#component","title":"Component","text":"<p>Trainer takes:</p> <ul> <li>tf.Examples used for training and eval.</li> <li>A user provided module file that defines the trainer logic.</li> <li>Protobuf definition of     train args and eval args.</li> <li>(Optional) A data schema created by a SchemaGen pipeline component and     optionally altered by the developer.</li> <li>(Optional) transform graph produced by an upstream Transform component.</li> <li>(Optional) pre-trained models used for scenarios such as warmstart.</li> <li>(Optional) hyperparameters, which will be passed to user module function.     Details of the integration with Tuner can be found here.</li> </ul> <p>Trainer emits: At least one model for inference/serving (typically in SavedModelFormat) and optionally another model for eval (typically an EvalSavedModel).</p> <p>We provide support for alternate model formats such as TFLite through the Model Rewriting Library. See the link to the Model Rewriting Library for examples of how to convert Keras models.</p>"},{"location":"guide/trainer/#generic-trainer","title":"Generic Trainer","text":"<p>Generic trainer enables developers to use any TensorFlow model API with the Trainer component. Developers can use Keras models or custom training loops. For details, please see the RFC for generic trainer.</p>"},{"location":"guide/trainer/#configuring-the-trainer-component","title":"Configuring the Trainer Component","text":"<p>Typical pipeline DSL code for the generic Trainer would look like this:</p> <pre><code>from tfx.components import Trainer\n\n...\n\ntrainer = Trainer(\n    module_file=module_file,\n    examples=transform.outputs['transformed_examples'],\n    transform_graph=transform.outputs['transform_graph'],\n    train_args=trainer_pb2.TrainArgs(num_steps=10000),\n    eval_args=trainer_pb2.EvalArgs(num_steps=5000))\n</code></pre> <p>Trainer invokes a training module, which is specified in the <code>module_file</code> parameter. A <code>run_fn</code> is required in the module file, and it needs to handle the training part and output the trained model to a the desired location given by FnArgs:</p> <pre><code>from tfx.components.trainer.fn_args_utils import FnArgs\n\ndef run_fn(fn_args: FnArgs) -&gt; None:\n  \"\"\"Build the TF model and train it.\"\"\"\n  model = _build_keras_model()\n  model.fit(...)\n  # Save model to fn_args.serving_model_dir.\n  model.save(fn_args.serving_model_dir, ...)\n</code></pre> <p>Here is an example module file with <code>run_fn</code>.</p> <p>Note that if the Transform component is not used in the pipeline, then the Trainer would take the examples from ExampleGen directly:</p> <pre><code>trainer = Trainer(\n    module_file=module_file,\n    examples=example_gen.outputs['examples'],\n    schema=infer_schema.outputs['schema'],\n    train_args=trainer_pb2.TrainArgs(num_steps=10000),\n    eval_args=trainer_pb2.EvalArgs(num_steps=5000))\n</code></pre> <p>More details are available in the Trainer API reference.</p>"},{"location":"guide/transform/","title":"The Transform TFX Pipeline Component","text":"<p>The Transform TFX pipeline component performs feature engineering on tf.Examples emitted from an ExampleGen component, using a data schema created by a SchemaGen component, and emits both a SavedModel as well as statistics on both pre-transform and post-transform data.  When executed, the SavedModel will accept tf.Examples emitted from an ExampleGen component and emit the transformed feature data.</p> <ul> <li>Consumes: tf.Examples from an ExampleGen component, and a data schema from a SchemaGen component.</li> <li>Emits: A SavedModel to a Trainer component, pre-transform and post-transform statistics.</li> </ul>"},{"location":"guide/transform/#configuring-a-transform-component","title":"Configuring a Transform Component","text":"<p>Once your <code>preprocessing_fn</code> is written, it needs to be defined in a python module that is then provided to the Transform component as an input.  This module will be loaded by transform and the function named <code>preprocessing_fn</code> will be found and used by Transform to construct the preprocessing pipeline.</p> <pre><code>transform = Transform(\n    examples=example_gen.outputs['examples'],\n    schema=schema_gen.outputs['schema'],\n    module_file=os.path.abspath(_taxi_transform_module_file))\n</code></pre> <p>Additionally, you may wish to provide options to the TFDV-based pre-transform or post-transform statistics computation. To do so, define a <code>stats_options_updater_fn</code> within the same module.</p>"},{"location":"guide/transform/#transform-and-tensorflow-transform","title":"Transform and TensorFlow Transform","text":"<p>Transform makes extensive use of TensorFlow Transform for performing feature engineering on your dataset.  TensorFlow Transform is a great tool for transforming feature data before it goes to your model and as a part of the training process. Common feature transformations include:</p> <ul> <li>Embedding: converting sparse features (like the integer IDs produced by a     vocabulary) into dense features by finding a meaningful mapping from high-     dimensional space to low dimensional space. See the Embeddings unit in the     Machine-learning Crash Course     for an introduction to embeddings.</li> <li>Vocabulary generation: converting strings or other non-numeric features     into integers by creating a vocabulary that maps each unique value to an ID     number.</li> <li>Normalizing values: transforming numeric features so that they all fall     within a similar range.</li> <li>Bucketization: converting continuous-valued features into categorical     features by assigning values to discrete buckets.</li> <li>Enriching text features: producing features from raw data like tokens,     n-grams, entities, sentiment, etc., to enrich the feature set.</li> </ul> <p>TensorFlow Transform provides support for these and many other kinds of transformations:</p> <ul> <li> <p>Automatically generate a vocabulary from your latest data.</p> </li> <li> <p>Perform arbitrary transformations on your data before sending it to your   model. TensorFlow Transform builds transformations into the TensorFlow graph for   your model so the same transformations are performed at training and inference   time. You can define transformations that refer to global properties of the   data, like the max value of a feature across all training instances.</p> </li> </ul> <p>You can transform your data however you like prior to running TFX. But if you do it within TensorFlow Transform, transforms become part of the TensorFlow graph. This approach helps avoid training/serving skew.</p> <p>Transformations inside your modeling code use FeatureColumns. Using FeatureColumns, you can define bucketizations, integerizations that use predefined vocabularies, or any other transformations that can be defined without looking at the data.</p> <p>By contrast, TensorFlow Transform is designed for transformations that require a full pass over the data to compute values that are not known in advance. For example, vocabulary generation requires a full pass over the data.</p> <p>Note</p> <p>These computations are implemented in Apache Beam under the hood.</p> <p>In addition to computing values using Apache Beam, TensorFlow Transform allows users to embed these values into a TensorFlow graph, which can then be loaded into the training graph. For example when normalizing features, the <code>tft.scale_to_z_score</code> function will compute the mean and standard deviation of a feature, and also a representation, in a TensorFlow graph, of the function that subtracts the mean and divides by the standard deviation. By emitting a TensorFlow graph, not just statistics, TensorFlow Transform simplifies the process of authoring your preprocessing pipeline.</p> <p>Since the preprocessing is expressed as a graph, it can happen on the server, and it's guaranteed to be consistent between training and serving. This consistency eliminates one source of training/serving skew.</p> <p>TensorFlow Transform allows users to specify their preprocessing pipeline using TensorFlow code. This means that a pipeline is constructed in the same manner as a TensorFlow graph. If only TensorFlow ops were used in this graph, the pipeline would be a pure map that accepts batches of input and returns batches of output. Such a pipeline would be equivalent to placing this graph inside your <code>input_fn</code> when using the <code>tf.Estimator</code> API. In order to specify full-pass operations such as computing quantiles, TensorFlow Transform provides special functions called <code>analyzers</code> that appear like TensorFlow ops, but in fact specify a deferred computation that will be done by Apache Beam, and the output inserted into the graph as a constant. While an ordinary TensorFlow op will take a single batch as its input, perform some computation on just that batch and emit a batch, an <code>analyzer</code> will perform a global reduction (implemented in Apache Beam) over all batches and return the result.</p> <p>By combining ordinary TensorFlow ops and TensorFlow Transform analyzers, users can create complex pipelines to preprocess their data. For example the <code>tft.scale_to_z_score</code> function takes an input tensor and returns that tensor normalized to have mean <code>0</code> and variance <code>1</code>. It does this by calling the <code>mean</code> and <code>var</code> analyzers under the hood, which will effectively generate constants in the graph equal to the mean and variance of the input tensor. It will then use TensorFlow ops to subtract the mean and divide by the standard deviation.</p>"},{"location":"guide/transform/#the-tensorflow-transform-preprocessing_fn","title":"The TensorFlow Transform <code>preprocessing_fn</code>","text":"<p>The TFX Transform component simplifies the use of Transform by handling the API calls related to reading and writing data, and writing the output SavedModel to disk.  As a TFX user, you only have to define a single function called the <code>preprocessing_fn</code>. In <code>preprocessing_fn</code> you define a series of functions that manipulate the input dict of tensors to produce the output dict of tensors. You can find helper functions like scale_to_0_1 and compute_and_apply_vocabulary the TensorFlow Transform API or use regular TensorFlow functions as shown below.</p> <pre><code>def preprocessing_fn(inputs):\n  \"\"\"tf.transform's callback function for preprocessing inputs.\n\n  Args:\n    inputs: map from feature keys to raw not-yet-transformed features.\n\n  Returns:\n    Map from string feature key to transformed feature operations.\n  \"\"\"\n  outputs = {}\n  for key in _DENSE_FLOAT_FEATURE_KEYS:\n    # If sparse make it dense, setting nan's to 0 or '', and apply zscore.\n    outputs[_transformed_name(key)] = transform.scale_to_z_score(\n        _fill_in_missing(inputs[key]))\n\n  for key in _VOCAB_FEATURE_KEYS:\n    # Build a vocabulary for this feature.\n    outputs[_transformed_name(\n        key)] = transform.compute_and_apply_vocabulary(\n            _fill_in_missing(inputs[key]),\n            top_k=_VOCAB_SIZE,\n            num_oov_buckets=_OOV_SIZE)\n\n  for key in _BUCKET_FEATURE_KEYS:\n    outputs[_transformed_name(key)] = transform.bucketize(\n        _fill_in_missing(inputs[key]), _FEATURE_BUCKET_COUNT)\n\n  for key in _CATEGORICAL_FEATURE_KEYS:\n    outputs[_transformed_name(key)] = _fill_in_missing(inputs[key])\n\n  # Was this passenger a big tipper?\n  taxi_fare = _fill_in_missing(inputs[_FARE_KEY])\n  tips = _fill_in_missing(inputs[_LABEL_KEY])\n  outputs[_transformed_name(_LABEL_KEY)] = tf.where(\n      tf.is_nan(taxi_fare),\n      tf.cast(tf.zeros_like(taxi_fare), tf.int64),\n      # Test if the tip was &gt; 20% of the fare.\n      tf.cast(\n          tf.greater(tips, tf.multiply(taxi_fare, tf.constant(0.2))), tf.int64))\n\n  return outputs\n</code></pre>"},{"location":"guide/transform/#understanding-the-inputs-to-the-preprocessing_fn","title":"Understanding the inputs to the preprocessing_fn","text":"<p>The <code>preprocessing_fn</code> describes a series of operations on tensors (that is, <code>Tensor</code>s, <code>SparseTensor</code>s, or <code>RaggedTensor</code>s). In order to define the <code>preprocessing_fn</code> correctly it is necessary to understand how the data is represented as tensors. The input to the <code>preprocessing_fn</code> is determined by the schema. A <code>Schema</code> proto is eventually converted to a \"feature spec\" (sometimes called a \"parsing spec\") that is used for data parsing, see more details about the conversion logic here.</p>"},{"location":"guide/transform/#using-tensorflow-transform-to-handle-string-labels","title":"Using TensorFlow Transform to handle string labels","text":"<p>Usually one wants to use TensorFlow Transform to both generate a vocabulary and apply that vocabulary to convert strings to integers. When following this workflow, the <code>input_fn</code> constructed in the model will output the integerized string. However labels are an exception, because in order for the model to be able to map the output (integer) labels back to strings, the model needs the <code>input_fn</code> to output a string label, together with a list of possible values of the label. E.g. if the labels are <code>cat</code> and <code>dog</code> then the output of the <code>input_fn</code> should be these raw strings, and the keys <code>[\"cat\", \"dog\"]</code> need to be passed into the estimator as a parameter (see details below).</p> <p>In order to handle the mapping of string labels to integers, you should use TensorFlow Transform to generate a vocabulary. We demonstrate this in the code snippet below:</p> <pre><code>def _preprocessing_fn(inputs):\n  \"\"\"Preprocess input features into transformed features.\"\"\"\n\n  ...\n\n\n  education = inputs[features.RAW_LABEL_KEY]\n  _ = tft.vocabulary(education, vocab_filename=features.RAW_LABEL_KEY)\n\n  ...\n</code></pre> <p>The preprocessing function above takes the raw input feature (which will also be returned as part of the output of the preprocessing function) and calls <code>tft.vocabulary</code> on it. This results in a vocabulary being generated for <code>education</code> that can be accessed in the model.</p> <p>The example also shows how to transform a label and then generate a vocabulary for the transformed label. In particular it takes the raw label <code>education</code> and converts all but the top 5 labels (by frequency) to <code>UNKNOWN</code>, without converting the label to an integer.</p> <p>In the model code, the classifier must be given the vocabulary generated by <code>tft.vocabulary</code> as the <code>label_vocabulary</code> argument. This is done by first reading this vocabulary as a list with a helper function. This is shown in the snippet below. Note the example code uses the transformed label discussed above but here we show code for using the raw label.</p> <pre><code>def create_estimator(pipeline_inputs, hparams):\n\n  ...\n\n  tf_transform_output = trainer_util.TFTransformOutput(\n      pipeline_inputs.transform_dir)\n\n  # vocabulary_by_name() returns a Python list.\n  label_vocabulary = tf_transform_output.vocabulary_by_name(\n      features.RAW_LABEL_KEY)\n\n  return tf.contrib.learn.DNNLinearCombinedClassifier(\n      ...\n      n_classes=len(label_vocab),\n      label_vocabulary=label_vocab,\n      ...)\n</code></pre>"},{"location":"guide/transform/#configuring-pre-transform-and-post-transform-statistics","title":"Configuring pre-transform and post-transform statistics","text":"<p>As mentioned above, the Transform component invokes TFDV to compute both pre-transform and post-transform statistics. TFDV takes as input an optional StatsOptions object. Users may wish to configure this object to enable certain additional statistics (e.g. NLP statistics) or to set thresholds that are validated (e.g. min / max token frequency). To do so, define a <code>stats_options_updater_fn</code> in the module file.</p> <pre><code>def stats_options_updater_fn(stats_type, stats_options):\n  ...\n  if stats_type == stats_options_util.StatsType.PRE_TRANSFORM:\n    # Update stats_options to modify pre-transform statistics computation.\n    # Most constraints are specified in the schema which can be accessed\n    # via stats_options.schema.\n  if stats_type == stats_options_util.StatsType.POST_TRANSFORM\n    # Update stats_options to modify post-transform statistics computation.\n    # Most constraints are specified in the schema which can be accessed\n    # via stats_options.schema.\n  return stats_options\n</code></pre> <p>Post-transform statistics often benefit from knowledge of the vocabulary being used for preprocessing a feature. The vocabulary name to path mapping is provided to StatsOptions (and hence TFDV) for every TFT-generated vocabulary. Additionally, mappings for externally-created vocabularies can be added by either (i) directly modifying the <code>vocab_paths</code> dictionary within StatsOptions or by (ii) using <code>tft.annotate_asset</code>.</p>"},{"location":"guide/tuner/","title":"The Tuner TFX Pipeline Component","text":"<p>The Tuner component tunes the hyperparameters for the model.</p>"},{"location":"guide/tuner/#tuner-component-and-kerastuner-library","title":"Tuner Component and KerasTuner Library","text":"<p>The Tuner component makes extensive use of the Python KerasTuner API for tuning hyperparameters.</p> <p>Note</p> <p>The KerasTuner library can be used for hyperparameter tuning regardless of the modeling API, not just for Keras models only.</p>"},{"location":"guide/tuner/#component","title":"Component","text":"<p>Tuner takes:</p> <ul> <li>tf.Examples used for training and eval.</li> <li>A user provided module file (or module fn) that defines the tuning logic,     including model definition, hyperparameter search space, objective etc.</li> <li>Protobuf definition of     train args and eval args.</li> <li>(Optional) Protobuf     definition of tuning args.</li> <li>(Optional) transform graph produced by an upstream Transform component.</li> <li>(Optional) A data schema created by a SchemaGen pipeline component and     optionally altered by the developer.</li> </ul> <p>With the given data, model, and objective, Tuner tunes the hyperparameters and emits the best result.</p>"},{"location":"guide/tuner/#instructions","title":"Instructions","text":"<p>A user module function <code>tuner_fn</code> with the following signature is required for Tuner:</p> <pre><code>...\nfrom keras_tuner.engine import base_tuner\n\nTunerFnResult = NamedTuple('TunerFnResult', [('tuner', base_tuner.BaseTuner),\n                                             ('fit_kwargs', Dict[Text, Any])])\n\ndef tuner_fn(fn_args: FnArgs) -&gt; TunerFnResult:\n  \"\"\"Build the tuner using the KerasTuner API.\n  Args:\n    fn_args: Holds args as name/value pairs.\n      - working_dir: working dir for tuning.\n      - train_files: List of file paths containing training tf.Example data.\n      - eval_files: List of file paths containing eval tf.Example data.\n      - train_steps: number of train steps.\n      - eval_steps: number of eval steps.\n      - schema_path: optional schema of the input data.\n      - transform_graph_path: optional transform graph produced by TFT.\n  Returns:\n    A namedtuple contains the following:\n      - tuner: A BaseTuner that will be used for tuning.\n      - fit_kwargs: Args to pass to tuner's run_trial function for fitting the\n                    model , e.g., the training and validation dataset. Required\n                    args depend on the above tuner's implementation.\n  \"\"\"\n  ...\n</code></pre> <p>In this function, you define both the model and hyperparameter search spaces, and choose the objective and algorithm for tuning. The Tuner component takes this module code as input, tunes the hyperparameters, and emits the best result.</p> <p>Trainer can take Tuner's output hyperparameters as input and utilize them in its user module code. The pipeline definition looks like this:</p> <pre><code>...\ntuner = Tuner(\n    module_file=module_file,  # Contains `tuner_fn`.\n    examples=transform.outputs['transformed_examples'],\n    transform_graph=transform.outputs['transform_graph'],\n    train_args=trainer_pb2.TrainArgs(num_steps=20),\n    eval_args=trainer_pb2.EvalArgs(num_steps=5))\n\ntrainer = Trainer(\n    module_file=module_file,  # Contains `run_fn`.\n    examples=transform.outputs['transformed_examples'],\n    transform_graph=transform.outputs['transform_graph'],\n    schema=schema_gen.outputs['schema'],\n    # This will be passed to `run_fn`.\n    hyperparameters=tuner.outputs['best_hyperparameters'],\n    train_args=trainer_pb2.TrainArgs(num_steps=100),\n    eval_args=trainer_pb2.EvalArgs(num_steps=5))\n...\n</code></pre> <p>You might not want to tune the hyperparameters every time you retrain your model. Once you have used Tuner to determine a good set of hyperparameters, you can remove Tuner from your pipeline and use <code>ImporterNode</code> to import the Tuner artifact from a previous training run to feed to Trainer.</p> <pre><code>hparams_importer = Importer(\n    # This can be Tuner's output file or manually edited file. The file contains\n    # text format of hyperparameters (keras_tuner.HyperParameters.get_config())\n    source_uri='path/to/best_hyperparameters.txt',\n    artifact_type=HyperParameters,\n).with_id('import_hparams')\n\ntrainer = Trainer(\n    ...\n    # An alternative is directly use the tuned hyperparameters in Trainer's user\n    # module code and set hyperparameters to None here.\n    hyperparameters = hparams_importer.outputs['result'])\n</code></pre>"},{"location":"guide/tuner/#tuning-on-google-cloud-platform-gcp","title":"Tuning on Google Cloud Platform (GCP)","text":"<p>When running on the Google Cloud Platform (GCP), the Tuner component can take advantage of two services:</p> <ul> <li>AI Platform Vizier     (via CloudTuner implementation)</li> <li>AI Platform Training     (as a flock manager for distributed tuning)</li> </ul>"},{"location":"guide/tuner/#ai-platform-vizier-as-the-backend-of-hyperparameter-tuning","title":"AI Platform Vizier as the backend of hyperparameter tuning","text":"<p>AI Platform Vizier is a managed service that performs black box optimization, based on the Google Vizier technology.</p> <p>CloudTuner is an implementation of KerasTuner which talks to the AI Platform Vizier service as the study backend. Since CloudTuner is a subclass of <code>keras_tuner.Tuner</code>, it can be used as a drop-in replacement in the <code>tuner_fn</code> module, and execute as a part of the TFX Tuner component.</p> <p>Below is a code snippet which shows how to use <code>CloudTuner</code>. Notice that configuration to <code>CloudTuner</code> requires items which are specific to GCP, such as the <code>project_id</code> and <code>region</code>.</p> <pre><code>...\nfrom tensorflow_cloud import CloudTuner\n\n...\ndef tuner_fn(fn_args: FnArgs) -&gt; TunerFnResult:\n  \"\"\"An implementation of tuner_fn that instantiates CloudTuner.\"\"\"\n\n  ...\n  tuner = CloudTuner(\n      _build_model,\n      hyperparameters=...,\n      ...\n      project_id=...,       # GCP Project ID\n      region=...,           # GCP Region where Vizier service is run.\n  )\n\n  ...\n  return TuneFnResult(\n      tuner=tuner,\n      fit_kwargs={...}\n  )\n</code></pre>"},{"location":"guide/tuner/#parallel-tuning-on-cloud-ai-platform-training-distributed-worker-flock","title":"Parallel tuning on Cloud AI Platform Training distributed worker flock","text":"<p>The KerasTuner framework as the underlying implementation of the Tuner component has ability to conduct hyperparameter search in parallel. While the stock Tuner component does not have ability to execute more than one search worker in parallel, by using the Google Cloud AI Platform extension Tuner component, it provides the ability to run parallel tuning, using an AI Platform Training Job as a distributed worker flock manager. TuneArgs is the configuration given to this component. This is a drop-in replacement of the stock Tuner component.</p> <pre><code>tuner = google_cloud_ai_platform.Tuner(\n    ...   # Same kwargs as the above stock Tuner component.\n    tune_args=proto.TuneArgs(num_parallel_trials=3),  # 3-worker parallel\n    custom_config={\n        # Configures Cloud AI Platform-specific configs . For for details, see\n        # https://cloud.google.com/ai-platform/training/docs/reference/rest/v1/projects.jobs#traininginput.\n        TUNING_ARGS_KEY:\n            {\n                'project': ...,\n                'region': ...,\n                # Configuration of machines for each master/worker in the flock.\n                'masterConfig': ...,\n                'workerConfig': ...,\n                ...\n            }\n    })\n...\n</code></pre> <p>The behavior and the output of the extension Tuner component is the same as the stock Tuner component, except that multiple hyperparameter searches are executed in parallel on different worker machines, and as a result, the <code>num_trials</code> will be completed faster. This is particularly effective when the search algorithm is embarrassingly parallelizable, such as <code>RandomSearch</code>. However, if the search algorithm uses information from results of prior trials, such as Google Vizier algorithm implemented in the AI Platform Vizier does, an excessively parallel search would negatively affect the efficacy of the search.</p> <p>It is also possible to use the new Vertex AI api as in the example shown below. </p><pre><code>from tfx.v1.extensions.google_cloud_ai_platform import Tuner\nai_platform_tuning_args = {\n    'project': GOOGLE_CLOUD_PROJECT,\n    'job_spec': {\n        # 'service_account': ACCOUNT,\n        'worker_pool_specs': [{'container_spec': {'image_uri': default_kfp_image},\n                               'machine_spec': {'machine_type': MACHINE_TYPE,\n                               'accelerator_type': accelerator_type,\n                               'accelerator_count': 1\n                                },\n                               'replica_count': 1}],\n\n        # \"enable_web_access\": True, #In case you need to debug from within the container\n        }\n    }\nvertex_job_spec = {\n    'project': GOOGLE_CLOUD_PROJECT,\n    'job_spec': {\n        'worker_pool_specs': [{\n            'machine_spec': {\n                'machine_type': MACHINE_TYPE,\n                'accelerator_type': accelerator_type,\n                'accelerator_count': 1\n                },\n            'replica_count': 1,\n            'container_spec': {\n                'image_uri': default_kfp_image,\n                },\n            }],\n        \"enable_web_access\": True,\n        }\n    }\ntuner = Tuner(\n    module_file=_tuner_module_file,\n    examples=transform.outputs['transformed_examples'],\n    transform_graph=transform.outputs['transform_graph'],\n    train_args=proto.TrainArgs(\n        splits=['train'], num_steps=int(\n            TRAINING_STEPS // 4)),\n    eval_args=proto.EvalArgs(\n        splits=['eval'], num_steps=int(\n            VAL_STEPS // 4)),\n    tune_args=proto.TuneArgs(num_parallel_trials=num_parallel_trials),\n    custom_config={\n        tfx.extensions.google_cloud_ai_platform.ENABLE_VERTEX_KEY:\n        True,\n        tfx.extensions.google_cloud_ai_platform.VERTEX_REGION_KEY:\n        GOOGLE_CLOUD_REGION,\n        tfx.extensions.google_cloud_ai_platform.experimental.TUNING_ARGS_KEY:\n        vertex_job_spec,\n        'use_gpu':\n        USE_GPU,\n        'ai_platform_tuning_args': ai_platform_tuning_args,\n        tfx.extensions.google_cloud_ai_platform.experimental.REMOTE_TRIALS_WORKING_DIR_KEY: os.path.join(PIPELINE_ROOT, 'trials'),\n\n        }\n    )\n</code></pre> <p>Note</p> <p>Each trial in each parallel search is conducted on a single machine in the worker flock, i.e., each trial does not take advantage of multi-worker distributed training. If multi-worker distribution is desired for each trial, refer to <code>DistributingCloudTuner</code>, instead of <code>CloudTuner</code>.</p> <p>Note</p> <p>Both <code>CloudTuner</code> and the Google Cloud AI Platform extensions Tuner component can be used together, in which case it allows distributed parallel tuning backed by the AI Platform Vizier's hyperparameter search algorithm. However, in order to do so, the Cloud AI Platform Job must be given access to the AI Platform Vizier service. See this guide to set up a custom service account. After that, you should specify the custom service account for your training job in the pipeline code. More details see E2E CloudTuner on GCP example.</p>"},{"location":"guide/tuner/#links","title":"Links","text":"<p>E2E Example</p> <p>E2E CloudTuner on GCP Example</p> <p>KerasTuner tutorial</p> <p>CloudTuner tutorial</p> <p>Proposal</p> <p>More details are available in the Tuner API reference.</p>"},{"location":"guide/understanding_custom_components/","title":"Understanding TFX Custom Components","text":"<p>TFX pipelines let you orchestrate your machine learning (ML) workflow on orchestrators, such as: Apache Airflow, Apache Beam, and Kubeflow Pipelines. Pipelines organize your workflow into a sequence of components, where each component performs a step in your ML workflow. TFX standard components provide proven functionality to help you get started building an ML workflow easily. You can also include custom components in your workflow. Custom components let you extend your ML workflow by:</p> <ul> <li>Building components that are tailored to meet your needs, such as ingesting     data from a proprietary system.</li> <li>Applying data augmentation, upsampling, or downsampling.</li> <li>Perform anomaly detection based on confidence intervals or autoencoder     reproduction error.</li> <li>Interfacing with external systems such as help desks for alerting and     monitoring.</li> <li>Applying labels to unlabeled examples.</li> <li>Integrating tools built with languages other than Python into your ML     workflow, such as performing data analysis using R.</li> </ul> <p>By mixing standard components and custom components, you can build an ML workflow that meets your needs while taking advantage of the best practices built into the TFX standard components.</p> <p>This guide describes the concepts required to understand TFX custom components, and the different ways that you can build custom components.</p>"},{"location":"guide/understanding_custom_components/#anatomy-of-a-tfx-component","title":"Anatomy of a TFX Component","text":"<p>This section provides a high-level overview of the composition of a TFX component. If you are new to TFX pipelines, learn the core concepts by reading the guide to understanding TFX pipelines.</p> <p>TFX components are composed of a component specification and an executor class which are packaged in a component interface class.</p> <p>A component specification defines the component's input and output contract. This contract specifies the component's input and output artifacts, and the parameters that are used for the component execution.</p> <p>A component's executor class provides the implementation for the work performed by the component.</p> <p>A component interface class combines the component specification with the executor for use as a component in a TFX pipeline.</p>"},{"location":"guide/understanding_custom_components/#tfx-components-at-runtime","title":"TFX components at runtime","text":"<p>When a pipeline runs a TFX component, the component is executed in three phases:</p> <ol> <li>First, the Driver uses the component specification to retrieve the required     artifacts from the metadata store and pass them into the component.</li> <li>Next, the Executor performs the component's work.</li> <li>Then the Publisher uses the component specification and the results from the     executor to store the component's outputs in the metadata store.</li> </ol> <p></p> <p>Most custom component implementations do not require you to customize the Driver or the Publisher. Typically, modifications to the Driver and Publisher should be necessary only if you want to change the interaction between your pipeline's components and the metadata store. If you only want to change the inputs, outputs, or parameters for your component you only need to modify the component specification.</p>"},{"location":"guide/understanding_custom_components/#types-of-custom-components","title":"Types of custom components","text":"<p>There are three types of custom components: Python function-based components, container-based components, and fully custom components. The following sections describe the different types of components and the cases when you should use each approach.</p>"},{"location":"guide/understanding_custom_components/#python-function-based-components","title":"Python function-based components","text":"<p>Python function-based components are easier to build than container-based components or fully custom components. The component specification is defined in the Python function's arguments using type annotations that describe if an argument is an input artifact, output artifact, or a parameter. The function body defines the component's executor. The component interface is defined by adding the <code>@component</code> decorator to your function.</p> <p>By decorating your function with the <code>@component</code> decorator and defining the function arguments with type annotations, you can create a component without the complexity of building a component specification, an executor, and a component interface.</p> <p>Learn how to build Python function-based components.</p>"},{"location":"guide/understanding_custom_components/#container-based-components","title":"Container-based components","text":"<p>Container-based components provide the flexibility to integrate code written in any language into your pipeline, so long as you can execute that code in a Docker container. To create a container-based component, you must build a Docker container image that contains your component's executable code. Then you must call the <code>create_container_component</code> function to define:</p> <ul> <li>The inputs, outputs, and parameters of your component specification.</li> <li>The container image and command that the component executor runs.</li> </ul> <p>This function returns an instance of a component that you can include in your pipeline definition.</p> <p>This approach is more complex than building a Python function-based component, since it requires packaging your code as a container image. This approach is most suitable for including non-Python code in your pipeline, or for building Python components with complex runtime environments or dependencies.</p> <p>Learn how to build container-based components.</p>"},{"location":"guide/understanding_custom_components/#fully-custom-components","title":"Fully custom components","text":"<p>Fully custom components let you build components by defining the component specification, executor, and component interface classes. This approach lets you reuse and extend a standard component to fit your needs.</p> <p>If an existing component is defined with the same inputs and outputs as the custom component that you're developing, you can simply override the Executor class of the existing component. This means that you can reuse a component specification and implement a new executor that derives from an existing component. In this way, you reuse functionality built into existing components and implement only the functionality that is required.</p> <p>If however the inputs and outputs of your new component are unique, you can define an entirely new component specification.</p> <p>This approach is best for reusing existing component specifications and executors.</p> <p>Learn how to build fully custom components.</p>"},{"location":"guide/understanding_tfx_pipelines/","title":"Understanding TFX Pipelines","text":"<p>MLOps is the practice of applying DevOps practices to help automate, manage, and audit machine learning (ML) workflows. ML workflows include steps to:</p> <ul> <li>Prepare, analyze, and transform data.</li> <li>Train and evaluate a model.</li> <li>Deploy trained models to production.</li> <li>Track ML artifacts and understand their dependencies.</li> </ul> <p>Managing these steps in an ad-hoc manner can be difficult and time-consuming.</p> <p>TFX makes it easier to implement MLOps by providing a toolkit that helps you orchestrate your ML process on various orchestrators, such as: Apache Airflow, Apache Beam, and Kubeflow Pipelines. By implementing your workflow as a TFX pipeline, you can:</p> <ul> <li>Automate your ML process, which lets you regularly retrain, evaluate, and     deploy your model.</li> <li>Utilize distributed compute resources for processing large datasets and     workloads.</li> <li>Increase the velocity of experimentation by running a pipeline with     different sets of hyperparameters.</li> </ul> <p>This guide describes the core concepts required to understand TFX pipelines.</p>"},{"location":"guide/understanding_tfx_pipelines/#artifact","title":"Artifact","text":"<p>The outputs of steps in a TFX pipeline are called artifacts. Subsequent steps in your workflow may use these artifacts as inputs. In this way, TFX lets you transfer data between workflow steps.</p> <p>For instance, the <code>ExampleGen</code> standard component emits serialized examples, which components such as the <code>StatisticsGen</code> standard component use as inputs.</p> <p>Artifacts must be strongly typed with an artifact type registered in the ML Metadata store. Learn more about the concepts used in ML Metadata.</p> <p>Artifact types have a name and define a schema of its properties. Artifact type names must be unique in your ML Metadata store. TFX provides several standard artifact types that describe complex data types and value types, such as: string, integer, and float. You can reuse these artifact types or define custom artifact types that derive from <code>Artifact</code>.</p>"},{"location":"guide/understanding_tfx_pipelines/#parameter","title":"Parameter","text":"<p>Parameters are inputs to pipelines that are known before your pipeline is executed. Parameters let you change the behavior of a pipeline, or a part of a pipeline, through configuration instead of code.</p> <p>For example, you can use parameters to run a pipeline with different sets of hyperparameters without changing the pipeline's code.</p> <p>Using parameters lets you increase the velocity of experimentation by making it easier to run your pipeline with different sets of parameters.</p> <p>Learn more about the RuntimeParameter class.</p>"},{"location":"guide/understanding_tfx_pipelines/#component","title":"Component","text":"<p>A component is an implementation of an ML task that you can use as a step in your TFX pipeline. Components are composed of:</p> <ul> <li>A component specification, which defines the component's input and output     artifacts, and the component's required parameters.</li> <li>An executor, which implements the code to perform a step in your ML     workflow, such as ingesting and transforming data or training and evaluating     a model.</li> <li>A component interface, which packages the component specification and     executor for use in a pipeline.</li> </ul> <p>TFX provides several standard components that you can use in your pipelines. If these components do not meet your needs, you can build custom components. Learn more about custom components.</p>"},{"location":"guide/understanding_tfx_pipelines/#pipeline","title":"Pipeline","text":"<p>A TFX pipeline is a portable implementation of an ML workflow that can be run on various orchestrators, such as: Apache Airflow, Apache Beam, and Kubeflow Pipelines. A pipeline is composed of component instances and input parameters.</p> <p>Component instances produce artifacts as outputs and typically depend on artifacts produced by upstream component instances as inputs. The execution sequence for component instances is determined by creating a directed acyclic graph of the artifact dependencies.</p> <p>For example, consider a pipeline that does the following:</p> <ul> <li>Ingests data directly from a proprietary system using a custom component.</li> <li>Calculates statistics for the training data using the StatisticsGen standard     component.</li> <li>Creates a data schema using the SchemaGen standard component.</li> <li>Checks the training data for anomalies using the ExampleValidator standard     component.</li> <li>Performs feature engineering on the dataset using the Transform standard     component.</li> <li>Trains a model using the Trainer standard component.</li> <li>Evaluates the trained model using the Evaluator component.</li> <li>If the model passes its evaluation, the pipeline enqueues the trained model     to a proprietary deployment system using a custom component.</li> </ul> <p></p> <p>To determine the execution sequence for the component instances, TFX analyzes the artifact dependencies.</p> <ul> <li>The data ingestion component does not have any artifact dependencies, so it     can be the first node in the graph.</li> <li>StatisticsGen depends on the examples produced by data ingestion, so it     must be executed after data ingestion.</li> <li>SchemaGen depends on the statistics created by StatisticsGen, so it must     be executed after StatisticsGen.</li> <li>ExampleValidator depends on the statistics created by StatisticsGen and     the schema created by SchemaGen, so it must be executed after     StatisticsGen and SchemaGen.</li> <li>Transform depends on the examples produced by data ingestion and the     schema created by SchemaGen, so it must be executed after data ingestion     and SchemaGen.</li> <li>Trainer depends on the examples produced by data ingestion, the schema     created by SchemaGen, and the saved model produced by Transform. The     Trainer can be executed only after data ingestion, SchemaGen, and Transform.</li> <li>Evaluator depends on the examples produced by data ingestion and the     saved model produced by the Trainer, so it must be executed after data     ingestion and the Trainer.</li> <li>The custom deployer depends on the saved model produced by the Trainer and     the analysis results created by the Evaluator, so the deployer must be     executed after the Trainer and the Evaluator.</li> </ul> <p>Based on this analysis, an orchestrator runs:</p> <ul> <li>The data ingestion, StatisticsGen, SchemaGen component instances     sequentially.</li> <li>The ExampleValidator and Transform components can run in parallel since they     share input artifact dependencies and do not depend on each other's output.</li> <li>After the Transform component is complete, the Trainer, Evaluator, and     custom deployer component instances run sequentially.</li> </ul> <p>Learn more about building a TFX pipeline.</p>"},{"location":"guide/understanding_tfx_pipelines/#tfx-pipeline-template","title":"TFX Pipeline Template","text":"<p>TFX Pipeline Templates make it easier to get started with pipeline development by providing a prebuilt pipeline that you can customize for your use case.</p> <p>Learn more about customizing a TFX pipeline template.</p>"},{"location":"guide/understanding_tfx_pipelines/#pipeline-run","title":"Pipeline Run","text":"<p>A run is a single execution of a pipeline.</p>"},{"location":"guide/understanding_tfx_pipelines/#orchestrator","title":"Orchestrator","text":"<p>An Orchestrator is a system where you can execute pipeline runs. TFX supports orchestrators such as: Apache Airflow, Apache Beam, and Kubeflow Pipelines. TFX also uses the term DagRunner to refer to an implementation that supports an orchestrator.</p>"},{"location":"guide/vertex/","title":"Orchestrating TFX Pipelines","text":""},{"location":"guide/vertex/#vertex-ai-pipelines","title":"Vertex AI Pipelines","text":"<p>Vertex AI Pipelines is a managed service in Google Cloud Platform which helps you to automate, monitor, and govern your ML systems by orchestrating your ML workflow in a managed, serverless manner.</p> <p>It is recommended to use TFX to define ML pipelines for Vertex AI Pipelines, if you use TensorFlow in an ML workflow that processes terabytes of structured data or text data. See also the Vertex AI guide</p> <p>Try the TFX on Cloud tutorials running in Colab to learn how to use Vertex AI Pipelines with TFX.</p>"},{"location":"tutorials/","title":"Tensorflow in Production Tutorials","text":"<p>These tutorials will get you started, and help you learn a few different ways of working with TFX for production workflows and deployments.  In particular, you'll learn the two main styles of developing a TFX pipeline:</p> <ul> <li>Using the <code>InteractiveContext</code> to develop a pipeline in a notebook, working   with one component at a time.  This style makes development easier and more   Pythonic.</li> <li>Defining an entire pipeline and executing it with a runner.  This is what your   pipelines will look like when you deploy them.</li> </ul>"},{"location":"tutorials/#getting-started-tutorials","title":"Getting Started Tutorials","text":"<ul> <li> <p>1. Starter Pipeline</p> <p>Probably the simplest pipeline you can build, to help you get started. Click the Run in Google\u00a0Colab button.</p> <p> Starter Pipeline</p> </li> <li> <p>2. Adding Data Validation</p> <p>Building on the simple pipeline to add data validation components.</p> <p> Data Validation</p> </li> <li> <p>3. Adding Feature Engineering</p> <p>Building on the data validation pipeline to add a feature engineering component.</p> <p> Feature Engineering</p> </li> <li> <p>4. Adding Model Analysis</p> <p>Building on the simple pipeline to add a model analysis component.</p> <p> Model Analysis</p> </li> </ul>"},{"location":"tutorials/#tfx-on-google-cloud","title":"TFX on Google Cloud","text":"<p>Google Cloud provides various products like BigQuery, Vertex AI to make your ML workflow cost-effective and scalable. You will learn how to use those products in your TFX pipeline.</p> <ul> <li> <p>Running on Vertex Pipelines</p> <p>Running pipelines on a managed pipeline service, Vertex Pipelines.</p> <p> Vertex Pipelines</p> </li> <li> <p>Read data from BigQuery</p> <p>Using BigQuery as a data source of ML pipelines.</p> <p> BigQuery</p> </li> <li> <p>Vertex AI Training and Serving</p> <p>Using cloud resources for ML training and serving with Vertex AI.</p> <p> Vertex Training and Serving</p> </li> <li> <p>TFX on Cloud AI Platform Pipelines</p> <p>An introduction to using TFX and Cloud AI Platform Pipelines.</p> <p> Cloud Pipelines</p> </li> </ul>"},{"location":"tutorials/#next-steps","title":"Next Steps","text":"<p>Once you have a basic understanding of TFX, check these additional tutorials and guides. And don't forget to read the TFX User Guide.</p> <ul> <li> <p>Complete Pipeline Tutorial</p> <p>A component-by-component introduction to TFX, including the interactive context, a very useful development tool. Click the Run in Google\u00a0Colab button.</p> <p> Keras</p> </li> <li> <p>Custom Component Tutorial</p> <p>A tutorial showing how to develop your own custom TFX components.</p> <p> Custom Component</p> </li> <li> <p>Data Validation</p> <p>This Google\u00a0Colab notebook demonstrates how TensorFlow Data Validation (TFDV) can be used to investigate and visualize a dataset, including generating descriptive statistics, inferring a schema, and finding anomalies.</p> <p> Data Validation</p> </li> <li> <p>Model Analysis</p> <p>This Google\u00a0Colab notebook demonstrates how TensorFlow Model Analysis (TFMA) can be used to investigate and visualize the characteristics of a dataset and evaluate the performance of a model along several axes of accuracy.</p> <p> Model Analysis</p> </li> <li> <p>Serve a Model</p> <p>This tutorial demonstrates how TensorFlow Serving can be used to serve a model using a simple REST API.</p> <p> Model Analysis</p> </li> </ul>"},{"location":"tutorials/#videos-and-updates","title":"Videos and Updates","text":"<p>Subscribe to the TFX YouTube Playlist and blog for the latest videos and updates.</p> <ul> <li>TFX: Production ML with TensorFlow in 2020</li> </ul> <ul> <li>TFX: Production ML pipelines with TensorFlow</li> </ul> <ul> <li>Taking Machine Learning from Research to Production</li> </ul>"},{"location":"tutorials/data_validation/tfdv_basic/","title":"TensorFlow Data Validation","text":"In\u00a0[\u00a0]: Copied! <pre>#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n</pre> #@title Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # https://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. <p>Note: We recommend running this tutorial in a Colab notebook, with no setup required!  Just click \"Run in Google Colab\".</p>        View on TensorFlow.org             Run in Google Colab             View source on GitHub             Download notebook      <p>This example colab notebook illustrates how TensorFlow Data Validation (TFDV) can be used to investigate and visualize your dataset.  That includes looking at descriptive statistics, inferring a schema, checking for and fixing anomalies, and checking for drift and skew in our dataset.  It's important to understand your dataset's characteristics, including how it might change over time in your production pipeline.  It's also important to look for anomalies in your data, and to compare your training, evaluation, and serving datasets to make sure that they're consistent.</p> <p>We'll use data from the Taxi Trips dataset released by the City of Chicago.</p> <p>Note: This site provides applications using data that has been modified for use from its original source, www.cityofchicago.org, the official website of the City of Chicago. The City of Chicago makes no claims as to the content, accuracy, timeliness, or completeness of any of the data provided at this site. The data provided at this site is subject to change at any time. It is understood that the data provided at this site is being used at one\u2019s own risk.</p> <p>Read more about the dataset in Google BigQuery. Explore the full dataset in the BigQuery UI.</p> <p>Key Point: As a modeler and developer, think about how this data is used and the potential benefits and harm a model's predictions can cause. A model like this could reinforce societal biases and disparities. Is a feature relevant to the problem you want to solve or will it introduce bias? For more information, read about ML fairness.</p> <p>The columns in the dataset are:</p> pickup_community_areafaretrip_start_month trip_start_hourtrip_start_daytrip_start_timestamp pickup_latitudepickup_longitudedropoff_latitude dropoff_longitudetrip_milespickup_census_tract dropoff_census_tractpayment_typecompany trip_secondsdropoff_community_areatips In\u00a0[\u00a0]: Copied! <pre>try:\n  import colab\n  !pip install --upgrade pip\nexcept:\n  pass\n</pre> try:   import colab   !pip install --upgrade pip except:   pass In\u00a0[\u00a0]: Copied! <pre>print('Installing TensorFlow Data Validation')\n!pip install --upgrade 'tensorflow_data_validation[visualization]&lt;2'\n</pre> print('Installing TensorFlow Data Validation') !pip install --upgrade 'tensorflow_data_validation[visualization]&lt;2' In\u00a0[\u00a0]: Copied! <pre>import pkg_resources\nimport importlib\nimportlib.reload(pkg_resources)\n</pre> import pkg_resources import importlib importlib.reload(pkg_resources) <p>Check the versions of TensorFlow and the Data Validation before proceeding.</p> In\u00a0[\u00a0]: Copied! <pre>import tensorflow as tf\nimport tensorflow_data_validation as tfdv\nprint('TF version:', tf.__version__)\nprint('TFDV version:', tfdv.version.__version__)\n</pre> import tensorflow as tf import tensorflow_data_validation as tfdv print('TF version:', tf.__version__) print('TFDV version:', tfdv.version.__version__) In\u00a0[\u00a0]: Copied! <pre>import os\nimport tempfile, urllib, zipfile\n\n# Set up some globals for our file paths\nBASE_DIR = tempfile.mkdtemp()\nDATA_DIR = os.path.join(BASE_DIR, 'data')\nOUTPUT_DIR = os.path.join(BASE_DIR, 'chicago_taxi_output')\nTRAIN_DATA = os.path.join(DATA_DIR, 'train', 'data.csv')\nEVAL_DATA = os.path.join(DATA_DIR, 'eval', 'data.csv')\nSERVING_DATA = os.path.join(DATA_DIR, 'serving', 'data.csv')\n\n# Download the zip file from GCP and unzip it\nzip, headers = urllib.request.urlretrieve('https://storage.googleapis.com/artifacts.tfx-oss-public.appspot.com/datasets/chicago_data.zip')\nzipfile.ZipFile(zip).extractall(BASE_DIR)\nzipfile.ZipFile(zip).close()\n\nprint(\"Here's what we downloaded:\")\n!ls -R {os.path.join(BASE_DIR, 'data')}\n</pre> import os import tempfile, urllib, zipfile  # Set up some globals for our file paths BASE_DIR = tempfile.mkdtemp() DATA_DIR = os.path.join(BASE_DIR, 'data') OUTPUT_DIR = os.path.join(BASE_DIR, 'chicago_taxi_output') TRAIN_DATA = os.path.join(DATA_DIR, 'train', 'data.csv') EVAL_DATA = os.path.join(DATA_DIR, 'eval', 'data.csv') SERVING_DATA = os.path.join(DATA_DIR, 'serving', 'data.csv')  # Download the zip file from GCP and unzip it zip, headers = urllib.request.urlretrieve('https://storage.googleapis.com/artifacts.tfx-oss-public.appspot.com/datasets/chicago_data.zip') zipfile.ZipFile(zip).extractall(BASE_DIR) zipfile.ZipFile(zip).close()  print(\"Here's what we downloaded:\") !ls -R {os.path.join(BASE_DIR, 'data')} In\u00a0[\u00a0]: Copied! <pre>train_stats = tfdv.generate_statistics_from_csv(data_location=TRAIN_DATA)\n</pre> train_stats = tfdv.generate_statistics_from_csv(data_location=TRAIN_DATA) <p>Now let's use <code>tfdv.visualize_statistics</code>, which uses Facets to create a succinct visualization of our training data:</p> <ul> <li>Notice that numeric features and catagorical features are visualized separately, and that charts are displayed showing the distributions for each feature.</li> <li>Notice that features with missing or zero values display a percentage in red as a visual indicator that there may be issues with examples in those features.  The percentage is the percentage of examples that have missing or zero values for that feature.</li> <li>Notice that there are no examples with values for <code>pickup_census_tract</code>.  This is an opportunity for dimensionality reduction!</li> <li>Try clicking \"expand\" above the charts to change the display</li> <li>Try hovering over bars in the charts to display bucket ranges and counts</li> <li>Try switching between the log and linear scales, and notice how the log scale reveals much more detail about the <code>payment_type</code> categorical feature</li> <li>Try selecting \"quantiles\" from the \"Chart to show\" menu, and hover over the markers to show the quantile percentages</li> </ul> In\u00a0[\u00a0]: Copied! <pre># docs-infra: no-execute\ntfdv.visualize_statistics(train_stats)\n</pre> # docs-infra: no-execute tfdv.visualize_statistics(train_stats) In\u00a0[\u00a0]: Copied! <pre>schema = tfdv.infer_schema(statistics=train_stats)\ntfdv.display_schema(schema=schema)\n</pre> schema = tfdv.infer_schema(statistics=train_stats) tfdv.display_schema(schema=schema) In\u00a0[\u00a0]: Copied! <pre># Compute stats for evaluation data\neval_stats = tfdv.generate_statistics_from_csv(data_location=EVAL_DATA)\n</pre> # Compute stats for evaluation data eval_stats = tfdv.generate_statistics_from_csv(data_location=EVAL_DATA) In\u00a0[\u00a0]: Copied! <pre># docs-infra: no-execute\n# Compare evaluation data with training data\ntfdv.visualize_statistics(lhs_statistics=eval_stats, rhs_statistics=train_stats,\n                          lhs_name='EVAL_DATASET', rhs_name='TRAIN_DATASET')\n</pre> # docs-infra: no-execute # Compare evaluation data with training data tfdv.visualize_statistics(lhs_statistics=eval_stats, rhs_statistics=train_stats,                           lhs_name='EVAL_DATASET', rhs_name='TRAIN_DATASET') In\u00a0[\u00a0]: Copied! <pre># Check eval data for errors by validating the eval data stats using the previously inferred schema.\nanomalies = tfdv.validate_statistics(statistics=eval_stats, schema=schema)\ntfdv.display_anomalies(anomalies)\n</pre> # Check eval data for errors by validating the eval data stats using the previously inferred schema. anomalies = tfdv.validate_statistics(statistics=eval_stats, schema=schema) tfdv.display_anomalies(anomalies) In\u00a0[\u00a0]: Copied! <pre># Relax the minimum fraction of values that must come from the domain for feature company.\ncompany = tfdv.get_feature(schema, 'company')\ncompany.distribution_constraints.min_domain_mass = 0.9\n\n# Add new value to the domain of feature payment_type.\npayment_type_domain = tfdv.get_domain(schema, 'payment_type')\npayment_type_domain.value.append('Prcard')\n\n# Validate eval stats after updating the schema \nupdated_anomalies = tfdv.validate_statistics(eval_stats, schema)\ntfdv.display_anomalies(updated_anomalies)\n</pre> # Relax the minimum fraction of values that must come from the domain for feature company. company = tfdv.get_feature(schema, 'company') company.distribution_constraints.min_domain_mass = 0.9  # Add new value to the domain of feature payment_type. payment_type_domain = tfdv.get_domain(schema, 'payment_type') payment_type_domain.value.append('Prcard')  # Validate eval stats after updating the schema  updated_anomalies = tfdv.validate_statistics(eval_stats, schema) tfdv.display_anomalies(updated_anomalies) <p>Hey, look at that!  We verified that the training and evaluation data are now consistent!  Thanks TFDV ;)</p> <p>We also split off a 'serving' dataset for this example, so we should check that too.  By default all datasets in a pipeline should use the same schema, but there are often exceptions. For example, in supervised learning we need to include labels in our dataset, but when we serve the model for inference the labels will not be included. In some cases introducing slight schema variations is necessary.</p> <p>Environments can be used to express such requirements. In particular, features in schema can be associated with a set of environments using <code>default_environment</code>, <code>in_environment</code> and <code>not_in_environment</code>.</p> <p>For example, in this dataset the <code>tips</code> feature is included as the label for training, but it's missing in the serving data. Without environment specified, it will show up as an anomaly.</p> In\u00a0[\u00a0]: Copied! <pre>serving_stats = tfdv.generate_statistics_from_csv(SERVING_DATA)\nserving_anomalies = tfdv.validate_statistics(serving_stats, schema)\n\ntfdv.display_anomalies(serving_anomalies)\n</pre> serving_stats = tfdv.generate_statistics_from_csv(SERVING_DATA) serving_anomalies = tfdv.validate_statistics(serving_stats, schema)  tfdv.display_anomalies(serving_anomalies) <p>We'll deal with the <code>tips</code> feature below.  We also have an INT value in our trip seconds, where our schema expected a FLOAT. By making us aware of that difference, TFDV helps uncover inconsistencies in the way the data is generated for training and serving. It's very easy to be unaware of problems like that until model performance suffers, sometimes catastrophically. It may or may not be a significant issue, but in any case this should be cause for further investigation.</p> <p>In this case, we can safely convert INT values to FLOATs, so we want to tell TFDV to use our schema to infer the type.  Let's do that now.</p> In\u00a0[\u00a0]: Copied! <pre>options = tfdv.StatsOptions(schema=schema, infer_type_from_schema=True)\nserving_stats = tfdv.generate_statistics_from_csv(SERVING_DATA, stats_options=options)\nserving_anomalies = tfdv.validate_statistics(serving_stats, schema)\n\ntfdv.display_anomalies(serving_anomalies)\n</pre> options = tfdv.StatsOptions(schema=schema, infer_type_from_schema=True) serving_stats = tfdv.generate_statistics_from_csv(SERVING_DATA, stats_options=options) serving_anomalies = tfdv.validate_statistics(serving_stats, schema)  tfdv.display_anomalies(serving_anomalies) <p>Now we just have the <code>tips</code> feature (which is our label) showing up as an anomaly ('Column dropped').  Of course we don't expect to have labels in our serving data, so let's tell TFDV to ignore that.</p> In\u00a0[\u00a0]: Copied! <pre># All features are by default in both TRAINING and SERVING environments.\nschema.default_environment.append('TRAINING')\nschema.default_environment.append('SERVING')\n\n# Specify that 'tips' feature is not in SERVING environment.\ntfdv.get_feature(schema, 'tips').not_in_environment.append('SERVING')\n\nserving_anomalies_with_env = tfdv.validate_statistics(\n    serving_stats, schema, environment='SERVING')\n\ntfdv.display_anomalies(serving_anomalies_with_env)\n</pre> # All features are by default in both TRAINING and SERVING environments. schema.default_environment.append('TRAINING') schema.default_environment.append('SERVING')  # Specify that 'tips' feature is not in SERVING environment. tfdv.get_feature(schema, 'tips').not_in_environment.append('SERVING')  serving_anomalies_with_env = tfdv.validate_statistics(     serving_stats, schema, environment='SERVING')  tfdv.display_anomalies(serving_anomalies_with_env) <p>In addition to checking whether a dataset conforms to the expectations set in the schema, TFDV also provides functionalities to detect drift and skew.  TFDV performs this check by comparing the statistics of the different datasets based on the drift/skew comparators specified in the schema.</p> In\u00a0[\u00a0]: Copied! <pre># Add skew comparator for 'payment_type' feature.\npayment_type = tfdv.get_feature(schema, 'payment_type')\npayment_type.skew_comparator.infinity_norm.threshold = 0.01\n\n# Add drift comparator for 'company' feature.\ncompany=tfdv.get_feature(schema, 'company')\ncompany.drift_comparator.infinity_norm.threshold = 0.001\n\nskew_anomalies = tfdv.validate_statistics(train_stats, schema,\n                                          previous_statistics=eval_stats,\n                                          serving_statistics=serving_stats)\n\ntfdv.display_anomalies(skew_anomalies)\n</pre> # Add skew comparator for 'payment_type' feature. payment_type = tfdv.get_feature(schema, 'payment_type') payment_type.skew_comparator.infinity_norm.threshold = 0.01  # Add drift comparator for 'company' feature. company=tfdv.get_feature(schema, 'company') company.drift_comparator.infinity_norm.threshold = 0.001  skew_anomalies = tfdv.validate_statistics(train_stats, schema,                                           previous_statistics=eval_stats,                                           serving_statistics=serving_stats)  tfdv.display_anomalies(skew_anomalies) <p>In this example we do see some drift, but it is well below the threshold that we've set.</p> In\u00a0[\u00a0]: Copied! <pre>from tensorflow.python.lib.io import file_io\nfrom google.protobuf import text_format\n\nfile_io.recursive_create_dir(OUTPUT_DIR)\nschema_file = os.path.join(OUTPUT_DIR, 'schema.pbtxt')\ntfdv.write_schema_text(schema, schema_file)\n\n!cat {schema_file}\n</pre> from tensorflow.python.lib.io import file_io from google.protobuf import text_format  file_io.recursive_create_dir(OUTPUT_DIR) schema_file = os.path.join(OUTPUT_DIR, 'schema.pbtxt') tfdv.write_schema_text(schema, schema_file)  !cat {schema_file}"},{"location":"tutorials/data_validation/tfdv_basic/#copyright-2020-the-tensorflow-authors","title":"Copyright 2020 The TensorFlow Authors.\u00b6","text":""},{"location":"tutorials/data_validation/tfdv_basic/#tensorflow-data-validation","title":"TensorFlow Data Validation\u00b6","text":"<p>An Example of a Key Component of TensorFlow Extended</p>"},{"location":"tutorials/data_validation/tfdv_basic/#install-and-import-packages","title":"Install and import packages\u00b6","text":"<p>Install the packages for TensorFlow Data Validation.</p>"},{"location":"tutorials/data_validation/tfdv_basic/#upgrade-pip","title":"Upgrade Pip\u00b6","text":"<p>To avoid upgrading Pip in a system when running locally, check to make sure that we're running in Colab.  Local systems can of course be upgraded separately.</p>"},{"location":"tutorials/data_validation/tfdv_basic/#install-data-validation-packages","title":"Install Data Validation packages\u00b6","text":"<p>Install the TensorFlow Data Validation packages and dependencies, which takes a few minutes. You may see warnings and errors regarding incompatible dependency versions, which you will resolve in the next section.</p>"},{"location":"tutorials/data_validation/tfdv_basic/#import-tensorflow-and-reload-updated-packages","title":"Import TensorFlow and reload updated packages\u00b6","text":"<p>The prior step updates the default packages in the Gooogle Colab environment, so you must reload the package resources to resolve the new dependencies.</p> <p>Note: This step resolves the dependency error from the installation. If you are still experiencing code execution problems after running this code, restart the runtime (Runtime &gt; Restart runtime ...).</p>"},{"location":"tutorials/data_validation/tfdv_basic/#load-the-dataset","title":"Load the dataset\u00b6","text":"<p>We will download our dataset from Google Cloud Storage.</p>"},{"location":"tutorials/data_validation/tfdv_basic/#compute-and-visualize-statistics","title":"Compute and visualize statistics\u00b6","text":"<p>First we'll use <code>tfdv.generate_statistics_from_csv</code> to compute statistics for our training data. (ignore the snappy warnings)</p> <p>TFDV can compute descriptive statistics that provide a quick overview of the data in terms of the features that are present and the shapes of their value distributions.</p> <p>Internally, TFDV uses Apache Beam's data-parallel processing framework to scale the computation of statistics over large datasets. For applications that wish to integrate deeper with TFDV (e.g., attach statistics generation at the end of a data-generation pipeline), the API also exposes a Beam PTransform for statistics generation.</p>"},{"location":"tutorials/data_validation/tfdv_basic/#infer-a-schema","title":"Infer a schema\u00b6","text":"<p>Now let's use <code>tfdv.infer_schema</code> to create a schema for our data.  A schema defines constraints for the data that are relevant for ML. Example constraints include the data type of each feature, whether it's numerical or categorical, or the frequency of its presence in the data.  For categorical features the schema also defines the domain - the list of acceptable values.  Since writing a schema can be a tedious task, especially for datasets with lots of features, TFDV provides a method to generate an initial version of the schema based on the descriptive statistics.</p> <p>Getting the schema right is important because the rest of our production pipeline will be relying on the schema that TFDV generates to be correct.  The schema also provides documentation for the data, and so is useful when different developers work on the same data.  Let's use <code>tfdv.display_schema</code> to display the inferred schema so that we can review it.</p>"},{"location":"tutorials/data_validation/tfdv_basic/#check-evaluation-data-for-errors","title":"Check evaluation data for errors\u00b6","text":"<p>So far we've only been looking at the training data.  It's important that our evaluation data is consistent with our training data, including that it uses the same schema.  It's also important that the evaluation data includes examples of roughly the same ranges of values for our numerical features as our training data, so that our coverage of the loss surface during evaluation is roughly the same as during training.  The same is true for categorical features.  Otherwise, we may have training issues that are not identified during evaluation, because we didn't evaluate part of our loss surface.</p> <ul> <li>Notice that each feature now includes statistics for both the training and evaluation datasets.</li> <li>Notice that the charts now have both the training and evaluation datasets overlaid, making it easy to compare them.</li> <li>Notice that the charts now include a percentages view, which can be combined with log or the default linear scales.</li> <li>Notice that the mean and median for <code>trip_miles</code> are different for the training versus the evaluation datasets.  Will that cause problems?</li> <li>Wow, the max <code>tips</code> is very different for the training versus the evaluation datasets.  Will that cause problems?</li> <li>Click expand on the Numeric Features chart, and select the log scale.  Review the <code>trip_seconds</code> feature, and notice the difference in the max.  Will evaluation miss parts of the loss surface?</li> </ul>"},{"location":"tutorials/data_validation/tfdv_basic/#check-for-evaluation-anomalies","title":"Check for evaluation anomalies\u00b6","text":"<p>Does our evaluation dataset match the schema from our training dataset?  This is especially important for categorical features, where we want to identify the range of acceptable values.</p> <p>Key Point: What would happen if we tried to evaluate using data with categorical feature values that were not in our training dataset?  What about numeric features that are outside the ranges in our training dataset?</p>"},{"location":"tutorials/data_validation/tfdv_basic/#fix-evaluation-anomalies-in-the-schema","title":"Fix evaluation anomalies in the schema\u00b6","text":"<p>Oops!  It looks like we have some new values for <code>company</code> in our evaluation data, that we didn't have in our training data.  We also have a new value for <code>payment_type</code>.  These should be considered anomalies, but what we decide to do about them depends on our domain knowledge of the data.  If an anomaly truly indicates a data error, then the underlying data should be fixed.  Otherwise, we can simply update the schema to include the values in the eval dataset.</p> <p>Key Point: How would our evaluation results be affected if we did not fix these problems?</p> <p>Unless we change our evaluation dataset we can't fix everything, but we can fix things in the schema that we're comfortable accepting.  That includes relaxing our view of what is and what is not an anomaly for particular features, as well as updating our schema to include missing values for categorical features.  TFDV has enabled us to discover what we need to fix.</p> <p>Let's make those fixes now, and then review one more time.</p>"},{"location":"tutorials/data_validation/tfdv_basic/#schema-environments","title":"Schema Environments\u00b6","text":""},{"location":"tutorials/data_validation/tfdv_basic/#check-for-drift-and-skew","title":"Check for drift and skew\u00b6","text":""},{"location":"tutorials/data_validation/tfdv_basic/#drift","title":"Drift\u00b6","text":"<p>Drift detection is supported for categorical features and between consecutive spans of data (i.e., between span N and span N+1), such as between different days of training data.  We express drift in terms of L-infinity distance, and you can set the threshold distance so that you receive warnings when the drift is higher than is acceptable.  Setting the correct distance is typically an iterative process requiring domain knowledge and experimentation.</p>"},{"location":"tutorials/data_validation/tfdv_basic/#skew","title":"Skew\u00b6","text":"<p>TFDV can detect three different kinds of skew in your data - schema skew, feature skew, and distribution skew.</p>"},{"location":"tutorials/data_validation/tfdv_basic/#schema-skew","title":"Schema Skew\u00b6","text":"<p>Schema skew occurs when the training and serving data do not conform to the same schema. Both training and serving data are expected to adhere to the same schema. Any expected deviations between the two (such as the label feature being only present in the training data but not in serving) should be specified through environments field in the schema.</p>"},{"location":"tutorials/data_validation/tfdv_basic/#feature-skew","title":"Feature Skew\u00b6","text":"<p>Feature skew occurs when the feature values that a model trains on are different from the feature values that it sees at serving time. For example, this can happen when:</p> <ul> <li>A data source that provides some feature values is modified between training and serving time</li> <li>There is different logic for generating features between training and serving. For example, if you apply some transformation only in one of the two code paths.</li> </ul>"},{"location":"tutorials/data_validation/tfdv_basic/#distribution-skew","title":"Distribution Skew\u00b6","text":"<p>Distribution skew occurs when the distribution of the training dataset is significantly different from the distribution of the serving dataset. One of the key causes for distribution skew is using different code or different data sources to generate the training dataset. Another reason is a faulty sampling mechanism that chooses a non-representative subsample of the serving data to train on.</p>"},{"location":"tutorials/data_validation/tfdv_basic/#freeze-the-schema","title":"Freeze the schema\u00b6","text":"<p>Now that the schema has been reviewed and curated, we will store it in a file to reflect its \"frozen\" state.</p>"},{"location":"tutorials/data_validation/tfdv_basic/#when-to-use-tfdv","title":"When to use TFDV\u00b6","text":"<p>It's easy to think of TFDV as only applying to the start of your training pipeline, as we did here, but in fact it has many uses.  Here's a few more:</p> <ul> <li>Validating new data for inference to make sure that we haven't suddenly started receiving bad features</li> <li>Validating new data for inference to make sure that our model has trained on that part of the decision surface</li> <li>Validating our data after we've transformed it and done feature engineering (probably using TensorFlow Transform) to make sure we haven't done something wrong</li> </ul>"},{"location":"tutorials/mlmd/mlmd_tutorial/","title":"Better ML Engineering with ML Metadata","text":"In\u00a0[\u00a0]: Copied! <pre>#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n</pre> #@title Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # https://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. <p>Note: We recommend running this tutorial in a Colab notebook, with no setup required!  Just click \"Run in Google Colab\".</p>        View on TensorFlow.org             Run in Google Colab             View source on GitHub             Download notebook      <p>Assume a scenario where you set up a production ML pipeline to classify penguins. The pipeline ingests your training data, trains and evaluates a model, and pushes it to production.</p> <p>However, when you later try using this model with a larger dataset that contains different kinds of penguins, you observe that your model does not behave as expected and starts classifying the species incorrectly.</p> <p>At this point, you are interested in knowing:</p> <ul> <li>What is the most efficient way to debug the model when the only available artifact is the model in production?</li> <li>Which training dataset was used to train the model?</li> <li>Which training run led to this erroneous model?</li> <li>Where are the model evaluation results?</li> <li>Where to begin debugging?</li> </ul> <p>ML Metadata (MLMD) is a library that leverages the metadata associated with ML models to help you answer these questions and more. A helpful analogy is to think of this metadata as the equivalent of logging in software development. MLMD enables you to reliably track the artifacts and lineage associated with the various components of your ML pipeline.</p> <p>In this tutorial, you set up a TFX Pipeline to create a model that classifies penguins into three species based on the body mass and the length and depth of their culmens, and the length of their flippers. You then use MLMD to track the lineage of pipeline components.</p> In\u00a0[\u00a0]: Copied! <pre>try:\n  import colab\n  !pip install --upgrade pip\nexcept:\n  pass\n</pre> try:   import colab   !pip install --upgrade pip except:   pass In\u00a0[\u00a0]: Copied! <pre> !pip install -q tfx\n</pre>  !pip install -q tfx In\u00a0[\u00a0]: Copied! <pre>import os\nimport tempfile\nimport urllib\nimport pandas as pd\n\nimport tensorflow_model_analysis as tfma\nfrom tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n</pre> import os import tempfile import urllib import pandas as pd  import tensorflow_model_analysis as tfma from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext <p>Check the TFX, and MLMD versions.</p> In\u00a0[\u00a0]: Copied! <pre>from tfx import v1 as tfx\nprint('TFX version: {}'.format(tfx.__version__))\nimport ml_metadata as mlmd\nprint('MLMD version: {}'.format(mlmd.__version__))\n</pre> from tfx import v1 as tfx print('TFX version: {}'.format(tfx.__version__)) import ml_metadata as mlmd print('MLMD version: {}'.format(mlmd.__version__)) In\u00a0[\u00a0]: Copied! <pre>DATA_PATH = 'https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/penguin/data/labelled/penguins_processed.csv'\n_data_root = tempfile.mkdtemp(prefix='tfx-data')\n_data_filepath = os.path.join(_data_root, \"penguins_processed.csv\")\nurllib.request.urlretrieve(DATA_PATH, _data_filepath)\n</pre> DATA_PATH = 'https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/penguin/data/labelled/penguins_processed.csv' _data_root = tempfile.mkdtemp(prefix='tfx-data') _data_filepath = os.path.join(_data_root, \"penguins_processed.csv\") urllib.request.urlretrieve(DATA_PATH, _data_filepath) In\u00a0[\u00a0]: Copied! <pre>interactive_context = InteractiveContext()\n</pre> interactive_context = InteractiveContext() <p>Note: Constructing a TFX Pipeline by setting up the individual components involves a lot of boilerplate code. For the purpose of this tutorial, it is alright if you do not fully understand every line of code in the pipeline setup.</p> In\u00a0[\u00a0]: Copied! <pre>example_gen = tfx.components.CsvExampleGen(input_base=_data_root)\ninteractive_context.run(example_gen)\n</pre> example_gen = tfx.components.CsvExampleGen(input_base=_data_root) interactive_context.run(example_gen) In\u00a0[\u00a0]: Copied! <pre>statistics_gen = tfx.components.StatisticsGen(\n    examples=example_gen.outputs['examples'])\ninteractive_context.run(statistics_gen)\n</pre> statistics_gen = tfx.components.StatisticsGen(     examples=example_gen.outputs['examples']) interactive_context.run(statistics_gen) In\u00a0[\u00a0]: Copied! <pre>infer_schema = tfx.components.SchemaGen(\n    statistics=statistics_gen.outputs['statistics'], infer_feature_shape=True)\ninteractive_context.run(infer_schema)\n</pre> infer_schema = tfx.components.SchemaGen(     statistics=statistics_gen.outputs['statistics'], infer_feature_shape=True) interactive_context.run(infer_schema) In\u00a0[\u00a0]: Copied! <pre># Define the module file for the Trainer component\ntrainer_module_file = 'penguin_trainer.py'\n</pre> # Define the module file for the Trainer component trainer_module_file = 'penguin_trainer.py' In\u00a0[\u00a0]: Copied! <pre>%%writefile {trainer_module_file}\n\n# Define the training algorithm for the Trainer module file\nimport os\nfrom typing import List, Text\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\nfrom tfx import v1 as tfx\nfrom tfx_bsl.public import tfxio\n\nfrom tensorflow_metadata.proto.v0 import schema_pb2\n\n# Features used for classification - culmen length and depth, flipper length,\n# body mass, and species.\n\n_LABEL_KEY = 'species'\n\n_FEATURE_KEYS = [\n    'culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g'\n]\n\n\ndef _input_fn(file_pattern: List[Text],\n              data_accessor: tfx.components.DataAccessor,\n              schema: schema_pb2.Schema, batch_size: int) -&gt; tf.data.Dataset:\n  return data_accessor.tf_dataset_factory(\n      file_pattern,\n      tfxio.TensorFlowDatasetOptions(\n          batch_size=batch_size, label_key=_LABEL_KEY), schema).repeat()\n\n\ndef _build_keras_model():\n  inputs = [keras.layers.Input(shape=(1,), name=f) for f in _FEATURE_KEYS]\n  d = keras.layers.concatenate(inputs)\n  d = keras.layers.Dense(8, activation='relu')(d)\n  d = keras.layers.Dense(8, activation='relu')(d)\n  outputs = keras.layers.Dense(3)(d)\n  model = keras.Model(inputs=inputs, outputs=outputs)\n  model.compile(\n      optimizer=keras.optimizers.Adam(1e-2),\n      loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n      metrics=[keras.metrics.SparseCategoricalAccuracy()])\n  return model\n\n\ndef run_fn(fn_args: tfx.components.FnArgs):\n  schema = schema_pb2.Schema()\n  tfx.utils.parse_pbtxt_file(fn_args.schema_path, schema)\n  train_dataset = _input_fn(\n      fn_args.train_files, fn_args.data_accessor, schema, batch_size=10)\n  eval_dataset = _input_fn(\n      fn_args.eval_files, fn_args.data_accessor, schema, batch_size=10)\n  model = _build_keras_model()\n  model.fit(\n      train_dataset,\n      epochs=int(fn_args.train_steps / 20),\n      steps_per_epoch=20,\n      validation_data=eval_dataset,\n      validation_steps=fn_args.eval_steps)\n  model.save(fn_args.serving_model_dir, save_format='tf')\n</pre> %%writefile {trainer_module_file}  # Define the training algorithm for the Trainer module file import os from typing import List, Text  import tensorflow as tf from tensorflow import keras  from tfx import v1 as tfx from tfx_bsl.public import tfxio  from tensorflow_metadata.proto.v0 import schema_pb2  # Features used for classification - culmen length and depth, flipper length, # body mass, and species.  _LABEL_KEY = 'species'  _FEATURE_KEYS = [     'culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g' ]   def _input_fn(file_pattern: List[Text],               data_accessor: tfx.components.DataAccessor,               schema: schema_pb2.Schema, batch_size: int) -&gt; tf.data.Dataset:   return data_accessor.tf_dataset_factory(       file_pattern,       tfxio.TensorFlowDatasetOptions(           batch_size=batch_size, label_key=_LABEL_KEY), schema).repeat()   def _build_keras_model():   inputs = [keras.layers.Input(shape=(1,), name=f) for f in _FEATURE_KEYS]   d = keras.layers.concatenate(inputs)   d = keras.layers.Dense(8, activation='relu')(d)   d = keras.layers.Dense(8, activation='relu')(d)   outputs = keras.layers.Dense(3)(d)   model = keras.Model(inputs=inputs, outputs=outputs)   model.compile(       optimizer=keras.optimizers.Adam(1e-2),       loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),       metrics=[keras.metrics.SparseCategoricalAccuracy()])   return model   def run_fn(fn_args: tfx.components.FnArgs):   schema = schema_pb2.Schema()   tfx.utils.parse_pbtxt_file(fn_args.schema_path, schema)   train_dataset = _input_fn(       fn_args.train_files, fn_args.data_accessor, schema, batch_size=10)   eval_dataset = _input_fn(       fn_args.eval_files, fn_args.data_accessor, schema, batch_size=10)   model = _build_keras_model()   model.fit(       train_dataset,       epochs=int(fn_args.train_steps / 20),       steps_per_epoch=20,       validation_data=eval_dataset,       validation_steps=fn_args.eval_steps)   model.save(fn_args.serving_model_dir, save_format='tf') <p>Run the <code>Trainer</code> component.</p> In\u00a0[\u00a0]: Copied! <pre>trainer = tfx.components.Trainer(\n    module_file=os.path.abspath(trainer_module_file),\n    examples=example_gen.outputs['examples'],\n    schema=infer_schema.outputs['schema'],\n    train_args=tfx.proto.TrainArgs(num_steps=100),\n    eval_args=tfx.proto.EvalArgs(num_steps=50))\ninteractive_context.run(trainer)\n</pre> trainer = tfx.components.Trainer(     module_file=os.path.abspath(trainer_module_file),     examples=example_gen.outputs['examples'],     schema=infer_schema.outputs['schema'],     train_args=tfx.proto.TrainArgs(num_steps=100),     eval_args=tfx.proto.EvalArgs(num_steps=50)) interactive_context.run(trainer) In\u00a0[\u00a0]: Copied! <pre>_serving_model_dir = os.path.join(tempfile.mkdtemp(),\n                                  'serving_model/penguins_classification')\n</pre> _serving_model_dir = os.path.join(tempfile.mkdtemp(),                                   'serving_model/penguins_classification') In\u00a0[\u00a0]: Copied! <pre>eval_config = tfma.EvalConfig(\n    model_specs=[\n        tfma.ModelSpec(label_key='species', signature_name='serving_default')\n    ],\n    metrics_specs=[\n        tfma.MetricsSpec(metrics=[\n            tfma.MetricConfig(\n                class_name='SparseCategoricalAccuracy',\n                threshold=tfma.MetricThreshold(\n                    value_threshold=tfma.GenericValueThreshold(\n                        lower_bound={'value': 0.6})))\n        ])\n    ],\n    slicing_specs=[tfma.SlicingSpec()])\n</pre> eval_config = tfma.EvalConfig(     model_specs=[         tfma.ModelSpec(label_key='species', signature_name='serving_default')     ],     metrics_specs=[         tfma.MetricsSpec(metrics=[             tfma.MetricConfig(                 class_name='SparseCategoricalAccuracy',                 threshold=tfma.MetricThreshold(                     value_threshold=tfma.GenericValueThreshold(                         lower_bound={'value': 0.6})))         ])     ],     slicing_specs=[tfma.SlicingSpec()]) In\u00a0[\u00a0]: Copied! <pre>evaluator = tfx.components.Evaluator(\n    examples=example_gen.outputs['examples'],\n    model=trainer.outputs['model'],\n    schema=infer_schema.outputs['schema'],\n    eval_config=eval_config)\ninteractive_context.run(evaluator)\n</pre> evaluator = tfx.components.Evaluator(     examples=example_gen.outputs['examples'],     model=trainer.outputs['model'],     schema=infer_schema.outputs['schema'],     eval_config=eval_config) interactive_context.run(evaluator) In\u00a0[\u00a0]: Copied! <pre>pusher = tfx.components.Pusher(\n    model=trainer.outputs['model'],\n    model_blessing=evaluator.outputs['blessing'],\n    push_destination=tfx.proto.PushDestination(\n        filesystem=tfx.proto.PushDestination.Filesystem(\n            base_directory=_serving_model_dir)))\ninteractive_context.run(pusher)\n</pre> pusher = tfx.components.Pusher(     model=trainer.outputs['model'],     model_blessing=evaluator.outputs['blessing'],     push_destination=tfx.proto.PushDestination(         filesystem=tfx.proto.PushDestination.Filesystem(             base_directory=_serving_model_dir))) interactive_context.run(pusher) <p>Running the TFX pipeline populates the MLMD Database. In the next section, you use the MLMD API to query this database for metadata information.</p> <p>Set up the metadata (MD) store with the <code>InteractiveContext</code> defined previously to query the MLMD database.</p> In\u00a0[\u00a0]: Copied! <pre>connection_config = interactive_context.metadata_connection_config\nstore = mlmd.MetadataStore(connection_config)\n\n# All TFX artifacts are stored in the base directory\nbase_dir = connection_config.sqlite.filename_uri.split('metadata.sqlite')[0]\n</pre> connection_config = interactive_context.metadata_connection_config store = mlmd.MetadataStore(connection_config)  # All TFX artifacts are stored in the base directory base_dir = connection_config.sqlite.filename_uri.split('metadata.sqlite')[0] <p>Create some helper functions to view the data from the MD store.</p> In\u00a0[\u00a0]: Copied! <pre>def display_types(types):\n  # Helper function to render dataframes for the artifact and execution types\n  table = {'id': [], 'name': []}\n  for a_type in types:\n    table['id'].append(a_type.id)\n    table['name'].append(a_type.name)\n  return pd.DataFrame(data=table)\n</pre> def display_types(types):   # Helper function to render dataframes for the artifact and execution types   table = {'id': [], 'name': []}   for a_type in types:     table['id'].append(a_type.id)     table['name'].append(a_type.name)   return pd.DataFrame(data=table) In\u00a0[\u00a0]: Copied! <pre>def display_artifacts(store, artifacts):\n  # Helper function to render dataframes for the input artifacts\n  table = {'artifact id': [], 'type': [], 'uri': []}\n  for a in artifacts:\n    table['artifact id'].append(a.id)\n    artifact_type = store.get_artifact_types_by_id([a.type_id])[0]\n    table['type'].append(artifact_type.name)\n    table['uri'].append(a.uri.replace(base_dir, './'))\n  return pd.DataFrame(data=table)\n</pre> def display_artifacts(store, artifacts):   # Helper function to render dataframes for the input artifacts   table = {'artifact id': [], 'type': [], 'uri': []}   for a in artifacts:     table['artifact id'].append(a.id)     artifact_type = store.get_artifact_types_by_id([a.type_id])[0]     table['type'].append(artifact_type.name)     table['uri'].append(a.uri.replace(base_dir, './'))   return pd.DataFrame(data=table) In\u00a0[\u00a0]: Copied! <pre>def display_properties(store, node):\n  # Helper function to render dataframes for artifact and execution properties\n  table = {'property': [], 'value': []}\n  for k, v in node.properties.items():\n    table['property'].append(k)\n    table['value'].append(\n        v.string_value if v.HasField('string_value') else v.int_value)\n  for k, v in node.custom_properties.items():\n    table['property'].append(k)\n    table['value'].append(\n        v.string_value if v.HasField('string_value') else v.int_value)\n  return pd.DataFrame(data=table)\n</pre> def display_properties(store, node):   # Helper function to render dataframes for artifact and execution properties   table = {'property': [], 'value': []}   for k, v in node.properties.items():     table['property'].append(k)     table['value'].append(         v.string_value if v.HasField('string_value') else v.int_value)   for k, v in node.custom_properties.items():     table['property'].append(k)     table['value'].append(         v.string_value if v.HasField('string_value') else v.int_value)   return pd.DataFrame(data=table) <p>First, query the MD store for a list of all its stored <code>ArtifactTypes</code>.</p> In\u00a0[\u00a0]: Copied! <pre>display_types(store.get_artifact_types())\n</pre> display_types(store.get_artifact_types()) <p>Next, query all <code>PushedModel</code> artifacts.</p> In\u00a0[\u00a0]: Copied! <pre>pushed_models = store.get_artifacts_by_type(\"PushedModel\")\ndisplay_artifacts(store, pushed_models)\n</pre> pushed_models = store.get_artifacts_by_type(\"PushedModel\") display_artifacts(store, pushed_models) <p>Query the MD store for the latest pushed model. This tutorial has only one pushed model.</p> In\u00a0[\u00a0]: Copied! <pre>pushed_model = pushed_models[-1]\ndisplay_properties(store, pushed_model)\n</pre> pushed_model = pushed_models[-1] display_properties(store, pushed_model) <p>One of the first steps in debugging a pushed model is to look at which trained model is pushed and to see which training data is used to train that model.</p> <p>MLMD provides traversal APIs to walk through the provenance graph, which you can use to analyze the model provenance.</p> In\u00a0[\u00a0]: Copied! <pre>def get_one_hop_parent_artifacts(store, artifacts):\n  # Get a list of artifacts within a 1-hop of the artifacts of interest\n  artifact_ids = [artifact.id for artifact in artifacts]\n  executions_ids = set(\n      event.execution_id\n      for event in store.get_events_by_artifact_ids(artifact_ids)\n      if event.type == mlmd.proto.Event.OUTPUT)\n  artifacts_ids = set(\n      event.artifact_id\n      for event in store.get_events_by_execution_ids(executions_ids)\n      if event.type == mlmd.proto.Event.INPUT)\n  return [artifact for artifact in store.get_artifacts_by_id(artifacts_ids)]\n</pre> def get_one_hop_parent_artifacts(store, artifacts):   # Get a list of artifacts within a 1-hop of the artifacts of interest   artifact_ids = [artifact.id for artifact in artifacts]   executions_ids = set(       event.execution_id       for event in store.get_events_by_artifact_ids(artifact_ids)       if event.type == mlmd.proto.Event.OUTPUT)   artifacts_ids = set(       event.artifact_id       for event in store.get_events_by_execution_ids(executions_ids)       if event.type == mlmd.proto.Event.INPUT)   return [artifact for artifact in store.get_artifacts_by_id(artifacts_ids)] <p>Query the parent artifacts for the pushed model.</p> In\u00a0[\u00a0]: Copied! <pre>parent_artifacts = get_one_hop_parent_artifacts(store, [pushed_model])\ndisplay_artifacts(store, parent_artifacts)\n</pre> parent_artifacts = get_one_hop_parent_artifacts(store, [pushed_model]) display_artifacts(store, parent_artifacts) <p>Query the properties for the model.</p> In\u00a0[\u00a0]: Copied! <pre>exported_model = parent_artifacts[0]\ndisplay_properties(store, exported_model)\n</pre> exported_model = parent_artifacts[0] display_properties(store, exported_model) <p>Query the upstream artifacts for the model.</p> In\u00a0[\u00a0]: Copied! <pre>model_parents = get_one_hop_parent_artifacts(store, [exported_model])\ndisplay_artifacts(store, model_parents)\n</pre> model_parents = get_one_hop_parent_artifacts(store, [exported_model]) display_artifacts(store, model_parents) <p>Get the training data the model trained with.</p> In\u00a0[\u00a0]: Copied! <pre>used_data = model_parents[0]\ndisplay_properties(store, used_data)\n</pre> used_data = model_parents[0] display_properties(store, used_data) <p>Now that you have the training data that the model trained with, query the database again to find the training step (execution). Query the MD store for a list of the registered execution types.</p> In\u00a0[\u00a0]: Copied! <pre>display_types(store.get_execution_types())\n</pre> display_types(store.get_execution_types()) <p>The training step is the <code>ExecutionType</code> named <code>tfx.components.trainer.component.Trainer</code>. Traverse the MD store to get the trainer run that corresponds to the pushed model.</p> In\u00a0[\u00a0]: Copied! <pre>def find_producer_execution(store, artifact):\n  executions_ids = set(\n      event.execution_id\n      for event in store.get_events_by_artifact_ids([artifact.id])\n      if event.type == mlmd.proto.Event.OUTPUT)\n  return store.get_executions_by_id(executions_ids)[0]\n\ntrainer = find_producer_execution(store, exported_model)\ndisplay_properties(store, trainer)\n</pre> def find_producer_execution(store, artifact):   executions_ids = set(       event.execution_id       for event in store.get_events_by_artifact_ids([artifact.id])       if event.type == mlmd.proto.Event.OUTPUT)   return store.get_executions_by_id(executions_ids)[0]  trainer = find_producer_execution(store, exported_model) display_properties(store, trainer)"},{"location":"tutorials/mlmd/mlmd_tutorial/#copyright-2020-the-tensorflow-authors","title":"Copyright 2020 The TensorFlow Authors.\u00b6","text":""},{"location":"tutorials/mlmd/mlmd_tutorial/#better-ml-engineering-with-ml-metadata","title":"Better ML Engineering with ML Metadata\u00b6","text":""},{"location":"tutorials/mlmd/mlmd_tutorial/#tfx-pipelines-in-colab","title":"TFX Pipelines in Colab\u00b6","text":"<p>Colab is a lightweight development environment which differs significantly from a production environment. In production, you may have various pipeline components like data ingestion, transformation, model training, run histories, etc. across multiple, distributed systems. For this tutorial, you should be aware that significant differences exist in Orchestration and Metadata storage - it is all handled locally within Colab. Learn more about TFX in Colab here.</p>"},{"location":"tutorials/mlmd/mlmd_tutorial/#setup","title":"Setup\u00b6","text":"<p>First, we install and import the necessary packages, set up paths, and download data.</p>"},{"location":"tutorials/mlmd/mlmd_tutorial/#upgrade-pip","title":"Upgrade Pip\u00b6","text":"<p>To avoid upgrading Pip in a system when running locally, check to make sure that we're running in Colab. Local systems can of course be upgraded separately.</p>"},{"location":"tutorials/mlmd/mlmd_tutorial/#install-and-import-tfx","title":"Install and import TFX\u00b6","text":""},{"location":"tutorials/mlmd/mlmd_tutorial/#import-packages","title":"Import packages\u00b6","text":""},{"location":"tutorials/mlmd/mlmd_tutorial/#did-you-restart-the-runtime","title":"Did you restart the runtime?\u00b6","text":"<p>If you are using Google Colab, the first time that you run the cell above, you must restart the runtime by clicking above \"RESTART RUNTIME\" button or using \"Runtime &gt; Restart runtime ...\" menu. This is because of the way that Colab loads packages.</p>"},{"location":"tutorials/mlmd/mlmd_tutorial/#download-the-dataset","title":"Download the dataset\u00b6","text":"<p>In this colab, we use the Palmer Penguins dataset which can be found on Github. We processed the dataset by leaving out any incomplete records, and drops <code>island</code> and <code>sex</code> columns, and converted labels to <code>int32</code>. The dataset contains 334 records of the body mass and the length and depth of penguins' culmens, and the length of their flippers. You use this data to classify penguins into one of three species.</p>"},{"location":"tutorials/mlmd/mlmd_tutorial/#create-an-interactivecontext","title":"Create an InteractiveContext\u00b6","text":"<p>To run TFX components interactively in this notebook, create an <code>InteractiveContext</code>. The <code>InteractiveContext</code> uses a temporary directory with an ephemeral MLMD database instance. Note that calls to <code>InteractiveContext</code> are no-ops outside the Colab environment.</p> <p>In general, it is a good practice to group similar pipeline runs under a <code>Context</code>.</p>"},{"location":"tutorials/mlmd/mlmd_tutorial/#construct-the-tfx-pipeline","title":"Construct the TFX Pipeline\u00b6","text":"<p>A TFX pipeline consists of several components that perform different aspects of the ML workflow. In this notebook, you create and run the <code>ExampleGen</code>, <code>StatisticsGen</code>, <code>SchemaGen</code>, and <code>Trainer</code> components and use the <code>Evaluator</code> and <code>Pusher</code> component to evaluate and push the trained model.</p> <p>Refer to the components tutorial for more information on TFX pipeline components.</p>"},{"location":"tutorials/mlmd/mlmd_tutorial/#instantiate-and-run-the-examplegen-component","title":"Instantiate and run the ExampleGen Component\u00b6","text":""},{"location":"tutorials/mlmd/mlmd_tutorial/#instantiate-and-run-the-statisticsgen-component","title":"Instantiate and run the StatisticsGen Component\u00b6","text":""},{"location":"tutorials/mlmd/mlmd_tutorial/#instantiate-and-run-the-schemagen-component","title":"Instantiate and run the SchemaGen Component\u00b6","text":""},{"location":"tutorials/mlmd/mlmd_tutorial/#instantiate-and-run-the-trainer-component","title":"Instantiate and run the Trainer Component\u00b6","text":""},{"location":"tutorials/mlmd/mlmd_tutorial/#evaluate-and-push-the-model","title":"Evaluate and push the model\u00b6","text":"<p>Use the <code>Evaluator</code> component to evaluate and 'bless' the model before using the <code>Pusher</code> component to push the model to a serving directory.</p>"},{"location":"tutorials/mlmd/mlmd_tutorial/#query-the-mlmd-database","title":"Query the MLMD Database\u00b6","text":"<p>The MLMD database stores three types of metadata:</p> <ul> <li>Metadata about the pipeline and lineage information associated with the pipeline components</li> <li>Metadata about artifacts that were generated during the pipeline run</li> <li>Metadata about the executions of the pipeline</li> </ul> <p>A typical production environment pipeline serves multiple models as new data arrives. When you encounter erroneous results in served models, you can query the MLMD database to isolate the erroneous models. You can then trace the lineage of the pipeline components that correspond to these models to debug your models</p>"},{"location":"tutorials/mlmd/mlmd_tutorial/#summary","title":"Summary\u00b6","text":"<p>In this tutorial, you learned about how you can leverage MLMD to trace the lineage of your TFX pipeline components and resolve issues.</p> <p>To learn more about how to use MLMD, check out these additional resources:</p> <ul> <li>MLMD API documentation</li> <li>MLMD guide</li> </ul>"},{"location":"tutorials/model_analysis/tfma_basic/","title":"TensorFlow Model Analysis","text":"In\u00a0[\u00a0]: Copied! <pre>#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n</pre> #@title Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # https://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. <p>Note: We recommend running this tutorial in a Colab notebook, with no setup required!  Just click \"Run in Google Colab\".</p>        View on TensorFlow.org             Run in Google Colab             View source on GitHub             Download notebook      <p>TensorFlow Model Analysis (TFMA) is a library for performing model evaluation across different slices of data. TFMA performs its computations in a distributed manner over large amounts of data using Apache Beam.</p> <p>This example colab notebook illustrates how  TFMA can be used to investigate and visualize the performance of a model with respect to characteristics of the dataset.  We'll use a model that we trained previously, and now you get to play with the results! The model we trained was for the Chicago Taxi Example, which uses the Taxi Trips dataset released by the City of Chicago. Explore the full dataset in the BigQuery UI.</p> <p>As a modeler and developer, think about how this data is used and the potential benefits and harm a model's predictions can cause. A model like this could reinforce societal biases and disparities. Is a feature relevant to the problem you want to solve or will it introduce bias? For more information, read about ML fairness.</p> <p>Note: In order to understand TFMA and how it works with Apache Beam, you'll need to know a little bit about Apache Beam itself.  The Beam Programming Guide is a great place to start.</p> <p>The columns in the dataset are:</p> pickup_community_areafaretrip_start_month trip_start_hourtrip_start_daytrip_start_timestamp pickup_latitudepickup_longitudedropoff_latitude dropoff_longitudetrip_milespickup_census_tract dropoff_census_tractpayment_typecompany trip_secondsdropoff_community_areatips In\u00a0[\u00a0]: Copied! <pre># Upgrade pip to the latest, and install TFMA.\n!pip install -U pip\n!pip install tensorflow-model-analysis\n</pre> # Upgrade pip to the latest, and install TFMA. !pip install -U pip !pip install tensorflow-model-analysis <p>Now you must restart the runtime before running the cells below.</p> In\u00a0[\u00a0]: Copied! <pre># This setup was tested with TF 2.10 and TFMA 0.41 (using colab), but it should\n# also work with the latest release.\nimport sys\n\n# Confirm that we're using Python 3\nassert sys.version_info.major==3, 'This notebook must be run using Python 3.'\n\nimport tensorflow as tf\nprint('TF version: {}'.format(tf.__version__))\nimport apache_beam as beam\nprint('Beam version: {}'.format(beam.__version__))\nimport tensorflow_model_analysis as tfma\nprint('TFMA version: {}'.format(tfma.__version__))\n</pre> # This setup was tested with TF 2.10 and TFMA 0.41 (using colab), but it should # also work with the latest release. import sys  # Confirm that we're using Python 3 assert sys.version_info.major==3, 'This notebook must be run using Python 3.'  import tensorflow as tf print('TF version: {}'.format(tf.__version__)) import apache_beam as beam print('Beam version: {}'.format(beam.__version__)) import tensorflow_model_analysis as tfma print('TFMA version: {}'.format(tfma.__version__)) <p>NOTE: The output above should be clear of errors before proceeding. Re-run the install if you are still seeing errors. Also, make sure to restart the runtime/kernel before moving to the next step.</p> In\u00a0[\u00a0]: Copied! <pre># Download the tar file from GCP and extract it\nimport io, os, tempfile\nTAR_NAME = 'saved_models-2.2'\nBASE_DIR = tempfile.mkdtemp()\nDATA_DIR = os.path.join(BASE_DIR, TAR_NAME, 'data')\nMODELS_DIR = os.path.join(BASE_DIR, TAR_NAME, 'models')\nSCHEMA = os.path.join(BASE_DIR, TAR_NAME, 'schema.pbtxt')\nOUTPUT_DIR = os.path.join(BASE_DIR, 'output')\n\n!curl -O https://storage.googleapis.com/artifacts.tfx-oss-public.appspot.com/datasets/{TAR_NAME}.tar\n!tar xf {TAR_NAME}.tar\n!mv {TAR_NAME} {BASE_DIR}\n!rm {TAR_NAME}.tar\n\nprint(\"Here's what we downloaded:\")\n!ls -R {BASE_DIR}\n</pre> # Download the tar file from GCP and extract it import io, os, tempfile TAR_NAME = 'saved_models-2.2' BASE_DIR = tempfile.mkdtemp() DATA_DIR = os.path.join(BASE_DIR, TAR_NAME, 'data') MODELS_DIR = os.path.join(BASE_DIR, TAR_NAME, 'models') SCHEMA = os.path.join(BASE_DIR, TAR_NAME, 'schema.pbtxt') OUTPUT_DIR = os.path.join(BASE_DIR, 'output')  !curl -O https://storage.googleapis.com/artifacts.tfx-oss-public.appspot.com/datasets/{TAR_NAME}.tar !tar xf {TAR_NAME}.tar !mv {TAR_NAME} {BASE_DIR} !rm {TAR_NAME}.tar  print(\"Here's what we downloaded:\") !ls -R {BASE_DIR} In\u00a0[\u00a0]: Copied! <pre>import tensorflow as tf\nfrom google.protobuf import text_format\nfrom tensorflow.python.lib.io import file_io\nfrom tensorflow_metadata.proto.v0 import schema_pb2\nfrom tensorflow.core.example import example_pb2\n\nschema = schema_pb2.Schema()\ncontents = file_io.read_file_to_string(SCHEMA)\nschema = text_format.Parse(contents, schema)\n</pre> import tensorflow as tf from google.protobuf import text_format from tensorflow.python.lib.io import file_io from tensorflow_metadata.proto.v0 import schema_pb2 from tensorflow.core.example import example_pb2  schema = schema_pb2.Schema() contents = file_io.read_file_to_string(SCHEMA) schema = text_format.Parse(contents, schema) In\u00a0[\u00a0]: Copied! <pre>import csv\n\ndatafile = os.path.join(DATA_DIR, 'eval', 'data.csv')\nreader = csv.DictReader(open(datafile, 'r'))\nexamples = []\nfor line in reader:\n  example = example_pb2.Example()\n  for feature in schema.feature:\n    key = feature.name\n    if feature.type == schema_pb2.FLOAT:\n      example.features.feature[key].float_list.value[:] = (\n          [float(line[key])] if len(line[key]) &gt; 0 else [])\n    elif feature.type == schema_pb2.INT:\n      example.features.feature[key].int64_list.value[:] = (\n          [int(line[key])] if len(line[key]) &gt; 0 else [])\n    elif feature.type == schema_pb2.BYTES:\n      example.features.feature[key].bytes_list.value[:] = (\n          [line[key].encode('utf8')] if len(line[key]) &gt; 0 else [])\n  # Add a new column 'big_tipper' that indicates if tips was &gt; 20% of the fare. \n  # TODO(b/157064428): Remove after label transformation is supported for Keras.\n  big_tipper = float(line['tips']) &gt; float(line['fare']) * 0.2\n  example.features.feature['big_tipper'].float_list.value[:] = [big_tipper]\n  examples.append(example)\n\ntfrecord_file = os.path.join(BASE_DIR, 'train_data.rio')\nwith tf.io.TFRecordWriter(tfrecord_file) as writer:\n  for example in examples:\n    writer.write(example.SerializeToString())\n\n!ls {tfrecord_file}\n</pre> import csv  datafile = os.path.join(DATA_DIR, 'eval', 'data.csv') reader = csv.DictReader(open(datafile, 'r')) examples = [] for line in reader:   example = example_pb2.Example()   for feature in schema.feature:     key = feature.name     if feature.type == schema_pb2.FLOAT:       example.features.feature[key].float_list.value[:] = (           [float(line[key])] if len(line[key]) &gt; 0 else [])     elif feature.type == schema_pb2.INT:       example.features.feature[key].int64_list.value[:] = (           [int(line[key])] if len(line[key]) &gt; 0 else [])     elif feature.type == schema_pb2.BYTES:       example.features.feature[key].bytes_list.value[:] = (           [line[key].encode('utf8')] if len(line[key]) &gt; 0 else [])   # Add a new column 'big_tipper' that indicates if tips was &gt; 20% of the fare.    # TODO(b/157064428): Remove after label transformation is supported for Keras.   big_tipper = float(line['tips']) &gt; float(line['fare']) * 0.2   example.features.feature['big_tipper'].float_list.value[:] = [big_tipper]   examples.append(example)  tfrecord_file = os.path.join(BASE_DIR, 'train_data.rio') with tf.io.TFRecordWriter(tfrecord_file) as writer:   for example in examples:     writer.write(example.SerializeToString())  !ls {tfrecord_file} In\u00a0[\u00a0]: Copied! <pre>import tensorflow_model_analysis as tfma\n\n# Setup tfma.EvalConfig settings\nkeras_eval_config = text_format.Parse(\"\"\"\n  ## Model information\n  model_specs {\n    # For keras (and serving models) we need to add a `label_key`.\n    label_key: \"big_tipper\"\n  }\n\n  ## Post training metric information. These will be merged with any built-in\n  ## metrics from training.\n  metrics_specs {\n    metrics { class_name: \"ExampleCount\" }\n    metrics { class_name: \"AUC\" }\n    metrics { class_name: \"Precision\" }\n    metrics { class_name: \"Recall\" }\n    metrics { class_name: \"MeanPrediction\" }\n    metrics { class_name: \"Calibration\" }\n    metrics { class_name: \"CalibrationPlot\" }\n    metrics { class_name: \"ConfusionMatrixPlot\" }\n    # ... add additional metrics and plots ...\n  }\n\n  ## Slicing information\n  slicing_specs {}  # overall slice\n  slicing_specs {\n    feature_keys: [\"trip_start_hour\"]\n  }\n  slicing_specs {\n    feature_keys: [\"trip_start_day\"]\n  }\n  slicing_specs {\n    feature_values: {\n      key: \"trip_start_month\"\n      value: \"1\"\n    }\n  }\n\"\"\", tfma.EvalConfig())\n\n# Create a tfma.EvalSharedModel that points at our keras model.\nkeras_model_path = os.path.join(MODELS_DIR, 'keras', '2')\nkeras_eval_shared_model = tfma.default_eval_shared_model(\n    eval_saved_model_path=keras_model_path,\n    eval_config=keras_eval_config)\n\nkeras_output_path = os.path.join(OUTPUT_DIR, 'keras')\n\n# Run TFMA\nkeras_eval_result = tfma.run_model_analysis(\n    eval_shared_model=keras_eval_shared_model,\n    eval_config=keras_eval_config,\n    data_location=tfrecord_file,\n    output_path=keras_output_path)\n</pre> import tensorflow_model_analysis as tfma  # Setup tfma.EvalConfig settings keras_eval_config = text_format.Parse(\"\"\"   ## Model information   model_specs {     # For keras (and serving models) we need to add a `label_key`.     label_key: \"big_tipper\"   }    ## Post training metric information. These will be merged with any built-in   ## metrics from training.   metrics_specs {     metrics { class_name: \"ExampleCount\" }     metrics { class_name: \"AUC\" }     metrics { class_name: \"Precision\" }     metrics { class_name: \"Recall\" }     metrics { class_name: \"MeanPrediction\" }     metrics { class_name: \"Calibration\" }     metrics { class_name: \"CalibrationPlot\" }     metrics { class_name: \"ConfusionMatrixPlot\" }     # ... add additional metrics and plots ...   }    ## Slicing information   slicing_specs {}  # overall slice   slicing_specs {     feature_keys: [\"trip_start_hour\"]   }   slicing_specs {     feature_keys: [\"trip_start_day\"]   }   slicing_specs {     feature_values: {       key: \"trip_start_month\"       value: \"1\"     }   } \"\"\", tfma.EvalConfig())  # Create a tfma.EvalSharedModel that points at our keras model. keras_model_path = os.path.join(MODELS_DIR, 'keras', '2') keras_eval_shared_model = tfma.default_eval_shared_model(     eval_saved_model_path=keras_model_path,     eval_config=keras_eval_config)  keras_output_path = os.path.join(OUTPUT_DIR, 'keras')  # Run TFMA keras_eval_result = tfma.run_model_analysis(     eval_shared_model=keras_eval_shared_model,     eval_config=keras_eval_config,     data_location=tfrecord_file,     output_path=keras_output_path) In\u00a0[\u00a0]: Copied! <pre>import tensorflow_model_analysis as tfma\n\n# Setup tfma.EvalConfig settings\nestimator_eval_config = text_format.Parse(\"\"\"\n  ## Model information\n  model_specs {\n    # To use EvalSavedModel set `signature_name` to \"eval\".\n    signature_name: \"eval\"\n  }\n\n  ## Post training metric information. These will be merged with any built-in\n  ## metrics from training.\n  metrics_specs {\n    metrics { class_name: \"ConfusionMatrixPlot\" }\n    # ... add additional metrics and plots ...\n  }\n\n  ## Slicing information\n  slicing_specs {}  # overall slice\n  slicing_specs {\n    feature_keys: [\"trip_start_hour\"]\n  }\n  slicing_specs {\n    feature_keys: [\"trip_start_day\"]\n  }\n  slicing_specs {\n    feature_values: {\n      key: \"trip_start_month\"\n      value: \"1\"\n    }\n  }\n\"\"\", tfma.EvalConfig())\n\n# Create a tfma.EvalSharedModel that points at our eval saved model.\nestimator_base_model_path = os.path.join(\n    MODELS_DIR, 'estimator', 'eval_model_dir')\nestimator_model_path = os.path.join(\n    estimator_base_model_path, os.listdir(estimator_base_model_path)[0])\nestimator_eval_shared_model = tfma.default_eval_shared_model(\n    eval_saved_model_path=estimator_model_path,\n    eval_config=estimator_eval_config)\n\nestimator_output_path = os.path.join(OUTPUT_DIR, 'estimator')\n\n# Run TFMA\nestimator_eval_result = tfma.run_model_analysis(\n    eval_shared_model=estimator_eval_shared_model,\n    eval_config=estimator_eval_config,\n    data_location=tfrecord_file,\n    output_path=estimator_output_path)\n</pre> import tensorflow_model_analysis as tfma  # Setup tfma.EvalConfig settings estimator_eval_config = text_format.Parse(\"\"\"   ## Model information   model_specs {     # To use EvalSavedModel set `signature_name` to \"eval\".     signature_name: \"eval\"   }    ## Post training metric information. These will be merged with any built-in   ## metrics from training.   metrics_specs {     metrics { class_name: \"ConfusionMatrixPlot\" }     # ... add additional metrics and plots ...   }    ## Slicing information   slicing_specs {}  # overall slice   slicing_specs {     feature_keys: [\"trip_start_hour\"]   }   slicing_specs {     feature_keys: [\"trip_start_day\"]   }   slicing_specs {     feature_values: {       key: \"trip_start_month\"       value: \"1\"     }   } \"\"\", tfma.EvalConfig())  # Create a tfma.EvalSharedModel that points at our eval saved model. estimator_base_model_path = os.path.join(     MODELS_DIR, 'estimator', 'eval_model_dir') estimator_model_path = os.path.join(     estimator_base_model_path, os.listdir(estimator_base_model_path)[0]) estimator_eval_shared_model = tfma.default_eval_shared_model(     eval_saved_model_path=estimator_model_path,     eval_config=estimator_eval_config)  estimator_output_path = os.path.join(OUTPUT_DIR, 'estimator')  # Run TFMA estimator_eval_result = tfma.run_model_analysis(     eval_shared_model=estimator_eval_shared_model,     eval_config=estimator_eval_config,     data_location=tfrecord_file,     output_path=estimator_output_path) In\u00a0[\u00a0]: Copied! <pre>eval_result_path = keras_output_path\n# eval_result_path = estimator_output_path\n\neval_result = keras_eval_result\n# eval_result = estimator_eval_result\n</pre> eval_result_path = keras_output_path # eval_result_path = estimator_output_path  eval_result = keras_eval_result # eval_result = estimator_eval_result In\u00a0[\u00a0]: Copied! <pre>import tensorflow_model_analysis.experimental.dataframe as tfma_dataframe\ndfs = tfma_dataframe.metrics_as_dataframes(\n  tfma.load_metrics(eval_result_path))\n\ndisplay(dfs.double_value.head())\n</pre> import tensorflow_model_analysis.experimental.dataframe as tfma_dataframe dfs = tfma_dataframe.metrics_as_dataframes(   tfma.load_metrics(eval_result_path))  display(dfs.double_value.head()) <p>Each of the DataFrames has a column multi-index with the top-level columns: <code>slices</code>, <code>metric_keys</code>, and <code>metric_values</code>. The exact columns of each group can change according to the payload. we can use <code>DataFrame.columns</code> API to inspect all the multi-index columns. For example, the slices columns are 'Overall', 'trip_start_day', 'trip_start_hour', and 'trip_start_month', which is configured by the <code>slicing_specs</code> in the <code>eval_config</code>.</p> In\u00a0[\u00a0]: Copied! <pre>print(dfs.double_value.columns)\n</pre> print(dfs.double_value.columns) In\u00a0[\u00a0]: Copied! <pre>tfma_dataframe.auto_pivot(dfs.double_value).head()\n</pre> tfma_dataframe.auto_pivot(dfs.double_value).head() In\u00a0[\u00a0]: Copied! <pre>df_double = dfs.double_value\ndf_filtered = (df_double\n  .loc[df_double.slices.trip_start_hour.isin([1,3,5,7])]\n)\ndisplay(tfma_dataframe.auto_pivot(df_filtered))\n</pre> df_double = dfs.double_value df_filtered = (df_double   .loc[df_double.slices.trip_start_hour.isin([1,3,5,7])] ) display(tfma_dataframe.auto_pivot(df_filtered)) In\u00a0[\u00a0]: Copied! <pre># Pivoted table sorted by AUC in ascending order.\ndf_sorted = (\n    tfma_dataframe.auto_pivot(df_double)\n    .sort_values(by='auc', ascending=True)\n    )\ndisplay(df_sorted.head())\n</pre> # Pivoted table sorted by AUC in ascending order. df_sorted = (     tfma_dataframe.auto_pivot(df_double)     .sort_values(by='auc', ascending=True)     ) display(df_sorted.head()) In\u00a0[\u00a0]: Copied! <pre>tfma.view.render_plot(\n    eval_result,\n    tfma.SlicingSpec(feature_values={'trip_start_hour': '1'}))\n</pre> tfma.view.render_plot(     eval_result,     tfma.SlicingSpec(feature_values={'trip_start_hour': '1'})) In\u00a0[\u00a0]: Copied! <pre># Note this re-uses the EvalConfig from the keras setup.\n\n# Run eval on each saved model\noutput_paths = []\nfor i in range(3):\n  # Create a tfma.EvalSharedModel that points at our saved model.\n  eval_shared_model = tfma.default_eval_shared_model(\n      eval_saved_model_path=os.path.join(MODELS_DIR, 'keras', str(i)),\n      eval_config=keras_eval_config)\n\n  output_path = os.path.join(OUTPUT_DIR, 'time_series', str(i))\n  output_paths.append(output_path)\n\n  # Run TFMA\n  tfma.run_model_analysis(eval_shared_model=eval_shared_model,\n                          eval_config=keras_eval_config,\n                          data_location=tfrecord_file,\n                          output_path=output_path)\n</pre> # Note this re-uses the EvalConfig from the keras setup.  # Run eval on each saved model output_paths = [] for i in range(3):   # Create a tfma.EvalSharedModel that points at our saved model.   eval_shared_model = tfma.default_eval_shared_model(       eval_saved_model_path=os.path.join(MODELS_DIR, 'keras', str(i)),       eval_config=keras_eval_config)    output_path = os.path.join(OUTPUT_DIR, 'time_series', str(i))   output_paths.append(output_path)    # Run TFMA   tfma.run_model_analysis(eval_shared_model=eval_shared_model,                           eval_config=keras_eval_config,                           data_location=tfrecord_file,                           output_path=output_path) <p>First, we'll imagine that we've trained and deployed our model yesterday, and now we want to see how it's doing on the new data coming in today.  The visualization will start by displaying AUC. From the UI you can:</p> <ul> <li>Add other metrics using the \"Add metric series\" menu.</li> <li>Close unwanted graphs by clicking on x</li> <li>Hover over data points (the ends of line segments in the graph) to get more details</li> </ul> <p>Note: In the metric series charts the X axis is the model directory name of the model run that you're examining.  These names themselves are not meaningful.</p> In\u00a0[\u00a0]: Copied! <pre>eval_results_from_disk = tfma.load_eval_results(output_paths[:2])\n\ntfma.view.render_time_series(eval_results_from_disk)\n</pre> eval_results_from_disk = tfma.load_eval_results(output_paths[:2])  tfma.view.render_time_series(eval_results_from_disk) <p>Now we'll imagine that another day has passed and we want to see how it's doing on the new data coming in today, compared to the previous two days:</p> In\u00a0[\u00a0]: Copied! <pre>eval_results_from_disk = tfma.load_eval_results(output_paths)\n\ntfma.view.render_time_series(eval_results_from_disk)\n</pre> eval_results_from_disk = tfma.load_eval_results(output_paths)  tfma.view.render_time_series(eval_results_from_disk) In\u00a0[\u00a0]: Copied! <pre># Setup tfma.EvalConfig setting\neval_config_with_thresholds = text_format.Parse(\"\"\"\n  ## Model information\n  model_specs {\n    name: \"candidate\"\n    # For keras we need to add a `label_key`.\n    label_key: \"big_tipper\"\n  }\n  model_specs {\n    name: \"baseline\"\n    # For keras we need to add a `label_key`.\n    label_key: \"big_tipper\"\n    is_baseline: true\n  }\n\n  ## Post training metric information\n  metrics_specs {\n    metrics { class_name: \"ExampleCount\" }\n    metrics { class_name: \"BinaryAccuracy\" }\n    metrics { class_name: \"BinaryCrossentropy\" }\n    metrics {\n      class_name: \"AUC\"\n      threshold {\n        # Ensure that AUC is always &gt; 0.9\n        value_threshold {\n          lower_bound { value: 0.9 }\n        }\n        # Ensure that AUC does not drop by more than a small epsilon\n        # e.g. (candidate - baseline) &gt; -1e-10 or candidate &gt; baseline - 1e-10\n        change_threshold {\n          direction: HIGHER_IS_BETTER\n          absolute { value: -1e-10 }\n        }\n      }\n    }\n    metrics { class_name: \"AUCPrecisionRecall\" }\n    metrics { class_name: \"Precision\" }\n    metrics { class_name: \"Recall\" }\n    metrics { class_name: \"MeanLabel\" }\n    metrics { class_name: \"MeanPrediction\" }\n    metrics { class_name: \"Calibration\" }\n    metrics { class_name: \"CalibrationPlot\" }\n    metrics { class_name: \"ConfusionMatrixPlot\" }\n    # ... add additional metrics and plots ...\n  }\n\n  ## Slicing information\n  slicing_specs {}  # overall slice\n  slicing_specs {\n    feature_keys: [\"trip_start_hour\"]\n  }\n  slicing_specs {\n    feature_keys: [\"trip_start_day\"]\n  }\n  slicing_specs {\n    feature_keys: [\"trip_start_month\"]\n  }\n  slicing_specs {\n    feature_keys: [\"trip_start_hour\", \"trip_start_day\"]\n  }\n\"\"\", tfma.EvalConfig())\n\n# Create tfma.EvalSharedModels that point at our keras models.\ncandidate_model_path = os.path.join(MODELS_DIR, 'keras', '2')\nbaseline_model_path = os.path.join(MODELS_DIR, 'keras', '1')\neval_shared_models = [\n  tfma.default_eval_shared_model(\n      model_name=tfma.CANDIDATE_KEY,\n      eval_saved_model_path=candidate_model_path,\n      eval_config=eval_config_with_thresholds),\n  tfma.default_eval_shared_model(\n      model_name=tfma.BASELINE_KEY,\n      eval_saved_model_path=baseline_model_path,\n      eval_config=eval_config_with_thresholds),\n]\n\nvalidation_output_path = os.path.join(OUTPUT_DIR, 'validation')\n\n# Run TFMA\neval_result_with_validation = tfma.run_model_analysis(\n    eval_shared_models,\n    eval_config=eval_config_with_thresholds,\n    data_location=tfrecord_file,\n    output_path=validation_output_path)\n</pre> # Setup tfma.EvalConfig setting eval_config_with_thresholds = text_format.Parse(\"\"\"   ## Model information   model_specs {     name: \"candidate\"     # For keras we need to add a `label_key`.     label_key: \"big_tipper\"   }   model_specs {     name: \"baseline\"     # For keras we need to add a `label_key`.     label_key: \"big_tipper\"     is_baseline: true   }    ## Post training metric information   metrics_specs {     metrics { class_name: \"ExampleCount\" }     metrics { class_name: \"BinaryAccuracy\" }     metrics { class_name: \"BinaryCrossentropy\" }     metrics {       class_name: \"AUC\"       threshold {         # Ensure that AUC is always &gt; 0.9         value_threshold {           lower_bound { value: 0.9 }         }         # Ensure that AUC does not drop by more than a small epsilon         # e.g. (candidate - baseline) &gt; -1e-10 or candidate &gt; baseline - 1e-10         change_threshold {           direction: HIGHER_IS_BETTER           absolute { value: -1e-10 }         }       }     }     metrics { class_name: \"AUCPrecisionRecall\" }     metrics { class_name: \"Precision\" }     metrics { class_name: \"Recall\" }     metrics { class_name: \"MeanLabel\" }     metrics { class_name: \"MeanPrediction\" }     metrics { class_name: \"Calibration\" }     metrics { class_name: \"CalibrationPlot\" }     metrics { class_name: \"ConfusionMatrixPlot\" }     # ... add additional metrics and plots ...   }    ## Slicing information   slicing_specs {}  # overall slice   slicing_specs {     feature_keys: [\"trip_start_hour\"]   }   slicing_specs {     feature_keys: [\"trip_start_day\"]   }   slicing_specs {     feature_keys: [\"trip_start_month\"]   }   slicing_specs {     feature_keys: [\"trip_start_hour\", \"trip_start_day\"]   } \"\"\", tfma.EvalConfig())  # Create tfma.EvalSharedModels that point at our keras models. candidate_model_path = os.path.join(MODELS_DIR, 'keras', '2') baseline_model_path = os.path.join(MODELS_DIR, 'keras', '1') eval_shared_models = [   tfma.default_eval_shared_model(       model_name=tfma.CANDIDATE_KEY,       eval_saved_model_path=candidate_model_path,       eval_config=eval_config_with_thresholds),   tfma.default_eval_shared_model(       model_name=tfma.BASELINE_KEY,       eval_saved_model_path=baseline_model_path,       eval_config=eval_config_with_thresholds), ]  validation_output_path = os.path.join(OUTPUT_DIR, 'validation')  # Run TFMA eval_result_with_validation = tfma.run_model_analysis(     eval_shared_models,     eval_config=eval_config_with_thresholds,     data_location=tfrecord_file,     output_path=validation_output_path) <p>When running evaluations with one or more models against a baseline, TFMA automatically adds diff metrics for all of the metrics computed during the evaluation. These metrics are named after the corresponding metric but with <code>_diff</code> appended to the metric name.</p> <p>Let's take a look at the metrics produced by our run:</p> In\u00a0[\u00a0]: Copied! <pre>tfma.view.render_time_series(eval_result_with_validation)\n</pre> tfma.view.render_time_series(eval_result_with_validation) <p>Now let's look at the output from our validation checks. To view the validation results we use <code>tfma.load_validator_result</code>. For our example, the validation fails because AUC is below the threshold.</p> In\u00a0[\u00a0]: Copied! <pre>validation_result = tfma.load_validation_result(validation_output_path)\nprint(validation_result.validation_ok)\n</pre> validation_result = tfma.load_validation_result(validation_output_path) print(validation_result.validation_ok) In\u00a0[\u00a0]: Copied! <pre>#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n</pre> #@title Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # https://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. <p>Note: This site provides applications using data that has been modified for use from its original source, www.cityofchicago.org, the official website of the City of Chicago. The City of Chicago makes no claims as to the content, accuracy, timeliness, or completeness of any of the data provided at this site. The data provided at this site is subject to change at any time. It is understood that the data provided at this site is being used at one\u2019s own risk.</p>"},{"location":"tutorials/model_analysis/tfma_basic/#copyright-2021-the-tensorflow-authors","title":"Copyright 2021 The TensorFlow Authors.\u00b6","text":""},{"location":"tutorials/model_analysis/tfma_basic/#tensorflow-model-analysis","title":"TensorFlow Model Analysis\u00b6","text":"<p>An Example of a Key Component of TensorFlow Extended (TFX)</p>"},{"location":"tutorials/model_analysis/tfma_basic/#install-jupyter-extensions","title":"Install Jupyter Extensions\u00b6","text":"<p>Note: If running in a local Jupyter notebook, then these Jupyter extensions must be installed in the environment before running Jupyter.</p> <pre>jupyter nbextension enable --py widgetsnbextension --sys-prefix \njupyter nbextension install --py --symlink tensorflow_model_analysis --sys-prefix \njupyter nbextension enable --py tensorflow_model_analysis --sys-prefix \n</pre>"},{"location":"tutorials/model_analysis/tfma_basic/#install-tensorflow-model-analysis-tfma","title":"Install TensorFlow Model Analysis (TFMA)\u00b6","text":"<p>This will pull in all the dependencies, and will take a minute.</p>"},{"location":"tutorials/model_analysis/tfma_basic/#load-the-files","title":"Load The Files\u00b6","text":"<p>We'll download a tar file that has everything we need.  That includes:</p> <ul> <li>Training and evaluation datasets</li> <li>Data schema</li> <li>Training and serving saved models (keras and estimator) and eval saved models (estimator).</li> </ul>"},{"location":"tutorials/model_analysis/tfma_basic/#parse-the-schema","title":"Parse the Schema\u00b6","text":"<p>Among the things we downloaded was a schema for our data that was created by TensorFlow Data Validation.  Let's parse that now so that we can use it with TFMA.</p>"},{"location":"tutorials/model_analysis/tfma_basic/#use-the-schema-to-create-tfrecords","title":"Use the Schema to Create TFRecords\u00b6","text":"<p>We need to give TFMA access to our dataset, so let's create a TFRecords file.  We can use our schema to create it, since it gives us the correct type for each feature.</p>"},{"location":"tutorials/model_analysis/tfma_basic/#setup-and-run-tfma","title":"Setup and Run TFMA\u00b6","text":"<p>TFMA supports a number of different model types including TF keras models, models based on generic TF2 signature APIs, as well TF estimator based models. The get_started guide has the full list of model types supported and any restrictions. For this example we are going to show how to configure a keras based model as well as an estimator based model that was saved as an <code>EvalSavedModel</code>. See the FAQ for examples of other configurations.</p> <p>TFMA provides support for calculating metrics that were used at training time (i.e. built-in metrics) as well metrics defined after the model was saved as part of the TFMA configuration settings. For our keras setup we will demonstrate adding our metrics and plots manually as part of our configuration (see the metrics guide for information on the metrics and plots that are supported). For the estimator setup we will use the built-in metrics that were saved with the model. Our setups also include a number of slicing specs which are discussed in more detail in the following sections.</p> <p>After creating a <code>tfma.EvalConfig</code> and <code>tfma.EvalSharedModel</code> we can then run TFMA using <code>tfma.run_model_analysis</code>. This will create a <code>tfma.EvalResult</code> which we can use later for rendering our metrics and plots.</p>"},{"location":"tutorials/model_analysis/tfma_basic/#keras","title":"Keras\u00b6","text":""},{"location":"tutorials/model_analysis/tfma_basic/#estimator","title":"Estimator\u00b6","text":""},{"location":"tutorials/model_analysis/tfma_basic/#visualizing-metrics-and-plots","title":"Visualizing Metrics and Plots\u00b6","text":"<p>Now that we've run the evaluation, let's take a look at our visualizations using TFMA. For the following examples, we will visualize the results from running the evaluation on the keras model. To view the estimator based model update the <code>eval_result_path</code> to point at our <code>estimator_output_path</code> variable.</p>"},{"location":"tutorials/model_analysis/tfma_basic/#rendering-metrics","title":"Rendering Metrics\u00b6","text":"<p>TFMA provides dataframe APIs in <code>tfma.experimental.dataframe</code> to load the materalized output as <code>Pandas DataFrames</code>. To view metrics you can use <code>metrics_as_dataframes(tfma.load_metrics(eval_path))</code>, which returns an object which potentially contains several DataFrames, one for each metric value type (<code>double_value</code>, <code>confusion_matrix_at_thresholds</code>,  <code>bytes_value</code>, and <code>array_value</code>). The specific DataFrames populated depends on the eval result. Here, we show the <code>double_value</code> DataFrame as an example.</p>"},{"location":"tutorials/model_analysis/tfma_basic/#auto-pivoting","title":"Auto pivoting\u00b6","text":"<p>The DataFrame is verbose by design so that there is no loss of information from the payload. However, sometimes, for direct consumption, we might want to organize the information in a more concise but lossy form: slices as rows and metrics as columns. TFMA provides an <code>auto_pivot</code> API for this purpose. The util pivots on all of the non-unique columns inside <code>metric_keys</code>, and condenses all the slices into one <code>stringified_slices</code> column by default.</p>"},{"location":"tutorials/model_analysis/tfma_basic/#filtering-slices","title":"Filtering slices\u00b6","text":"<p>Since the outputs are DataFrames, any native DataFrame APIs can be used to slice and dice the DataFrame. For example, if we are only interested in <code>trip_start_hour</code> of 1, 3, 5, 7 and not in <code>trip_start_day</code>, we can use DataFrame's <code>.loc</code> filtering logic. Again, we use the <code>auto_pivot</code> function to re-organize the DataFrame in the slice vs. metrics view after the filtering is performed.</p>"},{"location":"tutorials/model_analysis/tfma_basic/#sorting-by-metric-values","title":"Sorting by metric values\u00b6","text":"<p>We can also sort slices by metrics value. As an example, we show how to sort slices in the above DataFrame by ascending AUC, so that we can find poorly performing slices. This involves two steps: auto-pivoting so that slices are represented as rows and columns are metrics, and then sorting the pivoted DataFrame by the AUC column.</p>"},{"location":"tutorials/model_analysis/tfma_basic/#rendering-plots","title":"Rendering Plots\u00b6","text":"<p>Any plots that were added to the <code>tfma.EvalConfig</code> as post training <code>metric_specs</code> can be displayed using <code>tfma.view.render_plot</code>.</p> <p>As with metrics, plots can be viewed by slice. Unlike metrics, only plots for a particular slice value can be displayed so the <code>tfma.SlicingSpec</code> must be used and it must specify both a slice feature name and value. If no slice is provided then the plots for the <code>Overall</code> slice is used.</p> <p>In the example below we are displaying the <code>CalibrationPlot</code> and <code>ConfusionMatrixPlot</code> plots that were computed for the <code>trip_start_hour:1</code> slice.</p>"},{"location":"tutorials/model_analysis/tfma_basic/#tracking-model-performance-over-time","title":"Tracking Model Performance Over Time\u00b6","text":"<p>Your training dataset will be used for training your model, and will hopefully be representative of your test dataset and the data that will be sent to your model in production.  However, while the data in inference requests may remain the same as your training data, in many cases it will start to change enough so that the performance of your model will change.</p> <p>That means that you need to monitor and measure your model's performance on an ongoing basis, so that you can be aware of and react to changes.  Let's take a look at how TFMA can help.</p> <p>Let's load 3 different model runs and use TFMA to see how they compare using <code>render_time_series</code>.</p>"},{"location":"tutorials/model_analysis/tfma_basic/#model-validation","title":"Model Validation\u00b6","text":"<p>TFMA can be configured to evaluate multiple models at the same time. Typically this is done to compare a new model against a baseline (such as the currently serving model) to determine what the performance differences in metrics (e.g. AUC, etc) are relative to the baseline. When thresholds are configured, TFMA will produce a <code>tfma.ValidationResult</code> record indicating whether the performance matches expectations.</p> <p>Let's re-configure our keras evaluation to compare two models: a candidate and a baseline. We will also validate the candidate's performance against the baseline by setting a <code>tmfa.MetricThreshold</code> on the AUC metric.</p>"},{"location":"tutorials/model_analysis/tfma_basic/#copyright-2020-the-tensorflow-authors","title":"Copyright \u00a9 2020 The TensorFlow Authors.\u00b6","text":""},{"location":"tutorials/serving/rest_simple/","title":"Train and serve a TensorFlow model with TensorFlow Serving","text":"In\u00a0[\u00a0]: Copied! <pre>#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n</pre> #@title Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # https://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. <p>Note: We recommend running this tutorial in a Colab notebook, with no setup required!  Just click \"Run in Google Colab\".</p>        View on TensorFlow.org             Run in Google Colab             View source on GitHub             Download notebook      <p>This guide trains a neural network model to classify images of clothing, like sneakers and shirts, saves the trained model, and then serves it with TensorFlow Serving.  The focus is on TensorFlow Serving, rather than the modeling and training in TensorFlow, so for a complete example which focuses on the modeling and training see the Basic Classification example.</p> <p>This guide uses tf.keras, a high-level API to build and train models in TensorFlow.</p> In\u00a0[\u00a0]: Copied! <pre>import sys\n\n# Confirm that we're using Python 3\nassert sys.version_info.major == 3, 'Oops, not running Python 3. Use Runtime &gt; Change runtime type'\n</pre> import sys  # Confirm that we're using Python 3 assert sys.version_info.major == 3, 'Oops, not running Python 3. Use Runtime &gt; Change runtime type' In\u00a0[\u00a0]: Copied! <pre># TensorFlow and tf.keras\nprint(\"Installing dependencies for Colab environment\")\n!pip install -Uq grpcio==1.26.0\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\n# Helper libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport subprocess\n\nprint('TensorFlow version: {}'.format(tf.__version__))\n</pre> # TensorFlow and tf.keras print(\"Installing dependencies for Colab environment\") !pip install -Uq grpcio==1.26.0  import tensorflow as tf from tensorflow import keras  # Helper libraries import numpy as np import matplotlib.pyplot as plt import os import subprocess  print('TensorFlow version: {}'.format(tf.__version__)) In\u00a0[\u00a0]: Copied! <pre>fashion_mnist = keras.datasets.fashion_mnist\n(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n\n# scale the values to 0.0 to 1.0\ntrain_images = train_images / 255.0\ntest_images = test_images / 255.0\n\n# reshape for feeding into the model\ntrain_images = train_images.reshape(train_images.shape[0], 28, 28, 1)\ntest_images = test_images.reshape(test_images.shape[0], 28, 28, 1)\n\nclass_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n\nprint('\\ntrain_images.shape: {}, of {}'.format(train_images.shape, train_images.dtype))\nprint('test_images.shape: {}, of {}'.format(test_images.shape, test_images.dtype))\n</pre> fashion_mnist = keras.datasets.fashion_mnist (train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()  # scale the values to 0.0 to 1.0 train_images = train_images / 255.0 test_images = test_images / 255.0  # reshape for feeding into the model train_images = train_images.reshape(train_images.shape[0], 28, 28, 1) test_images = test_images.reshape(test_images.shape[0], 28, 28, 1)  class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',                'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']  print('\\ntrain_images.shape: {}, of {}'.format(train_images.shape, train_images.dtype)) print('test_images.shape: {}, of {}'.format(test_images.shape, test_images.dtype)) In\u00a0[\u00a0]: Copied! <pre>model = keras.Sequential([\n  keras.layers.Conv2D(input_shape=(28,28,1), filters=8, kernel_size=3, \n                      strides=2, activation='relu', name='Conv1'),\n  keras.layers.Flatten(),\n  keras.layers.Dense(10, name='Dense')\n])\nmodel.summary()\n\ntesting = False\nepochs = 5\n\nmodel.compile(optimizer='adam', \n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=[keras.metrics.SparseCategoricalAccuracy()])\nmodel.fit(train_images, train_labels, epochs=epochs)\n\ntest_loss, test_acc = model.evaluate(test_images, test_labels)\nprint('\\nTest accuracy: {}'.format(test_acc))\n</pre> model = keras.Sequential([   keras.layers.Conv2D(input_shape=(28,28,1), filters=8, kernel_size=3,                        strides=2, activation='relu', name='Conv1'),   keras.layers.Flatten(),   keras.layers.Dense(10, name='Dense') ]) model.summary()  testing = False epochs = 5  model.compile(optimizer='adam',                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),               metrics=[keras.metrics.SparseCategoricalAccuracy()]) model.fit(train_images, train_labels, epochs=epochs)  test_loss, test_acc = model.evaluate(test_images, test_labels) print('\\nTest accuracy: {}'.format(test_acc)) In\u00a0[\u00a0]: Copied! <pre># Fetch the Keras session and save the model\n# The signature definition is defined by the input and output tensors,\n# and stored with the default serving key\nimport tempfile\n\nMODEL_DIR = tempfile.gettempdir()\nversion = 1\nexport_path = os.path.join(MODEL_DIR, str(version))\nprint('export_path = {}\\n'.format(export_path))\n\ntf.keras.models.save_model(\n    model,\n    export_path,\n    overwrite=True,\n    include_optimizer=True,\n    save_format=None,\n    signatures=None,\n    options=None\n)\n\nprint('\\nSaved model:')\n!ls -l {export_path}\n</pre> # Fetch the Keras session and save the model # The signature definition is defined by the input and output tensors, # and stored with the default serving key import tempfile  MODEL_DIR = tempfile.gettempdir() version = 1 export_path = os.path.join(MODEL_DIR, str(version)) print('export_path = {}\\n'.format(export_path))  tf.keras.models.save_model(     model,     export_path,     overwrite=True,     include_optimizer=True,     save_format=None,     signatures=None,     options=None )  print('\\nSaved model:') !ls -l {export_path} In\u00a0[\u00a0]: Copied! <pre>!saved_model_cli show --dir {export_path} --all\n</pre> !saved_model_cli show --dir {export_path} --all <p>That tells us a lot about our model!  In this case we just trained our model, so we already know the inputs and outputs, but if we didn't this would be important information.  It doesn't tell us everything, like the fact that this is grayscale image data for example, but it's a great start.</p> In\u00a0[\u00a0]: Copied! <pre>import sys\n# We need sudo prefix if not on a Google Colab.\nif 'google.colab' not in sys.modules:\n  SUDO_IF_NEEDED = 'sudo'\nelse:\n  SUDO_IF_NEEDED = ''\n</pre> import sys # We need sudo prefix if not on a Google Colab. if 'google.colab' not in sys.modules:   SUDO_IF_NEEDED = 'sudo' else:   SUDO_IF_NEEDED = '' In\u00a0[\u00a0]: Copied! <pre># This is the same as you would do from your command line, but without the [arch=amd64], and no sudo\n# You would instead do:\n# echo \"deb [arch=amd64] http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal\" | sudo tee /etc/apt/sources.list.d/tensorflow-serving.list &amp;&amp; \\\n# curl https://storage.googleapis.com/tensorflow-serving-apt/tensorflow-serving.release.pub.gpg | sudo apt-key add -\n\n!echo \"deb http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal\" | {SUDO_IF_NEEDED} tee /etc/apt/sources.list.d/tensorflow-serving.list &amp;&amp; \\\ncurl https://storage.googleapis.com/tensorflow-serving-apt/tensorflow-serving.release.pub.gpg | {SUDO_IF_NEEDED} apt-key add -\n!{SUDO_IF_NEEDED} apt update\n</pre> # This is the same as you would do from your command line, but without the [arch=amd64], and no sudo # You would instead do: # echo \"deb [arch=amd64] http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal\" | sudo tee /etc/apt/sources.list.d/tensorflow-serving.list &amp;&amp; \\ # curl https://storage.googleapis.com/tensorflow-serving-apt/tensorflow-serving.release.pub.gpg | sudo apt-key add -  !echo \"deb http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal\" | {SUDO_IF_NEEDED} tee /etc/apt/sources.list.d/tensorflow-serving.list &amp;&amp; \\ curl https://storage.googleapis.com/tensorflow-serving-apt/tensorflow-serving.release.pub.gpg | {SUDO_IF_NEEDED} apt-key add - !{SUDO_IF_NEEDED} apt update In\u00a0[\u00a0]: Copied! <pre># TODO: Use the latest model server version when colab supports it.\n#!{SUDO_IF_NEEDED} apt-get install tensorflow-model-server\n# We need to install Tensorflow Model server 2.8 instead of latest version\n# Tensorflow Serving &gt;2.9.0 required `GLIBC_2.29` and `GLIBCXX_3.4.26`. Currently colab environment doesn't support latest version of`GLIBC`,so workaround is to use specific version of Tensorflow Serving `2.8.0` to mitigate issue.\n!wget 'http://storage.googleapis.com/tensorflow-serving-apt/pool/tensorflow-model-server-2.8.0/t/tensorflow-model-server/tensorflow-model-server_2.8.0_all.deb'\n!dpkg -i tensorflow-model-server_2.8.0_all.deb\n!pip3 install tensorflow-serving-api==2.8.0\n</pre> # TODO: Use the latest model server version when colab supports it. #!{SUDO_IF_NEEDED} apt-get install tensorflow-model-server # We need to install Tensorflow Model server 2.8 instead of latest version # Tensorflow Serving &gt;2.9.0 required `GLIBC_2.29` and `GLIBCXX_3.4.26`. Currently colab environment doesn't support latest version of`GLIBC`,so workaround is to use specific version of Tensorflow Serving `2.8.0` to mitigate issue. !wget 'http://storage.googleapis.com/tensorflow-serving-apt/pool/tensorflow-model-server-2.8.0/t/tensorflow-model-server/tensorflow-model-server_2.8.0_all.deb' !dpkg -i tensorflow-model-server_2.8.0_all.deb !pip3 install tensorflow-serving-api==2.8.0 In\u00a0[\u00a0]: Copied! <pre>os.environ[\"MODEL_DIR\"] = MODEL_DIR\n</pre> os.environ[\"MODEL_DIR\"] = MODEL_DIR In\u00a0[\u00a0]: Copied! <pre>%%bash --bg \nnohup tensorflow_model_server \\\n  --rest_api_port=8501 \\\n  --model_name=fashion_model \\\n  --model_base_path=\"${MODEL_DIR}\" &gt;server.log 2&gt;&amp;1\n</pre> %%bash --bg  nohup tensorflow_model_server \\   --rest_api_port=8501 \\   --model_name=fashion_model \\   --model_base_path=\"${MODEL_DIR}\" &gt;server.log 2&gt;&amp;1  In\u00a0[\u00a0]: Copied! <pre>!tail server.log\n</pre> !tail server.log In\u00a0[\u00a0]: Copied! <pre>def show(idx, title):\n  plt.figure()\n  plt.imshow(test_images[idx].reshape(28,28))\n  plt.axis('off')\n  plt.title('\\n\\n{}'.format(title), fontdict={'size': 16})\n\nimport random\nrando = random.randint(0,len(test_images)-1)\nshow(rando, 'An Example Image: {}'.format(class_names[test_labels[rando]]))\n</pre> def show(idx, title):   plt.figure()   plt.imshow(test_images[idx].reshape(28,28))   plt.axis('off')   plt.title('\\n\\n{}'.format(title), fontdict={'size': 16})  import random rando = random.randint(0,len(test_images)-1) show(rando, 'An Example Image: {}'.format(class_names[test_labels[rando]])) <p>Ok, that looks interesting.  How hard is that for you to recognize? Now let's create the JSON object for a batch of  three inference requests, and see how well our model recognizes things:</p> In\u00a0[\u00a0]: Copied! <pre>import json\ndata = json.dumps({\"signature_name\": \"serving_default\", \"instances\": test_images[0:3].tolist()})\nprint('Data: {} ... {}'.format(data[:50], data[len(data)-52:]))\n</pre> import json data = json.dumps({\"signature_name\": \"serving_default\", \"instances\": test_images[0:3].tolist()}) print('Data: {} ... {}'.format(data[:50], data[len(data)-52:])) In\u00a0[\u00a0]: Copied! <pre># docs_infra: no_execute\n!pip install -q requests\n\nimport requests\nheaders = {\"content-type\": \"application/json\"}\njson_response = requests.post('http://localhost:8501/v1/models/fashion_model:predict', data=data, headers=headers)\npredictions = json.loads(json_response.text)['predictions']\n\nshow(0, 'The model thought this was a {} (class {}), and it was actually a {} (class {})'.format(\n  class_names[np.argmax(predictions[0])], np.argmax(predictions[0]), class_names[test_labels[0]], test_labels[0]))\n</pre> # docs_infra: no_execute !pip install -q requests  import requests headers = {\"content-type\": \"application/json\"} json_response = requests.post('http://localhost:8501/v1/models/fashion_model:predict', data=data, headers=headers) predictions = json.loads(json_response.text)['predictions']  show(0, 'The model thought this was a {} (class {}), and it was actually a {} (class {})'.format(   class_names[np.argmax(predictions[0])], np.argmax(predictions[0]), class_names[test_labels[0]], test_labels[0])) In\u00a0[\u00a0]: Copied! <pre># docs_infra: no_execute\nheaders = {\"content-type\": \"application/json\"}\njson_response = requests.post('http://localhost:8501/v1/models/fashion_model/versions/1:predict', data=data, headers=headers)\npredictions = json.loads(json_response.text)['predictions']\n\nfor i in range(0,3):\n  show(i, 'The model thought this was a {} (class {}), and it was actually a {} (class {})'.format(\n    class_names[np.argmax(predictions[i])], np.argmax(predictions[i]), class_names[test_labels[i]], test_labels[i]))\n</pre> # docs_infra: no_execute headers = {\"content-type\": \"application/json\"} json_response = requests.post('http://localhost:8501/v1/models/fashion_model/versions/1:predict', data=data, headers=headers) predictions = json.loads(json_response.text)['predictions']  for i in range(0,3):   show(i, 'The model thought this was a {} (class {}), and it was actually a {} (class {})'.format(     class_names[np.argmax(predictions[i])], np.argmax(predictions[i]), class_names[test_labels[i]], test_labels[i]))"},{"location":"tutorials/serving/rest_simple/#copyright-2020-the-tensorflow-authors","title":"Copyright 2020 The TensorFlow Authors.\u00b6","text":""},{"location":"tutorials/serving/rest_simple/#train-and-serve-a-tensorflow-model-with-tensorflow-serving","title":"Train and serve a TensorFlow model with TensorFlow Serving\u00b6","text":""},{"location":"tutorials/serving/rest_simple/#create-your-model","title":"Create your model\u00b6","text":""},{"location":"tutorials/serving/rest_simple/#import-the-fashion-mnist-dataset","title":"Import the Fashion MNIST dataset\u00b6","text":"<p>This guide uses the Fashion MNIST dataset which contains 70,000 grayscale images in 10 categories. The images show individual articles of clothing at low resolution (28 by 28 pixels), as seen here:</p> Figure 1. Fashion-MNIST samples (by Zalando, MIT License). <p>Fashion MNIST is intended as a drop-in replacement for the classic MNIST dataset\u2014often used as the \"Hello, World\" of machine learning programs for computer vision. You can access the Fashion MNIST directly from TensorFlow, just import and load the data.</p> <p>Note: Although these are really images, they are loaded as NumPy arrays and not binary image objects.</p>"},{"location":"tutorials/serving/rest_simple/#train-and-evaluate-your-model","title":"Train and evaluate your model\u00b6","text":"<p>Let's use the simplest possible CNN, since we're not focused on the modeling part.</p>"},{"location":"tutorials/serving/rest_simple/#save-your-model","title":"Save your model\u00b6","text":"<p>To load our trained model into TensorFlow Serving we first need to save it in SavedModel format.  This will create a protobuf file in a well-defined directory hierarchy, and will include a version number.  TensorFlow Serving allows us to select which version of a model, or \"servable\" we want to use when we make inference requests.  Each version will be exported to a different sub-directory under the given path.</p>"},{"location":"tutorials/serving/rest_simple/#examine-your-saved-model","title":"Examine your saved model\u00b6","text":"<p>We'll use the command line utility <code>saved_model_cli</code> to look at the MetaGraphDefs (the models) and SignatureDefs (the methods you can call) in our SavedModel.  See this discussion of the SavedModel CLI in the TensorFlow Guide.</p>"},{"location":"tutorials/serving/rest_simple/#serve-your-model-with-tensorflow-serving","title":"Serve your model with TensorFlow Serving\u00b6","text":"<p>Warning: If you are running this NOT on a Google Colab, following cells will install packages on the system with root access. If you want to run it in a local Jupyter notebook, please proceed with caution.</p>"},{"location":"tutorials/serving/rest_simple/#add-tensorflow-serving-distribution-uri-as-a-package-source","title":"Add TensorFlow Serving distribution URI as a package source:\u00b6","text":"<p>We're preparing to install TensorFlow Serving using Aptitude since this Colab runs in a Debian environment.  We'll add the <code>tensorflow-model-server</code> package to the list of packages that Aptitude knows about.  Note that we're running as root.</p> <p>Note: This example is running TensorFlow Serving natively, but you can also run it in a Docker container, which is one of the easiest ways to get started using TensorFlow Serving.</p>"},{"location":"tutorials/serving/rest_simple/#install-tensorflow-serving","title":"Install TensorFlow Serving\u00b6","text":"<p>This is all you need - one command line!</p>"},{"location":"tutorials/serving/rest_simple/#start-running-tensorflow-serving","title":"Start running TensorFlow Serving\u00b6","text":"<p>This is where we start running TensorFlow Serving and load our model.  After it loads we can start making inference requests using REST.  There are some important parameters:</p> <ul> <li><code>rest_api_port</code>: The port that you'll use for REST requests.</li> <li><code>model_name</code>: You'll use this in the URL of REST requests.  It can be anything.</li> <li><code>model_base_path</code>: This is the path to the directory where you've saved your model.</li> </ul>"},{"location":"tutorials/serving/rest_simple/#make-a-request-to-your-model-in-tensorflow-serving","title":"Make a request to your model in TensorFlow Serving\u00b6","text":"<p>First, let's take a look at a random example from our test data.</p>"},{"location":"tutorials/serving/rest_simple/#make-rest-requests","title":"Make REST requests\u00b6","text":""},{"location":"tutorials/serving/rest_simple/#newest-version-of-the-servable","title":"Newest version of the servable\u00b6","text":"<p>We'll send a predict request as a POST to our server's REST endpoint, and pass it three examples.  We'll ask our server to give us the latest version of our servable by not specifying a particular version.</p>"},{"location":"tutorials/serving/rest_simple/#a-particular-version-of-the-servable","title":"A particular version of the servable\u00b6","text":"<p>Now let's specify a particular version of our servable.  Since we only have one, let's select version 1.  We'll also look at all three results.</p>"},{"location":"tutorials/tfx/airflow_workshop/","title":"TFX Airflow Tutorial","text":""},{"location":"tutorials/tfx/airflow_workshop/#overview","title":"Overview","text":""},{"location":"tutorials/tfx/airflow_workshop/#overview_1","title":"Overview","text":"<p>This tutorial is designed to help you learn to create your own machine learning pipelines using TensorFlow Extended (TFX) and Apache Airflow as the orchestrator. It runs on on Vertex AI Workbench, and shows integration with TFX and TensorBoard as well as interaction with TFX in a Jupyter Lab environment.</p>"},{"location":"tutorials/tfx/airflow_workshop/#what-youll-be-doing","title":"What you'll be doing?","text":"<p>You\u2019ll learn how to create an ML pipeline using TFX</p> <ul> <li>A TFX pipeline is a Directed Acyclic Graph, or \"DAG\". We will often refer to     pipelines as DAGs.</li> <li>TFX pipelines are appropriate when you will be deploying a production ML     application</li> <li>TFX pipelines are appropriate when datasets are large, or may grow to be     large</li> <li>TFX pipelines are appropriate when training/serving consistency is important</li> <li>TFX pipelines are appropriate when version management for inference is     important</li> <li>Google uses TFX pipelines for production ML</li> </ul> <p>Please see the TFX User Guide to learn more.</p> <p>You'll follow a typical ML development process:</p> <ul> <li>Ingesting, understanding, and cleaning our data</li> <li>Feature engineering</li> <li>Training</li> <li>Analyzing model performance</li> <li>Lather, rinse, repeat</li> <li>Ready for production</li> </ul>"},{"location":"tutorials/tfx/airflow_workshop/#apache-airflow-for-pipeline-orchestration","title":"Apache Airflow for Pipeline Orchestration","text":"<p>TFX orchestrators are responsible for scheduling components of the TFX pipeline based on the dependencies defined by the pipeline. TFX is designed to be portable to multiple environments and orchestration frameworks. One of the default orchestrators supported by TFX is Apache Airflow. This lab illustrates the use of Apache Airflow for TFX pipeline orchestration. Apache Airflow is a platform to programmatically author, schedule and monitor workflows. TFX uses Airflow to author workflows as directed acyclic graphs (DAGs) of tasks. The rich user interface makes it easy to visualize pipelines running in production, monitor progress, and troubleshoot issues when needed. Apache Airflow workflows are defined as code. This makes them more maintainable, versionable, testable, and collaborative. Apache Airflow is suited for batch processing pipelines. It is lightweight and easy to learn.</p> <p>In this example, we are going to run a TFX pipeline on an instance by manually setting up Airflow.</p> <p>The other default orchestrators supported by TFX are Apache Beam and Kubeflow. Apache Beam can run on multiple data processing backends (Beam Ruunners). Cloud Dataflow is one such beam runner which can be used for running TFX pipelines. Apache Beam can be used for both streaming and batch processing pipelines.</p> <p>Kubeflow is an open source ML platform dedicated to making deployments of machine learning (ML) workflows on Kubernetes simple, portable and scalable. Kubeflow can be used as an orchestrator for TFFX pipelines when they need to be deployed on Kubernetes clusters. In addition, you can also use your own custom orchestrator to run a TFX pipeline.</p> <p>Read more about Airflow here.</p>"},{"location":"tutorials/tfx/airflow_workshop/#chicago-taxi-dataset","title":"Chicago Taxi Dataset","text":"<p>You'll be using the Taxi Trips dataset released by the City of Chicago.</p> <p>Note</p> <p>This tutorial builds an application using data that has been modified for use from its original source, www.cityofchicago.org, the official website of the City of Chicago. The City of Chicago makes no claims as to the content, accuracy, timeliness, or completeness of any of the data provided at in this tutorial. The data provided at this site is subject to change at any time. It is understood that the data provided in this tutorial is being used at one\u2019s own risk.</p>"},{"location":"tutorials/tfx/airflow_workshop/#model-goal-binary-classification","title":"Model Goal - Binary classification","text":"<p>Will the customer tip more or less than 20%?</p>"},{"location":"tutorials/tfx/airflow_workshop/#setup-the-google-cloud-project","title":"Setup the Google Cloud Project","text":"<p>Before you click the Start Lab button Read these instructions. Labs are timed and you cannot pause them. The timer, which starts when you click Start Lab, shows how long Google Cloud resources will be made available to you.</p> <p>This hands-on lab lets you do the lab activities yourself in a real cloud environment, not in a simulation or demo environment. It does so by giving you new, temporary credentials that you use to sign in and access Google Cloud for the duration of the lab.</p> <p>What you need To complete this lab, you need:</p> <ul> <li>Access to a standard internet browser (Chrome browser recommended).</li> <li>Time to complete the lab.</li> </ul> <p>Note</p> <p>If you already have your own personal Google Cloud account or project, do not use it for this lab.</p> <p>Note</p> <p>If you are using a Chrome OS device, open an Incognito window to run this lab.</p> <p>How to start your lab and sign in to the Google Cloud Console 1. Click the Start Lab button. If you need to pay for the lab, a pop-up opens for you to select your payment method. On the left is a panel populated with the temporary credentials that you must use for this lab.</p> <p></p> <ol> <li>Copy the username, and then click Open Google Console. The lab spins up     resources, and then opens another tab that shows the Sign in page.</li> </ol> <p></p> <p>Tip: Open the tabs in separate windows, side-by-side.</p> <p></p> <ol> <li>In the Sign in page, paste the username that you copied from the left     panel. Then copy and paste the password.</li> </ol> <p>Important:- You must use the credentials from the left panel. Do not use your Google Cloud Training credentials. If you have your own Google Cloud account, do not use it for this lab (avoids incurring charges).</p> <ol> <li>Click through the subsequent pages:</li> <li> <p>Accept the terms and conditions.</p> </li> <li> <p>Do not add recovery options or two-factor authentication (because this is a     temporary account).</p> </li> <li> <p>Do not sign up for free trials.</p> </li> </ol> <p>After a few moments, the Cloud Console opens in this tab.</p> <p>Note</p> <p>You can view the menu with a list of Google Cloud Products and Services by clicking the Navigation menu at the top-left.</p> <p></p>"},{"location":"tutorials/tfx/airflow_workshop/#activate-cloud-shell","title":"Activate Cloud Shell","text":"<p>Cloud Shell is a virtual machine that is loaded with development tools. It offers a persistent 5GB home directory and runs on the Google Cloud. Cloud Shell provides command-line access to your Google Cloud resources.</p> <p>In the Cloud Console, in the top right toolbar, click the Activate Cloud Shell button.</p> <p></p> <p>Click Continue.</p> <p></p> <p>It takes a few moments to provision and connect to the environment. When you are connected, you are already authenticated, and the project is set to your PROJECT_ID. For example:</p> <p></p> <p><code>gcloud</code> is the command-line tool for Google Cloud. It comes pre-installed on Cloud Shell and supports tab-completion.</p> <p>You can list the active account name with this command:</p> <pre><code>gcloud auth list\n</code></pre> <p>(Output)</p> <p>ACTIVE: * ACCOUNT: student-01-xxxxxxxxxxxx@qwiklabs.net To set the active account, run: $ gcloud config set account <code>ACCOUNT</code></p> <p>You can list the project ID with this command: <code>gcloud config list project</code> (Output)</p> <p>[core] project = </p> <p>(Example output)</p> <p>[core] project = qwiklabs-gcp-44776a13dea667a6</p> <p>For full documentation of gcloud see the gcloud command-line tool overview.</p>"},{"location":"tutorials/tfx/airflow_workshop/#enable-google-cloud-services","title":"Enable Google Cloud services","text":"<ol> <li>In Cloud Shell, use gcloud to enable the services used in the lab. <code>gcloud     services enable notebooks.googleapis.com</code></li> </ol>"},{"location":"tutorials/tfx/airflow_workshop/#deploy-vertex-notebook-instance","title":"Deploy Vertex Notebook instance","text":"<ol> <li>Click on the Navigation Menu and navigate to Vertex AI, then to     Workbench.</li> </ol> <ol> <li> <p>On the Notebook instances page, click New Notebook.</p> </li> <li> <p>In the Customize instance menu, select TensorFlow Enterprise and choose     the version of TensorFlow Enterprise 2.x (with LTS) &gt; Without GPUs.</p> </li> </ol> <p></p> <ol> <li> <p>In the New notebook instance dialog, click the pencil icon to Edit     instance properties.</p> </li> <li> <p>For Instance name, enter a name for your instance.</p> </li> <li> <p>For Region, select <code>us-east1</code> and for Zone, select a zone within the     selected region.</p> </li> <li> <p>Scroll down to Machine configuration and select e2-standard-2 for     Machine type.</p> </li> <li> <p>Leave the remaining fields with their default and click Create.</p> </li> </ol> <p>After a few minutes, the Vertex AI console will display your instance name, followed by Open Jupyterlab.</p> <ol> <li>Click Open JupyterLab. A JupyterLab window will open in a new tab.</li> </ol>"},{"location":"tutorials/tfx/airflow_workshop/#setup-the-environment","title":"Setup the environment","text":""},{"location":"tutorials/tfx/airflow_workshop/#clone-the-lab-repository","title":"Clone the lab repository","text":"<p>Next you'll clone the <code>tfx</code> repository in your JupyterLab instance. 1. In JupyterLab, click the Terminal icon to open a new terminal.</p> <p>Note</p> <p>If prompted, click <code>Cancel</code> for Build Recommended.</p> <ol> <li>To clone the <code>tfx</code> Github repository, type in the following command, and     press Enter.</li> </ol> <pre><code>git clone https://github.com/tensorflow/tfx.git\n</code></pre> <ol> <li>To confirm that you have cloned the repository, double-click the <code>tfx</code>     directory and confirm that you can see its contents.</li> </ol> <p></p>"},{"location":"tutorials/tfx/airflow_workshop/#install-lab-dependencies","title":"Install lab dependencies","text":"<ol> <li>Run the following to go to the     <code>tfx/tfx/examples/airflow_workshop/taxi/setup/</code> folder, then run     <code>./setup_demo.sh</code> to install lab dependencies:</li> </ol> <pre><code>cd ~/tfx/tfx/examples/airflow_workshop/taxi/setup/\n./setup_demo.sh\n</code></pre> <p>The above code will</p> <ul> <li>Install the required packages.</li> <li>Create an <code>airflow</code> folder in the home folder.</li> <li>Copy the <code>dags</code> folder from <code>tfx/tfx/examples/airflow_workshop/taxi/setup/</code>     folder to <code>~/airflow/</code> folder.</li> <li>Copy the csv file from <code>tfx/tfx/examples/airflow_workshop/taxi/setup/data</code>     to <code>~/airflow/data</code>.</li> </ul> <p></p>"},{"location":"tutorials/tfx/airflow_workshop/#configuring-airflow-server","title":"Configuring Airflow server","text":""},{"location":"tutorials/tfx/airflow_workshop/#create-firewall-rule-to-access-to-airflow-server-in-the-browser","title":"Create firewall rule to access to airflow server in the browser","text":"<ol> <li>Go to <code>https://console.cloud.google.com/networking/firewalls/list</code> and make     sure the project name is selected appropriately</li> <li>Click on <code>CREATE FIREWALL RULE</code> option on top</li> </ol> <p>In the Create a firewall dialog, follow the steps listed below.</p> <ol> <li>For Name, put <code>airflow-tfx</code>.</li> <li>For Priority, select <code>1</code>.</li> <li>For Targets, select <code>All instances in the network</code>.</li> <li>For Source IPv4 ranges, select <code>0.0.0.0/0</code></li> <li>For Protocols and ports, click on <code>tcp</code> and enter <code>7000</code> in the box next     to <code>tcp</code></li> <li>Click <code>Create</code>.</li> </ol> <p></p>"},{"location":"tutorials/tfx/airflow_workshop/#run-airflow-server-from-your-shell","title":"Run airflow server from your shell","text":"<p>In the Jupyter Lab Terminal window, change to home directory, run the <code>airflow users create</code> command to create an admin user for Airflow:</p> <pre><code>cd\nairflow users  create --role Admin --username admin --email admin --firstname admin --lastname admin --password admin\n</code></pre> <p>Then run the <code>airflow webserver</code> and <code>airflow scheduler</code> command to run the server. Choose port <code>7000</code> since it is allowed through firewall.</p> <pre><code>nohup airflow webserver -p 7000 &amp;&gt; webserver.out &amp;\nnohup airflow scheduler &amp;&gt; scheduler.out &amp;\n</code></pre>"},{"location":"tutorials/tfx/airflow_workshop/#get-your-external-ip","title":"Get your external ip","text":"<ol> <li>In Cloud Shell, use <code>gcloud</code> to get the External IP.</li> </ol> <pre><code>gcloud compute instances list\n</code></pre>"},{"location":"tutorials/tfx/airflow_workshop/#running-a-dagpipeline","title":"Running a DAG/Pipeline","text":""},{"location":"tutorials/tfx/airflow_workshop/#in-a-browser","title":"In a browser","text":"<p>Open a browser and go to http://:7000</p> <ul> <li>In the login page, enter the username(<code>admin</code>) and password(<code>admin</code>) you     chose when running the <code>airflow users create</code> command.</li> </ul> <p></p> <p>Airflow loads DAGs from Python source files. It takes each file and executes it. Then it loads any DAG objects from that file. All <code>.py</code> files which define DAG objects will be listed as pipelines in the airflow homepage.</p> <p>In this tutorial, Airflow scans the <code>~/airflow/dags/</code> folder for DAG objects.</p> <p>If you open <code>~/airflow/dags/taxi_pipeline.py</code> and scroll to the bottom, you can see that it creates and stores a DAG object in a variable named <code>DAG</code>. Hence it will be listed as a pipeline in the airflow homepage as shown below:</p> <p></p> <p>If you click on taxi, you will be redirected to the grid view of the DAG. You can click the <code>Graph</code> option on top to get the graph view of the DAG.</p> <p></p>"},{"location":"tutorials/tfx/airflow_workshop/#trigger-the-taxi-pipeline","title":"Trigger the taxi pipeline","text":"<p>On the homepage you can see the buttons that can be used to interact with the DAG.</p> <p></p> <p>Under the actions header, click on the trigger button to trigger the pipeline.</p> <p>In the taxi DAG page, use the button on the right to refresh the state of the graph view of the DAG as the pipeline runs. Additionally, you can enable Auto Refresh to instruct Airflow to automatically refresh the graph view as and when the state changes.</p> <p></p> <p>You can also use the Airflow CLI in the terminal to enable and trigger your DAGs:</p> <pre><code># enable/disable\nairflow pause &lt;your DAG name&gt;\nairflow unpause &lt;your DAG name&gt;\n\n# trigger\nairflow trigger_dag &lt;your DAG name&gt;\n</code></pre>"},{"location":"tutorials/tfx/airflow_workshop/#waiting-for-the-pipeline-to-complete","title":"Waiting for the pipeline to complete","text":"<p>After you've triggered your pipeline, in the DAGs view, you can watch the progress of your pipeline while it is running. As each component runs, the outline color of the component in the DAG graph will change to show its state. When a component has finished processing the outline will turn dark green to show that it's done.</p> <p></p>"},{"location":"tutorials/tfx/airflow_workshop/#understanding-the-components","title":"Understanding the components","text":"<p>Now we will look at the components of this pipeline in detail, and individually look at the outputs produced by each step in the pipeline.</p> <ol> <li> <p>In JupyterLab go to <code>~/tfx/tfx/examples/airflow_workshop/taxi/notebooks/</code></p> </li> <li> <p>Open notebook.ipynb. </p> </li> <li> <p>Continue the lab in the notebook, and run each cell by clicking the Run     (     )     icon at the top of the screen. Alternatively, you can execute the code in a     cell with SHIFT + ENTER.</p> </li> </ol> <p>Read the narrative and make sure you understand what's happening in each cell.</p>"},{"location":"tutorials/tfx/cloud-ai-platform-pipelines/","title":"TFX on Cloud AI Platform Pipelines","text":""},{"location":"tutorials/tfx/cloud-ai-platform-pipelines/#introduction","title":"Introduction","text":"<p>This tutorial is designed to introduce TensorFlow Extended (TFX) and [AIPlatform Pipelines] (https://cloud.google.com/ai-platform/pipelines/docs/introduction), and help you learn to create your own machine learning pipelines on Google Cloud. It shows integration with TFX, AI Platform Pipelines, and Kubeflow, as well as interaction with TFX in Jupyter notebooks.</p> <p>At the end of this tutorial, you will have created and run an ML Pipeline, hosted on Google Cloud. You'll be able to visualize the results of each run, and view the lineage of the created artifacts.</p> <p>Key Term</p> <p>A TFX pipeline is a Directed Acyclic Graph, or \"DAG\". We will often refer to pipelines as DAGs.</p> <p>You'll follow a typical ML development process, starting by examining the dataset, and ending up with a complete working pipeline. Along the way you'll explore ways to debug and update your pipeline, and measure performance.</p> <p>Note</p> <p>Completing this tutorial may take 45-60 minutes.</p>"},{"location":"tutorials/tfx/cloud-ai-platform-pipelines/#chicago-taxi-dataset","title":"Chicago Taxi Dataset","text":"<p>You're using the Taxi Trips dataset released by the City of Chicago.</p> <p>Note</p> <p>This site provides applications using data that has been modified for use from its original source, www.cityofchicago.org, the official website of the City of Chicago. The City of Chicago makes no claims as to the content, accuracy, timeliness, or completeness of any of the data provided at this site. The data provided at this site is subject to change at any time. It is understood that the data provided at this site is being used at one\u2019s own risk.</p> <p>You can read more about the dataset in Google BigQuery. Explore the full dataset in the BigQuery UI.</p>"},{"location":"tutorials/tfx/cloud-ai-platform-pipelines/#model-goal-binary-classification","title":"Model Goal - Binary classification","text":"<p>Will the customer tip more or less than 20%?</p>"},{"location":"tutorials/tfx/cloud-ai-platform-pipelines/#1-set-up-a-google-cloud-project","title":"1. Set up a Google Cloud project","text":""},{"location":"tutorials/tfx/cloud-ai-platform-pipelines/#1a-set-up-your-environment-on-google-cloud","title":"1.a Set up your environment on Google Cloud","text":"<p>To get started, you need a Google Cloud Account. If you already have one, skip ahead to Create New Project.</p> <p>Warning</p> <p>This demo is designed to not exceed Google Cloud's Free Tier limits. If you already have a Google Account, you may have reached your Free Tier limits, or exhausted any free Google Cloud credits given to new users. If that is the case, following this demo will result in charges to your Google Cloud account.</p> <ol> <li> <p>Go to the Google Cloud Console.</p> </li> <li> <p>Agree to Google Cloud terms and conditions</p> <p></p> </li> <li> <p>If you would like to start with a free trial account, click on     Try For Free (or     Get started for free).</p> <ol> <li> <p>Select your country.</p> </li> <li> <p>Agree to the terms of service.</p> </li> <li> <p>Enter billing details.</p> <p>You will not be charged at this point. If you have no other Google Cloud projects, you can complete this tutorial without exceeding the Google Cloud Free Tier limits, which includes a max of 8 cores running at the same time.</p> </li> </ol> </li> </ol> <p>Note</p> <p>You can choose at this point to become a paid user instead of relying on the free trial. Since this tutorial stays within the Free Tier limits, you still won't be charged if this is your only project and you stay within those limits. For more details, see Google Cloud Cost Calculator and Google Cloud Platform Free Tier.</p>"},{"location":"tutorials/tfx/cloud-ai-platform-pipelines/#1b-create-a-new-project","title":"1.b Create a new project.","text":"<p>Note</p> <p>This tutorial assumes you want to work on this demo in a new project. You can, if you want, work in an existing project.</p> <p>Note</p> <p>You must have a verified credit card on file before creating the project.</p> <ol> <li>From the     main Google Cloud dashboard,     click the project dropdown next to the Google Cloud Platform header, and     select New Project.</li> <li>Give your project a name and enter other project details</li> <li>Once you have created a project, make sure to select it from the project drop-down.</li> </ol>"},{"location":"tutorials/tfx/cloud-ai-platform-pipelines/#2-set-up-and-deploy-an-ai-platform-pipeline-on-a-new-kubernetes-cluster","title":"2. Set up and deploy an AI Platform Pipeline on a new Kubernetes cluster","text":"<p>Note</p> <p>This will take up to 10 minutes, as it requires waiting at several points for resources to be provisioned.</p> <ol> <li> <p>Go to the     AI Platform Pipelines Clusters     page.</p> <p>Under the Main Navigation Menu: \u2261 &gt; AI Platform &gt; Pipelines</p> </li> <li> <p>Click + New Instance to create a new cluster.</p> <p></p> </li> <li> <p>On the Kubeflow Pipelines overview page, click Configure.</p> <p></p> </li> <li> <p>Click \"Enable\" to enable the Kubernetes Engine API</p> <p></p> <p>Note</p> <p>You may have to wait several minutes before moving on, while the Kubernetes Engine APIs are being enabled for you.</p> </li> <li> <p>On the Deploy Kubeflow Pipelines page:</p> <ol> <li> <p>Select a zone (or     \"region\") for your cluster. The network and subnetwork can be set, but     for the purposes of this tutorial we will leave them as defaults.</p> </li> <li> <p>IMPORTANT Check the box labeled Allow access to the following cloud     APIs. (This is required for this cluster to access the other pieces of     your project. If you miss this step, fixing it later is a bit tricky.)</p> <p></p> </li> <li> <p>Click Create New Cluster, and wait several minutes until the cluster     has been created.  This will take a few minutes.  When it completes you     will see a message like:</p> <p>Cluster \"cluster-1\" successfully created in zone \"us-central1-a\".</p> </li> <li> <p>Select a namespace and instance name (using the defaults is fine). For     the purposes of this tutorial do not check executor.emissary or     managedstorage.enabled.</p> </li> <li> <p>Click Deploy, and wait several moments until the pipeline has been     deployed. By deploying Kubeflow Pipelines, you accept the Terms of     Service.</p> </li> </ol> </li> </ol>"},{"location":"tutorials/tfx/cloud-ai-platform-pipelines/#3-set-up-cloud-ai-platform-notebook-instance","title":"3. Set up Cloud AI Platform Notebook instance.","text":"<ol> <li> <p>Go to the     Vertex AI Workbench     page.  The first time you run Workbench you will need to enable the     Notebooks API.</p> <p>Under the Main Navigation Menu: \u2261 -&gt; Vertex AI -&gt; Workbench</p> </li> <li> <p>If prompted, enable the Compute Engine API.</p> </li> <li> <p>Create a New Notebook with TensorFlow Enterprise 2.7 (or above)     installed.</p> <p></p> <p>New Notebook -&gt; TensorFlow Enterprise 2.7 -&gt; Without GPU</p> <p>Select a region and zone, and give the notebook instance a name.</p> <p>To stay within the Free Tier limits, you may need to change the default settings here to reduce the number of vCPUs available to this instance from 4 to 2:</p> <ol> <li>Select Advanced Options at the bottom of the New notebook form.</li> <li> <p>Under Machine configuration you may want to select a configuration     with 1 or 2 vCPUs if you need to stay in the free tier.</p> <p></p> </li> <li> <p>Wait for the new notebook to be created, and then click Enable    Notebooks API</p> </li> </ol> </li> </ol> <p>Note</p> <p>You may experience slow performance in your notebook if you use 1 or 2 vCPUs instead of the default or higher. This should not seriously hinder your completion of this tutorial. If would like to use the default settings, upgrade your account to at least 12 vCPUs. This will accrue charges. See Google Kubernetes Engine Pricing for more details on pricing, including a pricing calculator and information about the Google Cloud Free Tier.</p>"},{"location":"tutorials/tfx/cloud-ai-platform-pipelines/#4-launch-the-getting-started-notebook","title":"4. Launch the Getting Started Notebook","text":"<ol> <li> <p>Go to the [AI Platform Pipelines Clusters]     (https://console.cloud.google.com/ai-platform/pipelines) page.</p> <p>Under the Main Navigation Menu: \u2261 -&gt; AI Platform -&gt; Pipelines</p> </li> <li> <p>On the line for the cluster you are using in this tutorial, click Open     Pipelines Dashboard.</p> <p></p> </li> <li> <p>On the Getting Started page, click Open a Cloud AI Platform     Notebook on Google Cloud.</p> <p></p> </li> <li> <p>Select the Notebook instance you are using for this tutorial and     Continue, and then Confirm.</p> <p></p> </li> </ol>"},{"location":"tutorials/tfx/cloud-ai-platform-pipelines/#5-continue-working-in-the-notebook","title":"5. Continue working in the Notebook","text":"<p>Important: The rest of this tutorial should be completed in Jupyter Lab Notebook you opened in the previous step. The instructions and explanations are available here as a reference.</p>"},{"location":"tutorials/tfx/cloud-ai-platform-pipelines/#install","title":"Install","text":"<p>The Getting Started Notebook starts by installing TFX and Kubeflow Pipelines (KFP) into the VM which Jupyter Lab is running in.</p> <p>It then checks which version of TFX is installed, does an import, and sets and prints the Project ID:</p> <p></p>"},{"location":"tutorials/tfx/cloud-ai-platform-pipelines/#connect-with-your-google-cloud-services","title":"Connect with your Google Cloud services","text":"<p>The pipeline configuration needs your project ID, which you can get through the notebook and set as an environmental variable.</p> <pre><code># Read GCP project id from env.\nshell_output=!gcloud config list --format 'value(core.project)' 2&gt;/dev/null\nGCP_PROJECT_ID=shell_output[0]\nprint(\"GCP project ID:\" + GCP_PROJECT_ID)\n</code></pre> <p>Now set your KFP cluster endpoint.</p> <p>This can be found from the URL of the Pipelines dashboard. Go to the Kubeflow Pipeline dashboard and look at the URL. The endpoint is everything in the URL starting with the <code>https://</code>, up to, and including, <code>googleusercontent.com</code>.</p> <pre><code>ENDPOINT='' # Enter YOUR ENDPOINT here.\n</code></pre> <p>The notebook then sets a unique name for the custom Docker image:</p> <pre><code># Docker image name for the pipeline image\nCUSTOM_TFX_IMAGE='gcr.io/' + GCP_PROJECT_ID + '/tfx-pipeline'\n</code></pre>"},{"location":"tutorials/tfx/cloud-ai-platform-pipelines/#6-copy-a-template-into-your-project-directory","title":"6. Copy a template into your project directory","text":"<p>Edit the next notebook cell to set a name for your pipeline. In this tutorial we will use <code>my_pipeline</code>.</p> <pre><code>PIPELINE_NAME=\"my_pipeline\"\nPROJECT_DIR=os.path.join(os.path.expanduser(\"~\"),\"imported\",PIPELINE_NAME)\n</code></pre> <p>The notebook then uses the <code>tfx</code> CLI to copy the pipeline template. This tutorial uses the Chicago Taxi dataset to perform binary classification, so the template sets the model to <code>taxi</code>:</p> <pre><code>!tfx template copy \\\n  --pipeline-name={PIPELINE_NAME} \\\n  --destination-path={PROJECT_DIR} \\\n  --model=taxi\n</code></pre> <p>The notebook then changes its CWD context to the project directory:</p> <pre><code>%cd {PROJECT_DIR}\n</code></pre>"},{"location":"tutorials/tfx/cloud-ai-platform-pipelines/#browse-the-pipeline-files","title":"Browse the pipeline files","text":"<p>On the left-hand side of the Cloud AI Platform Notebook, you should see a file browser. There should be a directory with your pipeline name (<code>my_pipeline</code>). Open it and view the files. (You'll be able to open them and edit from the notebook environment as well.)</p> <pre><code># You can also list the files from the shell\n! ls\n</code></pre> <p>The <code>tfx template copy</code> command above created a basic scaffold of files that build a pipeline. These include Python source codes, sample data, and Jupyter notebooks. These are meant for this particular example. For your own pipelines these would be the supporting files that your pipeline requires.</p> <p>Here is brief description of the Python files.</p> <ul> <li><code>pipeline</code> - This directory contains the definition of the pipeline<ul> <li><code>configs.py</code> \u2014 defines common constants for pipeline runners</li> <li><code>pipeline.py</code> \u2014 defines TFX components and a pipeline</li> </ul> </li> <li><code>models</code> - This directory contains ML model definitions.<ul> <li><code>features.py</code> <code>features_test.py</code> \u2014 defines features for the model</li> <li><code>preprocessing.py</code> / <code>preprocessing_test.py</code> \u2014 defines preprocessing     jobs using <code>tf::Transform</code></li> <li><code>keras</code> - This directory contains a Keras based model.<ul> <li><code>constants.py</code> \u2014 defines constants of the model</li> <li><code>model.py</code> / <code>model_test.py</code> \u2014 defines DNN model using Keras</li> </ul> </li> </ul> </li> <li><code>beam_runner.py</code> / <code>kubeflow_runner.py</code> \u2014 define runners for each     orchestration engine</li> </ul>"},{"location":"tutorials/tfx/cloud-ai-platform-pipelines/#7-run-your-first-tfx-pipeline-on-kubeflow","title":"7. Run your first TFX pipeline on Kubeflow","text":"<p>The notebook will run the pipeline using the <code>tfx run</code> CLI command.</p>"},{"location":"tutorials/tfx/cloud-ai-platform-pipelines/#connect-to-storage","title":"Connect to storage","text":"<p>Running pipelines create artifacts which have to be stored in ML-Metadata. Artifacts refer to payloads, which are files that must be stored in a file system or block storage. For this tutorial, we'll use GCS to store our metadata payloads, using the bucket that was created automatically during setup. Its name will be <code>&lt;your-project-id&gt;-kubeflowpipelines-default</code>.</p>"},{"location":"tutorials/tfx/cloud-ai-platform-pipelines/#create-the-pipeline","title":"Create the pipeline","text":"<p>The notebook will upload our sample data to GCS bucket so that we can use it in our pipeline later.</p> <pre><code>!gsutil cp data/data.csv gs://{GOOGLE_CLOUD_PROJECT}-kubeflowpipelines-default/tfx-template/data/taxi/data.csv\n</code></pre> <p>The notebook then uses the <code>tfx pipeline create</code> command to create the pipeline.</p> <pre><code>!tfx pipeline create  \\\n--pipeline-path=kubeflow_runner.py \\\n--endpoint={ENDPOINT} \\\n--build-image\n</code></pre> <p>While creating a pipeline, <code>Dockerfile</code> will be generated to build a Docker image. Don't forget to add these files to your source control system (for example, git) along with other source files.</p>"},{"location":"tutorials/tfx/cloud-ai-platform-pipelines/#run-the-pipeline","title":"Run the pipeline","text":"<p>The notebook then uses the <code>tfx run create</code> command to start an execution run of your pipeline. You will also see this run listed under Experiments in the Kubeflow Pipelines Dashboard.</p> <pre><code>!tfx run create --pipeline-name={PIPELINE_NAME} --endpoint={ENDPOINT}\n</code></pre> <p>You can view your pipeline from the Kubeflow Pipelines Dashboard.</p> <p>Note</p> <p>If your pipeline run fails, you can see detailed logs in the KFP Dashboard. One of the major sources of failure is permission related problems. Make sure your KFP cluster has permissions to access Google Cloud APIs. This can be configured when you create a KFP cluster in GCP, or see Troubleshooting document in GCP.</p>"},{"location":"tutorials/tfx/cloud-ai-platform-pipelines/#8-validate-your-data","title":"8. Validate your data","text":"<p>The first task in any data science or ML project is to understand and clean the data.</p> <ul> <li>Understand the data types for each feature</li> <li>Look for anomalies and missing values</li> <li>Understand the distributions for each feature</li> </ul>"},{"location":"tutorials/tfx/cloud-ai-platform-pipelines/#components","title":"Components","text":"<ul> <li>ExampleGen ingests and     splits the input dataset.</li> <li>StatisticsGen calculates     statistics for the dataset.</li> <li>SchemaGen SchemaGen     examines the statistics and creates a data schema.</li> <li>ExampleValidator looks     for anomalies and missing values in the dataset.</li> </ul>"},{"location":"tutorials/tfx/cloud-ai-platform-pipelines/#in-jupyter-lab-file-editor","title":"In Jupyter lab file editor:","text":"<p>In <code>pipeline</code>/<code>pipeline.py</code>, uncomment the lines which append these components to your pipeline:</p> <pre><code># components.append(statistics_gen)\n# components.append(schema_gen)\n# components.append(example_validator)\n</code></pre> <p>(<code>ExampleGen</code> was already enabled when the template files were copied.)</p>"},{"location":"tutorials/tfx/cloud-ai-platform-pipelines/#update-the-pipeline-and-re-run-it","title":"Update the pipeline and re-run it","text":"<pre><code># Update the pipeline\n! tfx pipeline update \\\n  --pipeline-path=kubeflow_runner.py \\\n  --endpoint={ENDPOINT}\n\n! tfx run create --pipeline-name \"{PIPELINE_NAME}\"\n</code></pre>"},{"location":"tutorials/tfx/cloud-ai-platform-pipelines/#check-the-pipeline","title":"Check the pipeline","text":"<p>For Kubeflow Orchestrator, visit KFP dashboard and find pipeline outputs in the page for your pipeline run. Click \"Experiments\" tab on the left, and \"All runs\" in the Experiments page. You should be able to find the run with the name of your pipeline.</p>"},{"location":"tutorials/tfx/cloud-ai-platform-pipelines/#more-advanced-example","title":"More advanced example","text":"<p>The example presented here is really only meant to get you started. For a more advanced example see the TensorFlow Data Validation Colab.</p> <p>For more information on using TFDV to explore and validate a dataset, see the examples on tensorflow.org.</p>"},{"location":"tutorials/tfx/cloud-ai-platform-pipelines/#9-feature-engineering","title":"9. Feature engineering","text":"<p>You can increase the predictive quality of your data and/or reduce dimensionality with feature engineering.</p> <ul> <li>Feature crosses</li> <li>Vocabularies</li> <li>Embeddings</li> <li>PCA</li> <li>Categorical encoding</li> </ul> <p>One of the benefits of using TFX is that you will write your transformation code once, and the resulting transforms will be consistent between training and serving.</p>"},{"location":"tutorials/tfx/cloud-ai-platform-pipelines/#components_1","title":"Components","text":"<ul> <li>Transform performs feature     engineering on the dataset.</li> </ul>"},{"location":"tutorials/tfx/cloud-ai-platform-pipelines/#in-jupyter-lab-file-editor_1","title":"In Jupyter lab file editor:","text":"<p>In <code>pipeline</code>/<code>pipeline.py</code>, find and uncomment the line which appends Transform to the pipeline.</p> <pre><code># components.append(transform)\n</code></pre>"},{"location":"tutorials/tfx/cloud-ai-platform-pipelines/#update-the-pipeline-and-re-run-it_1","title":"Update the pipeline and re-run it","text":"<pre><code># Update the pipeline\n! tfx pipeline update \\\n  --pipeline-path=kubeflow_runner.py \\\n  --endpoint={ENDPOINT}\n\n! tfx run create --pipeline-name \"{PIPELINE_NAME}\"\n</code></pre>"},{"location":"tutorials/tfx/cloud-ai-platform-pipelines/#check-pipeline-outputs","title":"Check pipeline outputs","text":"<p>For Kubeflow Orchestrator, visit KFP dashboard and find pipeline outputs in the page for your pipeline run. Click \"Experiments\" tab on the left, and \"All runs\" in the Experiments page. You should be able to find the run with the name of your pipeline.</p>"},{"location":"tutorials/tfx/cloud-ai-platform-pipelines/#more-advanced-example_1","title":"More advanced example","text":"<p>The example presented here is really only meant to get you started. For a more advanced example see the TensorFlow Transform Colab.</p>"},{"location":"tutorials/tfx/cloud-ai-platform-pipelines/#10-training","title":"10. Training","text":"<p>Train a TensorFlow model with your nice, clean, transformed data.</p> <ul> <li>Include the transformations from the previous step so that they are applied     consistently</li> <li>Save the results as a SavedModel for production</li> <li>Visualize and explore the training process using TensorBoard</li> <li>Also save an EvalSavedModel for analysis of model performance</li> </ul>"},{"location":"tutorials/tfx/cloud-ai-platform-pipelines/#components_2","title":"Components","text":"<ul> <li>Trainer trains a TensorFlow     model.</li> </ul>"},{"location":"tutorials/tfx/cloud-ai-platform-pipelines/#in-jupyter-lab-file-editor_2","title":"In Jupyter lab file editor:","text":"<p>In <code>pipeline</code>/<code>pipeline.py</code>, find and uncomment the which appends Trainer to the pipeline:</p> <pre><code># components.append(trainer)\n</code></pre>"},{"location":"tutorials/tfx/cloud-ai-platform-pipelines/#update-the-pipeline-and-re-run-it_2","title":"Update the pipeline and re-run it","text":"<pre><code># Update the pipeline\n! tfx pipeline update \\\n  --pipeline-path=kubeflow_runner.py \\\n  --endpoint={ENDPOINT}\n\n! tfx run create --pipeline-name \"{PIPELINE_NAME}\"\n</code></pre>"},{"location":"tutorials/tfx/cloud-ai-platform-pipelines/#check-pipeline-outputs_1","title":"Check pipeline outputs","text":"<p>For Kubeflow Orchestrator, visit KFP dashboard and find pipeline outputs in the page for your pipeline run. Click \"Experiments\" tab on the left, and \"All runs\" in the Experiments page. You should be able to find the run with the name of your pipeline.</p>"},{"location":"tutorials/tfx/cloud-ai-platform-pipelines/#more-advanced-example_2","title":"More advanced example","text":"<p>The example presented here is really only meant to get you started. For a more advanced example see the TensorBoard Tutorial.</p>"},{"location":"tutorials/tfx/cloud-ai-platform-pipelines/#11-analyzing-model-performance","title":"11. Analyzing model performance","text":"<p>Understanding more than just the top level metrics.</p> <ul> <li>Users experience model performance for their queries only</li> <li>Poor performance on slices of data can be hidden by top level metrics</li> <li>Model fairness is important</li> <li>Often key subsets of users or data are very important, and may be small<ul> <li>Performance in critical but unusual conditions</li> <li>Performance for key audiences such as influencers</li> </ul> </li> <li>If you\u2019re replacing a model that is currently in production, first make sure     that the new one is better</li> </ul>"},{"location":"tutorials/tfx/cloud-ai-platform-pipelines/#components_3","title":"Components","text":"<ul> <li>Evaluator performs deep     analysis of the training results.</li> </ul>"},{"location":"tutorials/tfx/cloud-ai-platform-pipelines/#in-jupyter-lab-file-editor_3","title":"In Jupyter lab file editor:","text":"<p>In <code>pipeline</code>/<code>pipeline.py</code>, find and uncomment the line which appends Evaluator to the pipeline:</p> <pre><code>components.append(evaluator)\n</code></pre>"},{"location":"tutorials/tfx/cloud-ai-platform-pipelines/#update-the-pipeline-and-re-run-it_3","title":"Update the pipeline and re-run it","text":"<pre><code># Update the pipeline\n! tfx pipeline update \\\n  --pipeline-path=kubeflow_runner.py \\\n  --endpoint={ENDPOINT}\n\n! tfx run create --pipeline-name \"{PIPELINE_NAME}\"\n</code></pre>"},{"location":"tutorials/tfx/cloud-ai-platform-pipelines/#check-pipeline-outputs_2","title":"Check pipeline outputs","text":"<p>For Kubeflow Orchestrator, visit KFP dashboard and find pipeline outputs in the page for your pipeline run. Click \"Experiments\" tab on the left, and \"All runs\" in the Experiments page. You should be able to find the run with the name of your pipeline.</p>"},{"location":"tutorials/tfx/cloud-ai-platform-pipelines/#12-serving-the-model","title":"12. Serving the model","text":"<p>If the new model is ready, make it so.</p> <ul> <li>Pusher deploys SavedModels to well-known locations</li> </ul> <p>Deployment targets receive new models from well-known locations</p> <ul> <li>TensorFlow Serving</li> <li>TensorFlow Lite</li> <li>TensorFlow JS</li> <li>TensorFlow Hub</li> </ul>"},{"location":"tutorials/tfx/cloud-ai-platform-pipelines/#components_4","title":"Components","text":"<ul> <li>Pusher deploys the model to a     serving infrastructure.</li> </ul>"},{"location":"tutorials/tfx/cloud-ai-platform-pipelines/#in-jupyter-lab-file-editor_4","title":"In Jupyter lab file editor:","text":"<p>In <code>pipeline</code>/<code>pipeline.py</code>, find and uncomment the line that appends Pusher to the pipeline:</p> <pre><code># components.append(pusher)\n</code></pre>"},{"location":"tutorials/tfx/cloud-ai-platform-pipelines/#check-pipeline-outputs_3","title":"Check pipeline outputs","text":"<p>For Kubeflow Orchestrator, visit KFP dashboard and find pipeline outputs in the page for your pipeline run. Click \"Experiments\" tab on the left, and \"All runs\" in the Experiments page. You should be able to find the run with the name of your pipeline.</p>"},{"location":"tutorials/tfx/cloud-ai-platform-pipelines/#available-deployment-targets","title":"Available deployment targets","text":"<p>You have now trained and validated your model, and your model is now ready for production. You can now deploy your model to any of the TensorFlow deployment targets, including:</p> <ul> <li>TensorFlow Serving, for     serving your model on a server or server farm and processing REST and/or     gRPC inference requests.</li> <li>TensorFlow Lite, for including your model     in an Android or iOS native mobile application, or in a Raspberry Pi, IoT,     or microcontroller application.</li> <li>TensorFlow.js, for running your model in a     web browser or Node.JS application.</li> </ul>"},{"location":"tutorials/tfx/cloud-ai-platform-pipelines/#more-advanced-examples","title":"More advanced examples","text":"<p>The example presented above is really only meant to get you started. Below are some examples of integration with other Cloud services.</p>"},{"location":"tutorials/tfx/cloud-ai-platform-pipelines/#kubeflow-pipelines-resource-considerations","title":"Kubeflow Pipelines resource considerations","text":"<p>Depending on the requirements of your workload, the default configuration for your Kubeflow Pipelines deployment may or may not meet your needs.  You can customize your resource configurations using <code>pipeline_operator_funcs</code> in your call to <code>KubeflowDagRunnerConfig</code>.</p> <p><code>pipeline_operator_funcs</code> is a list of <code>OpFunc</code> items, which transforms all the generated <code>ContainerOp</code> instances in the KFP pipeline spec which is compiled from <code>KubeflowDagRunner</code>.</p> <p>For example, to configure memory we can use <code>set_memory_request</code> to declare the amount of memory needed. A typical way to do that is to create a wrapper for <code>set_memory_request</code> and use it to add to to the list of pipeline <code>OpFunc</code>s:</p> <pre><code>def request_more_memory():\n  def _set_memory_spec(container_op):\n    container_op.set_memory_request('32G')\n  return _set_memory_spec\n\n# Then use this opfunc in KubeflowDagRunner\npipeline_op_funcs = kubeflow_dag_runner.get_default_pipeline_operator_funcs()\npipeline_op_funcs.append(request_more_memory())\nconfig = KubeflowDagRunnerConfig(\n    pipeline_operator_funcs=pipeline_op_funcs,\n    ...\n)\nkubeflow_dag_runner.KubeflowDagRunner(config=config).run(pipeline)\n</code></pre> <p>Similar resource configuration functions include:</p> <ul> <li><code>set_memory_limit</code></li> <li><code>set_cpu_request</code></li> <li><code>set_cpu_limit</code></li> <li><code>set_gpu_limit</code></li> </ul>"},{"location":"tutorials/tfx/cloud-ai-platform-pipelines/#try-bigqueryexamplegen","title":"Try <code>BigQueryExampleGen</code>","text":"<p>BigQuery is a serverless, highly scalable, and cost-effective cloud data warehouse. BigQuery can be used as a source for training examples in TFX. In this step, we will add <code>BigQueryExampleGen</code> to the pipeline.</p>"},{"location":"tutorials/tfx/cloud-ai-platform-pipelines/#in-jupyter-lab-file-editor_5","title":"In Jupyter lab file editor:","text":"<p>Double-click to open <code>pipeline.py</code>. Comment out <code>CsvExampleGen</code> and uncomment the line which creates an instance of <code>BigQueryExampleGen</code>. You also need to uncomment the <code>query</code> argument of the <code>create_pipeline</code> function.</p> <p>We need to specify which GCP project to use for BigQuery, and this is done by setting <code>--project</code> in <code>beam_pipeline_args</code> when creating a pipeline.</p> <p>Double-click to open <code>configs.py</code>. Uncomment the definition of <code>BIG_QUERY_WITH_DIRECT_RUNNER_BEAM_PIPELINE_ARGS</code> and <code>BIG_QUERY_QUERY</code>. You should replace the project id and the region value in this file with the correct values for your GCP project.</p> <p>Note</p> <p>You MUST set your GCP project ID and region in the <code>configs.py</code> file before proceeding.</p> <p>Change directory one level up. Click the name of the directory above the file list. The name of the directory is the name of the pipeline which is <code>my_pipeline</code> if you didn't change the pipeline name.</p> <p>Double-click to open <code>kubeflow_runner.py</code>. Uncomment two arguments, <code>query</code> and <code>beam_pipeline_args</code>, for the <code>create_pipeline</code> function.</p> <p>Now the pipeline is ready to use BigQuery as an example source. Update the pipeline as before and create a new execution run as we did in step 5 and 6.</p>"},{"location":"tutorials/tfx/cloud-ai-platform-pipelines/#update-the-pipeline-and-re-run-it_4","title":"Update the pipeline and re-run it","text":"<pre><code># Update the pipeline\n!tfx pipeline update \\\n  --pipeline-path=kubeflow_runner.py \\\n  --endpoint={ENDPOINT}\n\n!tfx run create --pipeline-name {PIPELINE_NAME} --endpoint={ENDPOINT}\n</code></pre>"},{"location":"tutorials/tfx/cloud-ai-platform-pipelines/#try-dataflow","title":"Try Dataflow","text":"<p>Several TFX Components use Apache Beam to implement data-parallel pipelines, and it means that you can distribute data processing workloads using Google Cloud Dataflow. In this step, we will set the Kubeflow orchestrator to use Dataflow as the data processing back-end for Apache Beam.</p> <p>Note</p> <p>If the Dataflow API is not already enabled, you can enable it using the console, or from the CLI using this command (for example, in the Cloud Shell):</p> <pre><code># Select your project:\ngcloud config set project YOUR_PROJECT_ID\n\n# Get a list of services that you can enable in your project:\ngcloud services list --available | grep Dataflow\n\n# If you don't see dataflow.googleapis.com listed, that means you haven't been\n# granted access to enable the Dataflow API.  See your account adminstrator.\n\n# Enable the Dataflow service:\n\ngcloud services enable dataflow.googleapis.com\n</code></pre> <p>Note</p> <p>Execution speed may be limited by default Google Compute Engine (GCE) quota. We recommend setting a sufficient quota for approximately 250 Dataflow VMs: 250 CPUs, 250 IP Addresses, and 62500 GB of Persistent Disk. For more details, please see the GCE Quota and Dataflow Quota documentation. If you are blocked by IP Address quota, using a bigger <code>worker_type</code> will reduce the number of needed IPs.</p> <p>Double-click <code>pipeline</code> to change directory, and double-click to open <code>configs.py</code>. Uncomment the definition of <code>GOOGLE_CLOUD_REGION</code>, and <code>DATAFLOW_BEAM_PIPELINE_ARGS</code>.</p> <p>Change directory one level up. Click the name of the directory above the file list. The name of the directory is the name of the pipeline which is <code>my_pipeline</code> if you didn't change.</p> <p>Double-click to open <code>kubeflow_runner.py</code>. Uncomment <code>beam_pipeline_args</code>. (Also make sure to comment out current <code>beam_pipeline_args</code> that you added in Step 7.)</p>"},{"location":"tutorials/tfx/cloud-ai-platform-pipelines/#update-the-pipeline-and-re-run-it_5","title":"Update the pipeline and re-run it","text":"<pre><code># Update the pipeline\n!tfx pipeline update \\\n  --pipeline-path=kubeflow_runner.py \\\n  --endpoint={ENDPOINT}\n\n!tfx run create --pipeline-name {PIPELINE_NAME} --endpoint={ENDPOINT}\n</code></pre> <p>You can find your Dataflow jobs in Dataflow in Cloud Console.</p>"},{"location":"tutorials/tfx/cloud-ai-platform-pipelines/#try-cloud-ai-platform-training-and-prediction-with-kfp","title":"Try Cloud AI Platform Training and Prediction with KFP","text":"<p>TFX interoperates with several managed GCP services, such as Cloud AI Platform for Training and Prediction. You can set your <code>Trainer</code> component to use Cloud AI Platform Training, a managed service for training ML models. Moreover, when your model is built and ready to be served, you can push your model to Cloud AI Platform Prediction for serving. In this step, we will set our <code>Trainer</code> and <code>Pusher</code> component to use Cloud AI Platform services.</p> <p>Before editing files, you might first have to enable AI Platform Training &amp; Prediction API.</p> <p>Double-click <code>pipeline</code> to change directory, and double-click to open <code>configs.py</code>. Uncomment the definition of <code>GOOGLE_CLOUD_REGION</code>, <code>GCP_AI_PLATFORM_TRAINING_ARGS</code> and <code>GCP_AI_PLATFORM_SERVING_ARGS</code>. We will use our custom built container image to train a model in Cloud AI Platform Training, so we should set <code>masterConfig.imageUri</code> in <code>GCP_AI_PLATFORM_TRAINING_ARGS</code> to the same value as <code>CUSTOM_TFX_IMAGE</code> above.</p> <p>Change directory one level up, and double-click to open <code>kubeflow_runner.py</code>. Uncomment <code>ai_platform_training_args</code> and <code>ai_platform_serving_args</code>.</p> <p>Note</p> <p>If you receive a permissions error in the Training step, you may need to provide Storage Object Viewer permissions to the Cloud Machine Learning Engine (AI Platform Prediction &amp; Training) service account. More information is available in the Container Registry documentation.</p>"},{"location":"tutorials/tfx/cloud-ai-platform-pipelines/#update-the-pipeline-and-re-run-it_6","title":"Update the pipeline and re-run it","text":"<pre><code># Update the pipeline\n!tfx pipeline update \\\n  --pipeline-path=kubeflow_runner.py \\\n  --endpoint={ENDPOINT}\n\n!tfx run create --pipeline-name {PIPELINE_NAME} --endpoint={ENDPOINT}\n</code></pre> <p>You can find your training jobs in Cloud AI Platform Jobs. If your pipeline completed successfully, you can find your model in Cloud AI Platform Models.</p>"},{"location":"tutorials/tfx/cloud-ai-platform-pipelines/#14-use-your-own-data","title":"14. Use your own data","text":"<p>In this tutorial, you made a pipeline for a model using the Chicago Taxi dataset. Now try putting your own data into the pipeline. Your data can be stored anywhere the pipeline can access it, including Google Cloud Storage, BigQuery, or CSV files.</p> <p>You need to modify the pipeline definition to accommodate your data.</p>"},{"location":"tutorials/tfx/cloud-ai-platform-pipelines/#if-your-data-is-stored-in-files","title":"If your data is stored in files","text":"<ol> <li>Modify <code>DATA_PATH</code> in <code>kubeflow_runner.py</code>, indicating the location.</li> </ol>"},{"location":"tutorials/tfx/cloud-ai-platform-pipelines/#if-your-data-is-stored-in-bigquery","title":"If your data is stored in BigQuery","text":"<ol> <li>Modify <code>BIG_QUERY_QUERY</code> in configs.py to your query statement.</li> <li>Add features in <code>models</code>/<code>features.py</code>.</li> <li>Modify <code>models</code>/<code>preprocessing.py</code> to     transform input data for training.</li> <li>Modify <code>models</code>/<code>keras</code>/<code>model.py</code> and <code>models</code>/<code>keras</code>/<code>constants.py</code> to     describe your ML model.</li> </ol>"},{"location":"tutorials/tfx/cloud-ai-platform-pipelines/#learn-more-about-trainer","title":"Learn more about Trainer","text":"<p>See Trainer component guide for more details on Training pipelines.</p>"},{"location":"tutorials/tfx/cloud-ai-platform-pipelines/#cleaning-up","title":"Cleaning up","text":"<p>To clean up all Google Cloud resources used in this project, you can delete the Google Cloud project you used for the tutorial.</p> <p>Alternatively, you can clean up individual resources by visiting each consoles: - Google Cloud Storage - Google Container Registry - Google Kubernetes Engine</p>"},{"location":"tutorials/tfx/components/","title":"TFX Estimator Component Tutorial","text":"In\u00a0[\u00a0]: Copied! <pre>#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n</pre> #@title Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # https://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. <p>Note: We recommend running this tutorial in a Colab notebook, with no setup required!  Just click \"Run in Google Colab\".</p>        View on TensorFlow.org             Run in Google Colab             View source on GitHub             Download notebook      <p>Warning: Estimators are not recommended for new code.  Estimators run <code>v1.Session</code>-style code which is more difficult to write correctly, and can behave unexpectedly, especially when combined with TF 2 code. Estimators do fall under our compatibility guarantees, but will receive no fixes other than security vulnerabilities. See the migration guide for details.</p> <p>This Colab-based tutorial will interactively walk through each built-in component of TensorFlow Extended (TFX).</p> <p>It covers every step in an end-to-end machine learning pipeline, from data ingestion to pushing a model to serving.</p> <p>When you're done, the contents of this notebook can be automatically exported as TFX pipeline source code, which you can orchestrate with Apache Airflow and Apache Beam.</p> <p>Note: This notebook and its associated APIs are experimental and are in active development.  Major changes in functionality, behavior, and presentation are expected.</p> In\u00a0[\u00a0]: Copied! <pre>try:\n  import colab\n  !pip install --upgrade pip\nexcept:\n  pass\n</pre> try:   import colab   !pip install --upgrade pip except:   pass In\u00a0[\u00a0]: Copied! <pre># TFX has a constraint of 1.16 due to the removal of tf.estimator support.\n!pip install \"tfx&lt;1.16\"\n</pre> # TFX has a constraint of 1.16 due to the removal of tf.estimator support. !pip install \"tfx&lt;1.16\" In\u00a0[\u00a0]: Copied! <pre>import os\nimport pprint\nimport tempfile\nimport urllib\n\nimport absl\nimport tensorflow as tf\nimport tensorflow_model_analysis as tfma\ntf.get_logger().propagate = False\npp = pprint.PrettyPrinter()\n\nfrom tfx import v1 as tfx\nfrom tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n\n%load_ext tfx.orchestration.experimental.interactive.notebook_extensions.skip\n</pre> import os import pprint import tempfile import urllib  import absl import tensorflow as tf import tensorflow_model_analysis as tfma tf.get_logger().propagate = False pp = pprint.PrettyPrinter()  from tfx import v1 as tfx from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext  %load_ext tfx.orchestration.experimental.interactive.notebook_extensions.skip <p>Let's check the library versions.</p> In\u00a0[\u00a0]: Copied! <pre>print('TensorFlow version: {}'.format(tf.__version__))\nprint('TFX version: {}'.format(tfx.__version__))\n</pre> print('TensorFlow version: {}'.format(tf.__version__)) print('TFX version: {}'.format(tfx.__version__)) In\u00a0[\u00a0]: Copied! <pre># This is the root directory for your TFX pip package installation.\n_tfx_root = tfx.__path__[0]\n\n# This is the directory containing the TFX Chicago Taxi Pipeline example.\n_taxi_root = os.path.join(_tfx_root, 'examples/chicago_taxi_pipeline')\n\n# This is the path where your model will be pushed for serving.\n_serving_model_dir = os.path.join(\n    tempfile.mkdtemp(), 'serving_model/taxi_simple')\n\n# Set up logging.\nabsl.logging.set_verbosity(absl.logging.INFO)\n</pre> # This is the root directory for your TFX pip package installation. _tfx_root = tfx.__path__[0]  # This is the directory containing the TFX Chicago Taxi Pipeline example. _taxi_root = os.path.join(_tfx_root, 'examples/chicago_taxi_pipeline')  # This is the path where your model will be pushed for serving. _serving_model_dir = os.path.join(     tempfile.mkdtemp(), 'serving_model/taxi_simple')  # Set up logging. absl.logging.set_verbosity(absl.logging.INFO) In\u00a0[\u00a0]: Copied! <pre>_data_root = tempfile.mkdtemp(prefix='tfx-data')\nDATA_PATH = 'https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/chicago_taxi_pipeline/data/simple/data.csv'\n_data_filepath = os.path.join(_data_root, \"data.csv\")\nurllib.request.urlretrieve(DATA_PATH, _data_filepath)\n</pre> _data_root = tempfile.mkdtemp(prefix='tfx-data') DATA_PATH = 'https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/chicago_taxi_pipeline/data/simple/data.csv' _data_filepath = os.path.join(_data_root, \"data.csv\") urllib.request.urlretrieve(DATA_PATH, _data_filepath) <p>Take a quick look at the CSV file.</p> In\u00a0[\u00a0]: Copied! <pre>!head {_data_filepath}\n</pre> !head {_data_filepath} <p>Disclaimer: This site provides applications using data that has been modified for use from its original source, www.cityofchicago.org, the official website of the City of Chicago. The City of Chicago makes no claims as to the content, accuracy, timeliness, or completeness of any of the data provided at this site. The data provided at this site is subject to change at any time. It is understood that the data provided at this site is being used at one\u2019s own risk.</p> In\u00a0[\u00a0]: Copied! <pre># Here, we create an InteractiveContext using default parameters. This will\n# use a temporary directory with an ephemeral ML Metadata database instance.\n# To use your own pipeline root or database, the optional properties\n# `pipeline_root` and `metadata_connection_config` may be passed to\n# InteractiveContext. Calls to InteractiveContext are no-ops outside of the\n# notebook.\ncontext = InteractiveContext()\n</pre> # Here, we create an InteractiveContext using default parameters. This will # use a temporary directory with an ephemeral ML Metadata database instance. # To use your own pipeline root or database, the optional properties # `pipeline_root` and `metadata_connection_config` may be passed to # InteractiveContext. Calls to InteractiveContext are no-ops outside of the # notebook. context = InteractiveContext() In\u00a0[\u00a0]: Copied! <pre>example_gen = tfx.components.CsvExampleGen(input_base=_data_root)\ncontext.run(example_gen)\n</pre> example_gen = tfx.components.CsvExampleGen(input_base=_data_root) context.run(example_gen) <p>Let's examine the output artifacts of <code>ExampleGen</code>. This component produces two artifacts, training examples and evaluation examples:</p> In\u00a0[\u00a0]: Copied! <pre>artifact = example_gen.outputs['examples'].get()[0]\nprint(artifact.split_names, artifact.uri)\n</pre> artifact = example_gen.outputs['examples'].get()[0] print(artifact.split_names, artifact.uri) <p>We can also take a look at the first three training examples:</p> In\u00a0[\u00a0]: Copied! <pre># Get the URI of the output artifact representing the training examples, which is a directory\ntrain_uri = os.path.join(example_gen.outputs['examples'].get()[0].uri, 'Split-train')\n\n# Get the list of files in this directory (all compressed TFRecord files)\ntfrecord_filenames = [os.path.join(train_uri, name)\n                      for name in os.listdir(train_uri)]\n\n# Create a `TFRecordDataset` to read these files\ndataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")\n\n# Iterate over the first 3 records and decode them.\nfor tfrecord in dataset.take(3):\n  serialized_example = tfrecord.numpy()\n  example = tf.train.Example()\n  example.ParseFromString(serialized_example)\n  pp.pprint(example)\n</pre> # Get the URI of the output artifact representing the training examples, which is a directory train_uri = os.path.join(example_gen.outputs['examples'].get()[0].uri, 'Split-train')  # Get the list of files in this directory (all compressed TFRecord files) tfrecord_filenames = [os.path.join(train_uri, name)                       for name in os.listdir(train_uri)]  # Create a `TFRecordDataset` to read these files dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")  # Iterate over the first 3 records and decode them. for tfrecord in dataset.take(3):   serialized_example = tfrecord.numpy()   example = tf.train.Example()   example.ParseFromString(serialized_example)   pp.pprint(example) <p>Now that <code>ExampleGen</code> has finished ingesting the data, the next step is data analysis.</p> In\u00a0[\u00a0]: Copied! <pre>statistics_gen = tfx.components.StatisticsGen(examples=example_gen.outputs['examples'])\ncontext.run(statistics_gen)\n</pre> statistics_gen = tfx.components.StatisticsGen(examples=example_gen.outputs['examples']) context.run(statistics_gen) <p>After <code>StatisticsGen</code> finishes running, we can visualize the outputted statistics. Try playing with the different plots!</p> In\u00a0[\u00a0]: Copied! <pre>context.show(statistics_gen.outputs['statistics'])\n</pre> context.show(statistics_gen.outputs['statistics']) In\u00a0[\u00a0]: Copied! <pre>schema_gen = tfx.components.SchemaGen(\n    statistics=statistics_gen.outputs['statistics'],\n    infer_feature_shape=False)\ncontext.run(schema_gen)\n</pre> schema_gen = tfx.components.SchemaGen(     statistics=statistics_gen.outputs['statistics'],     infer_feature_shape=False) context.run(schema_gen) <p>After <code>SchemaGen</code> finishes running, we can visualize the generated schema as a table.</p> In\u00a0[\u00a0]: Copied! <pre>context.show(schema_gen.outputs['schema'])\n</pre> context.show(schema_gen.outputs['schema']) <p>Each feature in your dataset shows up as a row in the schema table, alongside its properties. The schema also captures all the values that a categorical feature takes on, denoted as its domain.</p> <p>To learn more about schemas, see the SchemaGen documentation.</p> In\u00a0[\u00a0]: Copied! <pre>example_validator = tfx.components.ExampleValidator(\n    statistics=statistics_gen.outputs['statistics'],\n    schema=schema_gen.outputs['schema'])\ncontext.run(example_validator)\n</pre> example_validator = tfx.components.ExampleValidator(     statistics=statistics_gen.outputs['statistics'],     schema=schema_gen.outputs['schema']) context.run(example_validator) <p>After <code>ExampleValidator</code> finishes running, we can visualize the anomalies as a table.</p> In\u00a0[\u00a0]: Copied! <pre>context.show(example_validator.outputs['anomalies'])\n</pre> context.show(example_validator.outputs['anomalies']) <p>In the anomalies table, we can see that there are no anomalies. This is what we'd expect, since this the first dataset that we've analyzed and the schema is tailored to it. You should review this schema -- anything unexpected means an anomaly in the data. Once reviewed, the schema can be used to guard future data, and anomalies produced here can be used to debug model performance, understand how your data evolves over time, and identify data errors.</p> In\u00a0[\u00a0]: Copied! <pre>_taxi_constants_module_file = 'taxi_constants.py'\n</pre> _taxi_constants_module_file = 'taxi_constants.py' In\u00a0[\u00a0]: Copied! <pre>%%writefile {_taxi_constants_module_file}\n\n# Categorical features are assumed to each have a maximum value in the dataset.\nMAX_CATEGORICAL_FEATURE_VALUES = [24, 31, 12]\n\nCATEGORICAL_FEATURE_KEYS = [\n    'trip_start_hour', 'trip_start_day', 'trip_start_month',\n    'pickup_census_tract', 'dropoff_census_tract', 'pickup_community_area',\n    'dropoff_community_area'\n]\n\nDENSE_FLOAT_FEATURE_KEYS = ['trip_miles', 'fare', 'trip_seconds']\n\n# Number of buckets used by tf.transform for encoding each feature.\nFEATURE_BUCKET_COUNT = 10\n\nBUCKET_FEATURE_KEYS = [\n    'pickup_latitude', 'pickup_longitude', 'dropoff_latitude',\n    'dropoff_longitude'\n]\n\n# Number of vocabulary terms used for encoding VOCAB_FEATURES by tf.transform\nVOCAB_SIZE = 1000\n\n# Count of out-of-vocab buckets in which unrecognized VOCAB_FEATURES are hashed.\nOOV_SIZE = 10\n\nVOCAB_FEATURE_KEYS = [\n    'payment_type',\n    'company',\n]\n\n# Keys\nLABEL_KEY = 'tips'\nFARE_KEY = 'fare'\n</pre> %%writefile {_taxi_constants_module_file}  # Categorical features are assumed to each have a maximum value in the dataset. MAX_CATEGORICAL_FEATURE_VALUES = [24, 31, 12]  CATEGORICAL_FEATURE_KEYS = [     'trip_start_hour', 'trip_start_day', 'trip_start_month',     'pickup_census_tract', 'dropoff_census_tract', 'pickup_community_area',     'dropoff_community_area' ]  DENSE_FLOAT_FEATURE_KEYS = ['trip_miles', 'fare', 'trip_seconds']  # Number of buckets used by tf.transform for encoding each feature. FEATURE_BUCKET_COUNT = 10  BUCKET_FEATURE_KEYS = [     'pickup_latitude', 'pickup_longitude', 'dropoff_latitude',     'dropoff_longitude' ]  # Number of vocabulary terms used for encoding VOCAB_FEATURES by tf.transform VOCAB_SIZE = 1000  # Count of out-of-vocab buckets in which unrecognized VOCAB_FEATURES are hashed. OOV_SIZE = 10  VOCAB_FEATURE_KEYS = [     'payment_type',     'company', ]  # Keys LABEL_KEY = 'tips' FARE_KEY = 'fare' <p>Next, we write a <code>preprocessing_fn</code> that takes in raw data as input, and returns transformed features that our model can train on:</p> In\u00a0[\u00a0]: Copied! <pre>_taxi_transform_module_file = 'taxi_transform.py'\n</pre> _taxi_transform_module_file = 'taxi_transform.py' In\u00a0[\u00a0]: Copied! <pre>%%writefile {_taxi_transform_module_file}\n\nimport tensorflow as tf\nimport tensorflow_transform as tft\n\nimport taxi_constants\n\n_DENSE_FLOAT_FEATURE_KEYS = taxi_constants.DENSE_FLOAT_FEATURE_KEYS\n_VOCAB_FEATURE_KEYS = taxi_constants.VOCAB_FEATURE_KEYS\n_VOCAB_SIZE = taxi_constants.VOCAB_SIZE\n_OOV_SIZE = taxi_constants.OOV_SIZE\n_FEATURE_BUCKET_COUNT = taxi_constants.FEATURE_BUCKET_COUNT\n_BUCKET_FEATURE_KEYS = taxi_constants.BUCKET_FEATURE_KEYS\n_CATEGORICAL_FEATURE_KEYS = taxi_constants.CATEGORICAL_FEATURE_KEYS\n_FARE_KEY = taxi_constants.FARE_KEY\n_LABEL_KEY = taxi_constants.LABEL_KEY\n\n\ndef preprocessing_fn(inputs):\n  \"\"\"tf.transform's callback function for preprocessing inputs.\n  Args:\n    inputs: map from feature keys to raw not-yet-transformed features.\n  Returns:\n    Map from string feature key to transformed feature operations.\n  \"\"\"\n  outputs = {}\n  for key in _DENSE_FLOAT_FEATURE_KEYS:\n    # If sparse make it dense, setting nan's to 0 or '', and apply zscore.\n    outputs[key] = tft.scale_to_z_score(\n        _fill_in_missing(inputs[key]))\n\n  for key in _VOCAB_FEATURE_KEYS:\n    # Build a vocabulary for this feature.\n    outputs[key] = tft.compute_and_apply_vocabulary(\n        _fill_in_missing(inputs[key]),\n        top_k=_VOCAB_SIZE,\n        num_oov_buckets=_OOV_SIZE)\n\n  for key in _BUCKET_FEATURE_KEYS:\n    outputs[key] = tft.bucketize(\n        _fill_in_missing(inputs[key]), _FEATURE_BUCKET_COUNT)\n\n  for key in _CATEGORICAL_FEATURE_KEYS:\n    outputs[key] = _fill_in_missing(inputs[key])\n\n  # Was this passenger a big tipper?\n  taxi_fare = _fill_in_missing(inputs[_FARE_KEY])\n  tips = _fill_in_missing(inputs[_LABEL_KEY])\n  outputs[_LABEL_KEY] = tf.where(\n      tf.math.is_nan(taxi_fare),\n      tf.cast(tf.zeros_like(taxi_fare), tf.int64),\n      # Test if the tip was &gt; 20% of the fare.\n      tf.cast(\n          tf.greater(tips, tf.multiply(taxi_fare, tf.constant(0.2))), tf.int64))\n\n  return outputs\n\n\ndef _fill_in_missing(x):\n  \"\"\"Replace missing values in a SparseTensor.\n  Fills in missing values of `x` with '' or 0, and converts to a dense tensor.\n  Args:\n    x: A `SparseTensor` of rank 2.  Its dense shape should have size at most 1\n      in the second dimension.\n  Returns:\n    A rank 1 tensor where missing values of `x` have been filled in.\n  \"\"\"\n  if not isinstance(x, tf.sparse.SparseTensor):\n    return x\n\n  default_value = '' if x.dtype == tf.string else 0\n  return tf.squeeze(\n      tf.sparse.to_dense(\n          tf.SparseTensor(x.indices, x.values, [x.dense_shape[0], 1]),\n          default_value),\n      axis=1)\n</pre> %%writefile {_taxi_transform_module_file}  import tensorflow as tf import tensorflow_transform as tft  import taxi_constants  _DENSE_FLOAT_FEATURE_KEYS = taxi_constants.DENSE_FLOAT_FEATURE_KEYS _VOCAB_FEATURE_KEYS = taxi_constants.VOCAB_FEATURE_KEYS _VOCAB_SIZE = taxi_constants.VOCAB_SIZE _OOV_SIZE = taxi_constants.OOV_SIZE _FEATURE_BUCKET_COUNT = taxi_constants.FEATURE_BUCKET_COUNT _BUCKET_FEATURE_KEYS = taxi_constants.BUCKET_FEATURE_KEYS _CATEGORICAL_FEATURE_KEYS = taxi_constants.CATEGORICAL_FEATURE_KEYS _FARE_KEY = taxi_constants.FARE_KEY _LABEL_KEY = taxi_constants.LABEL_KEY   def preprocessing_fn(inputs):   \"\"\"tf.transform's callback function for preprocessing inputs.   Args:     inputs: map from feature keys to raw not-yet-transformed features.   Returns:     Map from string feature key to transformed feature operations.   \"\"\"   outputs = {}   for key in _DENSE_FLOAT_FEATURE_KEYS:     # If sparse make it dense, setting nan's to 0 or '', and apply zscore.     outputs[key] = tft.scale_to_z_score(         _fill_in_missing(inputs[key]))    for key in _VOCAB_FEATURE_KEYS:     # Build a vocabulary for this feature.     outputs[key] = tft.compute_and_apply_vocabulary(         _fill_in_missing(inputs[key]),         top_k=_VOCAB_SIZE,         num_oov_buckets=_OOV_SIZE)    for key in _BUCKET_FEATURE_KEYS:     outputs[key] = tft.bucketize(         _fill_in_missing(inputs[key]), _FEATURE_BUCKET_COUNT)    for key in _CATEGORICAL_FEATURE_KEYS:     outputs[key] = _fill_in_missing(inputs[key])    # Was this passenger a big tipper?   taxi_fare = _fill_in_missing(inputs[_FARE_KEY])   tips = _fill_in_missing(inputs[_LABEL_KEY])   outputs[_LABEL_KEY] = tf.where(       tf.math.is_nan(taxi_fare),       tf.cast(tf.zeros_like(taxi_fare), tf.int64),       # Test if the tip was &gt; 20% of the fare.       tf.cast(           tf.greater(tips, tf.multiply(taxi_fare, tf.constant(0.2))), tf.int64))    return outputs   def _fill_in_missing(x):   \"\"\"Replace missing values in a SparseTensor.   Fills in missing values of `x` with '' or 0, and converts to a dense tensor.   Args:     x: A `SparseTensor` of rank 2.  Its dense shape should have size at most 1       in the second dimension.   Returns:     A rank 1 tensor where missing values of `x` have been filled in.   \"\"\"   if not isinstance(x, tf.sparse.SparseTensor):     return x    default_value = '' if x.dtype == tf.string else 0   return tf.squeeze(       tf.sparse.to_dense(           tf.SparseTensor(x.indices, x.values, [x.dense_shape[0], 1]),           default_value),       axis=1) <p>Now, we pass in this feature engineering code to the <code>Transform</code> component and run it to transform your data.</p> In\u00a0[\u00a0]: Copied! <pre>transform = tfx.components.Transform(\n    examples=example_gen.outputs['examples'],\n    schema=schema_gen.outputs['schema'],\n    module_file=os.path.abspath(_taxi_transform_module_file))\ncontext.run(transform)\n</pre> transform = tfx.components.Transform(     examples=example_gen.outputs['examples'],     schema=schema_gen.outputs['schema'],     module_file=os.path.abspath(_taxi_transform_module_file)) context.run(transform) <p>Let's examine the output artifacts of <code>Transform</code>. This component produces two types of outputs:</p> <ul> <li><code>transform_graph</code> is the graph that can perform the preprocessing operations (this graph will be included in the serving and evaluation models).</li> <li><code>transformed_examples</code> represents the preprocessed training and evaluation data.</li> </ul> In\u00a0[\u00a0]: Copied! <pre>transform.outputs\n</pre> transform.outputs <p>Take a peek at the <code>transform_graph</code> artifact.  It points to a directory containing three subdirectories.</p> In\u00a0[\u00a0]: Copied! <pre>train_uri = transform.outputs['transform_graph'].get()[0].uri\nos.listdir(train_uri)\n</pre> train_uri = transform.outputs['transform_graph'].get()[0].uri os.listdir(train_uri) <p>The <code>transformed_metadata</code> subdirectory contains the schema of the preprocessed data. The <code>transform_fn</code> subdirectory contains the actual preprocessing graph. The <code>metadata</code> subdirectory contains the schema of the original data.</p> <p>We can also take a look at the first three transformed examples:</p> In\u00a0[\u00a0]: Copied! <pre># Get the URI of the output artifact representing the transformed examples, which is a directory\ntrain_uri = os.path.join(transform.outputs['transformed_examples'].get()[0].uri, 'Split-train')\n\n# Get the list of files in this directory (all compressed TFRecord files)\ntfrecord_filenames = [os.path.join(train_uri, name)\n                      for name in os.listdir(train_uri)]\n\n# Create a `TFRecordDataset` to read these files\ndataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")\n\n# Iterate over the first 3 records and decode them.\nfor tfrecord in dataset.take(3):\n  serialized_example = tfrecord.numpy()\n  example = tf.train.Example()\n  example.ParseFromString(serialized_example)\n  pp.pprint(example)\n</pre> # Get the URI of the output artifact representing the transformed examples, which is a directory train_uri = os.path.join(transform.outputs['transformed_examples'].get()[0].uri, 'Split-train')  # Get the list of files in this directory (all compressed TFRecord files) tfrecord_filenames = [os.path.join(train_uri, name)                       for name in os.listdir(train_uri)]  # Create a `TFRecordDataset` to read these files dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")  # Iterate over the first 3 records and decode them. for tfrecord in dataset.take(3):   serialized_example = tfrecord.numpy()   example = tf.train.Example()   example.ParseFromString(serialized_example)   pp.pprint(example) <p>After the <code>Transform</code> component has transformed your data into features, and the next step is to train a model.</p> In\u00a0[\u00a0]: Copied! <pre>_taxi_trainer_module_file = 'taxi_trainer.py'\n</pre> _taxi_trainer_module_file = 'taxi_trainer.py' In\u00a0[\u00a0]: Copied! <pre>%%writefile {_taxi_trainer_module_file}\n\nimport tensorflow as tf\nimport tensorflow_model_analysis as tfma\nimport tensorflow_transform as tft\nfrom tensorflow_transform.tf_metadata import schema_utils\nfrom tfx_bsl.tfxio import dataset_options\n\nimport taxi_constants\n\n_DENSE_FLOAT_FEATURE_KEYS = taxi_constants.DENSE_FLOAT_FEATURE_KEYS\n_VOCAB_FEATURE_KEYS = taxi_constants.VOCAB_FEATURE_KEYS\n_VOCAB_SIZE = taxi_constants.VOCAB_SIZE\n_OOV_SIZE = taxi_constants.OOV_SIZE\n_FEATURE_BUCKET_COUNT = taxi_constants.FEATURE_BUCKET_COUNT\n_BUCKET_FEATURE_KEYS = taxi_constants.BUCKET_FEATURE_KEYS\n_CATEGORICAL_FEATURE_KEYS = taxi_constants.CATEGORICAL_FEATURE_KEYS\n_MAX_CATEGORICAL_FEATURE_VALUES = taxi_constants.MAX_CATEGORICAL_FEATURE_VALUES\n_LABEL_KEY = taxi_constants.LABEL_KEY\n\n\n# Tf.Transform considers these features as \"raw\"\ndef _get_raw_feature_spec(schema):\n  return schema_utils.schema_as_feature_spec(schema).feature_spec\n\n\ndef _build_estimator(config, hidden_units=None, warm_start_from=None):\n  \"\"\"Build an estimator for predicting the tipping behavior of taxi riders.\n  Args:\n    config: tf.estimator.RunConfig defining the runtime environment for the\n      estimator (including model_dir).\n    hidden_units: [int], the layer sizes of the DNN (input layer first)\n    warm_start_from: Optional directory to warm start from.\n  Returns:\n    A dict of the following:\n      - estimator: The estimator that will be used for training and eval.\n      - train_spec: Spec for training.\n      - eval_spec: Spec for eval.\n      - eval_input_receiver_fn: Input function for eval.\n  \"\"\"\n  real_valued_columns = [\n      tf.feature_column.numeric_column(key, shape=())\n      for key in _DENSE_FLOAT_FEATURE_KEYS\n  ]\n  categorical_columns = [\n      tf.feature_column.categorical_column_with_identity(\n          key, num_buckets=_VOCAB_SIZE + _OOV_SIZE, default_value=0)\n      for key in _VOCAB_FEATURE_KEYS\n  ]\n  categorical_columns += [\n      tf.feature_column.categorical_column_with_identity(\n          key, num_buckets=_FEATURE_BUCKET_COUNT, default_value=0)\n      for key in _BUCKET_FEATURE_KEYS\n  ]\n  categorical_columns += [\n      tf.feature_column.categorical_column_with_identity(  # pylint: disable=g-complex-comprehension\n          key,\n          num_buckets=num_buckets,\n          default_value=0) for key, num_buckets in zip(\n              _CATEGORICAL_FEATURE_KEYS,\n              _MAX_CATEGORICAL_FEATURE_VALUES)\n  ]\n  return tf.estimator.DNNLinearCombinedClassifier(\n      config=config,\n      linear_feature_columns=categorical_columns,\n      dnn_feature_columns=real_valued_columns,\n      dnn_hidden_units=hidden_units or [100, 70, 50, 25],\n      warm_start_from=warm_start_from)\n\n\ndef _example_serving_receiver_fn(tf_transform_graph, schema):\n  \"\"\"Build the serving in inputs.\n  Args:\n    tf_transform_graph: A TFTransformOutput.\n    schema: the schema of the input data.\n  Returns:\n    Tensorflow graph which parses examples, applying tf-transform to them.\n  \"\"\"\n  raw_feature_spec = _get_raw_feature_spec(schema)\n  raw_feature_spec.pop(_LABEL_KEY)\n\n  raw_input_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(\n      raw_feature_spec, default_batch_size=None)\n  serving_input_receiver = raw_input_fn()\n\n  transformed_features = tf_transform_graph.transform_raw_features(\n      serving_input_receiver.features)\n\n  return tf.estimator.export.ServingInputReceiver(\n      transformed_features, serving_input_receiver.receiver_tensors)\n\n\ndef _eval_input_receiver_fn(tf_transform_graph, schema):\n  \"\"\"Build everything needed for the tf-model-analysis to run the model.\n  Args:\n    tf_transform_graph: A TFTransformOutput.\n    schema: the schema of the input data.\n  Returns:\n    EvalInputReceiver function, which contains:\n      - Tensorflow graph which parses raw untransformed features, applies the\n        tf-transform preprocessing operators.\n      - Set of raw, untransformed features.\n      - Label against which predictions will be compared.\n  \"\"\"\n  # Notice that the inputs are raw features, not transformed features here.\n  raw_feature_spec = _get_raw_feature_spec(schema)\n\n  serialized_tf_example = tf.compat.v1.placeholder(\n      dtype=tf.string, shape=[None], name='input_example_tensor')\n\n  # Add a parse_example operator to the tensorflow graph, which will parse\n  # raw, untransformed, tf examples.\n  features = tf.io.parse_example(serialized_tf_example, raw_feature_spec)\n\n  # Now that we have our raw examples, process them through the tf-transform\n  # function computed during the preprocessing step.\n  transformed_features = tf_transform_graph.transform_raw_features(\n      features)\n\n  # The key name MUST be 'examples'.\n  receiver_tensors = {'examples': serialized_tf_example}\n\n  # NOTE: Model is driven by transformed features (since training works on the\n  # materialized output of TFT, but slicing will happen on raw features.\n  features.update(transformed_features)\n\n  return tfma.export.EvalInputReceiver(\n      features=features,\n      receiver_tensors=receiver_tensors,\n      labels=transformed_features[_LABEL_KEY])\n\n\ndef _input_fn(file_pattern, data_accessor, tf_transform_output, batch_size=200):\n  \"\"\"Generates features and label for tuning/training.\n\n  Args:\n    file_pattern: List of paths or patterns of input tfrecord files.\n    data_accessor: DataAccessor for converting input to RecordBatch.\n    tf_transform_output: A TFTransformOutput.\n    batch_size: representing the number of consecutive elements of returned\n      dataset to combine in a single batch\n\n  Returns:\n    A dataset that contains (features, indices) tuple where features is a\n      dictionary of Tensors, and indices is a single Tensor of label indices.\n  \"\"\"\n  return data_accessor.tf_dataset_factory(\n      file_pattern,\n      dataset_options.TensorFlowDatasetOptions(\n          batch_size=batch_size, label_key=_LABEL_KEY),\n      tf_transform_output.transformed_metadata.schema)\n\n\n# TFX will call this function\ndef trainer_fn(trainer_fn_args, schema):\n  \"\"\"Build the estimator using the high level API.\n  Args:\n    trainer_fn_args: Holds args used to train the model as name/value pairs.\n    schema: Holds the schema of the training examples.\n  Returns:\n    A dict of the following:\n      - estimator: The estimator that will be used for training and eval.\n      - train_spec: Spec for training.\n      - eval_spec: Spec for eval.\n      - eval_input_receiver_fn: Input function for eval.\n  \"\"\"\n  # Number of nodes in the first layer of the DNN\n  first_dnn_layer_size = 100\n  num_dnn_layers = 4\n  dnn_decay_factor = 0.7\n\n  train_batch_size = 40\n  eval_batch_size = 40\n\n  tf_transform_graph = tft.TFTransformOutput(trainer_fn_args.transform_output)\n\n  train_input_fn = lambda: _input_fn(  # pylint: disable=g-long-lambda\n      trainer_fn_args.train_files,\n      trainer_fn_args.data_accessor,\n      tf_transform_graph,\n      batch_size=train_batch_size)\n\n  eval_input_fn = lambda: _input_fn(  # pylint: disable=g-long-lambda\n      trainer_fn_args.eval_files,\n      trainer_fn_args.data_accessor,\n      tf_transform_graph,\n      batch_size=eval_batch_size)\n\n  train_spec = tf.estimator.TrainSpec(  # pylint: disable=g-long-lambda\n      train_input_fn,\n      max_steps=trainer_fn_args.train_steps)\n\n  serving_receiver_fn = lambda: _example_serving_receiver_fn(  # pylint: disable=g-long-lambda\n      tf_transform_graph, schema)\n\n  exporter = tf.estimator.FinalExporter('chicago-taxi', serving_receiver_fn)\n  eval_spec = tf.estimator.EvalSpec(\n      eval_input_fn,\n      steps=trainer_fn_args.eval_steps,\n      exporters=[exporter],\n      name='chicago-taxi-eval')\n\n  run_config = tf.estimator.RunConfig(\n      save_checkpoints_steps=999, keep_checkpoint_max=1)\n\n  run_config = run_config.replace(model_dir=trainer_fn_args.serving_model_dir)\n\n  estimator = _build_estimator(\n      # Construct layers sizes with exponetial decay\n      hidden_units=[\n          max(2, int(first_dnn_layer_size * dnn_decay_factor**i))\n          for i in range(num_dnn_layers)\n      ],\n      config=run_config,\n      warm_start_from=trainer_fn_args.base_model)\n\n  # Create an input receiver for TFMA processing\n  receiver_fn = lambda: _eval_input_receiver_fn(  # pylint: disable=g-long-lambda\n      tf_transform_graph, schema)\n\n  return {\n      'estimator': estimator,\n      'train_spec': train_spec,\n      'eval_spec': eval_spec,\n      'eval_input_receiver_fn': receiver_fn\n  }\n</pre> %%writefile {_taxi_trainer_module_file}  import tensorflow as tf import tensorflow_model_analysis as tfma import tensorflow_transform as tft from tensorflow_transform.tf_metadata import schema_utils from tfx_bsl.tfxio import dataset_options  import taxi_constants  _DENSE_FLOAT_FEATURE_KEYS = taxi_constants.DENSE_FLOAT_FEATURE_KEYS _VOCAB_FEATURE_KEYS = taxi_constants.VOCAB_FEATURE_KEYS _VOCAB_SIZE = taxi_constants.VOCAB_SIZE _OOV_SIZE = taxi_constants.OOV_SIZE _FEATURE_BUCKET_COUNT = taxi_constants.FEATURE_BUCKET_COUNT _BUCKET_FEATURE_KEYS = taxi_constants.BUCKET_FEATURE_KEYS _CATEGORICAL_FEATURE_KEYS = taxi_constants.CATEGORICAL_FEATURE_KEYS _MAX_CATEGORICAL_FEATURE_VALUES = taxi_constants.MAX_CATEGORICAL_FEATURE_VALUES _LABEL_KEY = taxi_constants.LABEL_KEY   # Tf.Transform considers these features as \"raw\" def _get_raw_feature_spec(schema):   return schema_utils.schema_as_feature_spec(schema).feature_spec   def _build_estimator(config, hidden_units=None, warm_start_from=None):   \"\"\"Build an estimator for predicting the tipping behavior of taxi riders.   Args:     config: tf.estimator.RunConfig defining the runtime environment for the       estimator (including model_dir).     hidden_units: [int], the layer sizes of the DNN (input layer first)     warm_start_from: Optional directory to warm start from.   Returns:     A dict of the following:       - estimator: The estimator that will be used for training and eval.       - train_spec: Spec for training.       - eval_spec: Spec for eval.       - eval_input_receiver_fn: Input function for eval.   \"\"\"   real_valued_columns = [       tf.feature_column.numeric_column(key, shape=())       for key in _DENSE_FLOAT_FEATURE_KEYS   ]   categorical_columns = [       tf.feature_column.categorical_column_with_identity(           key, num_buckets=_VOCAB_SIZE + _OOV_SIZE, default_value=0)       for key in _VOCAB_FEATURE_KEYS   ]   categorical_columns += [       tf.feature_column.categorical_column_with_identity(           key, num_buckets=_FEATURE_BUCKET_COUNT, default_value=0)       for key in _BUCKET_FEATURE_KEYS   ]   categorical_columns += [       tf.feature_column.categorical_column_with_identity(  # pylint: disable=g-complex-comprehension           key,           num_buckets=num_buckets,           default_value=0) for key, num_buckets in zip(               _CATEGORICAL_FEATURE_KEYS,               _MAX_CATEGORICAL_FEATURE_VALUES)   ]   return tf.estimator.DNNLinearCombinedClassifier(       config=config,       linear_feature_columns=categorical_columns,       dnn_feature_columns=real_valued_columns,       dnn_hidden_units=hidden_units or [100, 70, 50, 25],       warm_start_from=warm_start_from)   def _example_serving_receiver_fn(tf_transform_graph, schema):   \"\"\"Build the serving in inputs.   Args:     tf_transform_graph: A TFTransformOutput.     schema: the schema of the input data.   Returns:     Tensorflow graph which parses examples, applying tf-transform to them.   \"\"\"   raw_feature_spec = _get_raw_feature_spec(schema)   raw_feature_spec.pop(_LABEL_KEY)    raw_input_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(       raw_feature_spec, default_batch_size=None)   serving_input_receiver = raw_input_fn()    transformed_features = tf_transform_graph.transform_raw_features(       serving_input_receiver.features)    return tf.estimator.export.ServingInputReceiver(       transformed_features, serving_input_receiver.receiver_tensors)   def _eval_input_receiver_fn(tf_transform_graph, schema):   \"\"\"Build everything needed for the tf-model-analysis to run the model.   Args:     tf_transform_graph: A TFTransformOutput.     schema: the schema of the input data.   Returns:     EvalInputReceiver function, which contains:       - Tensorflow graph which parses raw untransformed features, applies the         tf-transform preprocessing operators.       - Set of raw, untransformed features.       - Label against which predictions will be compared.   \"\"\"   # Notice that the inputs are raw features, not transformed features here.   raw_feature_spec = _get_raw_feature_spec(schema)    serialized_tf_example = tf.compat.v1.placeholder(       dtype=tf.string, shape=[None], name='input_example_tensor')    # Add a parse_example operator to the tensorflow graph, which will parse   # raw, untransformed, tf examples.   features = tf.io.parse_example(serialized_tf_example, raw_feature_spec)    # Now that we have our raw examples, process them through the tf-transform   # function computed during the preprocessing step.   transformed_features = tf_transform_graph.transform_raw_features(       features)    # The key name MUST be 'examples'.   receiver_tensors = {'examples': serialized_tf_example}    # NOTE: Model is driven by transformed features (since training works on the   # materialized output of TFT, but slicing will happen on raw features.   features.update(transformed_features)    return tfma.export.EvalInputReceiver(       features=features,       receiver_tensors=receiver_tensors,       labels=transformed_features[_LABEL_KEY])   def _input_fn(file_pattern, data_accessor, tf_transform_output, batch_size=200):   \"\"\"Generates features and label for tuning/training.    Args:     file_pattern: List of paths or patterns of input tfrecord files.     data_accessor: DataAccessor for converting input to RecordBatch.     tf_transform_output: A TFTransformOutput.     batch_size: representing the number of consecutive elements of returned       dataset to combine in a single batch    Returns:     A dataset that contains (features, indices) tuple where features is a       dictionary of Tensors, and indices is a single Tensor of label indices.   \"\"\"   return data_accessor.tf_dataset_factory(       file_pattern,       dataset_options.TensorFlowDatasetOptions(           batch_size=batch_size, label_key=_LABEL_KEY),       tf_transform_output.transformed_metadata.schema)   # TFX will call this function def trainer_fn(trainer_fn_args, schema):   \"\"\"Build the estimator using the high level API.   Args:     trainer_fn_args: Holds args used to train the model as name/value pairs.     schema: Holds the schema of the training examples.   Returns:     A dict of the following:       - estimator: The estimator that will be used for training and eval.       - train_spec: Spec for training.       - eval_spec: Spec for eval.       - eval_input_receiver_fn: Input function for eval.   \"\"\"   # Number of nodes in the first layer of the DNN   first_dnn_layer_size = 100   num_dnn_layers = 4   dnn_decay_factor = 0.7    train_batch_size = 40   eval_batch_size = 40    tf_transform_graph = tft.TFTransformOutput(trainer_fn_args.transform_output)    train_input_fn = lambda: _input_fn(  # pylint: disable=g-long-lambda       trainer_fn_args.train_files,       trainer_fn_args.data_accessor,       tf_transform_graph,       batch_size=train_batch_size)    eval_input_fn = lambda: _input_fn(  # pylint: disable=g-long-lambda       trainer_fn_args.eval_files,       trainer_fn_args.data_accessor,       tf_transform_graph,       batch_size=eval_batch_size)    train_spec = tf.estimator.TrainSpec(  # pylint: disable=g-long-lambda       train_input_fn,       max_steps=trainer_fn_args.train_steps)    serving_receiver_fn = lambda: _example_serving_receiver_fn(  # pylint: disable=g-long-lambda       tf_transform_graph, schema)    exporter = tf.estimator.FinalExporter('chicago-taxi', serving_receiver_fn)   eval_spec = tf.estimator.EvalSpec(       eval_input_fn,       steps=trainer_fn_args.eval_steps,       exporters=[exporter],       name='chicago-taxi-eval')    run_config = tf.estimator.RunConfig(       save_checkpoints_steps=999, keep_checkpoint_max=1)    run_config = run_config.replace(model_dir=trainer_fn_args.serving_model_dir)    estimator = _build_estimator(       # Construct layers sizes with exponetial decay       hidden_units=[           max(2, int(first_dnn_layer_size * dnn_decay_factor**i))           for i in range(num_dnn_layers)       ],       config=run_config,       warm_start_from=trainer_fn_args.base_model)    # Create an input receiver for TFMA processing   receiver_fn = lambda: _eval_input_receiver_fn(  # pylint: disable=g-long-lambda       tf_transform_graph, schema)    return {       'estimator': estimator,       'train_spec': train_spec,       'eval_spec': eval_spec,       'eval_input_receiver_fn': receiver_fn   } <p>Now, we pass in this model code to the <code>Trainer</code> component and run it to train the model.</p> In\u00a0[\u00a0]: Copied! <pre>from tfx.components.trainer.executor import Executor\nfrom tfx.dsl.components.base import executor_spec\n\ntrainer = tfx.components.Trainer(\n    module_file=os.path.abspath(_taxi_trainer_module_file),\n    custom_executor_spec=executor_spec.ExecutorClassSpec(Executor),\n    examples=transform.outputs['transformed_examples'],\n    schema=schema_gen.outputs['schema'],\n    transform_graph=transform.outputs['transform_graph'],\n    train_args=tfx.proto.TrainArgs(num_steps=10000),\n    eval_args=tfx.proto.EvalArgs(num_steps=5000))\ncontext.run(trainer)\n</pre> from tfx.components.trainer.executor import Executor from tfx.dsl.components.base import executor_spec  trainer = tfx.components.Trainer(     module_file=os.path.abspath(_taxi_trainer_module_file),     custom_executor_spec=executor_spec.ExecutorClassSpec(Executor),     examples=transform.outputs['transformed_examples'],     schema=schema_gen.outputs['schema'],     transform_graph=transform.outputs['transform_graph'],     train_args=tfx.proto.TrainArgs(num_steps=10000),     eval_args=tfx.proto.EvalArgs(num_steps=5000)) context.run(trainer) In\u00a0[\u00a0]: Copied! <pre># Get the URI of the output artifact representing the training logs, which is a directory\nmodel_run_dir = trainer.outputs['model_run'].get()[0].uri\n\n%load_ext tensorboard\n%tensorboard --logdir {model_run_dir}\n</pre> # Get the URI of the output artifact representing the training logs, which is a directory model_run_dir = trainer.outputs['model_run'].get()[0].uri  %load_ext tensorboard %tensorboard --logdir {model_run_dir} In\u00a0[\u00a0]: Copied! <pre>eval_config = tfma.EvalConfig(\n    model_specs=[\n        # Using signature 'eval' implies the use of an EvalSavedModel. To use\n        # a serving model remove the signature to defaults to 'serving_default'\n        # and add a label_key.\n        tfma.ModelSpec(signature_name='eval')\n    ],\n    metrics_specs=[\n        tfma.MetricsSpec(\n            # The metrics added here are in addition to those saved with the\n            # model (assuming either a keras model or EvalSavedModel is used).\n            # Any metrics added into the saved model (for example using\n            # model.compile(..., metrics=[...]), etc) will be computed\n            # automatically.\n            metrics=[\n                tfma.MetricConfig(class_name='ExampleCount')\n            ],\n            # To add validation thresholds for metrics saved with the model,\n            # add them keyed by metric name to the thresholds map.\n            thresholds = {\n                'accuracy': tfma.MetricThreshold(\n                    value_threshold=tfma.GenericValueThreshold(\n                        lower_bound={'value': 0.5}),\n                    # Change threshold will be ignored if there is no\n                    # baseline model resolved from MLMD (first run).\n                    change_threshold=tfma.GenericChangeThreshold(\n                       direction=tfma.MetricDirection.HIGHER_IS_BETTER,\n                       absolute={'value': -1e-10}))\n            }\n        )\n    ],\n    slicing_specs=[\n        # An empty slice spec means the overall slice, i.e. the whole dataset.\n        tfma.SlicingSpec(),\n        # Data can be sliced along a feature column. In this case, data is\n        # sliced along feature column trip_start_hour.\n        tfma.SlicingSpec(feature_keys=['trip_start_hour'])\n    ])\n</pre> eval_config = tfma.EvalConfig(     model_specs=[         # Using signature 'eval' implies the use of an EvalSavedModel. To use         # a serving model remove the signature to defaults to 'serving_default'         # and add a label_key.         tfma.ModelSpec(signature_name='eval')     ],     metrics_specs=[         tfma.MetricsSpec(             # The metrics added here are in addition to those saved with the             # model (assuming either a keras model or EvalSavedModel is used).             # Any metrics added into the saved model (for example using             # model.compile(..., metrics=[...]), etc) will be computed             # automatically.             metrics=[                 tfma.MetricConfig(class_name='ExampleCount')             ],             # To add validation thresholds for metrics saved with the model,             # add them keyed by metric name to the thresholds map.             thresholds = {                 'accuracy': tfma.MetricThreshold(                     value_threshold=tfma.GenericValueThreshold(                         lower_bound={'value': 0.5}),                     # Change threshold will be ignored if there is no                     # baseline model resolved from MLMD (first run).                     change_threshold=tfma.GenericChangeThreshold(                        direction=tfma.MetricDirection.HIGHER_IS_BETTER,                        absolute={'value': -1e-10}))             }         )     ],     slicing_specs=[         # An empty slice spec means the overall slice, i.e. the whole dataset.         tfma.SlicingSpec(),         # Data can be sliced along a feature column. In this case, data is         # sliced along feature column trip_start_hour.         tfma.SlicingSpec(feature_keys=['trip_start_hour'])     ]) <p>Next, we give this configuration to <code>Evaluator</code> and run it.</p> In\u00a0[\u00a0]: Copied! <pre># Use TFMA to compute a evaluation statistics over features of a model and\n# validate them against a baseline.\n\n# The model resolver is only required if performing model validation in addition\n# to evaluation. In this case we validate against the latest blessed model. If\n# no model has been blessed before (as in this case) the evaluator will make our\n# candidate the first blessed model.\nmodel_resolver = tfx.dsl.Resolver(\n      strategy_class=tfx.dsl.experimental.LatestBlessedModelStrategy,\n      model=tfx.dsl.Channel(type=tfx.types.standard_artifacts.Model),\n      model_blessing=tfx.dsl.Channel(\n          type=tfx.types.standard_artifacts.ModelBlessing)).with_id(\n              'latest_blessed_model_resolver')\ncontext.run(model_resolver)\n\nevaluator = tfx.components.Evaluator(\n    examples=example_gen.outputs['examples'],\n    model=trainer.outputs['model'],\n    eval_config=eval_config)\ncontext.run(evaluator)\n</pre> # Use TFMA to compute a evaluation statistics over features of a model and # validate them against a baseline.  # The model resolver is only required if performing model validation in addition # to evaluation. In this case we validate against the latest blessed model. If # no model has been blessed before (as in this case) the evaluator will make our # candidate the first blessed model. model_resolver = tfx.dsl.Resolver(       strategy_class=tfx.dsl.experimental.LatestBlessedModelStrategy,       model=tfx.dsl.Channel(type=tfx.types.standard_artifacts.Model),       model_blessing=tfx.dsl.Channel(           type=tfx.types.standard_artifacts.ModelBlessing)).with_id(               'latest_blessed_model_resolver') context.run(model_resolver)  evaluator = tfx.components.Evaluator(     examples=example_gen.outputs['examples'],     model=trainer.outputs['model'],     eval_config=eval_config) context.run(evaluator) <p>Now let's examine the output artifacts of <code>Evaluator</code>.</p> In\u00a0[\u00a0]: Copied! <pre>evaluator.outputs\n</pre> evaluator.outputs <p>Using the <code>evaluation</code> output we can show the default visualization of global metrics on the entire evaluation set.</p> In\u00a0[\u00a0]: Copied! <pre>context.show(evaluator.outputs['evaluation'])\n</pre> context.show(evaluator.outputs['evaluation']) <p>To see the visualization for sliced evaluation metrics, we can directly call the TensorFlow Model Analysis library.</p> In\u00a0[\u00a0]: Copied! <pre>import tensorflow_model_analysis as tfma\n\n# Get the TFMA output result path and load the result.\nPATH_TO_RESULT = evaluator.outputs['evaluation'].get()[0].uri\ntfma_result = tfma.load_eval_result(PATH_TO_RESULT)\n\n# Show data sliced along feature column trip_start_hour.\ntfma.view.render_slicing_metrics(\n    tfma_result, slicing_column='trip_start_hour')\n</pre> import tensorflow_model_analysis as tfma  # Get the TFMA output result path and load the result. PATH_TO_RESULT = evaluator.outputs['evaluation'].get()[0].uri tfma_result = tfma.load_eval_result(PATH_TO_RESULT)  # Show data sliced along feature column trip_start_hour. tfma.view.render_slicing_metrics(     tfma_result, slicing_column='trip_start_hour') <p>This visualization shows the same metrics, but computed at every feature value of <code>trip_start_hour</code> instead of on the entire evaluation set.</p> <p>TensorFlow Model Analysis supports many other visualizations, such as Fairness Indicators and plotting a time series of model performance. To learn more, see the tutorial.</p> <p>Since we added thresholds to our config, validation output is also available. The precence of a <code>blessing</code> artifact indicates that our model passed validation. Since this is the first validation being performed the candidate is automatically blessed.</p> In\u00a0[\u00a0]: Copied! <pre>blessing_uri = evaluator.outputs['blessing'].get()[0].uri\n!ls -l {blessing_uri}\n</pre> blessing_uri = evaluator.outputs['blessing'].get()[0].uri !ls -l {blessing_uri} <p>Now can also verify the success by loading the validation result record:</p> In\u00a0[\u00a0]: Copied! <pre>PATH_TO_RESULT = evaluator.outputs['evaluation'].get()[0].uri\nprint(tfma.load_validation_result(PATH_TO_RESULT))\n</pre> PATH_TO_RESULT = evaluator.outputs['evaluation'].get()[0].uri print(tfma.load_validation_result(PATH_TO_RESULT)) In\u00a0[\u00a0]: Copied! <pre>pusher = tfx.components.Pusher(\n    model=trainer.outputs['model'],\n    model_blessing=evaluator.outputs['blessing'],\n    push_destination=tfx.proto.PushDestination(\n        filesystem=tfx.proto.PushDestination.Filesystem(\n            base_directory=_serving_model_dir)))\ncontext.run(pusher)\n</pre> pusher = tfx.components.Pusher(     model=trainer.outputs['model'],     model_blessing=evaluator.outputs['blessing'],     push_destination=tfx.proto.PushDestination(         filesystem=tfx.proto.PushDestination.Filesystem(             base_directory=_serving_model_dir))) context.run(pusher) <p>Let's examine the output artifacts of <code>Pusher</code>.</p> In\u00a0[\u00a0]: Copied! <pre>pusher.outputs\n</pre> pusher.outputs <p>In particular, the Pusher will export your model in the SavedModel format, which looks like this:</p> In\u00a0[\u00a0]: Copied! <pre>push_uri = pusher.outputs['pushed_model'].get()[0].uri\nmodel = tf.saved_model.load(push_uri)\n\nfor item in model.signatures.items():\n  pp.pprint(item)\n</pre> push_uri = pusher.outputs['pushed_model'].get()[0].uri model = tf.saved_model.load(push_uri)  for item in model.signatures.items():   pp.pprint(item) <p>We're finished our tour of built-in TFX components!</p>"},{"location":"tutorials/tfx/components/#copyright-2021-the-tensorflow-authors","title":"Copyright 2021 The TensorFlow Authors.\u00b6","text":""},{"location":"tutorials/tfx/components/#tfx-estimator-component-tutorial","title":"TFX Estimator Component Tutorial\u00b6","text":"<p>A Component-by-Component Introduction to TensorFlow Extended (TFX)</p>"},{"location":"tutorials/tfx/components/#background","title":"Background\u00b6","text":"<p>This notebook demonstrates how to use TFX in a Jupyter/Colab environment.  Here, we walk through the Chicago Taxi example in an interactive notebook.</p> <p>Working in an interactive notebook is a useful way to become familiar with the structure of a TFX pipeline.  It's also useful when doing development of your own pipelines as a lightweight development environment, but you should be aware that there are differences in the way interactive notebooks are orchestrated, and how they access metadata artifacts.</p>"},{"location":"tutorials/tfx/components/#orchestration","title":"Orchestration\u00b6","text":"<p>In a production deployment of TFX, you will use an orchestrator such as Apache Airflow, Kubeflow Pipelines, or Apache Beam to orchestrate a pre-defined pipeline graph of TFX components.  In an interactive notebook, the notebook itself is the orchestrator, running each TFX component as you execute the notebook cells.</p>"},{"location":"tutorials/tfx/components/#metadata","title":"Metadata\u00b6","text":"<p>In a production deployment of TFX, you will access metadata through the ML Metadata (MLMD) API.  MLMD stores metadata properties in a database such as MySQL or SQLite, and stores the metadata payloads in a persistent store such as on your filesystem.  In an interactive notebook, both properties and payloads are stored in an ephemeral SQLite database in the <code>/tmp</code> directory on the Jupyter notebook or Colab server.</p>"},{"location":"tutorials/tfx/components/#setup","title":"Setup\u00b6","text":"<p>First, we install and import the necessary packages, set up paths, and download data.</p>"},{"location":"tutorials/tfx/components/#upgrade-pip","title":"Upgrade Pip\u00b6","text":"<p>To avoid upgrading Pip in a system when running locally, check to make sure that we're running in Colab.  Local systems can of course be upgraded separately.</p>"},{"location":"tutorials/tfx/components/#install-tfx","title":"Install TFX\u00b6","text":"<p>Note: In Google Colab, because of package updates, the first time you run this cell you must restart the runtime (Runtime &gt; Restart runtime ...).</p>"},{"location":"tutorials/tfx/components/#did-you-restart-the-runtime","title":"Did you restart the runtime?\u00b6","text":"<p>If you are using Google Colab, the first time that you run the cell above, you must restart the runtime (Runtime &gt; Restart runtime ...). This is because of the way that Colab loads packages.</p>"},{"location":"tutorials/tfx/components/#import-packages","title":"Import packages\u00b6","text":"<p>We import necessary packages, including standard TFX component classes.</p>"},{"location":"tutorials/tfx/components/#set-up-pipeline-paths","title":"Set up pipeline paths\u00b6","text":""},{"location":"tutorials/tfx/components/#download-example-data","title":"Download example data\u00b6","text":"<p>We download the example dataset for use in our TFX pipeline.</p> <p>The dataset we're using is the Taxi Trips dataset released by the City of Chicago. The columns in this dataset are:</p> pickup_community_areafaretrip_start_month trip_start_hourtrip_start_daytrip_start_timestamp pickup_latitudepickup_longitudedropoff_latitude dropoff_longitudetrip_milespickup_census_tract dropoff_census_tractpayment_typecompany trip_secondsdropoff_community_areatips <p>With this dataset, we will build a model that predicts the <code>tips</code> of a trip.</p>"},{"location":"tutorials/tfx/components/#create-the-interactivecontext","title":"Create the InteractiveContext\u00b6","text":"<p>Last, we create an InteractiveContext, which will allow us to run TFX components interactively in this notebook.</p>"},{"location":"tutorials/tfx/components/#run-tfx-components-interactively","title":"Run TFX components interactively\u00b6","text":"<p>In the cells that follow, we create TFX components one-by-one, run each of them, and visualize their output artifacts.</p>"},{"location":"tutorials/tfx/components/#examplegen","title":"ExampleGen\u00b6","text":"<p>The <code>ExampleGen</code> component is usually at the start of a TFX pipeline. It will:</p> <ol> <li>Split data into training and evaluation sets (by default, 2/3 training + 1/3 eval)</li> <li>Convert data into the <code>tf.Example</code> format (learn more here)</li> <li>Copy data into the <code>_tfx_root</code> directory for other components to access</li> </ol> <p><code>ExampleGen</code> takes as input the path to your data source. In our case, this is the <code>_data_root</code> path that contains the downloaded CSV.</p> <p>Note: In this notebook, we can instantiate components one-by-one and run them with <code>InteractiveContext.run()</code>. By contrast, in a production setting, we would specify all the components upfront in a <code>Pipeline</code> to pass to the orchestrator (see the Building a TFX Pipeline Guide).</p>"},{"location":"tutorials/tfx/components/#statisticsgen","title":"StatisticsGen\u00b6","text":"<p>The <code>StatisticsGen</code> component computes statistics over your dataset for data analysis, as well as for use in downstream components. It uses the TensorFlow Data Validation library.</p> <p><code>StatisticsGen</code> takes as input the dataset we just ingested using <code>ExampleGen</code>.</p>"},{"location":"tutorials/tfx/components/#schemagen","title":"SchemaGen\u00b6","text":"<p>The <code>SchemaGen</code> component generates a schema based on your data statistics. (A schema defines the expected bounds, types, and properties of the features in your dataset.) It also uses the TensorFlow Data Validation library.</p> <p><code>SchemaGen</code> will take as input the statistics that we generated with <code>StatisticsGen</code>, looking at the training split by default.</p>"},{"location":"tutorials/tfx/components/#examplevalidator","title":"ExampleValidator\u00b6","text":"<p>The <code>ExampleValidator</code> component detects anomalies in your data, based on the expectations defined by the schema. It also uses the TensorFlow Data Validation library.</p> <p><code>ExampleValidator</code> will take as input the statistics from <code>StatisticsGen</code>, and the schema from <code>SchemaGen</code>.</p>"},{"location":"tutorials/tfx/components/#transform","title":"Transform\u00b6","text":"<p>The <code>Transform</code> component performs feature engineering for both training and serving. It uses the TensorFlow Transform library.</p> <p><code>Transform</code> will take as input the data from <code>ExampleGen</code>, the schema from <code>SchemaGen</code>, as well as a module that contains user-defined Transform code.</p> <p>Let's see an example of user-defined Transform code below (for an introduction to the TensorFlow Transform APIs, see the tutorial). First, we define a few constants for feature engineering:</p> <p>Note: The <code>%%writefile</code> cell magic will save the contents of the cell as a <code>.py</code> file on disk. This allows the <code>Transform</code> component to load your code as a module.</p>"},{"location":"tutorials/tfx/components/#trainer","title":"Trainer\u00b6","text":"<p>The <code>Trainer</code> component will train a model that you define in TensorFlow (either using the Estimator API or the Keras API with <code>model_to_estimator</code>).</p> <p><code>Trainer</code> takes as input the schema from <code>SchemaGen</code>, the transformed data and graph from <code>Transform</code>, training parameters, as well as a module that contains user-defined model code.</p> <p>Let's see an example of user-defined model code below (for an introduction to the TensorFlow Estimator APIs, see the tutorial):</p>"},{"location":"tutorials/tfx/components/#analyze-training-with-tensorboard","title":"Analyze Training with TensorBoard\u00b6","text":"<p>Optionally, we can connect TensorBoard to the Trainer to analyze our model's training curves.</p>"},{"location":"tutorials/tfx/components/#evaluator","title":"Evaluator\u00b6","text":"<p>The <code>Evaluator</code> component computes model performance metrics over the evaluation set. It uses the TensorFlow Model Analysis library. The <code>Evaluator</code> can also optionally validate that a newly trained model is better than the previous model. This is useful in a production pipeline setting where you may automatically train and validate a model every day. In this notebook, we only train one model, so the <code>Evaluator</code> automatically will label the model as \"good\".</p> <p><code>Evaluator</code> will take as input the data from <code>ExampleGen</code>, the trained model from <code>Trainer</code>, and slicing configuration. The slicing configuration allows you to slice your metrics on feature values (e.g. how does your model perform on taxi trips that start at 8am versus 8pm?). See an example of this configuration below:</p>"},{"location":"tutorials/tfx/components/#pusher","title":"Pusher\u00b6","text":"<p>The <code>Pusher</code> component is usually at the end of a TFX pipeline. It checks whether a model has passed validation, and if so, exports the model to <code>_serving_model_dir</code>.</p>"},{"location":"tutorials/tfx/components_keras/","title":"TFX Keras Component Tutorial","text":"In\u00a0[\u00a0]: Copied! <pre>#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n</pre> #@title Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # https://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. <p>Note: We recommend running this tutorial in a Colab notebook, with no setup required!  Just click \"Run in Google Colab\".</p>        View on TensorFlow.org             Run in Google Colab             View source on GitHub             Download notebook      <p>This Colab-based tutorial will interactively walk through each built-in component of TensorFlow Extended (TFX).</p> <p>It covers every step in an end-to-end machine learning pipeline, from data ingestion to pushing a model to serving.</p> <p>When you're done, the contents of this notebook can be automatically exported as TFX pipeline source code, which you can orchestrate with Apache Airflow and Apache Beam.</p> <p>Note: This notebook demonstrates the use of native Keras models in TFX pipelines. TFX only supports the TensorFlow 2 version of Keras.</p> In\u00a0[\u00a0]: Copied! <pre>import sys\nif 'google.colab' in sys.modules:\n  !pip install --upgrade pip\n</pre> import sys if 'google.colab' in sys.modules:   !pip install --upgrade pip In\u00a0[\u00a0]: Copied! <pre>!pip install tfx\n</pre> !pip install tfx In\u00a0[\u00a0]: Copied! <pre>import os\nimport pprint\nimport tempfile\nimport urllib\n\nimport absl\nimport tensorflow as tf\nimport tensorflow_model_analysis as tfma\ntf.get_logger().propagate = False\npp = pprint.PrettyPrinter()\n\nfrom tfx import v1 as tfx\nfrom tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n\n%load_ext tfx.orchestration.experimental.interactive.notebook_extensions.skip\n</pre> import os import pprint import tempfile import urllib  import absl import tensorflow as tf import tensorflow_model_analysis as tfma tf.get_logger().propagate = False pp = pprint.PrettyPrinter()  from tfx import v1 as tfx from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext  %load_ext tfx.orchestration.experimental.interactive.notebook_extensions.skip <p>Let's check the library versions.</p> In\u00a0[\u00a0]: Copied! <pre>print('TensorFlow version: {}'.format(tf.__version__))\nprint('TFX version: {}'.format(tfx.__version__))\n</pre> print('TensorFlow version: {}'.format(tf.__version__)) print('TFX version: {}'.format(tfx.__version__)) In\u00a0[\u00a0]: Copied! <pre># This is the root directory for your TFX pip package installation.\n_tfx_root = tfx.__path__[0]\n\n# This is the directory containing the TFX Chicago Taxi Pipeline example.\n_taxi_root = os.path.join(_tfx_root, 'examples/chicago_taxi_pipeline')\n\n# This is the path where your model will be pushed for serving.\n_serving_model_dir = os.path.join(\n    tempfile.mkdtemp(), 'serving_model/taxi_simple')\n\n# Set up logging.\nabsl.logging.set_verbosity(absl.logging.INFO)\n</pre> # This is the root directory for your TFX pip package installation. _tfx_root = tfx.__path__[0]  # This is the directory containing the TFX Chicago Taxi Pipeline example. _taxi_root = os.path.join(_tfx_root, 'examples/chicago_taxi_pipeline')  # This is the path where your model will be pushed for serving. _serving_model_dir = os.path.join(     tempfile.mkdtemp(), 'serving_model/taxi_simple')  # Set up logging. absl.logging.set_verbosity(absl.logging.INFO) In\u00a0[\u00a0]: Copied! <pre>_data_root = tempfile.mkdtemp(prefix='tfx-data')\nDATA_PATH = 'https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/chicago_taxi_pipeline/data/simple/data.csv'\n_data_filepath = os.path.join(_data_root, \"data.csv\")\nurllib.request.urlretrieve(DATA_PATH, _data_filepath)\n</pre> _data_root = tempfile.mkdtemp(prefix='tfx-data') DATA_PATH = 'https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/chicago_taxi_pipeline/data/simple/data.csv' _data_filepath = os.path.join(_data_root, \"data.csv\") urllib.request.urlretrieve(DATA_PATH, _data_filepath) <p>Take a quick look at the CSV file.</p> In\u00a0[\u00a0]: Copied! <pre>!head {_data_filepath}\n</pre> !head {_data_filepath} <p>Disclaimer: This site provides applications using data that has been modified for use from its original source, www.cityofchicago.org, the official website of the City of Chicago. The City of Chicago makes no claims as to the content, accuracy, timeliness, or completeness of any of the data provided at this site. The data provided at this site is subject to change at any time. It is understood that the data provided at this site is being used at one\u2019s own risk.</p> In\u00a0[\u00a0]: Copied! <pre># Here, we create an InteractiveContext using default parameters. This will\n# use a temporary directory with an ephemeral ML Metadata database instance.\n# To use your own pipeline root or database, the optional properties\n# `pipeline_root` and `metadata_connection_config` may be passed to\n# InteractiveContext. Calls to InteractiveContext are no-ops outside of the\n# notebook.\ncontext = InteractiveContext()\n</pre> # Here, we create an InteractiveContext using default parameters. This will # use a temporary directory with an ephemeral ML Metadata database instance. # To use your own pipeline root or database, the optional properties # `pipeline_root` and `metadata_connection_config` may be passed to # InteractiveContext. Calls to InteractiveContext are no-ops outside of the # notebook. context = InteractiveContext() In\u00a0[\u00a0]: Copied! <pre>example_gen = tfx.components.CsvExampleGen(input_base=_data_root)\ncontext.run(example_gen, enable_cache=True)\n</pre> example_gen = tfx.components.CsvExampleGen(input_base=_data_root) context.run(example_gen, enable_cache=True) <p>Let's examine the output artifacts of <code>ExampleGen</code>. This component produces two artifacts, training examples and evaluation examples:</p> In\u00a0[\u00a0]: Copied! <pre>artifact = example_gen.outputs['examples'].get()[0]\nprint(artifact.split_names, artifact.uri)\n</pre> artifact = example_gen.outputs['examples'].get()[0] print(artifact.split_names, artifact.uri) <p>We can also take a look at the first three training examples:</p> In\u00a0[\u00a0]: Copied! <pre># Get the URI of the output artifact representing the training examples, which is a directory\ntrain_uri = os.path.join(example_gen.outputs['examples'].get()[0].uri, 'Split-train')\n\n# Get the list of files in this directory (all compressed TFRecord files)\ntfrecord_filenames = [os.path.join(train_uri, name)\n                      for name in os.listdir(train_uri)]\n\n# Create a `TFRecordDataset` to read these files\ndataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")\n\n# Iterate over the first 3 records and decode them.\nfor tfrecord in dataset.take(3):\n  serialized_example = tfrecord.numpy()\n  example = tf.train.Example()\n  example.ParseFromString(serialized_example)\n  pp.pprint(example)\n</pre> # Get the URI of the output artifact representing the training examples, which is a directory train_uri = os.path.join(example_gen.outputs['examples'].get()[0].uri, 'Split-train')  # Get the list of files in this directory (all compressed TFRecord files) tfrecord_filenames = [os.path.join(train_uri, name)                       for name in os.listdir(train_uri)]  # Create a `TFRecordDataset` to read these files dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")  # Iterate over the first 3 records and decode them. for tfrecord in dataset.take(3):   serialized_example = tfrecord.numpy()   example = tf.train.Example()   example.ParseFromString(serialized_example)   pp.pprint(example) <p>Now that <code>ExampleGen</code> has finished ingesting the data, the next step is data analysis.</p> In\u00a0[\u00a0]: Copied! <pre>statistics_gen = tfx.components.StatisticsGen(\n    examples=example_gen.outputs['examples'])\ncontext.run(statistics_gen, enable_cache=True)\n</pre> statistics_gen = tfx.components.StatisticsGen(     examples=example_gen.outputs['examples']) context.run(statistics_gen, enable_cache=True) <p>After <code>StatisticsGen</code> finishes running, we can visualize the outputted statistics. Try playing with the different plots!</p> In\u00a0[\u00a0]: Copied! <pre>context.show(statistics_gen.outputs['statistics'])\n</pre> context.show(statistics_gen.outputs['statistics']) In\u00a0[\u00a0]: Copied! <pre>schema_gen = tfx.components.SchemaGen(\n    statistics=statistics_gen.outputs['statistics'],\n    infer_feature_shape=False)\ncontext.run(schema_gen, enable_cache=True)\n</pre> schema_gen = tfx.components.SchemaGen(     statistics=statistics_gen.outputs['statistics'],     infer_feature_shape=False) context.run(schema_gen, enable_cache=True) <p>After <code>SchemaGen</code> finishes running, we can visualize the generated schema as a table.</p> In\u00a0[\u00a0]: Copied! <pre>context.show(schema_gen.outputs['schema'])\n</pre> context.show(schema_gen.outputs['schema']) <p>Each feature in your dataset shows up as a row in the schema table, alongside its properties. The schema also captures all the values that a categorical feature takes on, denoted as its domain.</p> <p>To learn more about schemas, see the SchemaGen documentation.</p> In\u00a0[\u00a0]: Copied! <pre>example_validator = tfx.components.ExampleValidator(\n    statistics=statistics_gen.outputs['statistics'],\n    schema=schema_gen.outputs['schema'])\ncontext.run(example_validator, enable_cache=True)\n</pre> example_validator = tfx.components.ExampleValidator(     statistics=statistics_gen.outputs['statistics'],     schema=schema_gen.outputs['schema']) context.run(example_validator, enable_cache=True) <p>After <code>ExampleValidator</code> finishes running, we can visualize the anomalies as a table.</p> In\u00a0[\u00a0]: Copied! <pre>context.show(example_validator.outputs['anomalies'])\n</pre> context.show(example_validator.outputs['anomalies']) <p>In the anomalies table, we can see that there are no anomalies. This is what we'd expect, since this the first dataset that we've analyzed and the schema is tailored to it. You should review this schema -- anything unexpected means an anomaly in the data. Once reviewed, the schema can be used to guard future data, and anomalies produced here can be used to debug model performance, understand how your data evolves over time, and identify data errors.</p> In\u00a0[\u00a0]: Copied! <pre>_taxi_constants_module_file = 'taxi_constants.py'\n</pre> _taxi_constants_module_file = 'taxi_constants.py' In\u00a0[\u00a0]: Copied! <pre>%%writefile {_taxi_constants_module_file}\n\nNUMERICAL_FEATURES = ['trip_miles', 'fare', 'trip_seconds']\n\nBUCKET_FEATURES = [\n    'pickup_latitude', 'pickup_longitude', 'dropoff_latitude',\n    'dropoff_longitude'\n]\n# Number of buckets used by tf.transform for encoding each feature.\nFEATURE_BUCKET_COUNT = 10\n\nCATEGORICAL_NUMERICAL_FEATURES = [\n    'trip_start_hour', 'trip_start_day', 'trip_start_month',\n    'pickup_census_tract', 'dropoff_census_tract', 'pickup_community_area',\n    'dropoff_community_area'\n]\n\nCATEGORICAL_STRING_FEATURES = [\n    'payment_type',\n    'company',\n]\n\n# Number of vocabulary terms used for encoding categorical features.\nVOCAB_SIZE = 1000\n\n# Count of out-of-vocab buckets in which unrecognized categorical are hashed.\nOOV_SIZE = 10\n\n# Keys\nLABEL_KEY = 'tips'\nFARE_KEY = 'fare'\n\ndef t_name(key):\n  \"\"\"\n  Rename the feature keys so that they don't clash with the raw keys when\n  running the Evaluator component.\n  Args:\n    key: The original feature key\n  Returns:\n    key with '_xf' appended\n  \"\"\"\n  return key + '_xf'\n</pre> %%writefile {_taxi_constants_module_file}  NUMERICAL_FEATURES = ['trip_miles', 'fare', 'trip_seconds']  BUCKET_FEATURES = [     'pickup_latitude', 'pickup_longitude', 'dropoff_latitude',     'dropoff_longitude' ] # Number of buckets used by tf.transform for encoding each feature. FEATURE_BUCKET_COUNT = 10  CATEGORICAL_NUMERICAL_FEATURES = [     'trip_start_hour', 'trip_start_day', 'trip_start_month',     'pickup_census_tract', 'dropoff_census_tract', 'pickup_community_area',     'dropoff_community_area' ]  CATEGORICAL_STRING_FEATURES = [     'payment_type',     'company', ]  # Number of vocabulary terms used for encoding categorical features. VOCAB_SIZE = 1000  # Count of out-of-vocab buckets in which unrecognized categorical are hashed. OOV_SIZE = 10  # Keys LABEL_KEY = 'tips' FARE_KEY = 'fare'  def t_name(key):   \"\"\"   Rename the feature keys so that they don't clash with the raw keys when   running the Evaluator component.   Args:     key: The original feature key   Returns:     key with '_xf' appended   \"\"\"   return key + '_xf' <p>Next, we write a <code>preprocessing_fn</code> that takes in raw data as input, and returns transformed features that our model can train on:</p> In\u00a0[\u00a0]: Copied! <pre>_taxi_transform_module_file = 'taxi_transform.py'\n</pre> _taxi_transform_module_file = 'taxi_transform.py' In\u00a0[\u00a0]: Copied! <pre>%%writefile {_taxi_transform_module_file}\n\nimport tensorflow as tf\nimport tensorflow_transform as tft\n\n# Imported files such as taxi_constants are normally cached, so changes are\n# not honored after the first import.  Normally this is good for efficiency, but\n# during development when we may be iterating code it can be a problem. To\n# avoid this problem during development, reload the file.\nimport taxi_constants\nimport sys\nif 'google.colab' in sys.modules:  # Testing to see if we're doing development\n  import importlib\n  importlib.reload(taxi_constants)\n\n_NUMERICAL_FEATURES = taxi_constants.NUMERICAL_FEATURES\n_BUCKET_FEATURES = taxi_constants.BUCKET_FEATURES\n_FEATURE_BUCKET_COUNT = taxi_constants.FEATURE_BUCKET_COUNT\n_CATEGORICAL_NUMERICAL_FEATURES = taxi_constants.CATEGORICAL_NUMERICAL_FEATURES\n_CATEGORICAL_STRING_FEATURES = taxi_constants.CATEGORICAL_STRING_FEATURES\n_VOCAB_SIZE = taxi_constants.VOCAB_SIZE\n_OOV_SIZE = taxi_constants.OOV_SIZE\n_FARE_KEY = taxi_constants.FARE_KEY\n_LABEL_KEY = taxi_constants.LABEL_KEY\n\n\ndef _make_one_hot(x, key):\n  \"\"\"Make a one-hot tensor to encode categorical features.\n  Args:\n    X: A dense tensor\n    key: A string key for the feature in the input\n  Returns:\n    A dense one-hot tensor as a float list\n  \"\"\"\n  integerized = tft.compute_and_apply_vocabulary(x,\n          top_k=_VOCAB_SIZE,\n          num_oov_buckets=_OOV_SIZE,\n          vocab_filename=key, name=key)\n  depth = (\n      tft.experimental.get_vocabulary_size_by_name(key) + _OOV_SIZE)\n  one_hot_encoded = tf.one_hot(\n      integerized,\n      depth=tf.cast(depth, tf.int32),\n      on_value=1.0,\n      off_value=0.0)\n  return tf.reshape(one_hot_encoded, [-1, depth])\n\n\ndef _fill_in_missing(x):\n  \"\"\"Replace missing values in a SparseTensor.\n  Fills in missing values of `x` with '' or 0, and converts to a dense tensor.\n  Args:\n    x: A `SparseTensor` of rank 2.  Its dense shape should have size at most 1\n      in the second dimension.\n  Returns:\n    A rank 1 tensor where missing values of `x` have been filled in.\n  \"\"\"\n  if not isinstance(x, tf.sparse.SparseTensor):\n    return x\n\n  default_value = '' if x.dtype == tf.string else 0\n  return tf.squeeze(\n      tf.sparse.to_dense(\n          tf.SparseTensor(x.indices, x.values, [x.dense_shape[0], 1]),\n          default_value),\n      axis=1)\n\n\ndef preprocessing_fn(inputs):\n  \"\"\"tf.transform's callback function for preprocessing inputs.\n  Args:\n    inputs: map from feature keys to raw not-yet-transformed features.\n  Returns:\n    Map from string feature key to transformed feature operations.\n  \"\"\"\n  outputs = {}\n  for key in _NUMERICAL_FEATURES:\n    # If sparse make it dense, setting nan's to 0 or '', and apply zscore.\n    outputs[taxi_constants.t_name(key)] = tft.scale_to_z_score(\n        _fill_in_missing(inputs[key]), name=key)\n\n  for key in _BUCKET_FEATURES:\n    outputs[taxi_constants.t_name(key)] = tf.cast(tft.bucketize(\n            _fill_in_missing(inputs[key]), _FEATURE_BUCKET_COUNT, name=key),\n            dtype=tf.float32)\n\n  for key in _CATEGORICAL_STRING_FEATURES:\n    outputs[taxi_constants.t_name(key)] = _make_one_hot(_fill_in_missing(inputs[key]), key)\n\n  for key in _CATEGORICAL_NUMERICAL_FEATURES:\n    outputs[taxi_constants.t_name(key)] = _make_one_hot(tf.strings.strip(\n        tf.strings.as_string(_fill_in_missing(inputs[key]))), key)\n\n  # Was this passenger a big tipper?\n  taxi_fare = _fill_in_missing(inputs[_FARE_KEY])\n  tips = _fill_in_missing(inputs[_LABEL_KEY])\n  outputs[_LABEL_KEY] = tf.where(\n      tf.math.is_nan(taxi_fare),\n      tf.cast(tf.zeros_like(taxi_fare), tf.int64),\n      # Test if the tip was &gt; 20% of the fare.\n      tf.cast(\n          tf.greater(tips, tf.multiply(taxi_fare, tf.constant(0.2))), tf.int64))\n\n  return outputs\n</pre> %%writefile {_taxi_transform_module_file}  import tensorflow as tf import tensorflow_transform as tft  # Imported files such as taxi_constants are normally cached, so changes are # not honored after the first import.  Normally this is good for efficiency, but # during development when we may be iterating code it can be a problem. To # avoid this problem during development, reload the file. import taxi_constants import sys if 'google.colab' in sys.modules:  # Testing to see if we're doing development   import importlib   importlib.reload(taxi_constants)  _NUMERICAL_FEATURES = taxi_constants.NUMERICAL_FEATURES _BUCKET_FEATURES = taxi_constants.BUCKET_FEATURES _FEATURE_BUCKET_COUNT = taxi_constants.FEATURE_BUCKET_COUNT _CATEGORICAL_NUMERICAL_FEATURES = taxi_constants.CATEGORICAL_NUMERICAL_FEATURES _CATEGORICAL_STRING_FEATURES = taxi_constants.CATEGORICAL_STRING_FEATURES _VOCAB_SIZE = taxi_constants.VOCAB_SIZE _OOV_SIZE = taxi_constants.OOV_SIZE _FARE_KEY = taxi_constants.FARE_KEY _LABEL_KEY = taxi_constants.LABEL_KEY   def _make_one_hot(x, key):   \"\"\"Make a one-hot tensor to encode categorical features.   Args:     X: A dense tensor     key: A string key for the feature in the input   Returns:     A dense one-hot tensor as a float list   \"\"\"   integerized = tft.compute_and_apply_vocabulary(x,           top_k=_VOCAB_SIZE,           num_oov_buckets=_OOV_SIZE,           vocab_filename=key, name=key)   depth = (       tft.experimental.get_vocabulary_size_by_name(key) + _OOV_SIZE)   one_hot_encoded = tf.one_hot(       integerized,       depth=tf.cast(depth, tf.int32),       on_value=1.0,       off_value=0.0)   return tf.reshape(one_hot_encoded, [-1, depth])   def _fill_in_missing(x):   \"\"\"Replace missing values in a SparseTensor.   Fills in missing values of `x` with '' or 0, and converts to a dense tensor.   Args:     x: A `SparseTensor` of rank 2.  Its dense shape should have size at most 1       in the second dimension.   Returns:     A rank 1 tensor where missing values of `x` have been filled in.   \"\"\"   if not isinstance(x, tf.sparse.SparseTensor):     return x    default_value = '' if x.dtype == tf.string else 0   return tf.squeeze(       tf.sparse.to_dense(           tf.SparseTensor(x.indices, x.values, [x.dense_shape[0], 1]),           default_value),       axis=1)   def preprocessing_fn(inputs):   \"\"\"tf.transform's callback function for preprocessing inputs.   Args:     inputs: map from feature keys to raw not-yet-transformed features.   Returns:     Map from string feature key to transformed feature operations.   \"\"\"   outputs = {}   for key in _NUMERICAL_FEATURES:     # If sparse make it dense, setting nan's to 0 or '', and apply zscore.     outputs[taxi_constants.t_name(key)] = tft.scale_to_z_score(         _fill_in_missing(inputs[key]), name=key)    for key in _BUCKET_FEATURES:     outputs[taxi_constants.t_name(key)] = tf.cast(tft.bucketize(             _fill_in_missing(inputs[key]), _FEATURE_BUCKET_COUNT, name=key),             dtype=tf.float32)    for key in _CATEGORICAL_STRING_FEATURES:     outputs[taxi_constants.t_name(key)] = _make_one_hot(_fill_in_missing(inputs[key]), key)    for key in _CATEGORICAL_NUMERICAL_FEATURES:     outputs[taxi_constants.t_name(key)] = _make_one_hot(tf.strings.strip(         tf.strings.as_string(_fill_in_missing(inputs[key]))), key)    # Was this passenger a big tipper?   taxi_fare = _fill_in_missing(inputs[_FARE_KEY])   tips = _fill_in_missing(inputs[_LABEL_KEY])   outputs[_LABEL_KEY] = tf.where(       tf.math.is_nan(taxi_fare),       tf.cast(tf.zeros_like(taxi_fare), tf.int64),       # Test if the tip was &gt; 20% of the fare.       tf.cast(           tf.greater(tips, tf.multiply(taxi_fare, tf.constant(0.2))), tf.int64))    return outputs <p>Now, we pass in this feature engineering code to the <code>Transform</code> component and run it to transform your data.</p> In\u00a0[\u00a0]: Copied! <pre>transform = tfx.components.Transform(\n    examples=example_gen.outputs['examples'],\n    schema=schema_gen.outputs['schema'],\n    module_file=os.path.abspath(_taxi_transform_module_file))\ncontext.run(transform, enable_cache=True)\n</pre> transform = tfx.components.Transform(     examples=example_gen.outputs['examples'],     schema=schema_gen.outputs['schema'],     module_file=os.path.abspath(_taxi_transform_module_file)) context.run(transform, enable_cache=True) <p>Let's examine the output artifacts of <code>Transform</code>. This component produces two types of outputs:</p> <ul> <li><code>transform_graph</code> is the graph that can perform the preprocessing operations (this graph will be included in the serving and evaluation models).</li> <li><code>transformed_examples</code> represents the preprocessed training and evaluation data.</li> </ul> In\u00a0[\u00a0]: Copied! <pre>transform.outputs\n</pre> transform.outputs <p>Take a peek at the <code>transform_graph</code> artifact.  It points to a directory containing three subdirectories.</p> In\u00a0[\u00a0]: Copied! <pre>train_uri = transform.outputs['transform_graph'].get()[0].uri\nos.listdir(train_uri)\n</pre> train_uri = transform.outputs['transform_graph'].get()[0].uri os.listdir(train_uri) <p>The <code>transformed_metadata</code> subdirectory contains the schema of the preprocessed data. The <code>transform_fn</code> subdirectory contains the actual preprocessing graph. The <code>metadata</code> subdirectory contains the schema of the original data.</p> <p>We can also take a look at the first three transformed examples:</p> In\u00a0[\u00a0]: Copied! <pre># Get the URI of the output artifact representing the transformed examples, which is a directory\ntrain_uri = os.path.join(transform.outputs['transformed_examples'].get()[0].uri, 'Split-train')\n\n# Get the list of files in this directory (all compressed TFRecord files)\ntfrecord_filenames = [os.path.join(train_uri, name)\n                      for name in os.listdir(train_uri)]\n\n# Create a `TFRecordDataset` to read these files\ndataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")\n\n# Iterate over the first 3 records and decode them.\nfor tfrecord in dataset.take(3):\n  serialized_example = tfrecord.numpy()\n  example = tf.train.Example()\n  example.ParseFromString(serialized_example)\n  pp.pprint(example)\n</pre> # Get the URI of the output artifact representing the transformed examples, which is a directory train_uri = os.path.join(transform.outputs['transformed_examples'].get()[0].uri, 'Split-train')  # Get the list of files in this directory (all compressed TFRecord files) tfrecord_filenames = [os.path.join(train_uri, name)                       for name in os.listdir(train_uri)]  # Create a `TFRecordDataset` to read these files dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")  # Iterate over the first 3 records and decode them. for tfrecord in dataset.take(3):   serialized_example = tfrecord.numpy()   example = tf.train.Example()   example.ParseFromString(serialized_example)   pp.pprint(example) <p>After the <code>Transform</code> component has transformed your data into features, and the next step is to train a model.</p> In\u00a0[\u00a0]: Copied! <pre>_taxi_trainer_module_file = 'taxi_trainer.py'\n</pre> _taxi_trainer_module_file = 'taxi_trainer.py' In\u00a0[\u00a0]: Copied! <pre>%%writefile {_taxi_trainer_module_file}\n\nfrom typing import Dict, List, Text\n\nimport os\nimport glob\nfrom absl import logging\n\nimport datetime\nimport tensorflow as tf\nimport tensorflow_transform as tft\n\nfrom tfx import v1 as tfx\nfrom tfx_bsl.public import tfxio\nfrom tensorflow_transform import TFTransformOutput\n\n# Imported files such as taxi_constants are normally cached, so changes are\n# not honored after the first import.  Normally this is good for efficiency, but\n# during development when we may be iterating code it can be a problem. To\n# avoid this problem during development, reload the file.\nimport taxi_constants\nimport sys\nif 'google.colab' in sys.modules:  # Testing to see if we're doing development\n  import importlib\n  importlib.reload(taxi_constants)\n\n_LABEL_KEY = taxi_constants.LABEL_KEY\n\n_BATCH_SIZE = 40\n\n\ndef _input_fn(file_pattern: List[Text],\n              data_accessor: tfx.components.DataAccessor,\n              tf_transform_output: tft.TFTransformOutput,\n              batch_size: int = 200) -&gt; tf.data.Dataset:\n  \"\"\"Generates features and label for tuning/training.\n\n  Args:\n    file_pattern: List of paths or patterns of input tfrecord files.\n    data_accessor: DataAccessor for converting input to RecordBatch.\n    tf_transform_output: A TFTransformOutput.\n    batch_size: representing the number of consecutive elements of returned\n      dataset to combine in a single batch\n\n  Returns:\n    A dataset that contains (features, indices) tuple where features is a\n      dictionary of Tensors, and indices is a single Tensor of label indices.\n  \"\"\"\n  return data_accessor.tf_dataset_factory(\n      file_pattern,\n      tfxio.TensorFlowDatasetOptions(\n          batch_size=batch_size, label_key=_LABEL_KEY),\n      tf_transform_output.transformed_metadata.schema)\n\ndef _get_tf_examples_serving_signature(model, tf_transform_output):\n  \"\"\"Returns a serving signature that accepts `tensorflow.Example`.\"\"\"\n\n  # We need to track the layers in the model in order to save it.\n  # TODO(b/162357359): Revise once the bug is resolved.\n  model.tft_layer_inference = tf_transform_output.transform_features_layer()\n\n  @tf.function(input_signature=[\n      tf.TensorSpec(shape=[None], dtype=tf.string, name='examples')\n  ])\n  def serve_tf_examples_fn(serialized_tf_example):\n    \"\"\"Returns the output to be used in the serving signature.\"\"\"\n    raw_feature_spec = tf_transform_output.raw_feature_spec()\n    # Remove label feature since these will not be present at serving time.\n    raw_feature_spec.pop(_LABEL_KEY)\n    raw_features = tf.io.parse_example(serialized_tf_example, raw_feature_spec)\n    transformed_features = model.tft_layer_inference(raw_features)\n    logging.info('serve_transformed_features = %s', transformed_features)\n\n    outputs = model(transformed_features)\n    # TODO(b/154085620): Convert the predicted labels from the model using a\n    # reverse-lookup (opposite of transform.py).\n    return {'outputs': outputs}\n\n  return serve_tf_examples_fn\n\n\ndef _get_transform_features_signature(model, tf_transform_output):\n  \"\"\"Returns a serving signature that applies tf.Transform to features.\"\"\"\n\n  # We need to track the layers in the model in order to save it.\n  # TODO(b/162357359): Revise once the bug is resolved.\n  model.tft_layer_eval = tf_transform_output.transform_features_layer()\n\n  @tf.function(input_signature=[\n      tf.TensorSpec(shape=[None], dtype=tf.string, name='examples')\n  ])\n  def transform_features_fn(serialized_tf_example):\n    \"\"\"Returns the transformed_features to be fed as input to evaluator.\"\"\"\n    raw_feature_spec = tf_transform_output.raw_feature_spec()\n    raw_features = tf.io.parse_example(serialized_tf_example, raw_feature_spec)\n    transformed_features = model.tft_layer_eval(raw_features)\n    logging.info('eval_transformed_features = %s', transformed_features)\n    return transformed_features\n\n  return transform_features_fn\n\n\ndef export_serving_model(tf_transform_output, model, output_dir):\n  \"\"\"Exports a keras model for serving.\n  Args:\n    tf_transform_output: Wrapper around output of tf.Transform.\n    model: A keras model to export for serving.\n    output_dir: A directory where the model will be exported to.\n  \"\"\"\n  # The layer has to be saved to the model for keras tracking purpases.\n  model.tft_layer = tf_transform_output.transform_features_layer()\n\n  signatures = {\n      'serving_default':\n          _get_tf_examples_serving_signature(model, tf_transform_output),\n      'transform_features':\n          _get_transform_features_signature(model, tf_transform_output),\n  }\n\n  model.save(output_dir, save_format='tf', signatures=signatures)\n\n\ndef _build_keras_model(tf_transform_output: TFTransformOutput\n                       ) -&gt; tf.keras.Model:\n  \"\"\"Creates a DNN Keras model for classifying taxi data.\n\n  Args:\n    tf_transform_output: [TFTransformOutput], the outputs from Transform\n\n  Returns:\n    A keras Model.\n  \"\"\"\n  feature_spec = tf_transform_output.transformed_feature_spec().copy()\n  feature_spec.pop(_LABEL_KEY)\n\n  inputs = {}\n  for key, spec in feature_spec.items():\n    if isinstance(spec, tf.io.VarLenFeature):\n      inputs[key] = tf.keras.layers.Input(\n          shape=[None], name=key, dtype=spec.dtype, sparse=True)\n    elif isinstance(spec, tf.io.FixedLenFeature):\n      # TODO(b/208879020): Move into schema such that spec.shape is [1] and not\n      # [] for scalars.\n      inputs[key] = tf.keras.layers.Input(\n          shape=spec.shape or [1], name=key, dtype=spec.dtype)\n    else:\n      raise ValueError('Spec type is not supported: ', key, spec)\n\n  output = tf.keras.layers.Concatenate()(tf.nest.flatten(inputs))\n  output = tf.keras.layers.Dense(100, activation='relu')(output)\n  output = tf.keras.layers.Dense(70, activation='relu')(output)\n  output = tf.keras.layers.Dense(50, activation='relu')(output)\n  output = tf.keras.layers.Dense(20, activation='relu')(output)\n  output = tf.keras.layers.Dense(1)(output)\n  return tf.keras.Model(inputs=inputs, outputs=output)\n\n\n# TFX Trainer will call this function.\ndef run_fn(fn_args: tfx.components.FnArgs):\n  \"\"\"Train the model based on given args.\n\n  Args:\n    fn_args: Holds args used to train the model as name/value pairs.\n  \"\"\"\n  tf_transform_output = tft.TFTransformOutput(fn_args.transform_output)\n\n  train_dataset = _input_fn(fn_args.train_files, fn_args.data_accessor,\n                            tf_transform_output, _BATCH_SIZE)\n  eval_dataset = _input_fn(fn_args.eval_files, fn_args.data_accessor,\n                           tf_transform_output, _BATCH_SIZE)\n\n  model = _build_keras_model(tf_transform_output)\n\n  model.compile(\n      loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n      optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n      metrics=[tf.keras.metrics.BinaryAccuracy()])\n\n  tensorboard_callback = tf.keras.callbacks.TensorBoard(\n      log_dir=fn_args.model_run_dir, update_freq='batch')\n\n  model.fit(\n      train_dataset,\n      steps_per_epoch=fn_args.train_steps,\n      validation_data=eval_dataset,\n      validation_steps=fn_args.eval_steps,\n      callbacks=[tensorboard_callback])\n\n  # Export the model.\n  export_serving_model(tf_transform_output, model, fn_args.serving_model_dir)\n</pre> %%writefile {_taxi_trainer_module_file}  from typing import Dict, List, Text  import os import glob from absl import logging  import datetime import tensorflow as tf import tensorflow_transform as tft  from tfx import v1 as tfx from tfx_bsl.public import tfxio from tensorflow_transform import TFTransformOutput  # Imported files such as taxi_constants are normally cached, so changes are # not honored after the first import.  Normally this is good for efficiency, but # during development when we may be iterating code it can be a problem. To # avoid this problem during development, reload the file. import taxi_constants import sys if 'google.colab' in sys.modules:  # Testing to see if we're doing development   import importlib   importlib.reload(taxi_constants)  _LABEL_KEY = taxi_constants.LABEL_KEY  _BATCH_SIZE = 40   def _input_fn(file_pattern: List[Text],               data_accessor: tfx.components.DataAccessor,               tf_transform_output: tft.TFTransformOutput,               batch_size: int = 200) -&gt; tf.data.Dataset:   \"\"\"Generates features and label for tuning/training.    Args:     file_pattern: List of paths or patterns of input tfrecord files.     data_accessor: DataAccessor for converting input to RecordBatch.     tf_transform_output: A TFTransformOutput.     batch_size: representing the number of consecutive elements of returned       dataset to combine in a single batch    Returns:     A dataset that contains (features, indices) tuple where features is a       dictionary of Tensors, and indices is a single Tensor of label indices.   \"\"\"   return data_accessor.tf_dataset_factory(       file_pattern,       tfxio.TensorFlowDatasetOptions(           batch_size=batch_size, label_key=_LABEL_KEY),       tf_transform_output.transformed_metadata.schema)  def _get_tf_examples_serving_signature(model, tf_transform_output):   \"\"\"Returns a serving signature that accepts `tensorflow.Example`.\"\"\"    # We need to track the layers in the model in order to save it.   # TODO(b/162357359): Revise once the bug is resolved.   model.tft_layer_inference = tf_transform_output.transform_features_layer()    @tf.function(input_signature=[       tf.TensorSpec(shape=[None], dtype=tf.string, name='examples')   ])   def serve_tf_examples_fn(serialized_tf_example):     \"\"\"Returns the output to be used in the serving signature.\"\"\"     raw_feature_spec = tf_transform_output.raw_feature_spec()     # Remove label feature since these will not be present at serving time.     raw_feature_spec.pop(_LABEL_KEY)     raw_features = tf.io.parse_example(serialized_tf_example, raw_feature_spec)     transformed_features = model.tft_layer_inference(raw_features)     logging.info('serve_transformed_features = %s', transformed_features)      outputs = model(transformed_features)     # TODO(b/154085620): Convert the predicted labels from the model using a     # reverse-lookup (opposite of transform.py).     return {'outputs': outputs}    return serve_tf_examples_fn   def _get_transform_features_signature(model, tf_transform_output):   \"\"\"Returns a serving signature that applies tf.Transform to features.\"\"\"    # We need to track the layers in the model in order to save it.   # TODO(b/162357359): Revise once the bug is resolved.   model.tft_layer_eval = tf_transform_output.transform_features_layer()    @tf.function(input_signature=[       tf.TensorSpec(shape=[None], dtype=tf.string, name='examples')   ])   def transform_features_fn(serialized_tf_example):     \"\"\"Returns the transformed_features to be fed as input to evaluator.\"\"\"     raw_feature_spec = tf_transform_output.raw_feature_spec()     raw_features = tf.io.parse_example(serialized_tf_example, raw_feature_spec)     transformed_features = model.tft_layer_eval(raw_features)     logging.info('eval_transformed_features = %s', transformed_features)     return transformed_features    return transform_features_fn   def export_serving_model(tf_transform_output, model, output_dir):   \"\"\"Exports a keras model for serving.   Args:     tf_transform_output: Wrapper around output of tf.Transform.     model: A keras model to export for serving.     output_dir: A directory where the model will be exported to.   \"\"\"   # The layer has to be saved to the model for keras tracking purpases.   model.tft_layer = tf_transform_output.transform_features_layer()    signatures = {       'serving_default':           _get_tf_examples_serving_signature(model, tf_transform_output),       'transform_features':           _get_transform_features_signature(model, tf_transform_output),   }    model.save(output_dir, save_format='tf', signatures=signatures)   def _build_keras_model(tf_transform_output: TFTransformOutput                        ) -&gt; tf.keras.Model:   \"\"\"Creates a DNN Keras model for classifying taxi data.    Args:     tf_transform_output: [TFTransformOutput], the outputs from Transform    Returns:     A keras Model.   \"\"\"   feature_spec = tf_transform_output.transformed_feature_spec().copy()   feature_spec.pop(_LABEL_KEY)    inputs = {}   for key, spec in feature_spec.items():     if isinstance(spec, tf.io.VarLenFeature):       inputs[key] = tf.keras.layers.Input(           shape=[None], name=key, dtype=spec.dtype, sparse=True)     elif isinstance(spec, tf.io.FixedLenFeature):       # TODO(b/208879020): Move into schema such that spec.shape is [1] and not       # [] for scalars.       inputs[key] = tf.keras.layers.Input(           shape=spec.shape or [1], name=key, dtype=spec.dtype)     else:       raise ValueError('Spec type is not supported: ', key, spec)    output = tf.keras.layers.Concatenate()(tf.nest.flatten(inputs))   output = tf.keras.layers.Dense(100, activation='relu')(output)   output = tf.keras.layers.Dense(70, activation='relu')(output)   output = tf.keras.layers.Dense(50, activation='relu')(output)   output = tf.keras.layers.Dense(20, activation='relu')(output)   output = tf.keras.layers.Dense(1)(output)   return tf.keras.Model(inputs=inputs, outputs=output)   # TFX Trainer will call this function. def run_fn(fn_args: tfx.components.FnArgs):   \"\"\"Train the model based on given args.    Args:     fn_args: Holds args used to train the model as name/value pairs.   \"\"\"   tf_transform_output = tft.TFTransformOutput(fn_args.transform_output)    train_dataset = _input_fn(fn_args.train_files, fn_args.data_accessor,                             tf_transform_output, _BATCH_SIZE)   eval_dataset = _input_fn(fn_args.eval_files, fn_args.data_accessor,                            tf_transform_output, _BATCH_SIZE)    model = _build_keras_model(tf_transform_output)    model.compile(       loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),       optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),       metrics=[tf.keras.metrics.BinaryAccuracy()])    tensorboard_callback = tf.keras.callbacks.TensorBoard(       log_dir=fn_args.model_run_dir, update_freq='batch')    model.fit(       train_dataset,       steps_per_epoch=fn_args.train_steps,       validation_data=eval_dataset,       validation_steps=fn_args.eval_steps,       callbacks=[tensorboard_callback])    # Export the model.   export_serving_model(tf_transform_output, model, fn_args.serving_model_dir) <p>Now, we pass in this model code to the <code>Trainer</code> component and run it to train the model.</p> In\u00a0[\u00a0]: Copied! <pre>trainer = tfx.components.Trainer(\n    module_file=os.path.abspath(_taxi_trainer_module_file),\n    examples=transform.outputs['transformed_examples'],\n    transform_graph=transform.outputs['transform_graph'],\n    schema=schema_gen.outputs['schema'],\n    train_args=tfx.proto.TrainArgs(num_steps=10000),\n    eval_args=tfx.proto.EvalArgs(num_steps=5000))\ncontext.run(trainer, enable_cache=True)\n</pre> trainer = tfx.components.Trainer(     module_file=os.path.abspath(_taxi_trainer_module_file),     examples=transform.outputs['transformed_examples'],     transform_graph=transform.outputs['transform_graph'],     schema=schema_gen.outputs['schema'],     train_args=tfx.proto.TrainArgs(num_steps=10000),     eval_args=tfx.proto.EvalArgs(num_steps=5000)) context.run(trainer, enable_cache=True) In\u00a0[\u00a0]: Copied! <pre>model_artifact_dir = trainer.outputs['model'].get()[0].uri\npp.pprint(os.listdir(model_artifact_dir))\nmodel_dir = os.path.join(model_artifact_dir, 'Format-Serving')\npp.pprint(os.listdir(model_dir))\n</pre> model_artifact_dir = trainer.outputs['model'].get()[0].uri pp.pprint(os.listdir(model_artifact_dir)) model_dir = os.path.join(model_artifact_dir, 'Format-Serving') pp.pprint(os.listdir(model_dir)) <p>Optionally, we can connect TensorBoard to the Trainer to analyze our model's training curves.</p> In\u00a0[\u00a0]: Copied! <pre>model_run_artifact_dir = trainer.outputs['model_run'].get()[0].uri\n\n%load_ext tensorboard\n%tensorboard --logdir {model_run_artifact_dir}\n</pre> model_run_artifact_dir = trainer.outputs['model_run'].get()[0].uri  %load_ext tensorboard %tensorboard --logdir {model_run_artifact_dir} In\u00a0[\u00a0]: Copied! <pre># Imported files such as taxi_constants are normally cached, so changes are\n# not honored after the first import.  Normally this is good for efficiency, but\n# during development when we may be iterating code it can be a problem. To\n# avoid this problem during development, reload the file.\nimport taxi_constants\nimport sys\nif 'google.colab' in sys.modules:  # Testing to see if we're doing development\n  import importlib\n  importlib.reload(taxi_constants)\n\neval_config = tfma.EvalConfig(\n    model_specs=[\n        # This assumes a serving model with signature 'serving_default'. If\n        # using estimator based EvalSavedModel, add signature_name: 'eval' and\n        # remove the label_key.\n        tfma.ModelSpec(\n            signature_name='serving_default',\n            label_key=taxi_constants.LABEL_KEY,\n            preprocessing_function_names=['transform_features'],\n            )\n        ],\n    metrics_specs=[\n        tfma.MetricsSpec(\n            # The metrics added here are in addition to those saved with the\n            # model (assuming either a keras model or EvalSavedModel is used).\n            # Any metrics added into the saved model (for example using\n            # model.compile(..., metrics=[...]), etc) will be computed\n            # automatically.\n            # To add validation thresholds for metrics saved with the model,\n            # add them keyed by metric name to the thresholds map.\n            metrics=[\n                tfma.MetricConfig(class_name='ExampleCount'),\n                tfma.MetricConfig(class_name='BinaryAccuracy',\n                  threshold=tfma.MetricThreshold(\n                      value_threshold=tfma.GenericValueThreshold(\n                          lower_bound={'value': 0.5}),\n                      # Change threshold will be ignored if there is no\n                      # baseline model resolved from MLMD (first run).\n                      change_threshold=tfma.GenericChangeThreshold(\n                          direction=tfma.MetricDirection.HIGHER_IS_BETTER,\n                          absolute={'value': -1e-10})))\n            ]\n        )\n    ],\n    slicing_specs=[\n        # An empty slice spec means the overall slice, i.e. the whole dataset.\n        tfma.SlicingSpec(),\n        # Data can be sliced along a feature column. In this case, data is\n        # sliced along feature column trip_start_hour.\n        tfma.SlicingSpec(\n            feature_keys=['trip_start_hour'])\n    ])\n</pre> # Imported files such as taxi_constants are normally cached, so changes are # not honored after the first import.  Normally this is good for efficiency, but # during development when we may be iterating code it can be a problem. To # avoid this problem during development, reload the file. import taxi_constants import sys if 'google.colab' in sys.modules:  # Testing to see if we're doing development   import importlib   importlib.reload(taxi_constants)  eval_config = tfma.EvalConfig(     model_specs=[         # This assumes a serving model with signature 'serving_default'. If         # using estimator based EvalSavedModel, add signature_name: 'eval' and         # remove the label_key.         tfma.ModelSpec(             signature_name='serving_default',             label_key=taxi_constants.LABEL_KEY,             preprocessing_function_names=['transform_features'],             )         ],     metrics_specs=[         tfma.MetricsSpec(             # The metrics added here are in addition to those saved with the             # model (assuming either a keras model or EvalSavedModel is used).             # Any metrics added into the saved model (for example using             # model.compile(..., metrics=[...]), etc) will be computed             # automatically.             # To add validation thresholds for metrics saved with the model,             # add them keyed by metric name to the thresholds map.             metrics=[                 tfma.MetricConfig(class_name='ExampleCount'),                 tfma.MetricConfig(class_name='BinaryAccuracy',                   threshold=tfma.MetricThreshold(                       value_threshold=tfma.GenericValueThreshold(                           lower_bound={'value': 0.5}),                       # Change threshold will be ignored if there is no                       # baseline model resolved from MLMD (first run).                       change_threshold=tfma.GenericChangeThreshold(                           direction=tfma.MetricDirection.HIGHER_IS_BETTER,                           absolute={'value': -1e-10})))             ]         )     ],     slicing_specs=[         # An empty slice spec means the overall slice, i.e. the whole dataset.         tfma.SlicingSpec(),         # Data can be sliced along a feature column. In this case, data is         # sliced along feature column trip_start_hour.         tfma.SlicingSpec(             feature_keys=['trip_start_hour'])     ]) <p>Next, we give this configuration to <code>Evaluator</code> and run it.</p> In\u00a0[\u00a0]: Copied! <pre># Use TFMA to compute a evaluation statistics over features of a model and\n# validate them against a baseline.\n\n# The model resolver is only required if performing model validation in addition\n# to evaluation. In this case we validate against the latest blessed model. If\n# no model has been blessed before (as in this case) the evaluator will make our\n# candidate the first blessed model.\nmodel_resolver = tfx.dsl.Resolver(\n      strategy_class=tfx.dsl.experimental.LatestBlessedModelStrategy,\n      model=tfx.dsl.Channel(type=tfx.types.standard_artifacts.Model),\n      model_blessing=tfx.dsl.Channel(\n          type=tfx.types.standard_artifacts.ModelBlessing)).with_id(\n              'latest_blessed_model_resolver')\ncontext.run(model_resolver, enable_cache=True)\n\nevaluator = tfx.components.Evaluator(\n    examples=example_gen.outputs['examples'],\n    model=trainer.outputs['model'],\n    baseline_model=model_resolver.outputs['model'],\n    eval_config=eval_config)\ncontext.run(evaluator, enable_cache=True)\n</pre> # Use TFMA to compute a evaluation statistics over features of a model and # validate them against a baseline.  # The model resolver is only required if performing model validation in addition # to evaluation. In this case we validate against the latest blessed model. If # no model has been blessed before (as in this case) the evaluator will make our # candidate the first blessed model. model_resolver = tfx.dsl.Resolver(       strategy_class=tfx.dsl.experimental.LatestBlessedModelStrategy,       model=tfx.dsl.Channel(type=tfx.types.standard_artifacts.Model),       model_blessing=tfx.dsl.Channel(           type=tfx.types.standard_artifacts.ModelBlessing)).with_id(               'latest_blessed_model_resolver') context.run(model_resolver, enable_cache=True)  evaluator = tfx.components.Evaluator(     examples=example_gen.outputs['examples'],     model=trainer.outputs['model'],     baseline_model=model_resolver.outputs['model'],     eval_config=eval_config) context.run(evaluator, enable_cache=True) <p>Now let's examine the output artifacts of <code>Evaluator</code>.</p> In\u00a0[\u00a0]: Copied! <pre>evaluator.outputs\n</pre> evaluator.outputs <p>Using the <code>evaluation</code> output we can show the default visualization of global metrics on the entire evaluation set.</p> In\u00a0[\u00a0]: Copied! <pre>context.show(evaluator.outputs['evaluation'])\n</pre> context.show(evaluator.outputs['evaluation']) <p>To see the visualization for sliced evaluation metrics, we can directly call the TensorFlow Model Analysis library.</p> In\u00a0[\u00a0]: Copied! <pre>import tensorflow_model_analysis as tfma\n\n# Get the TFMA output result path and load the result.\nPATH_TO_RESULT = evaluator.outputs['evaluation'].get()[0].uri\ntfma_result = tfma.load_eval_result(PATH_TO_RESULT)\n\n# Show data sliced along feature column trip_start_hour.\ntfma.view.render_slicing_metrics(\n    tfma_result, slicing_column='trip_start_hour')\n</pre> import tensorflow_model_analysis as tfma  # Get the TFMA output result path and load the result. PATH_TO_RESULT = evaluator.outputs['evaluation'].get()[0].uri tfma_result = tfma.load_eval_result(PATH_TO_RESULT)  # Show data sliced along feature column trip_start_hour. tfma.view.render_slicing_metrics(     tfma_result, slicing_column='trip_start_hour') <p>This visualization shows the same metrics, but computed at every feature value of <code>trip_start_hour</code> instead of on the entire evaluation set.</p> <p>TensorFlow Model Analysis supports many other visualizations, such as Fairness Indicators and plotting a time series of model performance. To learn more, see the tutorial.</p> <p>Since we added thresholds to our config, validation output is also available. The precence of a <code>blessing</code> artifact indicates that our model passed validation. Since this is the first validation being performed the candidate is automatically blessed.</p> In\u00a0[\u00a0]: Copied! <pre>blessing_uri = evaluator.outputs['blessing'].get()[0].uri\n!ls -l {blessing_uri}\n</pre> blessing_uri = evaluator.outputs['blessing'].get()[0].uri !ls -l {blessing_uri} <p>Now can also verify the success by loading the validation result record:</p> In\u00a0[\u00a0]: Copied! <pre>PATH_TO_RESULT = evaluator.outputs['evaluation'].get()[0].uri\nprint(tfma.load_validation_result(PATH_TO_RESULT))\n</pre> PATH_TO_RESULT = evaluator.outputs['evaluation'].get()[0].uri print(tfma.load_validation_result(PATH_TO_RESULT)) In\u00a0[\u00a0]: Copied! <pre>pusher = tfx.components.Pusher(\n    model=trainer.outputs['model'],\n    model_blessing=evaluator.outputs['blessing'],\n    push_destination=tfx.proto.PushDestination(\n        filesystem=tfx.proto.PushDestination.Filesystem(\n            base_directory=_serving_model_dir)))\ncontext.run(pusher, enable_cache=True)\n</pre> pusher = tfx.components.Pusher(     model=trainer.outputs['model'],     model_blessing=evaluator.outputs['blessing'],     push_destination=tfx.proto.PushDestination(         filesystem=tfx.proto.PushDestination.Filesystem(             base_directory=_serving_model_dir))) context.run(pusher, enable_cache=True) <p>Let's examine the output artifacts of <code>Pusher</code>.</p> In\u00a0[\u00a0]: Copied! <pre>pusher.outputs\n</pre> pusher.outputs <p>In particular, the Pusher will export your model in the SavedModel format, which looks like this:</p> In\u00a0[\u00a0]: Copied! <pre>push_uri = pusher.outputs['pushed_model'].get()[0].uri\nmodel = tf.saved_model.load(push_uri)\n\nfor item in model.signatures.items():\n  pp.pprint(item)\n</pre> push_uri = pusher.outputs['pushed_model'].get()[0].uri model = tf.saved_model.load(push_uri)  for item in model.signatures.items():   pp.pprint(item) <p>We're finished our tour of built-in TFX components!</p>"},{"location":"tutorials/tfx/components_keras/#copyright-2021-the-tensorflow-authors","title":"Copyright 2021 The TensorFlow Authors.\u00b6","text":""},{"location":"tutorials/tfx/components_keras/#tfx-keras-component-tutorial","title":"TFX Keras Component Tutorial\u00b6","text":"<p>A Component-by-Component Introduction to TensorFlow Extended (TFX)</p>"},{"location":"tutorials/tfx/components_keras/#background","title":"Background\u00b6","text":"<p>This notebook demonstrates how to use TFX in a Jupyter/Colab environment.  Here, we walk through the Chicago Taxi example in an interactive notebook.</p> <p>Working in an interactive notebook is a useful way to become familiar with the structure of a TFX pipeline.  It's also useful when doing development of your own pipelines as a lightweight development environment, but you should be aware that there are differences in the way interactive notebooks are orchestrated, and how they access metadata artifacts.</p>"},{"location":"tutorials/tfx/components_keras/#orchestration","title":"Orchestration\u00b6","text":"<p>In a production deployment of TFX, you will use an orchestrator such as Apache Airflow, Kubeflow Pipelines, or Apache Beam to orchestrate a pre-defined pipeline graph of TFX components.  In an interactive notebook, the notebook itself is the orchestrator, running each TFX component as you execute the notebook cells.</p>"},{"location":"tutorials/tfx/components_keras/#metadata","title":"Metadata\u00b6","text":"<p>In a production deployment of TFX, you will access metadata through the ML Metadata (MLMD) API.  MLMD stores metadata properties in a database such as MySQL or SQLite, and stores the metadata payloads in a persistent store such as on your filesystem.  In an interactive notebook, both properties and payloads are stored in an ephemeral SQLite database in the <code>/tmp</code> directory on the Jupyter notebook or Colab server.</p>"},{"location":"tutorials/tfx/components_keras/#setup","title":"Setup\u00b6","text":"<p>First, we install and import the necessary packages, set up paths, and download data.</p>"},{"location":"tutorials/tfx/components_keras/#upgrade-pip","title":"Upgrade Pip\u00b6","text":"<p>To avoid upgrading Pip in a system when running locally, check to make sure that we're running in Colab.  Local systems can of course be upgraded separately.</p>"},{"location":"tutorials/tfx/components_keras/#install-tfx","title":"Install TFX\u00b6","text":"<p>Note: In Google Colab, because of package updates, the first time you run this cell you must restart the runtime (Runtime &gt; Restart runtime ...).</p>"},{"location":"tutorials/tfx/components_keras/#did-you-restart-the-runtime","title":"Did you restart the runtime?\u00b6","text":"<p>If you are using Google Colab, the first time that you run the cell above, you must restart the runtime (Runtime &gt; Restart runtime ...). This is because of the way that Colab loads packages.</p>"},{"location":"tutorials/tfx/components_keras/#import-packages","title":"Import packages\u00b6","text":"<p>We import necessary packages, including standard TFX component classes.</p>"},{"location":"tutorials/tfx/components_keras/#set-up-pipeline-paths","title":"Set up pipeline paths\u00b6","text":""},{"location":"tutorials/tfx/components_keras/#download-example-data","title":"Download example data\u00b6","text":"<p>We download the example dataset for use in our TFX pipeline.</p> <p>The dataset we're using is the Taxi Trips dataset released by the City of Chicago. The columns in this dataset are:</p> pickup_community_areafaretrip_start_month trip_start_hourtrip_start_daytrip_start_timestamp pickup_latitudepickup_longitudedropoff_latitude dropoff_longitudetrip_milespickup_census_tract dropoff_census_tractpayment_typecompany trip_secondsdropoff_community_areatips <p>With this dataset, we will build a model that predicts the <code>tips</code> of a trip.</p>"},{"location":"tutorials/tfx/components_keras/#create-the-interactivecontext","title":"Create the InteractiveContext\u00b6","text":"<p>Last, we create an InteractiveContext, which will allow us to run TFX components interactively in this notebook.</p>"},{"location":"tutorials/tfx/components_keras/#run-tfx-components-interactively","title":"Run TFX components interactively\u00b6","text":"<p>In the cells that follow, we create TFX components one-by-one, run each of them, and visualize their output artifacts.</p>"},{"location":"tutorials/tfx/components_keras/#examplegen","title":"ExampleGen\u00b6","text":"<p>The <code>ExampleGen</code> component is usually at the start of a TFX pipeline. It will:</p> <ol> <li>Split data into training and evaluation sets (by default, 2/3 training + 1/3 eval)</li> <li>Convert data into the <code>tf.Example</code> format (learn more here)</li> <li>Copy data into the <code>_tfx_root</code> directory for other components to access</li> </ol> <p><code>ExampleGen</code> takes as input the path to your data source. In our case, this is the <code>_data_root</code> path that contains the downloaded CSV.</p> <p>Note: In this notebook, we can instantiate components one-by-one and run them with <code>InteractiveContext.run()</code>. By contrast, in a production setting, we would specify all the components upfront in a <code>Pipeline</code> to pass to the orchestrator (see the Building a TFX Pipeline Guide).</p>"},{"location":"tutorials/tfx/components_keras/#enabling-the-cache","title":"Enabling the Cache\u00b6","text":"<p>When using the <code>InteractiveContext</code> in a notebook to develop a pipeline you can control when individual components will cache their outputs.  Set <code>enable_cache</code> to <code>True</code> when you want to reuse the previous output artifacts that the component generated.  Set <code>enable_cache</code> to <code>False</code> when you want to recompute the output artifacts for a component, if you are making changes to the code for example.</p>"},{"location":"tutorials/tfx/components_keras/#statisticsgen","title":"StatisticsGen\u00b6","text":"<p>The <code>StatisticsGen</code> component computes statistics over your dataset for data analysis, as well as for use in downstream components. It uses the TensorFlow Data Validation library.</p> <p><code>StatisticsGen</code> takes as input the dataset we just ingested using <code>ExampleGen</code>.</p>"},{"location":"tutorials/tfx/components_keras/#schemagen","title":"SchemaGen\u00b6","text":"<p>The <code>SchemaGen</code> component generates a schema based on your data statistics. (A schema defines the expected bounds, types, and properties of the features in your dataset.) It also uses the TensorFlow Data Validation library.</p> <p>Note: The generated schema is best-effort and only tries to infer basic properties of the data. It is expected that you review and modify it as needed.</p> <p><code>SchemaGen</code> will take as input the statistics that we generated with <code>StatisticsGen</code>, looking at the training split by default.</p>"},{"location":"tutorials/tfx/components_keras/#examplevalidator","title":"ExampleValidator\u00b6","text":"<p>The <code>ExampleValidator</code> component detects anomalies in your data, based on the expectations defined by the schema. It also uses the TensorFlow Data Validation library.</p> <p><code>ExampleValidator</code> will take as input the statistics from <code>StatisticsGen</code>, and the schema from <code>SchemaGen</code>.</p>"},{"location":"tutorials/tfx/components_keras/#transform","title":"Transform\u00b6","text":"<p>The <code>Transform</code> component performs feature engineering for both training and serving. It uses the TensorFlow Transform library.</p> <p><code>Transform</code> will take as input the data from <code>ExampleGen</code>, the schema from <code>SchemaGen</code>, as well as a module that contains user-defined Transform code.</p> <p>Let's see an example of user-defined Transform code below (for an introduction to the TensorFlow Transform APIs, see the tutorial). First, we define a few constants for feature engineering:</p> <p>Note: The <code>%%writefile</code> cell magic will save the contents of the cell as a <code>.py</code> file on disk. This allows the <code>Transform</code> component to load your code as a module.</p>"},{"location":"tutorials/tfx/components_keras/#trainer","title":"Trainer\u00b6","text":"<p>The <code>Trainer</code> component will train a model that you define in TensorFlow.</p> <p><code>Trainer</code> takes as input the schema from <code>SchemaGen</code>, the transformed data and graph from <code>Transform</code>, training parameters, as well as a module that contains user-defined model code.</p> <p>Let's see an example of user-defined model code below (for an introduction to the TensorFlow Keras APIs, see the tutorial):</p>"},{"location":"tutorials/tfx/components_keras/#analyze-training-with-tensorboard","title":"Analyze Training with TensorBoard\u00b6","text":"<p>Take a peek at the trainer artifact. It points to a directory containing the model subdirectories.</p>"},{"location":"tutorials/tfx/components_keras/#evaluator","title":"Evaluator\u00b6","text":"<p>The <code>Evaluator</code> component computes model performance metrics over the evaluation set. It uses the TensorFlow Model Analysis library. The <code>Evaluator</code> can also optionally validate that a newly trained model is better than the previous model. This is useful in a production pipeline setting where you may automatically train and validate a model every day. In this notebook, we only train one model, so the <code>Evaluator</code> automatically will label the model as \"good\".</p> <p><code>Evaluator</code> will take as input the data from <code>ExampleGen</code>, the trained model from <code>Trainer</code>, and slicing configuration. The slicing configuration allows you to slice your metrics on feature values (e.g. how does your model perform on taxi trips that start at 8am versus 8pm?). See an example of this configuration below:</p>"},{"location":"tutorials/tfx/components_keras/#pusher","title":"Pusher\u00b6","text":"<p>The <code>Pusher</code> component is usually at the end of a TFX pipeline. It checks whether a model has passed validation, and if so, exports the model to <code>_serving_model_dir</code>.</p>"},{"location":"tutorials/tfx/gpt2_finetuning_and_conversion/","title":"Licensed under the Apache License, Version 2.0 (the \"License\");","text":"In\u00a0[\u00a0]: Copied! <pre># you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n</pre> # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # https://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. <p>Note: We recommend running this tutorial in a Colab notebook, with no setup required!  Just click \"Run in Google Colab\".</p>        View on TensorFlow.org             Run in Google Colab             View source on GitHub             Download notebook      In\u00a0[\u00a0]: Copied! <pre>try:\n  import colab\n  !pip install --upgrade pip\n\nexcept:\n  pass\n</pre> try:   import colab   !pip install --upgrade pip  except:   pass In\u00a0[\u00a0]: Copied! <pre>!pip install -q tfx tensorflow-text more_itertools tensorflow_datasets\n!pip install -q --upgrade keras-nlp\n!pip install -q --upgrade keras\n</pre> !pip install -q tfx tensorflow-text more_itertools tensorflow_datasets !pip install -q --upgrade keras-nlp !pip install -q --upgrade keras <p>Note: pip's dependency resolver errors can be ignored. The required packages for this tutorial works as expected.</p> In\u00a0[\u00a0]: Copied! <pre>import os\nos.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n\nimport tensorflow as tf\nprint('TensorFlow version: {}'.format(tf.__version__))\nfrom tfx import v1 as tfx\nprint('TFX version: {}'.format(tfx.__version__))\nimport keras\nprint('Keras version: {}'.format(keras.__version__))\nimport keras_nlp\nprint('Keras NLP version: {}'.format(keras_nlp.__version__))\n\nkeras.mixed_precision.set_global_policy(\"mixed_float16\")\n</pre> import os os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"  import tensorflow as tf print('TensorFlow version: {}'.format(tf.__version__)) from tfx import v1 as tfx print('TFX version: {}'.format(tfx.__version__)) import keras print('Keras version: {}'.format(keras.__version__)) import keras_nlp print('Keras NLP version: {}'.format(keras_nlp.__version__))  keras.mixed_precision.set_global_policy(\"mixed_float16\") <p>An interactive context is used to provide global context when running a TFX pipeline in a notebook without using a runner or orchestrator such as Apache Airflow or Kubeflow. This style of development is only useful when developing the code for a pipeline, and cannot currently be used to deploy a working pipeline to production.</p> In\u00a0[\u00a0]: Copied! <pre>from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\ncontext = InteractiveContext()\n</pre> from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext context = InteractiveContext() In\u00a0[\u00a0]: Copied! <pre>from typing import Any, Dict, List, Text\nimport tensorflow_datasets as tfds\nimport apache_beam as beam\nimport json\nfrom tfx.components.example_gen.base_example_gen_executor import BaseExampleGenExecutor\nfrom tfx.components.example_gen.component import FileBasedExampleGen\nfrom tfx.components.example_gen import utils\nfrom tfx.dsl.components.base import executor_spec\nimport os\nimport pprint\npp = pprint.PrettyPrinter()\n</pre> from typing import Any, Dict, List, Text import tensorflow_datasets as tfds import apache_beam as beam import json from tfx.components.example_gen.base_example_gen_executor import BaseExampleGenExecutor from tfx.components.example_gen.component import FileBasedExampleGen from tfx.components.example_gen import utils from tfx.dsl.components.base import executor_spec import os import pprint pp = pprint.PrettyPrinter() In\u00a0[\u00a0]: Copied! <pre>@beam.ptransform_fn\n@beam.typehints.with_input_types(beam.Pipeline)\n@beam.typehints.with_output_types(tf.train.Example)\ndef _TFDatasetToExample(\n    pipeline: beam.Pipeline,\n    exec_properties: Dict[str, Any],\n    split_pattern: str\n    ) -&gt; beam.pvalue.PCollection:\n    \"\"\"Read a TensorFlow Dataset and create tf.Examples\"\"\"\n    custom_config = json.loads(exec_properties['custom_config'])\n    dataset_name = custom_config['dataset']\n    split_name = custom_config['split']\n\n    builder = tfds.builder(dataset_name)\n    builder.download_and_prepare()\n\n    return (pipeline\n            | 'MakeExamples' &gt;&gt; tfds.beam.ReadFromTFDS(builder, split=split_name)\n            | 'AsNumpy' &gt;&gt; beam.Map(tfds.as_numpy)\n            | 'ToDict' &gt;&gt; beam.Map(dict)\n            | 'ToTFExample' &gt;&gt; beam.Map(utils.dict_to_example)\n            )\n\nclass TFDSExecutor(BaseExampleGenExecutor):\n  def GetInputSourceToExamplePTransform(self) -&gt; beam.PTransform:\n    \"\"\"Returns PTransform for TF Dataset to TF examples.\"\"\"\n    return _TFDatasetToExample\n</pre> @beam.ptransform_fn @beam.typehints.with_input_types(beam.Pipeline) @beam.typehints.with_output_types(tf.train.Example) def _TFDatasetToExample(     pipeline: beam.Pipeline,     exec_properties: Dict[str, Any],     split_pattern: str     ) -&gt; beam.pvalue.PCollection:     \"\"\"Read a TensorFlow Dataset and create tf.Examples\"\"\"     custom_config = json.loads(exec_properties['custom_config'])     dataset_name = custom_config['dataset']     split_name = custom_config['split']      builder = tfds.builder(dataset_name)     builder.download_and_prepare()      return (pipeline             | 'MakeExamples' &gt;&gt; tfds.beam.ReadFromTFDS(builder, split=split_name)             | 'AsNumpy' &gt;&gt; beam.Map(tfds.as_numpy)             | 'ToDict' &gt;&gt; beam.Map(dict)             | 'ToTFExample' &gt;&gt; beam.Map(utils.dict_to_example)             )  class TFDSExecutor(BaseExampleGenExecutor):   def GetInputSourceToExamplePTransform(self) -&gt; beam.PTransform:     \"\"\"Returns PTransform for TF Dataset to TF examples.\"\"\"     return _TFDatasetToExample <p>For this demonstration, we're using a subset of the IMDb reviews dataset, representing 20% of the total data. This allows for a more manageable training process. You can modify the \"custom_config\" settings to experiment with larger amounts of data, up to the full dataset, depending on your computational resources.</p> In\u00a0[\u00a0]: Copied! <pre>example_gen = FileBasedExampleGen(\n    input_base='dummy',\n    custom_config={'dataset':'imdb_reviews', 'split':'train[:20%]'},\n    custom_executor_spec=executor_spec.BeamExecutorSpec(TFDSExecutor))\ncontext.run(example_gen, enable_cache=False)\n</pre> example_gen = FileBasedExampleGen(     input_base='dummy',     custom_config={'dataset':'imdb_reviews', 'split':'train[:20%]'},     custom_executor_spec=executor_spec.BeamExecutorSpec(TFDSExecutor)) context.run(example_gen, enable_cache=False) <p>We've developed a handy utility for examining datasets composed of TFExamples. When used with the reviews dataset, this tool returns a clear dictionary containing both the text and the corresponding label.</p> In\u00a0[\u00a0]: Copied! <pre>def inspect_examples(component,\n                     channel_name='examples',\n                     split_name='train',\n                     num_examples=1):\n  # Get the URI of the output artifact, which is a directory\n  full_split_name = 'Split-{}'.format(split_name)\n  print('channel_name: {}, split_name: {} (\\\"{}\\\"), num_examples: {}\\n'.format(\n      channel_name, split_name, full_split_name, num_examples))\n  train_uri = os.path.join(\n      component.outputs[channel_name].get()[0].uri, full_split_name)\n  print('train_uri: {}'.format(train_uri))\n\n  # Get the list of files in this directory (all compressed TFRecord files)\n  tfrecord_filenames = [os.path.join(train_uri, name)\n                        for name in os.listdir(train_uri)]\n\n  # Create a `TFRecordDataset` to read these files\n  dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")\n\n  # Iterate over the records and print them\n  print()\n  for tfrecord in dataset.take(num_examples):\n    serialized_example = tfrecord.numpy()\n    example = tf.train.Example()\n    example.ParseFromString(serialized_example)\n    pp.pprint(example)\n</pre> def inspect_examples(component,                      channel_name='examples',                      split_name='train',                      num_examples=1):   # Get the URI of the output artifact, which is a directory   full_split_name = 'Split-{}'.format(split_name)   print('channel_name: {}, split_name: {} (\\\"{}\\\"), num_examples: {}\\n'.format(       channel_name, split_name, full_split_name, num_examples))   train_uri = os.path.join(       component.outputs[channel_name].get()[0].uri, full_split_name)   print('train_uri: {}'.format(train_uri))    # Get the list of files in this directory (all compressed TFRecord files)   tfrecord_filenames = [os.path.join(train_uri, name)                         for name in os.listdir(train_uri)]    # Create a `TFRecordDataset` to read these files   dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")    # Iterate over the records and print them   print()   for tfrecord in dataset.take(num_examples):     serialized_example = tfrecord.numpy()     example = tf.train.Example()     example.ParseFromString(serialized_example)     pp.pprint(example) In\u00a0[\u00a0]: Copied! <pre>inspect_examples(example_gen, num_examples=1, split_name='eval')\n</pre> inspect_examples(example_gen, num_examples=1, split_name='eval') In\u00a0[\u00a0]: Copied! <pre>from tfx.components import StatisticsGen\n</pre> from tfx.components import StatisticsGen In\u00a0[\u00a0]: Copied! <pre>statistics_gen = tfx.components.StatisticsGen(\n    examples=example_gen.outputs['examples'], exclude_splits=['eval']\n)\ncontext.run(statistics_gen, enable_cache=False)\n</pre> statistics_gen = tfx.components.StatisticsGen(     examples=example_gen.outputs['examples'], exclude_splits=['eval'] ) context.run(statistics_gen, enable_cache=False) In\u00a0[\u00a0]: Copied! <pre>context.show(statistics_gen.outputs['statistics'])\n</pre> context.show(statistics_gen.outputs['statistics']) In\u00a0[\u00a0]: Copied! <pre>schema_gen = tfx.components.SchemaGen(\n    statistics=statistics_gen.outputs['statistics'],\n    infer_feature_shape=False,\n    exclude_splits=['eval'],\n)\ncontext.run(schema_gen, enable_cache=False)\n</pre> schema_gen = tfx.components.SchemaGen(     statistics=statistics_gen.outputs['statistics'],     infer_feature_shape=False,     exclude_splits=['eval'], ) context.run(schema_gen, enable_cache=False) In\u00a0[\u00a0]: Copied! <pre>context.show(schema_gen.outputs['schema'])\n</pre> context.show(schema_gen.outputs['schema']) In\u00a0[\u00a0]: Copied! <pre>example_validator = tfx.components.ExampleValidator(\n    statistics=statistics_gen.outputs['statistics'],\n    schema=schema_gen.outputs['schema'],\n    exclude_splits=['eval'],\n)\ncontext.run(example_validator, enable_cache=False)\n</pre> example_validator = tfx.components.ExampleValidator(     statistics=statistics_gen.outputs['statistics'],     schema=schema_gen.outputs['schema'],     exclude_splits=['eval'], ) context.run(example_validator, enable_cache=False) <p>After <code>ExampleValidator</code> finishes running, we can visualize the anomalies as a table.</p> In\u00a0[\u00a0]: Copied! <pre>context.show(example_validator.outputs['anomalies'])\n</pre> context.show(example_validator.outputs['anomalies']) In\u00a0[\u00a0]: Copied! <pre>import os\nif not os.path.exists(\"modules\"):\n  os.mkdir(\"modules\")\n</pre> import os if not os.path.exists(\"modules\"):   os.mkdir(\"modules\") In\u00a0[\u00a0]: Copied! <pre>_transform_module_file = 'modules/_transform_module.py'\n</pre> _transform_module_file = 'modules/_transform_module.py' In\u00a0[\u00a0]: Copied! <pre>%%writefile {_transform_module_file}\n\nimport tensorflow as tf\n\ndef _fill_in_missing(x, default_value):\n  \"\"\"Replace missing values in a SparseTensor.\n\n  Fills in missing values of `x` with the default_value.\n\n  Args:\n    x: A `SparseTensor` of rank 2.  Its dense shape should have size at most 1\n      in the second dimension.\n    default_value: the value with which to replace the missing values.\n\n  Returns:\n    A rank 1 tensor where missing values of `x` have been filled in.\n  \"\"\"\n  if not isinstance(x, tf.sparse.SparseTensor):\n    return x\n  return tf.squeeze(\n      tf.sparse.to_dense(\n          tf.SparseTensor(x.indices, x.values, [x.dense_shape[0], 1]),\n          default_value),\n      axis=1)\n\ndef preprocessing_fn(inputs):\n  outputs = {}\n  # outputs[\"summary\"] = _fill_in_missing(inputs[\"summary\"],\"\")\n  outputs[\"summary\"] = _fill_in_missing(inputs[\"text\"],\"\")\n  return outputs\n</pre> %%writefile {_transform_module_file}  import tensorflow as tf  def _fill_in_missing(x, default_value):   \"\"\"Replace missing values in a SparseTensor.    Fills in missing values of `x` with the default_value.    Args:     x: A `SparseTensor` of rank 2.  Its dense shape should have size at most 1       in the second dimension.     default_value: the value with which to replace the missing values.    Returns:     A rank 1 tensor where missing values of `x` have been filled in.   \"\"\"   if not isinstance(x, tf.sparse.SparseTensor):     return x   return tf.squeeze(       tf.sparse.to_dense(           tf.SparseTensor(x.indices, x.values, [x.dense_shape[0], 1]),           default_value),       axis=1)  def preprocessing_fn(inputs):   outputs = {}   # outputs[\"summary\"] = _fill_in_missing(inputs[\"summary\"],\"\")   outputs[\"summary\"] = _fill_in_missing(inputs[\"text\"],\"\")   return outputs In\u00a0[\u00a0]: Copied! <pre>preprocessor = tfx.components.Transform(\n    examples=example_gen.outputs['examples'],\n    schema=schema_gen.outputs['schema'],\n    module_file=os.path.abspath(_transform_module_file))\n</pre> preprocessor = tfx.components.Transform(     examples=example_gen.outputs['examples'],     schema=schema_gen.outputs['schema'],     module_file=os.path.abspath(_transform_module_file)) In\u00a0[\u00a0]: Copied! <pre>context.run(preprocessor, enable_cache=False)\n</pre> context.run(preprocessor, enable_cache=False) <p>Let's take a look at some of the transformed examples and check that they are indeed processed as intended.</p> In\u00a0[\u00a0]: Copied! <pre>def pprint_examples(artifact, n_examples=2):\n  print(\"artifact:\", artifact, \"\\n\")\n  uri = os.path.join(artifact.uri, \"Split-eval\")\n  print(\"uri:\", uri, \"\\n\")\n  tfrecord_filenames = [os.path.join(uri, name) for name in os.listdir(uri)]\n  print(\"tfrecord_filenames:\", tfrecord_filenames, \"\\n\")\n  dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")\n  for tfrecord in dataset.take(n_examples):\n    serialized_example = tfrecord.numpy()\n    example = tf.train.Example.FromString(serialized_example)\n    pp.pprint(example)\n</pre> def pprint_examples(artifact, n_examples=2):   print(\"artifact:\", artifact, \"\\n\")   uri = os.path.join(artifact.uri, \"Split-eval\")   print(\"uri:\", uri, \"\\n\")   tfrecord_filenames = [os.path.join(uri, name) for name in os.listdir(uri)]   print(\"tfrecord_filenames:\", tfrecord_filenames, \"\\n\")   dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")   for tfrecord in dataset.take(n_examples):     serialized_example = tfrecord.numpy()     example = tf.train.Example.FromString(serialized_example)     pp.pprint(example) In\u00a0[\u00a0]: Copied! <pre>pprint_examples(preprocessor.outputs['transformed_examples'].get()[0])\n</pre> pprint_examples(preprocessor.outputs['transformed_examples'].get()[0]) In\u00a0[\u00a0]: Copied! <pre>model_file = \"modules/model.py\"\nmodel_fn = \"modules.model.run_fn\"\n</pre> model_file = \"modules/model.py\" model_fn = \"modules.model.run_fn\" <p>Now, we write the run_fn function:</p> <p>This run_fn function first gets the training data from the <code>fn_args.examples</code> argument. It then gets the schema of the training data from the <code>fn_args.schema</code> argument. Next, it loads finetuned GPT-2 model along with its preprocessor. The model is then trained on the training data using the model.train() method. Finally, the trained model weights are saved to the <code>fn_args.serving_model_dir</code> argument.</p> <p>Now, we are going to work with Keras NLP's GPT-2 Model! You can learn about the full GPT-2 model implementation in KerasNLP on GitHub or can read and interactively test the model on Google IO2023 colab notebook.</p> In\u00a0[\u00a0]: Copied! <pre>import keras_nlp\nimport keras\nimport tensorflow as tf\n</pre> import keras_nlp import keras import tensorflow as tf <p>Note: To accommodate the limited resources of a free Colab GPU, we've adjusted the GPT-2 model's <code>sequence_length</code> parameter to <code>128</code> from its default <code>256</code>. This optimization enables efficient model training on the T4 GPU, facilitating faster fine-tuning while adhering to resource constraints.</p> In\u00a0[\u00a0]: Copied! <pre>%%writefile {model_file}\n\nimport os\nimport time\nfrom absl import logging\nimport keras_nlp\nimport more_itertools\nimport pandas as pd\nimport tensorflow as tf\nimport keras\nimport tfx\nimport tfx.components.trainer.fn_args_utils\nimport gc\n\n\n_EPOCH = 1\n_BATCH_SIZE = 20\n_INITIAL_LEARNING_RATE = 5e-5\n_END_LEARNING_RATE = 0.0\n_SEQUENCE_LENGTH = 128 # default value is 256\n\ndef _input_fn(file_pattern: str) -&gt; list:\n  \"\"\"Retrieves training data and returns a list of articles for training.\n\n  For each row in the TFRecordDataset, generated in the previous ExampleGen\n  component, create a new tf.train.Example object and parse the TFRecord into\n  the example object. Articles, which are initially in bytes objects, are\n  decoded into a string.\n\n  Args:\n    file_pattern: Path to the TFRecord file of the training dataset.\n\n  Returns:\n    A list of training articles.\n\n  Raises:\n    FileNotFoundError: If TFRecord dataset is not found in the file_pattern\n    directory.\n  \"\"\"\n\n  if os.path.basename(file_pattern) == '*':\n    file_loc = os.path.dirname(file_pattern)\n\n  else:\n    raise FileNotFoundError(\n        f\"There is no file in the current directory: '{file_pattern}.\"\n    )\n\n  file_paths = [os.path.join(file_loc, name) for name in os.listdir(file_loc)]\n  train_articles = []\n  parsed_dataset = tf.data.TFRecordDataset(file_paths, compression_type=\"GZIP\")\n  for raw_record in parsed_dataset:\n    example = tf.train.Example()\n    example.ParseFromString(raw_record.numpy())\n    train_articles.append(\n        example.features.feature[\"summary\"].bytes_list.value[0].decode('utf-8')\n    )\n  return train_articles\n\ndef run_fn(fn_args: tfx.components.trainer.fn_args_utils.FnArgs) -&gt; None:\n  \"\"\"Trains the model and outputs the trained model to a the desired location given by FnArgs.\n\n  Args:\n    FnArgs :  Args to pass to user defined training/tuning function(s)\n  \"\"\"\n\n  train_articles =  pd.Series(_input_fn(\n          fn_args.train_files[0],\n      ))\n  tf_train_ds = tf.data.Dataset.from_tensor_slices(train_articles)\n\n  gpt2_preprocessor = keras_nlp.models.GPT2CausalLMPreprocessor.from_preset(\n      'gpt2_base_en',\n      sequence_length=_SEQUENCE_LENGTH,\n      add_end_token=True,\n  )\n  gpt2_lm = keras_nlp.models.GPT2CausalLM.from_preset(\n      'gpt2_base_en', preprocessor=gpt2_preprocessor\n  )\n\n  processed_ds = (\n      tf_train_ds\n      .batch(_BATCH_SIZE)\n      .cache()\n      .prefetch(tf.data.AUTOTUNE)\n  )\n\n  gpt2_lm.include_preprocessing = False\n\n  lr = tf.keras.optimizers.schedules.PolynomialDecay(\n      5e-5,\n      decay_steps=processed_ds.cardinality() * _EPOCH,\n      end_learning_rate=0.0,\n  )\n  loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n\n  gpt2_lm.compile(\n      optimizer=keras.optimizers.Adam(lr),\n      loss=loss,\n      weighted_metrics=['accuracy'],\n  )\n\n  gpt2_lm.fit(processed_ds, epochs=_EPOCH)\n  if os.path.exists(fn_args.serving_model_dir):\n    os.rmdir(fn_args.serving_model_dir)\n  os.mkdir(fn_args.serving_model_dir)\n  gpt2_lm.save_weights(\n      filepath=os.path.join(fn_args.serving_model_dir, \"model_weights.weights.h5\")\n  )\n  del gpt2_lm, gpt2_preprocessor, processed_ds, tf_train_ds\n  gc.collect()\n</pre> %%writefile {model_file}  import os import time from absl import logging import keras_nlp import more_itertools import pandas as pd import tensorflow as tf import keras import tfx import tfx.components.trainer.fn_args_utils import gc   _EPOCH = 1 _BATCH_SIZE = 20 _INITIAL_LEARNING_RATE = 5e-5 _END_LEARNING_RATE = 0.0 _SEQUENCE_LENGTH = 128 # default value is 256  def _input_fn(file_pattern: str) -&gt; list:   \"\"\"Retrieves training data and returns a list of articles for training.    For each row in the TFRecordDataset, generated in the previous ExampleGen   component, create a new tf.train.Example object and parse the TFRecord into   the example object. Articles, which are initially in bytes objects, are   decoded into a string.    Args:     file_pattern: Path to the TFRecord file of the training dataset.    Returns:     A list of training articles.    Raises:     FileNotFoundError: If TFRecord dataset is not found in the file_pattern     directory.   \"\"\"    if os.path.basename(file_pattern) == '*':     file_loc = os.path.dirname(file_pattern)    else:     raise FileNotFoundError(         f\"There is no file in the current directory: '{file_pattern}.\"     )    file_paths = [os.path.join(file_loc, name) for name in os.listdir(file_loc)]   train_articles = []   parsed_dataset = tf.data.TFRecordDataset(file_paths, compression_type=\"GZIP\")   for raw_record in parsed_dataset:     example = tf.train.Example()     example.ParseFromString(raw_record.numpy())     train_articles.append(         example.features.feature[\"summary\"].bytes_list.value[0].decode('utf-8')     )   return train_articles  def run_fn(fn_args: tfx.components.trainer.fn_args_utils.FnArgs) -&gt; None:   \"\"\"Trains the model and outputs the trained model to a the desired location given by FnArgs.    Args:     FnArgs :  Args to pass to user defined training/tuning function(s)   \"\"\"    train_articles =  pd.Series(_input_fn(           fn_args.train_files[0],       ))   tf_train_ds = tf.data.Dataset.from_tensor_slices(train_articles)    gpt2_preprocessor = keras_nlp.models.GPT2CausalLMPreprocessor.from_preset(       'gpt2_base_en',       sequence_length=_SEQUENCE_LENGTH,       add_end_token=True,   )   gpt2_lm = keras_nlp.models.GPT2CausalLM.from_preset(       'gpt2_base_en', preprocessor=gpt2_preprocessor   )    processed_ds = (       tf_train_ds       .batch(_BATCH_SIZE)       .cache()       .prefetch(tf.data.AUTOTUNE)   )    gpt2_lm.include_preprocessing = False    lr = tf.keras.optimizers.schedules.PolynomialDecay(       5e-5,       decay_steps=processed_ds.cardinality() * _EPOCH,       end_learning_rate=0.0,   )   loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)    gpt2_lm.compile(       optimizer=keras.optimizers.Adam(lr),       loss=loss,       weighted_metrics=['accuracy'],   )    gpt2_lm.fit(processed_ds, epochs=_EPOCH)   if os.path.exists(fn_args.serving_model_dir):     os.rmdir(fn_args.serving_model_dir)   os.mkdir(fn_args.serving_model_dir)   gpt2_lm.save_weights(       filepath=os.path.join(fn_args.serving_model_dir, \"model_weights.weights.h5\")   )   del gpt2_lm, gpt2_preprocessor, processed_ds, tf_train_ds   gc.collect() In\u00a0[\u00a0]: Copied! <pre>trainer = tfx.components.Trainer(\n    run_fn=model_fn,\n    examples=preprocessor.outputs['transformed_examples'],\n    train_args=tfx.proto.TrainArgs(splits=['train']),\n    eval_args=tfx.proto.EvalArgs(splits=['train']),\n    schema=schema_gen.outputs['schema'],\n)\n</pre> trainer = tfx.components.Trainer(     run_fn=model_fn,     examples=preprocessor.outputs['transformed_examples'],     train_args=tfx.proto.TrainArgs(splits=['train']),     eval_args=tfx.proto.EvalArgs(splits=['train']),     schema=schema_gen.outputs['schema'], ) In\u00a0[\u00a0]: Copied! <pre>context.run(trainer, enable_cache=False)\n</pre> context.run(trainer, enable_cache=False) In\u00a0[\u00a0]: Copied! <pre>from tfx.types import artifact\nfrom tfx import types\n\nProperty = artifact.Property\nPropertyType = artifact.PropertyType\n\nDURATION_PROPERTY = Property(type=PropertyType.FLOAT)\nEVAL_OUTPUT_PROPERTY = Property(type=PropertyType.STRING)\n\nclass EvaluationMetric(types.Artifact):\n  \"\"\"Artifact that contains metrics for a model.\n\n  * Properties:\n\n     - 'model_prediction_time' : time it took for the model to make predictions\n     based on the input text.\n     - 'model_evaluation_output_path' : saves the path to the CSV file that\n     contains the model's prediction based on the testing inputs.\n  \"\"\"\n  TYPE_NAME = 'Evaluation_Metric'\n  PROPERTIES = {\n      'model_prediction_time': DURATION_PROPERTY,\n      'model_evaluation_output_path': EVAL_OUTPUT_PROPERTY,\n  }\n</pre> from tfx.types import artifact from tfx import types  Property = artifact.Property PropertyType = artifact.PropertyType  DURATION_PROPERTY = Property(type=PropertyType.FLOAT) EVAL_OUTPUT_PROPERTY = Property(type=PropertyType.STRING)  class EvaluationMetric(types.Artifact):   \"\"\"Artifact that contains metrics for a model.    * Properties:       - 'model_prediction_time' : time it took for the model to make predictions      based on the input text.      - 'model_evaluation_output_path' : saves the path to the CSV file that      contains the model's prediction based on the testing inputs.   \"\"\"   TYPE_NAME = 'Evaluation_Metric'   PROPERTIES = {       'model_prediction_time': DURATION_PROPERTY,       'model_evaluation_output_path': EVAL_OUTPUT_PROPERTY,   } <p>These helper functions contribute to the evaluation of a language model (LLM) by providing tools for calculating perplexity, a key metric reflecting the model's ability to predict the next word in a sequence, and by facilitating the extraction, preparation, and processing of evaluation data. The <code>input_fn</code> function retrieves training data from a specified TFRecord file, while the <code>trim_sentence</code> function ensures consistency by limiting sentence length. A lower perplexity score indicates higher prediction confidence and generally better model performance, making these functions essential for comprehensive evaluation within the LLM pipeline.</p> In\u00a0[\u00a0]: Copied! <pre>\"\"\"This is an evaluation component for the LLM pipeline takes in a\nstandard trainer artifact and outputs a custom evaluation artifact.\nIt displays the evaluation output in the colab notebook.\n\"\"\"\nimport os\nimport time\nimport keras_nlp\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport tfx.v1 as tfx\n\ndef input_fn(file_pattern: str) -&gt; list:\n  \"\"\"Retrieves training data and returns a list of articles for training.\n\n  Args:\n    file_pattern: Path to the TFRecord file of the training dataset.\n\n  Returns:\n    A list of test articles\n\n  Raises:\n    FileNotFoundError: If the file path does not exist.\n  \"\"\"\n  if os.path.exists(file_pattern):\n    file_paths = [os.path.join(file_pattern, name) for name in os.listdir(file_pattern)]\n    test_articles = []\n    parsed_dataset = tf.data.TFRecordDataset(file_paths, compression_type=\"GZIP\")\n    for raw_record in parsed_dataset:\n      example = tf.train.Example()\n      example.ParseFromString(raw_record.numpy())\n      test_articles.append(\n          example.features.feature[\"summary\"].bytes_list.value[0].decode('utf-8')\n      )\n    return test_articles\n  else:\n    raise FileNotFoundError(f'File path \"{file_pattern}\" does not exist.')\n\ndef trim_sentence(sentence: str, max_words: int = 20):\n  \"\"\"Trims the sentence to include up to the given number of words.\n\n  Args:\n    sentence: The sentence to trim.\n    max_words: The maximum number of words to include in the trimmed sentence.\n\n  Returns:\n    The trimmed sentence.\n  \"\"\"\n  words = sentence.split(' ')\n  if len(words) &lt;= max_words:\n    return sentence\n  return ' '.join(words[:max_words])\n</pre> \"\"\"This is an evaluation component for the LLM pipeline takes in a standard trainer artifact and outputs a custom evaluation artifact. It displays the evaluation output in the colab notebook. \"\"\" import os import time import keras_nlp import numpy as np import pandas as pd import tensorflow as tf import tfx.v1 as tfx  def input_fn(file_pattern: str) -&gt; list:   \"\"\"Retrieves training data and returns a list of articles for training.    Args:     file_pattern: Path to the TFRecord file of the training dataset.    Returns:     A list of test articles    Raises:     FileNotFoundError: If the file path does not exist.   \"\"\"   if os.path.exists(file_pattern):     file_paths = [os.path.join(file_pattern, name) for name in os.listdir(file_pattern)]     test_articles = []     parsed_dataset = tf.data.TFRecordDataset(file_paths, compression_type=\"GZIP\")     for raw_record in parsed_dataset:       example = tf.train.Example()       example.ParseFromString(raw_record.numpy())       test_articles.append(           example.features.feature[\"summary\"].bytes_list.value[0].decode('utf-8')       )     return test_articles   else:     raise FileNotFoundError(f'File path \"{file_pattern}\" does not exist.')  def trim_sentence(sentence: str, max_words: int = 20):   \"\"\"Trims the sentence to include up to the given number of words.    Args:     sentence: The sentence to trim.     max_words: The maximum number of words to include in the trimmed sentence.    Returns:     The trimmed sentence.   \"\"\"   words = sentence.split(' ')   if len(words) &lt;= max_words:     return sentence   return ' '.join(words[:max_words]) <p></p> <p>One of the useful metrics for evaluating a Large Language Model is Perplexity. Perplexity is a measure of how well a language model predicts the next token in a sequence. It is calculated by taking the exponentiation of the average negative log-likelihood of the next token. A lower perplexity score indicates that the language model is better at predicting the next token.</p> <p>This is the formula for calculating perplexity.</p> <p>$\\text{Perplexity} = \\exp(-1 * $ Average Negative Log Likelihood $) =   \\exp\\left(-\\frac{1}{T} \\sum_{t=1}^T \\log p(w_t | w_{&lt;t})\\right)$.</p> <p>In this colab notebook, we calculate perplexity using keras_nlp's perplexity.</p> <p>Computing Perplexity for Base GPT-2 Model and Finetuned Model</p> <p>The code below is the function which will be used later in the notebook for computing perplexity for the base GPT-2 model and the finetuned model.</p> In\u00a0[\u00a0]: Copied! <pre>def calculate_perplexity(gpt2_model, gpt2_tokenizer, sentence) -&gt; int:\n  \"\"\"Calculates perplexity of a model given a sentence.\n\n  Args:\n    gpt2_model: GPT-2 Language Model\n    gpt2_tokenizer: A GPT-2 tokenizer using Byte-Pair Encoding subword segmentation.\n    sentence: Sentence that the model's perplexity is calculated upon.\n\n  Returns:\n    A perplexity score.\n  \"\"\"\n  # gpt2_tokenizer([sentence])[0] produces a tensor containing an array of tokens that form the sentence.\n  tokens = gpt2_tokenizer([sentence])[0].numpy()\n  # decoded_sentences is an array containing sentences that increase by one token in size.\n  # e.g. if tokens for a sentence \"I love dogs\" are [\"I\", \"love\", \"dogs\"], then decoded_sentences = [\"I love\", \"I love dogs\"]\n  decoded_sentences = [gpt2_tokenizer.detokenize([tokens[:i]])[0].numpy() for i in range(1, len(tokens))]\n  predictions = gpt2_model.predict(decoded_sentences)\n  logits = [predictions[i - 1][i] for i in range(1, len(tokens))]\n  target = tokens[1:].reshape(len(tokens) - 1, 1)\n  perplexity = keras_nlp.metrics.Perplexity(from_logits=True)\n  perplexity.update_state(target, logits)\n  result = perplexity.result()\n  return result.numpy()\n\ndef average_perplexity(gpt2_model, gpt2_tokenizer, sentences):\n  perplexity_lst = [calculate_perplexity(gpt2_model, gpt2_tokenizer, sent) for sent in sentences]\n  return np.mean(perplexity_lst)\n</pre> def calculate_perplexity(gpt2_model, gpt2_tokenizer, sentence) -&gt; int:   \"\"\"Calculates perplexity of a model given a sentence.    Args:     gpt2_model: GPT-2 Language Model     gpt2_tokenizer: A GPT-2 tokenizer using Byte-Pair Encoding subword segmentation.     sentence: Sentence that the model's perplexity is calculated upon.    Returns:     A perplexity score.   \"\"\"   # gpt2_tokenizer([sentence])[0] produces a tensor containing an array of tokens that form the sentence.   tokens = gpt2_tokenizer([sentence])[0].numpy()   # decoded_sentences is an array containing sentences that increase by one token in size.   # e.g. if tokens for a sentence \"I love dogs\" are [\"I\", \"love\", \"dogs\"], then decoded_sentences = [\"I love\", \"I love dogs\"]   decoded_sentences = [gpt2_tokenizer.detokenize([tokens[:i]])[0].numpy() for i in range(1, len(tokens))]   predictions = gpt2_model.predict(decoded_sentences)   logits = [predictions[i - 1][i] for i in range(1, len(tokens))]   target = tokens[1:].reshape(len(tokens) - 1, 1)   perplexity = keras_nlp.metrics.Perplexity(from_logits=True)   perplexity.update_state(target, logits)   result = perplexity.result()   return result.numpy()  def average_perplexity(gpt2_model, gpt2_tokenizer, sentences):   perplexity_lst = [calculate_perplexity(gpt2_model, gpt2_tokenizer, sent) for sent in sentences]   return np.mean(perplexity_lst) In\u00a0[\u00a0]: Copied! <pre>@tfx.dsl.components.component\ndef Evaluator(\n    examples: tfx.dsl.components.InputArtifact[\n        tfx.types.standard_artifacts.Examples\n    ],\n    trained_model: tfx.dsl.components.InputArtifact[\n        tfx.types.standard_artifacts.Model\n    ],\n    max_length: tfx.dsl.components.Parameter[int],\n    evaluation: tfx.dsl.components.OutputArtifact[EvaluationMetric],\n) -&gt; None:\n  \"\"\"Makes inferences with base model, finetuned model, TFlite model, and quantized model.\n\n  Args:\n    examples: Standard TFX examples artifacts for retreiving test dataset.\n    trained_model: Standard TFX trained model artifact finetuned with imdb-reviews\n      dataset.\n    tflite_model: Unquantized TFLite model.\n    quantized_model: Quantized TFLite model.\n    max_length: Length of the text that the model generates given custom input\n      statements.\n    evaluation: An evaluation artifact that saves predicted outcomes of custom\n      inputs in a csv document and inference speed of the model.\n  \"\"\"\n  _TEST_SIZE = 10\n  _INPUT_LENGTH = 10\n  _SEQUENCE_LENGTH = 128\n\n  path = os.path.join(examples.uri, 'Split-eval')\n  test_data = input_fn(path)\n  evaluation_inputs = [\n      trim_sentence(article, max_words=_INPUT_LENGTH)\n      for article in test_data[:_TEST_SIZE]\n  ]\n  true_test = [\n      trim_sentence(article, max_words=max_length)\n      for article in test_data[:_TEST_SIZE]\n  ]\n\n  # Loading base model, making inference, and calculating perplexity on the base model.\n  gpt2_preprocessor = keras_nlp.models.GPT2CausalLMPreprocessor.from_preset(\n      'gpt2_base_en',\n      sequence_length=_SEQUENCE_LENGTH,\n      add_end_token=True,\n  )\n  gpt2_lm = keras_nlp.models.GPT2CausalLM.from_preset(\n      'gpt2_base_en', preprocessor=gpt2_preprocessor\n  )\n  gpt2_tokenizer = keras_nlp.models.GPT2Tokenizer.from_preset('gpt2_base_en')\n\n  base_average_perplexity = average_perplexity(\n      gpt2_lm, gpt2_tokenizer, true_test\n  )\n\n  start_base_model = time.time()\n  base_evaluation = [\n      gpt2_lm.generate(input, max_length)\n      for input in evaluation_inputs\n  ]\n  end_base_model = time.time()\n\n  # Loading finetuned model and making inferences with the finetuned model.\n  model_weights_path = os.path.join(\n      trained_model.uri, \"Format-Serving\", \"model_weights.weights.h5\"\n  )\n  gpt2_lm.load_weights(model_weights_path)\n\n  trained_model_average_perplexity = average_perplexity(\n      gpt2_lm, gpt2_tokenizer, true_test\n  )\n\n  start_trained = time.time()\n  trained_evaluation = [\n      gpt2_lm.generate(input, max_length)\n      for input in evaluation_inputs\n  ]\n  end_trained = time.time()\n\n  # Building an inference table.\n  inference_data = {\n      'input': evaluation_inputs,\n      'actual_test_output': true_test,\n      'base_model_prediction': base_evaluation,\n      'trained_model_prediction': trained_evaluation,\n  }\n\n  models = [\n      'Base Model',\n      'Finetuned Model',\n  ]\n  inference_time = [\n      (end_base_model - start_base_model),\n      (end_trained - start_trained),\n  ]\n  average_inference_time = [time / _TEST_SIZE for time in inference_time]\n  average_perplexity_lst = [\n      base_average_perplexity,\n      trained_model_average_perplexity,\n  ]\n  evaluation_data = {\n      'Model': models,\n      'Average Inference Time (sec)': average_inference_time,\n      'Average Perplexity': average_perplexity_lst,\n  }\n\n  # creating directory in examples artifact to save metric dataframes\n  metrics_path = os.path.join(evaluation.uri, 'metrics')\n  if not os.path.exists(metrics_path):\n      os.mkdir(metrics_path)\n\n  evaluation_df = pd.DataFrame(evaluation_data).set_index('Model').transpose()\n  evaluation_path = os.path.join(metrics_path, 'evaluation_output.csv')\n  evaluation_df.to_csv(evaluation_path)\n\n  inference_df = pd.DataFrame(inference_data)\n  inference_path = os.path.join(metrics_path, 'inference_output.csv')\n  inference_df.to_csv(inference_path)\n  evaluation.model_evaluation_output_path = inference_path\n</pre> @tfx.dsl.components.component def Evaluator(     examples: tfx.dsl.components.InputArtifact[         tfx.types.standard_artifacts.Examples     ],     trained_model: tfx.dsl.components.InputArtifact[         tfx.types.standard_artifacts.Model     ],     max_length: tfx.dsl.components.Parameter[int],     evaluation: tfx.dsl.components.OutputArtifact[EvaluationMetric], ) -&gt; None:   \"\"\"Makes inferences with base model, finetuned model, TFlite model, and quantized model.    Args:     examples: Standard TFX examples artifacts for retreiving test dataset.     trained_model: Standard TFX trained model artifact finetuned with imdb-reviews       dataset.     tflite_model: Unquantized TFLite model.     quantized_model: Quantized TFLite model.     max_length: Length of the text that the model generates given custom input       statements.     evaluation: An evaluation artifact that saves predicted outcomes of custom       inputs in a csv document and inference speed of the model.   \"\"\"   _TEST_SIZE = 10   _INPUT_LENGTH = 10   _SEQUENCE_LENGTH = 128    path = os.path.join(examples.uri, 'Split-eval')   test_data = input_fn(path)   evaluation_inputs = [       trim_sentence(article, max_words=_INPUT_LENGTH)       for article in test_data[:_TEST_SIZE]   ]   true_test = [       trim_sentence(article, max_words=max_length)       for article in test_data[:_TEST_SIZE]   ]    # Loading base model, making inference, and calculating perplexity on the base model.   gpt2_preprocessor = keras_nlp.models.GPT2CausalLMPreprocessor.from_preset(       'gpt2_base_en',       sequence_length=_SEQUENCE_LENGTH,       add_end_token=True,   )   gpt2_lm = keras_nlp.models.GPT2CausalLM.from_preset(       'gpt2_base_en', preprocessor=gpt2_preprocessor   )   gpt2_tokenizer = keras_nlp.models.GPT2Tokenizer.from_preset('gpt2_base_en')    base_average_perplexity = average_perplexity(       gpt2_lm, gpt2_tokenizer, true_test   )    start_base_model = time.time()   base_evaluation = [       gpt2_lm.generate(input, max_length)       for input in evaluation_inputs   ]   end_base_model = time.time()    # Loading finetuned model and making inferences with the finetuned model.   model_weights_path = os.path.join(       trained_model.uri, \"Format-Serving\", \"model_weights.weights.h5\"   )   gpt2_lm.load_weights(model_weights_path)    trained_model_average_perplexity = average_perplexity(       gpt2_lm, gpt2_tokenizer, true_test   )    start_trained = time.time()   trained_evaluation = [       gpt2_lm.generate(input, max_length)       for input in evaluation_inputs   ]   end_trained = time.time()    # Building an inference table.   inference_data = {       'input': evaluation_inputs,       'actual_test_output': true_test,       'base_model_prediction': base_evaluation,       'trained_model_prediction': trained_evaluation,   }    models = [       'Base Model',       'Finetuned Model',   ]   inference_time = [       (end_base_model - start_base_model),       (end_trained - start_trained),   ]   average_inference_time = [time / _TEST_SIZE for time in inference_time]   average_perplexity_lst = [       base_average_perplexity,       trained_model_average_perplexity,   ]   evaluation_data = {       'Model': models,       'Average Inference Time (sec)': average_inference_time,       'Average Perplexity': average_perplexity_lst,   }    # creating directory in examples artifact to save metric dataframes   metrics_path = os.path.join(evaluation.uri, 'metrics')   if not os.path.exists(metrics_path):       os.mkdir(metrics_path)    evaluation_df = pd.DataFrame(evaluation_data).set_index('Model').transpose()   evaluation_path = os.path.join(metrics_path, 'evaluation_output.csv')   evaluation_df.to_csv(evaluation_path)    inference_df = pd.DataFrame(inference_data)   inference_path = os.path.join(metrics_path, 'inference_output.csv')   inference_df.to_csv(inference_path)   evaluation.model_evaluation_output_path = inference_path In\u00a0[\u00a0]: Copied! <pre>evaluator = Evaluator(examples = preprocessor.outputs['transformed_examples'],\n                      trained_model = trainer.outputs['model'],\n                      max_length = 50)\n</pre> evaluator = Evaluator(examples = preprocessor.outputs['transformed_examples'],                       trained_model = trainer.outputs['model'],                       max_length = 50) In\u00a0[\u00a0]: Copied! <pre>context.run(evaluator, enable_cache = False)\n</pre> context.run(evaluator, enable_cache = False) <p>Once our evaluation component execution is completed, we will load the evaluation metrics from evaluator URI and display them.</p> <p>Note:</p> <p>Perplexity Calculation: Perplexity is only one of many ways to evaluate LLMs. LLM evaluation is an active research topic and a comprehensive treatment is beyond the scope of this notebook.</p> In\u00a0[\u00a0]: Copied! <pre>evaluation_path = os.path.join(evaluator.outputs['evaluation']._artifacts[0].uri, 'metrics')\ninference_df = pd.read_csv(os.path.join(evaluation_path, 'inference_output.csv'), index_col=0)\nevaluation_df = pd.read_csv(os.path.join(evaluation_path, 'evaluation_output.csv'), index_col=0)\n</pre> evaluation_path = os.path.join(evaluator.outputs['evaluation']._artifacts[0].uri, 'metrics') inference_df = pd.read_csv(os.path.join(evaluation_path, 'inference_output.csv'), index_col=0) evaluation_df = pd.read_csv(os.path.join(evaluation_path, 'evaluation_output.csv'), index_col=0) <p>The fine-tuned GPT-2 model exhibits a slight improvement in perplexity compared to the baseline model. Further training with more epochs or a larger dataset may yield more substantial perplexity reductions.</p> In\u00a0[\u00a0]: Copied! <pre>from IPython import display\ndisplay.display(display.HTML(inference_df.to_html()))\ndisplay.display(display.HTML(evaluation_df.to_html()))\n</pre> from IPython import display display.display(display.HTML(inference_df.to_html())) display.display(display.HTML(evaluation_df.to_html())) <p>Note: For running below section, a more substantial amount of GPU memory is required. Therefore, Colab Pro or a local machine equipped with a higher-capacity GPU is recommended for running below pipeline.</p> <p>TFX supports multiple orchestrators to run pipelines. In this tutorial we will use LocalDagRunner which is included in the TFX Python package and runs pipelines on local environment. We often call TFX pipelines \"DAGs\" which stands for directed acyclic graph.</p> <p>LocalDagRunner provides fast iterations for development and debugging. TFX also supports other orchestrators including Kubeflow Pipelines and Apache Airflow which are suitable for production use cases. See TFX on Cloud AI Platform Pipelines or TFX Airflow Tutorial to learn more about other orchestration systems.</p> <p>Now we create a LocalDagRunner and pass a Pipeline object created from the function we already defined. The pipeline runs directly and you can see logs for the progress of the pipeline including ML model training.</p> In\u00a0[\u00a0]: Copied! <pre>import urllib.request\nimport tempfile\nimport os\n\nPIPELINE_NAME = \"tfx-llm-imdb-reviews\"\nmodel_fn = \"modules.model.run_fn\"\n_transform_module_file = \"modules/_transform_module.py\"\n\n# Output directory to store artifacts generated from the pipeline.\nPIPELINE_ROOT = os.path.join('pipelines', PIPELINE_NAME)\n# Path to a SQLite DB file to use as an MLMD storage.\nMETADATA_PATH = os.path.join('metadata', PIPELINE_NAME, 'metadata.db')\n# Output directory where created models from the pipeline will be exported.\nSERVING_MODEL_DIR = os.path.join('serving_model', PIPELINE_NAME)\n\nfrom absl import logging\nlogging.set_verbosity(logging.INFO)  # Set default logging level.\n</pre> import urllib.request import tempfile import os  PIPELINE_NAME = \"tfx-llm-imdb-reviews\" model_fn = \"modules.model.run_fn\" _transform_module_file = \"modules/_transform_module.py\"  # Output directory to store artifacts generated from the pipeline. PIPELINE_ROOT = os.path.join('pipelines', PIPELINE_NAME) # Path to a SQLite DB file to use as an MLMD storage. METADATA_PATH = os.path.join('metadata', PIPELINE_NAME, 'metadata.db') # Output directory where created models from the pipeline will be exported. SERVING_MODEL_DIR = os.path.join('serving_model', PIPELINE_NAME)  from absl import logging logging.set_verbosity(logging.INFO)  # Set default logging level. In\u00a0[\u00a0]: Copied! <pre>def _create_pipeline(\n    pipeline_name: str,\n    pipeline_root: str,\n    model_fn: str,\n    serving_model_dir: str,\n    metadata_path: str,\n) -&gt; tfx.dsl.Pipeline:\n  \"\"\"Creates a Pipeline for Fine-Tuning and Converting an Large Language Model with TFX.\"\"\"\n\n  example_gen = FileBasedExampleGen(\n    input_base='dummy',\n    custom_config={'dataset':'imdb_reviews', 'split':'train[:5%]'},\n    custom_executor_spec=executor_spec.BeamExecutorSpec(TFDSExecutor))\n\n  statistics_gen = tfx.components.StatisticsGen(\n      examples=example_gen.outputs['examples'], exclude_splits=['eval']\n  )\n\n  schema_gen = tfx.components.SchemaGen(\n      statistics=statistics_gen.outputs['statistics'],\n      infer_feature_shape=False,\n      exclude_splits=['eval'],\n  )\n\n  example_validator = tfx.components.ExampleValidator(\n      statistics=statistics_gen.outputs['statistics'],\n      schema=schema_gen.outputs['schema'],\n      exclude_splits=['eval'],\n  )\n\n  preprocessor = tfx.components.Transform(\n    examples=example_gen.outputs['examples'],\n    schema=schema_gen.outputs['schema'],\n    module_file= _transform_module_file,\n  )\n\n  trainer = tfx.components.Trainer(\n      run_fn=model_fn,\n      examples=preprocessor.outputs['transformed_examples'],\n      train_args=tfx.proto.TrainArgs(splits=['train']),\n      eval_args=tfx.proto.EvalArgs(splits=['train']),\n      schema=schema_gen.outputs['schema'],\n  )\n\n\n  evaluator = Evaluator(\n      examples=preprocessor.outputs['transformed_examples'],\n      trained_model=trainer.outputs['model'],\n      max_length=50,\n  )\n\n  # Following 7 components will be included in the pipeline.\n  components = [\n      example_gen,\n      statistics_gen,\n      schema_gen,\n      example_validator,\n      preprocessor,\n      trainer,\n      evaluator,\n  ]\n\n  return tfx.dsl.Pipeline(\n      pipeline_name=pipeline_name,\n      pipeline_root=pipeline_root,\n      metadata_connection_config=tfx.orchestration.metadata.sqlite_metadata_connection_config(\n          metadata_path\n      ),\n      components=components,\n  )\n</pre> def _create_pipeline(     pipeline_name: str,     pipeline_root: str,     model_fn: str,     serving_model_dir: str,     metadata_path: str, ) -&gt; tfx.dsl.Pipeline:   \"\"\"Creates a Pipeline for Fine-Tuning and Converting an Large Language Model with TFX.\"\"\"    example_gen = FileBasedExampleGen(     input_base='dummy',     custom_config={'dataset':'imdb_reviews', 'split':'train[:5%]'},     custom_executor_spec=executor_spec.BeamExecutorSpec(TFDSExecutor))    statistics_gen = tfx.components.StatisticsGen(       examples=example_gen.outputs['examples'], exclude_splits=['eval']   )    schema_gen = tfx.components.SchemaGen(       statistics=statistics_gen.outputs['statistics'],       infer_feature_shape=False,       exclude_splits=['eval'],   )    example_validator = tfx.components.ExampleValidator(       statistics=statistics_gen.outputs['statistics'],       schema=schema_gen.outputs['schema'],       exclude_splits=['eval'],   )    preprocessor = tfx.components.Transform(     examples=example_gen.outputs['examples'],     schema=schema_gen.outputs['schema'],     module_file= _transform_module_file,   )    trainer = tfx.components.Trainer(       run_fn=model_fn,       examples=preprocessor.outputs['transformed_examples'],       train_args=tfx.proto.TrainArgs(splits=['train']),       eval_args=tfx.proto.EvalArgs(splits=['train']),       schema=schema_gen.outputs['schema'],   )     evaluator = Evaluator(       examples=preprocessor.outputs['transformed_examples'],       trained_model=trainer.outputs['model'],       max_length=50,   )    # Following 7 components will be included in the pipeline.   components = [       example_gen,       statistics_gen,       schema_gen,       example_validator,       preprocessor,       trainer,       evaluator,   ]    return tfx.dsl.Pipeline(       pipeline_name=pipeline_name,       pipeline_root=pipeline_root,       metadata_connection_config=tfx.orchestration.metadata.sqlite_metadata_connection_config(           metadata_path       ),       components=components,   ) In\u00a0[\u00a0]: Copied! <pre>tfx.orchestration.LocalDagRunner().run(\n    _create_pipeline(\n        pipeline_name=PIPELINE_NAME,\n        pipeline_root=PIPELINE_ROOT,\n        model_fn=model_fn,\n        serving_model_dir=SERVING_MODEL_DIR,\n        metadata_path=METADATA_PATH,\n    )\n)\n</pre> tfx.orchestration.LocalDagRunner().run(     _create_pipeline(         pipeline_name=PIPELINE_NAME,         pipeline_root=PIPELINE_ROOT,         model_fn=model_fn,         serving_model_dir=SERVING_MODEL_DIR,         metadata_path=METADATA_PATH,     ) ) <p>You should see INFO:absl:Component Evaluator is finished.\" at the end of the logs if the pipeline finished successfully because evaluator component is the last component of the pipeline.</p>"},{"location":"tutorials/tfx/gpt2_finetuning_and_conversion/#copyright-2024-the-tensorflow-authors","title":"Copyright 2024 The TensorFlow Authors.\u00b6","text":""},{"location":"tutorials/tfx/gpt2_finetuning_and_conversion/#licensed-under-the-apache-license-version-20-the-license","title":"Licensed under the Apache License, Version 2.0 (the \"License\");\u00b6","text":""},{"location":"tutorials/tfx/gpt2_finetuning_and_conversion/#why-is-this-pipeline-useful","title":"Why is this pipeline useful?\u00b6","text":"<p>TFX pipelines provide a powerful and structured approach to building and managing machine learning workflows, particularly those involving large language models. They offer significant advantages over traditional Python code, including:</p> <ol> <li><p>Enhanced Reproducibility: TFX pipelines ensure consistent results by capturing all steps and dependencies, eliminating the inconsistencies often associated with manual workflows.</p> </li> <li><p>Scalability and Modularity: TFX allows for breaking down complex workflows into manageable, reusable components, promoting code organization.</p> </li> <li><p>Streamlined Fine-Tuning and Conversion: The pipeline structure streamlines the fine-tuning and conversion processes of large language models, significantly reducing manual effort and time.</p> </li> <li><p>Comprehensive Lineage Tracking: Through metadata tracking, TFX pipelines provide a clear understanding of data and model provenance, making debugging, auditing, and performance analysis much easier and more efficient.</p> </li> </ol> <p>By leveraging the benefits of TFX pipelines, organizations can effectively manage the complexity of large language model development and deployment, achieving greater efficiency and control over their machine learning processes.</p>"},{"location":"tutorials/tfx/gpt2_finetuning_and_conversion/#note","title":"Note\u00b6","text":"<p>GPT-2 is used here only to demonstrate the end-to-end process; the techniques and tooling introduced in this codelab are potentially transferrable to other generative language models such as Google T5.</p>"},{"location":"tutorials/tfx/gpt2_finetuning_and_conversion/#before-you-begin","title":"Before You Begin\u00b6","text":"<p>Colab offers different kinds of runtimes. Make sure to go to Runtime -&gt; Change runtime type and choose the GPU Hardware Accelerator runtime since you will finetune the GPT-2 model.</p> <p>This tutorial's interactive pipeline is designed to function seamlessly with free Colab GPUs. However, for users opting to run the pipeline using the LocalDagRunner orchestrator (code provided at the end of this tutorial), a more substantial amount of GPU memory is required. Therefore, Colab Pro or a local machine equipped with a higher-capacity GPU is recommended for this approach.</p>"},{"location":"tutorials/tfx/gpt2_finetuning_and_conversion/#set-up","title":"Set Up\u00b6","text":"<p>We first install required python packages.</p>"},{"location":"tutorials/tfx/gpt2_finetuning_and_conversion/#upgrade-pip","title":"Upgrade Pip\u00b6","text":"<p>To avoid upgrading Pip in a system when running locally, check to make sure that we are running in Colab. Local systems can of course be upgraded separately.</p>"},{"location":"tutorials/tfx/gpt2_finetuning_and_conversion/#install-tfx-keras-3-kerasnlp-and-required-libraries","title":"Install TFX, Keras 3, KerasNLP and required Libraries\u00b6","text":""},{"location":"tutorials/tfx/gpt2_finetuning_and_conversion/#did-you-restart-the-runtime","title":"Did you restart the runtime?\u00b6","text":"<p>If you are using Google Colab, the first time that you run the cell above, you must restart the runtime by clicking above \"RESTART SESSION\" button or using <code>\"Runtime &gt; Restart session\"</code> menu. This is because of the way that Colab loads packages.</p> <p>Let's check the TensorFlow, Keras, Keras-nlp and TFX library versions.</p>"},{"location":"tutorials/tfx/gpt2_finetuning_and_conversion/#using-tfx-interactive-context","title":"Using TFX Interactive Context\u00b6","text":""},{"location":"tutorials/tfx/gpt2_finetuning_and_conversion/#pipeline-overview","title":"Pipeline Overview\u00b6","text":"<p>Below are the components that this pipeline follows.</p> <ul> <li><p>Custom Artifacts are artifacts that we have created for this pipeline. Artifacts are data that is produced by a component or consumed by a component. Artifacts are stored in a system for managing the storage and versioning of artifacts called MLMD.</p> </li> <li><p>Components are defined as the implementation of an ML task that you can use as a step in your pipeline</p> </li> <li><p>Aside from artifacts, Parameters are passed into the components to specify an argument.</p> </li> </ul>"},{"location":"tutorials/tfx/gpt2_finetuning_and_conversion/#examplegen","title":"ExampleGen\u00b6","text":"<p>We create a custom ExampleGen component which we use to load a TensorFlow Datasets (TFDS) dataset. This uses a custom executor in a FileBasedExampleGen.</p>"},{"location":"tutorials/tfx/gpt2_finetuning_and_conversion/#statisticsgen","title":"StatisticsGen\u00b6","text":"<p><code>StatisticsGen</code> component computes statistics over your dataset for data analysis, such as the number of examples, the number of features, and the data types of the features. It uses the TensorFlow Data Validation library. <code>StatisticsGen</code> takes as input the dataset we just ingested using <code>ExampleGen</code>.</p> <p>Note that the statistics generator is appropriate for tabular data, and therefore, text dataset for this LLM tutorial may not be the optimal dataset for the analysis with statistics generator.</p>"},{"location":"tutorials/tfx/gpt2_finetuning_and_conversion/#schemagen","title":"SchemaGen\u00b6","text":"<p>The <code>SchemaGen</code> component generates a schema based on your data statistics. (A schema defines the expected bounds, types, and properties of the features in your dataset.) It also uses the TensorFlow Data Validation library.</p> <p>Note: The generated schema is best-effort and only tries to infer basic properties of the data. It is expected that you review and modify it as needed.</p> <p><code>SchemaGen</code> will take as input the statistics that we generated with <code>StatisticsGen</code>, looking at the training split by default.</p>"},{"location":"tutorials/tfx/gpt2_finetuning_and_conversion/#examplevalidator","title":"ExampleValidator\u00b6","text":"<p>The <code>ExampleValidator</code> component detects anomalies in your data, based on the expectations defined by the schema. It also uses the TensorFlow Data Validation library.</p> <p><code>ExampleValidator</code> will take as input the statistics from <code>StatisticsGen</code>, and the schema from <code>SchemaGen</code>.</p>"},{"location":"tutorials/tfx/gpt2_finetuning_and_conversion/#transform","title":"Transform\u00b6","text":"<p>For a structured and repeatable design of a TFX pipeline we will need a scalable approach to feature engineering. The <code>Transform</code> component performs feature engineering for both training and serving. It uses the TensorFlow Transform library.</p> <p>The Transform component uses a module file to supply user code for the feature engineering what we want to do, so our first step is to create that module file. We will only be working with the summary field.</p> <p>Note: The %%writefile {_movies_transform_module_file} cell magic below creates and writes the contents of that cell to a file on the notebook server where this notebook is running (for example, the Colab VM). When doing this outside of a notebook you would just create a Python file.</p>"},{"location":"tutorials/tfx/gpt2_finetuning_and_conversion/#trainer","title":"Trainer\u00b6","text":"<p>Trainer component trains an ML model, and it requires a model definition code from users.</p> <p>The <code>run_fn</code> function in TFX's Trainer component is the entry point for training a machine learning model. It is a user-supplied function that takes in a set of arguments and returns a model artifact.</p> <p>The <code>run_fn</code> function is responsible for:</p> <ul> <li>Building the machine learning model.</li> <li>Training the model on the training data.</li> <li>Saving the trained model to the serving model directory.</li> </ul>"},{"location":"tutorials/tfx/gpt2_finetuning_and_conversion/#write-model-training-code","title":"Write model training code\u00b6","text":"<p>We will create a very simple fine-tuned model, with the preprocessing GPT-2 model. First, we need to create a module that contains the <code>run_fn</code> function for TFX Trainer because TFX Trainer expects the <code>run_fn</code> function to be defined in a module.</p>"},{"location":"tutorials/tfx/gpt2_finetuning_and_conversion/#inference-and-evaluation","title":"Inference and Evaluation\u00b6","text":"<p>With our model fine-tuned, let's evaluate its performance by generating inferences. To capture and preserve these results, we'll create an EvaluationMetric artifact.</p>"},{"location":"tutorials/tfx/gpt2_finetuning_and_conversion/#evaluator","title":"Evaluator\u00b6","text":"<p>Having established the necessary helper functions for evaluation, we proceed to define the Evaluator component. This component facilitates model inference using both base and fine-tuned models, computes perplexity scores for all models, and measures inference time. The Evaluator's output provides comprehensive insights for a thorough comparison and assessment of each model's performance.</p>"},{"location":"tutorials/tfx/gpt2_finetuning_and_conversion/#evaluator-results","title":"Evaluator Results\u00b6","text":""},{"location":"tutorials/tfx/gpt2_finetuning_and_conversion/#running-the-entire-pipeline","title":"Running the Entire Pipeline\u00b6","text":""},{"location":"tutorials/tfx/neural_structured_learning/","title":"Graph-based Neural Structured Learning in TFX","text":"In\u00a0[\u00a0]: Copied! <pre>#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n</pre> #@title Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # https://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. <p>Note: We recommend running this tutorial in a Colab notebook, with no setup required!  Just click \"Run in Google Colab\".</p>        View on TensorFlow.org             Run in Google Colab             View source on GitHub             Download notebook      <p>Warning: Estimators are not recommended for new code.  Estimators run &lt;a href=\"https://www.tensorflow.org/api_docs/python/tf/compat/v1/Session\\\"&gt;<code>v1.Session</code>-style code which is more difficult to write correctly, and can behave unexpectedly, especially when combined with TF 2 code. Estimators do fall under our compatibility guarantees, but will receive no fixes other than security vulnerabilities. See the migration guide for details.</p> <p>This notebook classifies movie reviews as positive or negative using the text of the review. This is an example of binary classification, an important and widely applicable kind of machine learning problem.</p> <p>We will demonstrate the use of graph regularization in this notebook by building a graph from the given input. The general recipe for building a graph-regularized model using the Neural Structured Learning (NSL) framework when the input does not contain an explicit graph is as follows:</p> <ol> <li>Create embeddings for each text sample in the input. This can be done using pre-trained models such as word2vec, Swivel, BERT etc.</li> <li>Build a graph based on these embeddings by using a similarity metric such as the 'L2' distance, 'cosine' distance, etc. Nodes in the graph correspond to samples and edges in the graph correspond to similarity between pairs of samples.</li> <li>Generate training data from the above synthesized graph and sample features. The resulting training data will contain neighbor features in addition to the original node features.</li> <li>Create a neural network as a base model using Estimators.</li> <li>Wrap the base model with the <code>add_graph_regularization</code> wrapper function, which is provided by the NSL framework, to create a new graph Estimator model. This new model will include a graph regularization loss as the regularization term in its training objective.</li> <li>Train and evaluate the graph Estimator model.</li> </ol> <p>In this tutorial, we integrate the above workflow in a TFX pipeline using several custom TFX components as well as a custom graph-regularized trainer component.</p> <p>Below is the schematic for our TFX pipeline. Orange boxes represent off-the-shelf TFX components and pink boxes represent custom TFX components.</p> <p></p> In\u00a0[\u00a0]: Copied! <pre>import sys\nif 'google.colab' in sys.modules:\n  !pip install --upgrade pip\n</pre> import sys if 'google.colab' in sys.modules:   !pip install --upgrade pip In\u00a0[\u00a0]: Copied! <pre># TFX has a constraint of 1.16 due to the removal of tf.estimator support.\n!pip install -q \\\n  \"tfx&lt;1.16\" \\\n  neural-structured-learning \\\n  tensorflow-hub \\\n  tensorflow-datasets\n</pre> # TFX has a constraint of 1.16 due to the removal of tf.estimator support. !pip install -q \\   \"tfx&lt;1.16\" \\   neural-structured-learning \\   tensorflow-hub \\   tensorflow-datasets In\u00a0[\u00a0]: Copied! <pre>import apache_beam as beam\nimport gzip as gzip_lib\nimport numpy as np\nimport os\nimport pprint\nimport shutil\nimport tempfile\nimport urllib\nimport uuid\npp = pprint.PrettyPrinter()\n\nimport tensorflow as tf\nimport neural_structured_learning as nsl\n\nimport tfx\nfrom tfx.components.evaluator.component import Evaluator\nfrom tfx.components.example_gen.import_example_gen.component import ImportExampleGen\nfrom tfx.components.example_validator.component import ExampleValidator\nfrom tfx.components.model_validator.component import ModelValidator\nfrom tfx.components.pusher.component import Pusher\nfrom tfx.components.schema_gen.component import SchemaGen\nfrom tfx.components.statistics_gen.component import StatisticsGen\nfrom tfx.components.trainer import executor as trainer_executor\nfrom tfx.components.trainer.component import Trainer\nfrom tfx.components.transform.component import Transform\nfrom tfx.dsl.components.base import executor_spec\nfrom tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\nfrom tfx.proto import evaluator_pb2\nfrom tfx.proto import example_gen_pb2\nfrom tfx.proto import pusher_pb2\nfrom tfx.proto import trainer_pb2\n\nfrom tfx.types import artifact\nfrom tfx.types import artifact_utils\nfrom tfx.types import channel\nfrom tfx.types import standard_artifacts\nfrom tfx.types.standard_artifacts import Examples\n\nfrom tfx.dsl.component.experimental.annotations import InputArtifact\nfrom tfx.dsl.component.experimental.annotations import OutputArtifact\nfrom tfx.dsl.component.experimental.annotations import Parameter\nfrom tfx.dsl.component.experimental.decorators import component\n\nfrom tensorflow_metadata.proto.v0 import anomalies_pb2\nfrom tensorflow_metadata.proto.v0 import schema_pb2\nfrom tensorflow_metadata.proto.v0 import statistics_pb2\n\nimport tensorflow_data_validation as tfdv\nimport tensorflow_transform as tft\nimport tensorflow_model_analysis as tfma\nimport tensorflow_hub as hub\nimport tensorflow_datasets as tfds\n\nprint(\"TF Version: \", tf.__version__)\nprint(\"Eager mode: \", tf.executing_eagerly())\nprint(\n    \"GPU is\",\n    \"available\" if tf.config.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")\nprint(\"NSL Version: \", nsl.__version__)\nprint(\"TFX Version: \", tfx.__version__)\nprint(\"TFDV version: \", tfdv.__version__)\nprint(\"TFT version: \", tft.__version__)\nprint(\"TFMA version: \", tfma.__version__)\nprint(\"Hub version: \", hub.__version__)\nprint(\"Beam version: \", beam.__version__)\n</pre> import apache_beam as beam import gzip as gzip_lib import numpy as np import os import pprint import shutil import tempfile import urllib import uuid pp = pprint.PrettyPrinter()  import tensorflow as tf import neural_structured_learning as nsl  import tfx from tfx.components.evaluator.component import Evaluator from tfx.components.example_gen.import_example_gen.component import ImportExampleGen from tfx.components.example_validator.component import ExampleValidator from tfx.components.model_validator.component import ModelValidator from tfx.components.pusher.component import Pusher from tfx.components.schema_gen.component import SchemaGen from tfx.components.statistics_gen.component import StatisticsGen from tfx.components.trainer import executor as trainer_executor from tfx.components.trainer.component import Trainer from tfx.components.transform.component import Transform from tfx.dsl.components.base import executor_spec from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext from tfx.proto import evaluator_pb2 from tfx.proto import example_gen_pb2 from tfx.proto import pusher_pb2 from tfx.proto import trainer_pb2  from tfx.types import artifact from tfx.types import artifact_utils from tfx.types import channel from tfx.types import standard_artifacts from tfx.types.standard_artifacts import Examples  from tfx.dsl.component.experimental.annotations import InputArtifact from tfx.dsl.component.experimental.annotations import OutputArtifact from tfx.dsl.component.experimental.annotations import Parameter from tfx.dsl.component.experimental.decorators import component  from tensorflow_metadata.proto.v0 import anomalies_pb2 from tensorflow_metadata.proto.v0 import schema_pb2 from tensorflow_metadata.proto.v0 import statistics_pb2  import tensorflow_data_validation as tfdv import tensorflow_transform as tft import tensorflow_model_analysis as tfma import tensorflow_hub as hub import tensorflow_datasets as tfds  print(\"TF Version: \", tf.__version__) print(\"Eager mode: \", tf.executing_eagerly()) print(     \"GPU is\",     \"available\" if tf.config.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\") print(\"NSL Version: \", nsl.__version__) print(\"TFX Version: \", tfx.__version__) print(\"TFDV version: \", tfdv.__version__) print(\"TFT version: \", tft.__version__) print(\"TFMA version: \", tfma.__version__) print(\"Hub version: \", hub.__version__) print(\"Beam version: \", beam.__version__) In\u00a0[\u00a0]: Copied! <pre>train_set, eval_set = tfds.load(\n    \"imdb_reviews:1.0.0\",\n    split=[\"train[:10000]+unsupervised[:10000]\", \"test[:10000]\"],\n    shuffle_files=False)\n</pre> train_set, eval_set = tfds.load(     \"imdb_reviews:1.0.0\",     split=[\"train[:10000]+unsupervised[:10000]\", \"test[:10000]\"],     shuffle_files=False) <p>Let's look at a few reviews from the training set:</p> In\u00a0[\u00a0]: Copied! <pre>for tfrecord in train_set.take(4):\n  print(\"Review: {}\".format(tfrecord[\"text\"].numpy().decode(\"utf-8\")[:300]))\n  print(\"Label: {}\\n\".format(tfrecord[\"label\"].numpy()))\n</pre> for tfrecord in train_set.take(4):   print(\"Review: {}\".format(tfrecord[\"text\"].numpy().decode(\"utf-8\")[:300]))   print(\"Label: {}\\n\".format(tfrecord[\"label\"].numpy())) In\u00a0[\u00a0]: Copied! <pre>def _dict_to_example(instance):\n  \"\"\"Decoded CSV to tf example.\"\"\"\n  feature = {}\n  for key, value in instance.items():\n    if value is None:\n      feature[key] = tf.train.Feature()\n    elif value.dtype == np.integer:\n      feature[key] = tf.train.Feature(\n          int64_list=tf.train.Int64List(value=value.tolist()))\n    elif value.dtype == np.float32:\n      feature[key] = tf.train.Feature(\n          float_list=tf.train.FloatList(value=value.tolist()))\n    else:\n      feature[key] = tf.train.Feature(\n          bytes_list=tf.train.BytesList(value=value.tolist()))\n  return tf.train.Example(features=tf.train.Features(feature=feature))\n\n\nexamples_path = tempfile.mkdtemp(prefix=\"tfx-data\")\ntrain_path = os.path.join(examples_path, \"train.tfrecord\")\neval_path = os.path.join(examples_path, \"eval.tfrecord\")\n\nfor path, dataset in [(train_path, train_set), (eval_path, eval_set)]:\n  with tf.io.TFRecordWriter(path) as writer:\n    for example in dataset:\n      writer.write(\n          _dict_to_example({\n              \"label\": np.array([example[\"label\"].numpy()]),\n              \"text\": np.array([example[\"text\"].numpy()]),\n          }).SerializeToString())\n</pre> def _dict_to_example(instance):   \"\"\"Decoded CSV to tf example.\"\"\"   feature = {}   for key, value in instance.items():     if value is None:       feature[key] = tf.train.Feature()     elif value.dtype == np.integer:       feature[key] = tf.train.Feature(           int64_list=tf.train.Int64List(value=value.tolist()))     elif value.dtype == np.float32:       feature[key] = tf.train.Feature(           float_list=tf.train.FloatList(value=value.tolist()))     else:       feature[key] = tf.train.Feature(           bytes_list=tf.train.BytesList(value=value.tolist()))   return tf.train.Example(features=tf.train.Features(feature=feature))   examples_path = tempfile.mkdtemp(prefix=\"tfx-data\") train_path = os.path.join(examples_path, \"train.tfrecord\") eval_path = os.path.join(examples_path, \"eval.tfrecord\")  for path, dataset in [(train_path, train_set), (eval_path, eval_set)]:   with tf.io.TFRecordWriter(path) as writer:     for example in dataset:       writer.write(           _dict_to_example({               \"label\": np.array([example[\"label\"].numpy()]),               \"text\": np.array([example[\"text\"].numpy()]),           }).SerializeToString()) In\u00a0[\u00a0]: Copied! <pre>context = InteractiveContext()\n</pre> context = InteractiveContext() In\u00a0[\u00a0]: Copied! <pre>input_config = example_gen_pb2.Input(splits=[\n    example_gen_pb2.Input.Split(name='train', pattern='train.tfrecord'),\n    example_gen_pb2.Input.Split(name='eval', pattern='eval.tfrecord')\n])\n\nexample_gen = ImportExampleGen(input_base=examples_path, input_config=input_config)\n\ncontext.run(example_gen, enable_cache=True)\n</pre> input_config = example_gen_pb2.Input(splits=[     example_gen_pb2.Input.Split(name='train', pattern='train.tfrecord'),     example_gen_pb2.Input.Split(name='eval', pattern='eval.tfrecord') ])  example_gen = ImportExampleGen(input_base=examples_path, input_config=input_config)  context.run(example_gen, enable_cache=True) In\u00a0[\u00a0]: Copied! <pre>for artifact in example_gen.outputs['examples'].get():\n  print(artifact)\n\nprint('\\nexample_gen.outputs is a {}'.format(type(example_gen.outputs)))\nprint(example_gen.outputs)\n\nprint(example_gen.outputs['examples'].get()[0].split_names)\n</pre> for artifact in example_gen.outputs['examples'].get():   print(artifact)  print('\\nexample_gen.outputs is a {}'.format(type(example_gen.outputs))) print(example_gen.outputs)  print(example_gen.outputs['examples'].get()[0].split_names) <p>The component's outputs include 2 artifacts:</p> <ul> <li>the training examples (10,000 labeled reviews + 10,000 unlabeled reviews)</li> <li>the eval examples (10,000 labeled reviews)</li> </ul> In\u00a0[\u00a0]: Copied! <pre>def make_example_with_unique_id(example, id_feature_name):\n  \"\"\"Adds a unique ID to the given `tf.train.Example` proto.\n\n  This function uses Python's 'uuid' module to generate a universally unique\n  identifier for each example.\n\n  Args:\n    example: An instance of a `tf.train.Example` proto.\n    id_feature_name: The name of the feature in the resulting `tf.train.Example`\n      that will contain the unique identifier.\n\n  Returns:\n    A new `tf.train.Example` proto that includes a unique identifier as an\n    additional feature.\n  \"\"\"\n  result = tf.train.Example()\n  result.CopyFrom(example)\n  unique_id = uuid.uuid4()\n  result.features.feature.get_or_create(\n      id_feature_name).bytes_list.MergeFrom(\n          tf.train.BytesList(value=[str(unique_id).encode('utf-8')]))\n  return result\n\n\n@component\ndef IdentifyExamples(orig_examples: InputArtifact[Examples],\n                     identified_examples: OutputArtifact[Examples],\n                     id_feature_name: Parameter[str],\n                     component_name: Parameter[str]) -&gt; None:\n\n  # Get a list of the splits in input_data\n  splits_list = artifact_utils.decode_split_names(\n      split_names=orig_examples.split_names)\n  # For completeness, encode the splits names and payload_format.\n  # We could also just use input_data.split_names.\n  identified_examples.split_names = artifact_utils.encode_split_names(\n      splits=splits_list)\n  # TODO(b/168616829): Remove populating payload_format after tfx 0.25.0.\n  identified_examples.set_string_custom_property(\n      \"payload_format\",\n      orig_examples.get_string_custom_property(\"payload_format\"))\n\n\n  for split in splits_list:\n    input_dir = artifact_utils.get_split_uri([orig_examples], split)\n    output_dir = artifact_utils.get_split_uri([identified_examples], split)\n    os.mkdir(output_dir)\n    with beam.Pipeline() as pipeline:\n      (pipeline\n       | 'ReadExamples' &gt;&gt; beam.io.ReadFromTFRecord(\n           os.path.join(input_dir, '*'),\n           coder=beam.coders.coders.ProtoCoder(tf.train.Example))\n       | 'AddUniqueId' &gt;&gt; beam.Map(make_example_with_unique_id, id_feature_name)\n       | 'WriteIdentifiedExamples' &gt;&gt; beam.io.WriteToTFRecord(\n           file_path_prefix=os.path.join(output_dir, 'data_tfrecord'),\n           coder=beam.coders.coders.ProtoCoder(tf.train.Example),\n           file_name_suffix='.gz'))\n\n  return\n</pre> def make_example_with_unique_id(example, id_feature_name):   \"\"\"Adds a unique ID to the given `tf.train.Example` proto.    This function uses Python's 'uuid' module to generate a universally unique   identifier for each example.    Args:     example: An instance of a `tf.train.Example` proto.     id_feature_name: The name of the feature in the resulting `tf.train.Example`       that will contain the unique identifier.    Returns:     A new `tf.train.Example` proto that includes a unique identifier as an     additional feature.   \"\"\"   result = tf.train.Example()   result.CopyFrom(example)   unique_id = uuid.uuid4()   result.features.feature.get_or_create(       id_feature_name).bytes_list.MergeFrom(           tf.train.BytesList(value=[str(unique_id).encode('utf-8')]))   return result   @component def IdentifyExamples(orig_examples: InputArtifact[Examples],                      identified_examples: OutputArtifact[Examples],                      id_feature_name: Parameter[str],                      component_name: Parameter[str]) -&gt; None:    # Get a list of the splits in input_data   splits_list = artifact_utils.decode_split_names(       split_names=orig_examples.split_names)   # For completeness, encode the splits names and payload_format.   # We could also just use input_data.split_names.   identified_examples.split_names = artifact_utils.encode_split_names(       splits=splits_list)   # TODO(b/168616829): Remove populating payload_format after tfx 0.25.0.   identified_examples.set_string_custom_property(       \"payload_format\",       orig_examples.get_string_custom_property(\"payload_format\"))     for split in splits_list:     input_dir = artifact_utils.get_split_uri([orig_examples], split)     output_dir = artifact_utils.get_split_uri([identified_examples], split)     os.mkdir(output_dir)     with beam.Pipeline() as pipeline:       (pipeline        | 'ReadExamples' &gt;&gt; beam.io.ReadFromTFRecord(            os.path.join(input_dir, '*'),            coder=beam.coders.coders.ProtoCoder(tf.train.Example))        | 'AddUniqueId' &gt;&gt; beam.Map(make_example_with_unique_id, id_feature_name)        | 'WriteIdentifiedExamples' &gt;&gt; beam.io.WriteToTFRecord(            file_path_prefix=os.path.join(output_dir, 'data_tfrecord'),            coder=beam.coders.coders.ProtoCoder(tf.train.Example),            file_name_suffix='.gz'))    return In\u00a0[\u00a0]: Copied! <pre>identify_examples = IdentifyExamples(\n    orig_examples=example_gen.outputs['examples'],\n    component_name=u'IdentifyExamples',\n    id_feature_name=u'id')\ncontext.run(identify_examples, enable_cache=False)\n</pre> identify_examples = IdentifyExamples(     orig_examples=example_gen.outputs['examples'],     component_name=u'IdentifyExamples',     id_feature_name=u'id') context.run(identify_examples, enable_cache=False) In\u00a0[\u00a0]: Copied! <pre># Computes statistics over data for visualization and example validation.\nstatistics_gen = StatisticsGen(\n    examples=identify_examples.outputs[\"identified_examples\"])\ncontext.run(statistics_gen, enable_cache=True)\n</pre> # Computes statistics over data for visualization and example validation. statistics_gen = StatisticsGen(     examples=identify_examples.outputs[\"identified_examples\"]) context.run(statistics_gen, enable_cache=True) In\u00a0[\u00a0]: Copied! <pre># Generates schema based on statistics files.\nschema_gen = SchemaGen(\n    statistics=statistics_gen.outputs['statistics'], infer_feature_shape=False)\ncontext.run(schema_gen, enable_cache=True)\n</pre> # Generates schema based on statistics files. schema_gen = SchemaGen(     statistics=statistics_gen.outputs['statistics'], infer_feature_shape=False) context.run(schema_gen, enable_cache=True) <p>The generated artifact is just a <code>schema.pbtxt</code> containing a text representation of a <code>schema_pb2.Schema</code> protobuf:</p> In\u00a0[\u00a0]: Copied! <pre>train_uri = schema_gen.outputs['schema'].get()[0].uri\nschema_filename = os.path.join(train_uri, 'schema.pbtxt')\nschema = tfx.utils.io_utils.parse_pbtxt_file(\n    file_name=schema_filename, message=schema_pb2.Schema())\n</pre> train_uri = schema_gen.outputs['schema'].get()[0].uri schema_filename = os.path.join(train_uri, 'schema.pbtxt') schema = tfx.utils.io_utils.parse_pbtxt_file(     file_name=schema_filename, message=schema_pb2.Schema()) <p>It can be visualized using <code>tfdv.display_schema()</code> (we will look at this in more detail in a subsequent lab):</p> In\u00a0[\u00a0]: Copied! <pre>tfdv.display_schema(schema)\n</pre> tfdv.display_schema(schema) In\u00a0[\u00a0]: Copied! <pre># Performs anomaly detection based on statistics and data schema.\nvalidate_stats = ExampleValidator(\n    statistics=statistics_gen.outputs['statistics'],\n    schema=schema_gen.outputs['schema'])\ncontext.run(validate_stats, enable_cache=False)\n</pre> # Performs anomaly detection based on statistics and data schema. validate_stats = ExampleValidator(     statistics=statistics_gen.outputs['statistics'],     schema=schema_gen.outputs['schema']) context.run(validate_stats, enable_cache=False) <p>Graph construction involves creating embeddings for text samples and then using a similarity function to compare the embeddings.</p> <p>We will use pretrained Swivel embeddings to create embeddings in the <code>tf.train.Example</code> format for each sample in the input. We will store the resulting embeddings in the <code>TFRecord</code> format along with the sample's ID. This is important and will allow us match sample embeddings with corresponding nodes in the graph later.</p> <p>Once we have the sample embeddings, we will use them to build a similarity graph, i.e, nodes in this graph will correspond to samples and edges in this graph will correspond to similarity between pairs of nodes.</p> <p>Neural Structured Learning provides a graph building library to build a graph based on sample embeddings. It uses cosine similarity as the similarity measure to compare embeddings and build edges between them. It also allows us to specify a similarity threshold, which can be used to discard dissimilar edges from the final graph. In the following example, using 0.99 as the similarity threshold, we end up with a graph that has 111,066 bi-directional edges.</p> <p>Note: Graph quality and by extension, embedding quality, are very important for graph regularization. While we use Swivel embeddings in this notebook, using BERT embeddings for instance, will likely capture review semantics more accurately. We encourage users to use embeddings of their choice and as appropriate to their needs.</p> In\u00a0[\u00a0]: Copied! <pre>swivel_url = 'https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1'\nhub_layer = hub.KerasLayer(swivel_url, input_shape=[], dtype=tf.string)\n\n\ndef _bytes_feature(value):\n  \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n  return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))\n\n\ndef _float_feature(value):\n  \"\"\"Returns a float_list from a float / double.\"\"\"\n  return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n\n\ndef create_embedding_example(example):\n  \"\"\"Create tf.Example containing the sample's embedding and its ID.\"\"\"\n  sentence_embedding = hub_layer(tf.sparse.to_dense(example['text']))\n\n  # Flatten the sentence embedding back to 1-D.\n  sentence_embedding = tf.reshape(sentence_embedding, shape=[-1])\n\n  feature_dict = {\n      'id': _bytes_feature(tf.sparse.to_dense(example['id']).numpy()),\n      'embedding': _float_feature(sentence_embedding.numpy().tolist())\n  }\n\n  return tf.train.Example(features=tf.train.Features(feature=feature_dict))\n\n\ndef create_dataset(uri):\n  tfrecord_filenames = [os.path.join(uri, name) for name in os.listdir(uri)]\n  return tf.data.TFRecordDataset(tfrecord_filenames, compression_type='GZIP')\n\n\ndef create_embeddings(train_path, output_path):\n  dataset = create_dataset(train_path)\n  embeddings_path = os.path.join(output_path, 'embeddings.tfr')\n\n  feature_map = {\n      'label': tf.io.FixedLenFeature([], tf.int64),\n      'id': tf.io.VarLenFeature(tf.string),\n      'text': tf.io.VarLenFeature(tf.string)\n  }\n\n  with tf.io.TFRecordWriter(embeddings_path) as writer:\n    for tfrecord in dataset:\n      tensor_dict = tf.io.parse_single_example(tfrecord, feature_map)\n      embedding_example = create_embedding_example(tensor_dict)\n      writer.write(embedding_example.SerializeToString())\n\n\ndef build_graph(output_path, similarity_threshold):\n  embeddings_path = os.path.join(output_path, 'embeddings.tfr')\n  graph_path = os.path.join(output_path, 'graph.tsv')\n  graph_builder_config = nsl.configs.GraphBuilderConfig(\n      similarity_threshold=similarity_threshold,\n      lsh_splits=32,\n      lsh_rounds=15,\n      random_seed=12345)\n  nsl.tools.build_graph_from_config([embeddings_path], graph_path,\n                                    graph_builder_config)\n</pre> swivel_url = 'https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1' hub_layer = hub.KerasLayer(swivel_url, input_shape=[], dtype=tf.string)   def _bytes_feature(value):   \"\"\"Returns a bytes_list from a string / byte.\"\"\"   return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))   def _float_feature(value):   \"\"\"Returns a float_list from a float / double.\"\"\"   return tf.train.Feature(float_list=tf.train.FloatList(value=value))   def create_embedding_example(example):   \"\"\"Create tf.Example containing the sample's embedding and its ID.\"\"\"   sentence_embedding = hub_layer(tf.sparse.to_dense(example['text']))    # Flatten the sentence embedding back to 1-D.   sentence_embedding = tf.reshape(sentence_embedding, shape=[-1])    feature_dict = {       'id': _bytes_feature(tf.sparse.to_dense(example['id']).numpy()),       'embedding': _float_feature(sentence_embedding.numpy().tolist())   }    return tf.train.Example(features=tf.train.Features(feature=feature_dict))   def create_dataset(uri):   tfrecord_filenames = [os.path.join(uri, name) for name in os.listdir(uri)]   return tf.data.TFRecordDataset(tfrecord_filenames, compression_type='GZIP')   def create_embeddings(train_path, output_path):   dataset = create_dataset(train_path)   embeddings_path = os.path.join(output_path, 'embeddings.tfr')    feature_map = {       'label': tf.io.FixedLenFeature([], tf.int64),       'id': tf.io.VarLenFeature(tf.string),       'text': tf.io.VarLenFeature(tf.string)   }    with tf.io.TFRecordWriter(embeddings_path) as writer:     for tfrecord in dataset:       tensor_dict = tf.io.parse_single_example(tfrecord, feature_map)       embedding_example = create_embedding_example(tensor_dict)       writer.write(embedding_example.SerializeToString())   def build_graph(output_path, similarity_threshold):   embeddings_path = os.path.join(output_path, 'embeddings.tfr')   graph_path = os.path.join(output_path, 'graph.tsv')   graph_builder_config = nsl.configs.GraphBuilderConfig(       similarity_threshold=similarity_threshold,       lsh_splits=32,       lsh_rounds=15,       random_seed=12345)   nsl.tools.build_graph_from_config([embeddings_path], graph_path,                                     graph_builder_config) In\u00a0[\u00a0]: Copied! <pre>\"\"\"Custom Artifact type\"\"\"\n\n\nclass SynthesizedGraph(tfx.types.artifact.Artifact):\n  \"\"\"Output artifact of the SynthesizeGraph component\"\"\"\n  TYPE_NAME = 'SynthesizedGraphPath'\n  PROPERTIES = {\n      'span': standard_artifacts.SPAN_PROPERTY,\n      'split_names': standard_artifacts.SPLIT_NAMES_PROPERTY,\n  }\n\n\n@component\ndef SynthesizeGraph(identified_examples: InputArtifact[Examples],\n                    synthesized_graph: OutputArtifact[SynthesizedGraph],\n                    similarity_threshold: Parameter[float],\n                    component_name: Parameter[str]) -&gt; None:\n\n  # Get a list of the splits in input_data\n  splits_list = artifact_utils.decode_split_names(\n      split_names=identified_examples.split_names)\n\n  # We build a graph only based on the 'Split-train' split which includes both\n  # labeled and unlabeled examples.\n  train_input_examples_uri = os.path.join(identified_examples.uri,\n                                          'Split-train')\n  output_graph_uri = os.path.join(synthesized_graph.uri, 'Split-train')\n  os.mkdir(output_graph_uri)\n\n  print('Creating embeddings...')\n  create_embeddings(train_input_examples_uri, output_graph_uri)\n\n  print('Synthesizing graph...')\n  build_graph(output_graph_uri, similarity_threshold)\n\n  synthesized_graph.split_names = artifact_utils.encode_split_names(\n      splits=['Split-train'])\n\n  return\n</pre> \"\"\"Custom Artifact type\"\"\"   class SynthesizedGraph(tfx.types.artifact.Artifact):   \"\"\"Output artifact of the SynthesizeGraph component\"\"\"   TYPE_NAME = 'SynthesizedGraphPath'   PROPERTIES = {       'span': standard_artifacts.SPAN_PROPERTY,       'split_names': standard_artifacts.SPLIT_NAMES_PROPERTY,   }   @component def SynthesizeGraph(identified_examples: InputArtifact[Examples],                     synthesized_graph: OutputArtifact[SynthesizedGraph],                     similarity_threshold: Parameter[float],                     component_name: Parameter[str]) -&gt; None:    # Get a list of the splits in input_data   splits_list = artifact_utils.decode_split_names(       split_names=identified_examples.split_names)    # We build a graph only based on the 'Split-train' split which includes both   # labeled and unlabeled examples.   train_input_examples_uri = os.path.join(identified_examples.uri,                                           'Split-train')   output_graph_uri = os.path.join(synthesized_graph.uri, 'Split-train')   os.mkdir(output_graph_uri)    print('Creating embeddings...')   create_embeddings(train_input_examples_uri, output_graph_uri)    print('Synthesizing graph...')   build_graph(output_graph_uri, similarity_threshold)    synthesized_graph.split_names = artifact_utils.encode_split_names(       splits=['Split-train'])    return In\u00a0[\u00a0]: Copied! <pre>synthesize_graph = SynthesizeGraph(\n    identified_examples=identify_examples.outputs['identified_examples'],\n    component_name=u'SynthesizeGraph',\n    similarity_threshold=0.99)\ncontext.run(synthesize_graph, enable_cache=False)\n</pre> synthesize_graph = SynthesizeGraph(     identified_examples=identify_examples.outputs['identified_examples'],     component_name=u'SynthesizeGraph',     similarity_threshold=0.99) context.run(synthesize_graph, enable_cache=False) In\u00a0[\u00a0]: Copied! <pre>train_uri = synthesize_graph.outputs[\"synthesized_graph\"].get()[0].uri\nos.listdir(train_uri)\n</pre> train_uri = synthesize_graph.outputs[\"synthesized_graph\"].get()[0].uri os.listdir(train_uri) In\u00a0[\u00a0]: Copied! <pre>graph_path = os.path.join(train_uri, \"Split-train\", \"graph.tsv\")\nprint(\"node 1\\t\\t\\t\\t\\tnode 2\\t\\t\\t\\t\\tsimilarity\")\n!head {graph_path}\nprint(\"...\")\n!tail {graph_path}\n</pre> graph_path = os.path.join(train_uri, \"Split-train\", \"graph.tsv\") print(\"node 1\\t\\t\\t\\t\\tnode 2\\t\\t\\t\\t\\tsimilarity\") !head {graph_path} print(\"...\") !tail {graph_path} In\u00a0[\u00a0]: Copied! <pre>!wc -l {graph_path}\n</pre> !wc -l {graph_path} <p>Each sample will include the following three features:</p> <ol> <li>id: The node ID of the sample.</li> <li>text_xf: An int64 list containing word IDs.</li> <li>label_xf: A singleton int64 identifying the target class of the review: 0=negative, 1=positive.</li> </ol> <p>Let's define a module containing the <code>preprocessing_fn()</code> function that we will pass to the <code>Transform</code> component:</p> In\u00a0[\u00a0]: Copied! <pre>_transform_module_file = 'imdb_transform.py'\n</pre> _transform_module_file = 'imdb_transform.py' In\u00a0[\u00a0]: Copied! <pre>%%writefile {_transform_module_file}\n\nimport tensorflow as tf\n\nimport tensorflow_transform as tft\n\nSEQUENCE_LENGTH = 100\nVOCAB_SIZE = 10000\nOOV_SIZE = 100\n\ndef tokenize_reviews(reviews, sequence_length=SEQUENCE_LENGTH):\n  reviews = tf.strings.lower(reviews)\n  reviews = tf.strings.regex_replace(reviews, r\" '| '|^'|'$\", \" \")\n  reviews = tf.strings.regex_replace(reviews, \"[^a-z' ]\", \" \")\n  tokens = tf.strings.split(reviews)[:, :sequence_length]\n  start_tokens = tf.fill([tf.shape(reviews)[0], 1], \"&lt;START&gt;\")\n  end_tokens = tf.fill([tf.shape(reviews)[0], 1], \"&lt;END&gt;\")\n  tokens = tf.concat([start_tokens, tokens, end_tokens], axis=1)\n  tokens = tokens[:, :sequence_length]\n  tokens = tokens.to_tensor(default_value=\"&lt;PAD&gt;\")\n  pad = sequence_length - tf.shape(tokens)[1]\n  tokens = tf.pad(tokens, [[0, 0], [0, pad]], constant_values=\"&lt;PAD&gt;\")\n  return tf.reshape(tokens, [-1, sequence_length])\n\ndef preprocessing_fn(inputs):\n  \"\"\"tf.transform's callback function for preprocessing inputs.\n\n  Args:\n    inputs: map from feature keys to raw not-yet-transformed features.\n\n  Returns:\n    Map from string feature key to transformed feature operations.\n  \"\"\"\n  outputs = {}\n  outputs[\"id\"] = inputs[\"id\"]\n  tokens = tokenize_reviews(_fill_in_missing(inputs[\"text\"], ''))\n  outputs[\"text_xf\"] = tft.compute_and_apply_vocabulary(\n      tokens,\n      top_k=VOCAB_SIZE,\n      num_oov_buckets=OOV_SIZE)\n  outputs[\"label_xf\"] = _fill_in_missing(inputs[\"label\"], -1)\n  return outputs\n\ndef _fill_in_missing(x, default_value):\n  \"\"\"Replace missing values in a SparseTensor.\n\n  Fills in missing values of `x` with the default_value.\n\n  Args:\n    x: A `SparseTensor` of rank 2.  Its dense shape should have size at most 1\n      in the second dimension.\n    default_value: the value with which to replace the missing values.\n\n  Returns:\n    A rank 1 tensor where missing values of `x` have been filled in.\n  \"\"\"\n  if not isinstance(x, tf.sparse.SparseTensor):\n    return x\n  return tf.squeeze(\n      tf.sparse.to_dense(\n          tf.SparseTensor(x.indices, x.values, [x.dense_shape[0], 1]),\n          default_value),\n      axis=1)\n</pre> %%writefile {_transform_module_file}  import tensorflow as tf  import tensorflow_transform as tft  SEQUENCE_LENGTH = 100 VOCAB_SIZE = 10000 OOV_SIZE = 100  def tokenize_reviews(reviews, sequence_length=SEQUENCE_LENGTH):   reviews = tf.strings.lower(reviews)   reviews = tf.strings.regex_replace(reviews, r\" '| '|^'|'$\", \" \")   reviews = tf.strings.regex_replace(reviews, \"[^a-z' ]\", \" \")   tokens = tf.strings.split(reviews)[:, :sequence_length]   start_tokens = tf.fill([tf.shape(reviews)[0], 1], \"\")   end_tokens = tf.fill([tf.shape(reviews)[0], 1], \"\")   tokens = tf.concat([start_tokens, tokens, end_tokens], axis=1)   tokens = tokens[:, :sequence_length]   tokens = tokens.to_tensor(default_value=\"\")   pad = sequence_length - tf.shape(tokens)[1]   tokens = tf.pad(tokens, [[0, 0], [0, pad]], constant_values=\"\")   return tf.reshape(tokens, [-1, sequence_length])  def preprocessing_fn(inputs):   \"\"\"tf.transform's callback function for preprocessing inputs.    Args:     inputs: map from feature keys to raw not-yet-transformed features.    Returns:     Map from string feature key to transformed feature operations.   \"\"\"   outputs = {}   outputs[\"id\"] = inputs[\"id\"]   tokens = tokenize_reviews(_fill_in_missing(inputs[\"text\"], ''))   outputs[\"text_xf\"] = tft.compute_and_apply_vocabulary(       tokens,       top_k=VOCAB_SIZE,       num_oov_buckets=OOV_SIZE)   outputs[\"label_xf\"] = _fill_in_missing(inputs[\"label\"], -1)   return outputs  def _fill_in_missing(x, default_value):   \"\"\"Replace missing values in a SparseTensor.    Fills in missing values of `x` with the default_value.    Args:     x: A `SparseTensor` of rank 2.  Its dense shape should have size at most 1       in the second dimension.     default_value: the value with which to replace the missing values.    Returns:     A rank 1 tensor where missing values of `x` have been filled in.   \"\"\"   if not isinstance(x, tf.sparse.SparseTensor):     return x   return tf.squeeze(       tf.sparse.to_dense(           tf.SparseTensor(x.indices, x.values, [x.dense_shape[0], 1]),           default_value),       axis=1) <p>Create and run the <code>Transform</code> component, referring to the files that were created above.</p> In\u00a0[\u00a0]: Copied! <pre># Performs transformations and feature engineering in training and serving.\ntransform = Transform(\n    examples=identify_examples.outputs['identified_examples'],\n    schema=schema_gen.outputs['schema'],\n    module_file=_transform_module_file)\ncontext.run(transform, enable_cache=True)\n</pre> # Performs transformations and feature engineering in training and serving. transform = Transform(     examples=identify_examples.outputs['identified_examples'],     schema=schema_gen.outputs['schema'],     module_file=_transform_module_file) context.run(transform, enable_cache=True) <p>The <code>Transform</code> component has 2 types of outputs:</p> <ul> <li><code>transform_graph</code> is the graph that can perform the preprocessing operations (this graph will be included in the serving and evaluation models).</li> <li><code>transformed_examples</code> represents the preprocessed training and evaluation data.</li> </ul> In\u00a0[\u00a0]: Copied! <pre>transform.outputs\n</pre> transform.outputs <p>Take a peek at the <code>transform_graph</code> artifact: it points to a directory containing 3 subdirectories:</p> In\u00a0[\u00a0]: Copied! <pre>train_uri = transform.outputs['transform_graph'].get()[0].uri\nos.listdir(train_uri)\n</pre> train_uri = transform.outputs['transform_graph'].get()[0].uri os.listdir(train_uri) <p>The <code>transform_fn</code> subdirectory contains the actual preprocessing graph. The <code>metadata</code> subdirectory contains the schema of the original data. The <code>transformed_metadata</code> subdirectory contains the schema of the preprocessed data.</p> <p>Take a look at some of the transformed examples and check that they are indeed processed as intended.</p> In\u00a0[\u00a0]: Copied! <pre>def pprint_examples(artifact, n_examples=3):\n  print(\"artifact:\", artifact)\n  uri = os.path.join(artifact.uri, \"Split-train\")\n  print(\"uri:\", uri)\n  tfrecord_filenames = [os.path.join(uri, name) for name in os.listdir(uri)]\n  print(\"tfrecord_filenames:\", tfrecord_filenames)\n  dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")\n  for tfrecord in dataset.take(n_examples):\n    serialized_example = tfrecord.numpy()\n    example = tf.train.Example.FromString(serialized_example)\n    pp.pprint(example)\n</pre> def pprint_examples(artifact, n_examples=3):   print(\"artifact:\", artifact)   uri = os.path.join(artifact.uri, \"Split-train\")   print(\"uri:\", uri)   tfrecord_filenames = [os.path.join(uri, name) for name in os.listdir(uri)]   print(\"tfrecord_filenames:\", tfrecord_filenames)   dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")   for tfrecord in dataset.take(n_examples):     serialized_example = tfrecord.numpy()     example = tf.train.Example.FromString(serialized_example)     pp.pprint(example) In\u00a0[\u00a0]: Copied! <pre>pprint_examples(transform.outputs['transformed_examples'].get()[0])\n</pre> pprint_examples(transform.outputs['transformed_examples'].get()[0]) In\u00a0[\u00a0]: Copied! <pre>def split_train_and_unsup(input_uri):\n  'Separate the labeled and unlabeled instances.'\n\n  tmp_dir = tempfile.mkdtemp(prefix='tfx-data')\n  tfrecord_filenames = [\n      os.path.join(input_uri, filename) for filename in os.listdir(input_uri)\n  ]\n  train_path = os.path.join(tmp_dir, 'train.tfrecord')\n  unsup_path = os.path.join(tmp_dir, 'unsup.tfrecord')\n  with tf.io.TFRecordWriter(train_path) as train_writer, \\\n       tf.io.TFRecordWriter(unsup_path) as unsup_writer:\n    for tfrecord in tf.data.TFRecordDataset(\n        tfrecord_filenames, compression_type='GZIP'):\n      example = tf.train.Example()\n      example.ParseFromString(tfrecord.numpy())\n      if ('label_xf' not in example.features.feature or\n          example.features.feature['label_xf'].int64_list.value[0] == -1):\n        writer = unsup_writer\n      else:\n        writer = train_writer\n      writer.write(tfrecord.numpy())\n  return train_path, unsup_path\n\n\ndef gzip(filepath):\n  with open(filepath, 'rb') as f_in:\n    with gzip_lib.open(filepath + '.gz', 'wb') as f_out:\n      shutil.copyfileobj(f_in, f_out)\n  os.remove(filepath)\n\n\ndef copy_tfrecords(input_uri, output_uri):\n  for filename in os.listdir(input_uri):\n    input_filename = os.path.join(input_uri, filename)\n    output_filename = os.path.join(output_uri, filename)\n    shutil.copyfile(input_filename, output_filename)\n\n\n@component\ndef GraphAugmentation(identified_examples: InputArtifact[Examples],\n                      synthesized_graph: InputArtifact[SynthesizedGraph],\n                      augmented_examples: OutputArtifact[Examples],\n                      num_neighbors: Parameter[int],\n                      component_name: Parameter[str]) -&gt; None:\n\n  # Get a list of the splits in input_data\n  splits_list = artifact_utils.decode_split_names(\n      split_names=identified_examples.split_names)\n\n  train_input_uri = os.path.join(identified_examples.uri, 'Split-train')\n  eval_input_uri = os.path.join(identified_examples.uri, 'Split-eval')\n  train_graph_uri = os.path.join(synthesized_graph.uri, 'Split-train')\n  train_output_uri = os.path.join(augmented_examples.uri, 'Split-train')\n  eval_output_uri = os.path.join(augmented_examples.uri, 'Split-eval')\n\n  os.mkdir(train_output_uri)\n  os.mkdir(eval_output_uri)\n\n  # Separate the labeled and unlabeled examples from the 'Split-train' split.\n  train_path, unsup_path = split_train_and_unsup(train_input_uri)\n\n  output_path = os.path.join(train_output_uri, 'nsl_train_data.tfr')\n  pack_nbrs_args = dict(\n      labeled_examples_path=train_path,\n      unlabeled_examples_path=unsup_path,\n      graph_path=os.path.join(train_graph_uri, 'graph.tsv'),\n      output_training_data_path=output_path,\n      add_undirected_edges=True,\n      max_nbrs=num_neighbors)\n  print('nsl.tools.pack_nbrs arguments:', pack_nbrs_args)\n  nsl.tools.pack_nbrs(**pack_nbrs_args)\n\n  # Downstream components expect gzip'ed TFRecords.\n  gzip(output_path)\n\n  # The test examples are left untouched and are simply copied over.\n  copy_tfrecords(eval_input_uri, eval_output_uri)\n\n  augmented_examples.split_names = identified_examples.split_names\n\n  return\n</pre> def split_train_and_unsup(input_uri):   'Separate the labeled and unlabeled instances.'    tmp_dir = tempfile.mkdtemp(prefix='tfx-data')   tfrecord_filenames = [       os.path.join(input_uri, filename) for filename in os.listdir(input_uri)   ]   train_path = os.path.join(tmp_dir, 'train.tfrecord')   unsup_path = os.path.join(tmp_dir, 'unsup.tfrecord')   with tf.io.TFRecordWriter(train_path) as train_writer, \\        tf.io.TFRecordWriter(unsup_path) as unsup_writer:     for tfrecord in tf.data.TFRecordDataset(         tfrecord_filenames, compression_type='GZIP'):       example = tf.train.Example()       example.ParseFromString(tfrecord.numpy())       if ('label_xf' not in example.features.feature or           example.features.feature['label_xf'].int64_list.value[0] == -1):         writer = unsup_writer       else:         writer = train_writer       writer.write(tfrecord.numpy())   return train_path, unsup_path   def gzip(filepath):   with open(filepath, 'rb') as f_in:     with gzip_lib.open(filepath + '.gz', 'wb') as f_out:       shutil.copyfileobj(f_in, f_out)   os.remove(filepath)   def copy_tfrecords(input_uri, output_uri):   for filename in os.listdir(input_uri):     input_filename = os.path.join(input_uri, filename)     output_filename = os.path.join(output_uri, filename)     shutil.copyfile(input_filename, output_filename)   @component def GraphAugmentation(identified_examples: InputArtifact[Examples],                       synthesized_graph: InputArtifact[SynthesizedGraph],                       augmented_examples: OutputArtifact[Examples],                       num_neighbors: Parameter[int],                       component_name: Parameter[str]) -&gt; None:    # Get a list of the splits in input_data   splits_list = artifact_utils.decode_split_names(       split_names=identified_examples.split_names)    train_input_uri = os.path.join(identified_examples.uri, 'Split-train')   eval_input_uri = os.path.join(identified_examples.uri, 'Split-eval')   train_graph_uri = os.path.join(synthesized_graph.uri, 'Split-train')   train_output_uri = os.path.join(augmented_examples.uri, 'Split-train')   eval_output_uri = os.path.join(augmented_examples.uri, 'Split-eval')    os.mkdir(train_output_uri)   os.mkdir(eval_output_uri)    # Separate the labeled and unlabeled examples from the 'Split-train' split.   train_path, unsup_path = split_train_and_unsup(train_input_uri)    output_path = os.path.join(train_output_uri, 'nsl_train_data.tfr')   pack_nbrs_args = dict(       labeled_examples_path=train_path,       unlabeled_examples_path=unsup_path,       graph_path=os.path.join(train_graph_uri, 'graph.tsv'),       output_training_data_path=output_path,       add_undirected_edges=True,       max_nbrs=num_neighbors)   print('nsl.tools.pack_nbrs arguments:', pack_nbrs_args)   nsl.tools.pack_nbrs(**pack_nbrs_args)    # Downstream components expect gzip'ed TFRecords.   gzip(output_path)    # The test examples are left untouched and are simply copied over.   copy_tfrecords(eval_input_uri, eval_output_uri)    augmented_examples.split_names = identified_examples.split_names    return In\u00a0[\u00a0]: Copied! <pre># Augments training data with graph neighbors.\ngraph_augmentation = GraphAugmentation(\n    identified_examples=transform.outputs['transformed_examples'],\n    synthesized_graph=synthesize_graph.outputs['synthesized_graph'],\n    component_name=u'GraphAugmentation',\n    num_neighbors=3)\ncontext.run(graph_augmentation, enable_cache=False)\n</pre> # Augments training data with graph neighbors. graph_augmentation = GraphAugmentation(     identified_examples=transform.outputs['transformed_examples'],     synthesized_graph=synthesize_graph.outputs['synthesized_graph'],     component_name=u'GraphAugmentation',     num_neighbors=3) context.run(graph_augmentation, enable_cache=False) In\u00a0[\u00a0]: Copied! <pre>pprint_examples(graph_augmentation.outputs['augmented_examples'].get()[0], 6)\n</pre> pprint_examples(graph_augmentation.outputs['augmented_examples'].get()[0], 6) In\u00a0[\u00a0]: Copied! <pre># Setup paths.\n_trainer_module_file = 'imdb_trainer.py'\n</pre> # Setup paths. _trainer_module_file = 'imdb_trainer.py' In\u00a0[\u00a0]: Copied! <pre>%%writefile {_trainer_module_file}\n\nimport neural_structured_learning as nsl\n\nimport tensorflow as tf\n\nimport tensorflow_model_analysis as tfma\nimport tensorflow_transform as tft\nfrom tensorflow_transform.tf_metadata import schema_utils\n\n\nNBR_FEATURE_PREFIX = 'NL_nbr_'\nNBR_WEIGHT_SUFFIX = '_weight'\nLABEL_KEY = 'label'\nID_FEATURE_KEY = 'id'\n\ndef _transformed_name(key):\n  return key + '_xf'\n\n\ndef _transformed_names(keys):\n  return [_transformed_name(key) for key in keys]\n\n\n# Hyperparameters:\n#\n# We will use an instance of `HParams` to inclue various hyperparameters and\n# constants used for training and evaluation. We briefly describe each of them\n# below:\n#\n# -   max_seq_length: This is the maximum number of words considered from each\n#                     movie review in this example.\n# -   vocab_size: This is the size of the vocabulary considered for this\n#                 example.\n# -   oov_size: This is the out-of-vocabulary size considered for this example.\n# -   distance_type: This is the distance metric used to regularize the sample\n#                    with its neighbors.\n# -   graph_regularization_multiplier: This controls the relative weight of the\n#                                      graph regularization term in the overall\n#                                      loss function.\n# -   num_neighbors: The number of neighbors used for graph regularization. This\n#                    value has to be less than or equal to the `num_neighbors`\n#                    argument used above in the GraphAugmentation component when\n#                    invoking `nsl.tools.pack_nbrs`.\n# -   num_fc_units: The number of units in the fully connected layer of the\n#                   neural network.\nclass HParams(object):\n  \"\"\"Hyperparameters used for training.\"\"\"\n  def __init__(self):\n    ### dataset parameters\n    # The following 3 values should match those defined in the Transform\n    # Component.\n    self.max_seq_length = 100\n    self.vocab_size = 10000\n    self.oov_size = 100\n    ### Neural Graph Learning parameters\n    self.distance_type = nsl.configs.DistanceType.L2\n    self.graph_regularization_multiplier = 0.1\n    # The following value has to be at most the value of 'num_neighbors' used\n    # in the GraphAugmentation component.\n    self.num_neighbors = 1\n    ### Model Architecture\n    self.num_embedding_dims = 16\n    self.num_fc_units = 64\n\nHPARAMS = HParams()\n\n\ndef optimizer_fn():\n  \"\"\"Returns an instance of `tf.Optimizer`.\"\"\"\n  return tf.compat.v1.train.RMSPropOptimizer(\n    learning_rate=0.0001, decay=1e-6)\n\n\ndef build_train_op(loss, global_step):\n  \"\"\"Builds a train op to optimize the given loss using gradient descent.\"\"\"\n  with tf.name_scope('train'):\n    optimizer = optimizer_fn()\n    train_op = optimizer.minimize(loss=loss, global_step=global_step)\n  return train_op\n\n\n# Building the model:\n#\n# A neural network is created by stacking layers\u2014this requires two main\n# architectural decisions:\n# * How many layers to use in the model?\n# * How many *hidden units* to use for each layer?\n#\n# In this example, the input data consists of an array of word-indices. The\n# labels to predict are either 0 or 1. We will use a feed-forward neural network\n# as our base model in this tutorial.\ndef feed_forward_model(features, is_training, reuse=tf.compat.v1.AUTO_REUSE):\n  \"\"\"Builds a simple 2 layer feed forward neural network.\n\n  The layers are effectively stacked sequentially to build the classifier. The\n  first layer is an Embedding layer, which takes the integer-encoded vocabulary\n  and looks up the embedding vector for each word-index. These vectors are\n  learned as the model trains. The vectors add a dimension to the output array.\n  The resulting dimensions are: (batch, sequence, embedding). Next is a global\n  average pooling 1D layer, which reduces the dimensionality of its inputs from\n  3D to 2D. This fixed-length output vector is piped through a fully-connected\n  (Dense) layer with 16 hidden units. The last layer is densely connected with a\n  single output node. Using the sigmoid activation function, this value is a\n  float between 0 and 1, representing a probability, or confidence level.\n\n  Args:\n    features: A dictionary containing batch features returned from the\n      `input_fn`, that include sample features, corresponding neighbor features,\n      and neighbor weights.\n    is_training: a Python Boolean value or a Boolean scalar Tensor, indicating\n      whether to apply dropout.\n    reuse: a Python Boolean value for reusing variable scope.\n\n  Returns:\n    logits: Tensor of shape [batch_size, 1].\n    representations: Tensor of shape [batch_size, _] for graph regularization.\n      This is the representation of each example at the graph regularization\n      layer.\n  \"\"\"\n\n  with tf.compat.v1.variable_scope('ff', reuse=reuse):\n    inputs = features[_transformed_name('text')]\n    embeddings = tf.compat.v1.get_variable(\n        'embeddings',\n        shape=[\n            HPARAMS.vocab_size + HPARAMS.oov_size, HPARAMS.num_embedding_dims\n        ])\n    embedding_layer = tf.nn.embedding_lookup(embeddings, inputs)\n\n    pooling_layer = tf.compat.v1.layers.AveragePooling1D(\n        pool_size=HPARAMS.max_seq_length, strides=HPARAMS.max_seq_length)(\n            embedding_layer)\n    # Shape of pooling_layer is now [batch_size, 1, HPARAMS.num_embedding_dims]\n    pooling_layer = tf.reshape(pooling_layer, [-1, HPARAMS.num_embedding_dims])\n\n    dense_layer = tf.compat.v1.layers.Dense(\n        16, activation='relu')(\n            pooling_layer)\n\n    output_layer = tf.compat.v1.layers.Dense(\n        1, activation='sigmoid')(\n            dense_layer)\n\n    # Graph regularization will be done on the penultimate (dense) layer\n    # because the output layer is a single floating point number.\n    return output_layer, dense_layer\n\n\n# A note on hidden units:\n#\n# The above model has two intermediate or \"hidden\" layers, between the input and\n# output, and excluding the Embedding layer. The number of outputs (units,\n# nodes, or neurons) is the dimension of the representational space for the\n# layer. In other words, the amount of freedom the network is allowed when\n# learning an internal representation. If a model has more hidden units\n# (a higher-dimensional representation space), and/or more layers, then the\n# network can learn more complex representations. However, it makes the network\n# more computationally expensive and may lead to learning unwanted\n# patterns\u2014patterns that improve performance on training data but not on the\n# test data. This is called overfitting.\n\n\n# This function will be used to generate the embeddings for samples and their\n# corresponding neighbors, which will then be used for graph regularization.\ndef embedding_fn(features, mode, **params):\n  \"\"\"Returns the embedding corresponding to the given features.\n\n  Args:\n    features: A dictionary containing batch features returned from the\n      `input_fn`, that include sample features, corresponding neighbor features,\n      and neighbor weights.\n    mode: Specifies if this is training, evaluation, or prediction. See\n      tf.estimator.ModeKeys.\n\n  Returns:\n    The embedding that will be used for graph regularization.\n  \"\"\"\n  is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n  _, embedding = feed_forward_model(features, is_training)\n  return embedding\n\n\ndef feed_forward_model_fn(features, labels, mode, params, config):\n  \"\"\"Implementation of the model_fn for the base feed-forward model.\n\n  Args:\n    features: This is the first item returned from the `input_fn` passed to\n      `train`, `evaluate`, and `predict`. This should be a single `Tensor` or\n      `dict` of same.\n    labels: This is the second item returned from the `input_fn` passed to\n      `train`, `evaluate`, and `predict`. This should be a single `Tensor` or\n      `dict` of same (for multi-head models). If mode is `ModeKeys.PREDICT`,\n      `labels=None` will be passed. If the `model_fn`'s signature does not\n      accept `mode`, the `model_fn` must still be able to handle `labels=None`.\n    mode: Optional. Specifies if this training, evaluation or prediction. See\n      `ModeKeys`.\n    params: An HParams instance as returned by get_hyper_parameters().\n    config: Optional configuration object. Will receive what is passed to\n      Estimator in `config` parameter, or the default `config`. Allows updating\n      things in your model_fn based on configuration such as `num_ps_replicas`,\n      or `model_dir`. Unused currently.\n\n  Returns:\n     A `tf.estimator.EstimatorSpec` for the base feed-forward model. This does\n     not include graph-based regularization.\n  \"\"\"\n\n  is_training = mode == tf.estimator.ModeKeys.TRAIN\n\n  # Build the computation graph.\n  probabilities, _ = feed_forward_model(features, is_training)\n  predictions = tf.round(probabilities)\n\n  if mode == tf.estimator.ModeKeys.PREDICT:\n    # labels will be None, and no loss to compute.\n    cross_entropy_loss = None\n    eval_metric_ops = None\n  else:\n    # Loss is required in train and eval modes.\n    # Flatten 'probabilities' to 1-D.\n    probabilities = tf.reshape(probabilities, shape=[-1])\n    cross_entropy_loss = tf.compat.v1.keras.losses.binary_crossentropy(\n        labels, probabilities)\n    eval_metric_ops = {\n        'accuracy': tf.compat.v1.metrics.accuracy(labels, predictions)\n    }\n\n  if is_training:\n    global_step = tf.compat.v1.train.get_or_create_global_step()\n    train_op = build_train_op(cross_entropy_loss, global_step)\n  else:\n    train_op = None\n\n  return tf.estimator.EstimatorSpec(\n      mode=mode,\n      predictions={\n          'probabilities': probabilities,\n          'predictions': predictions\n      },\n      loss=cross_entropy_loss,\n      train_op=train_op,\n      eval_metric_ops=eval_metric_ops)\n\n\n# Tf.Transform considers these features as \"raw\"\ndef _get_raw_feature_spec(schema):\n  return schema_utils.schema_as_feature_spec(schema).feature_spec\n\n\ndef _gzip_reader_fn(filenames):\n  \"\"\"Small utility returning a record reader that can read gzip'ed files.\"\"\"\n  return tf.data.TFRecordDataset(\n      filenames,\n      compression_type='GZIP')\n\n\ndef _example_serving_receiver_fn(tf_transform_output, schema):\n  \"\"\"Build the serving in inputs.\n\n  Args:\n    tf_transform_output: A TFTransformOutput.\n    schema: the schema of the input data.\n\n  Returns:\n    Tensorflow graph which parses examples, applying tf-transform to them.\n  \"\"\"\n  raw_feature_spec = _get_raw_feature_spec(schema)\n  raw_feature_spec.pop(LABEL_KEY)\n\n  # We don't need the ID feature for serving.\n  raw_feature_spec.pop(ID_FEATURE_KEY)\n\n  raw_input_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(\n      raw_feature_spec, default_batch_size=None)\n  serving_input_receiver = raw_input_fn()\n\n  transformed_features = tf_transform_output.transform_raw_features(\n      serving_input_receiver.features)\n\n  # Even though, LABEL_KEY was removed from 'raw_feature_spec', the transform\n  # operation would have injected the transformed LABEL_KEY feature with a\n  # default value.\n  transformed_features.pop(_transformed_name(LABEL_KEY))\n  return tf.estimator.export.ServingInputReceiver(\n      transformed_features, serving_input_receiver.receiver_tensors)\n\n\ndef _eval_input_receiver_fn(tf_transform_output, schema):\n  \"\"\"Build everything needed for the tf-model-analysis to run the model.\n\n  Args:\n    tf_transform_output: A TFTransformOutput.\n    schema: the schema of the input data.\n\n  Returns:\n    EvalInputReceiver function, which contains:\n      - Tensorflow graph which parses raw untransformed features, applies the\n        tf-transform preprocessing operators.\n      - Set of raw, untransformed features.\n      - Label against which predictions will be compared.\n  \"\"\"\n  # Notice that the inputs are raw features, not transformed features here.\n  raw_feature_spec = _get_raw_feature_spec(schema)\n\n  # We don't need the ID feature for TFMA.\n  raw_feature_spec.pop(ID_FEATURE_KEY)\n\n  raw_input_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(\n      raw_feature_spec, default_batch_size=None)\n  serving_input_receiver = raw_input_fn()\n\n  transformed_features = tf_transform_output.transform_raw_features(\n      serving_input_receiver.features)\n\n  labels = transformed_features.pop(_transformed_name(LABEL_KEY))\n  return tfma.export.EvalInputReceiver(\n      features=transformed_features,\n      receiver_tensors=serving_input_receiver.receiver_tensors,\n      labels=labels)\n\n\ndef _augment_feature_spec(feature_spec, num_neighbors):\n  \"\"\"Augments `feature_spec` to include neighbor features.\n    Args:\n      feature_spec: Dictionary of feature keys mapping to TF feature types.\n      num_neighbors: Number of neighbors to use for feature key augmentation.\n    Returns:\n      An augmented `feature_spec` that includes neighbor feature keys.\n  \"\"\"\n  for i in range(num_neighbors):\n    feature_spec['{}{}_{}'.format(NBR_FEATURE_PREFIX, i, 'id')] = \\\n        tf.io.VarLenFeature(dtype=tf.string)\n    # We don't care about the neighbor features corresponding to\n    # _transformed_name(LABEL_KEY) because the LABEL_KEY feature will be\n    # removed from the feature spec during training/evaluation.\n    feature_spec['{}{}_{}'.format(NBR_FEATURE_PREFIX, i, 'text_xf')] = \\\n        tf.io.FixedLenFeature(shape=[HPARAMS.max_seq_length], dtype=tf.int64,\n                              default_value=tf.constant(0, dtype=tf.int64,\n                                                        shape=[HPARAMS.max_seq_length]))\n    # The 'NL_num_nbrs' features is currently not used.\n\n  # Set the neighbor weight feature keys.\n  for i in range(num_neighbors):\n    feature_spec['{}{}{}'.format(NBR_FEATURE_PREFIX, i, NBR_WEIGHT_SUFFIX)] = \\\n        tf.io.FixedLenFeature(shape=[1], dtype=tf.float32, default_value=[0.0])\n\n  return feature_spec\n\n\ndef _input_fn(filenames, tf_transform_output, is_training, batch_size=200):\n  \"\"\"Generates features and labels for training or evaluation.\n\n  Args:\n    filenames: [str] list of CSV files to read data from.\n    tf_transform_output: A TFTransformOutput.\n    is_training: Boolean indicating if we are in training mode.\n    batch_size: int First dimension size of the Tensors returned by input_fn\n\n  Returns:\n    A (features, indices) tuple where features is a dictionary of\n      Tensors, and indices is a single Tensor of label indices.\n  \"\"\"\n  transformed_feature_spec = (\n      tf_transform_output.transformed_feature_spec().copy())\n\n  # During training, NSL uses augmented training data (which includes features\n  # from graph neighbors). So, update the feature spec accordingly. This needs\n  # to be done because we are using different schemas for NSL training and eval,\n  # but the Trainer Component only accepts a single schema.\n  if is_training:\n    transformed_feature_spec =_augment_feature_spec(transformed_feature_spec,\n                                                    HPARAMS.num_neighbors)\n\n  dataset = tf.data.experimental.make_batched_features_dataset(\n      filenames, batch_size, transformed_feature_spec, reader=_gzip_reader_fn)\n\n  transformed_features = tf.compat.v1.data.make_one_shot_iterator(\n      dataset).get_next()\n  # We pop the label because we do not want to use it as a feature while we're\n  # training.\n  return transformed_features, transformed_features.pop(\n      _transformed_name(LABEL_KEY))\n\n\n# TFX will call this function\ndef trainer_fn(hparams, schema):\n  \"\"\"Build the estimator using the high level API.\n  Args:\n    hparams: Holds hyperparameters used to train the model as name/value pairs.\n    schema: Holds the schema of the training examples.\n  Returns:\n    A dict of the following:\n      - estimator: The estimator that will be used for training and eval.\n      - train_spec: Spec for training.\n      - eval_spec: Spec for eval.\n      - eval_input_receiver_fn: Input function for eval.\n  \"\"\"\n  train_batch_size = 40\n  eval_batch_size = 40\n\n  tf_transform_output = tft.TFTransformOutput(hparams.transform_output)\n\n  train_input_fn = lambda: _input_fn(\n      hparams.train_files,\n      tf_transform_output,\n      is_training=True,\n      batch_size=train_batch_size)\n\n  eval_input_fn = lambda: _input_fn(\n      hparams.eval_files,\n      tf_transform_output,\n      is_training=False,\n      batch_size=eval_batch_size)\n\n  train_spec = tf.estimator.TrainSpec(\n      train_input_fn,\n      max_steps=hparams.train_steps)\n\n  serving_receiver_fn = lambda: _example_serving_receiver_fn(\n      tf_transform_output, schema)\n\n  exporter = tf.estimator.FinalExporter('imdb', serving_receiver_fn)\n  eval_spec = tf.estimator.EvalSpec(\n      eval_input_fn,\n      steps=hparams.eval_steps,\n      exporters=[exporter],\n      name='imdb-eval')\n\n  run_config = tf.estimator.RunConfig(\n      save_checkpoints_steps=999, keep_checkpoint_max=1)\n\n  run_config = run_config.replace(model_dir=hparams.serving_model_dir)\n\n  estimator = tf.estimator.Estimator(\n      model_fn=feed_forward_model_fn, config=run_config, params=HPARAMS)\n\n  # Create a graph regularization config.\n  graph_reg_config = nsl.configs.make_graph_reg_config(\n      max_neighbors=HPARAMS.num_neighbors,\n      multiplier=HPARAMS.graph_regularization_multiplier,\n      distance_type=HPARAMS.distance_type,\n      sum_over_axis=-1)\n\n  # Invoke the Graph Regularization Estimator wrapper to incorporate\n  # graph-based regularization for training.\n  graph_nsl_estimator = nsl.estimator.add_graph_regularization(\n      estimator,\n      embedding_fn,\n      optimizer_fn=optimizer_fn,\n      graph_reg_config=graph_reg_config)\n\n  # Create an input receiver for TFMA processing\n  receiver_fn = lambda: _eval_input_receiver_fn(\n      tf_transform_output, schema)\n\n  return {\n      'estimator': graph_nsl_estimator,\n      'train_spec': train_spec,\n      'eval_spec': eval_spec,\n      'eval_input_receiver_fn': receiver_fn\n  }\n</pre> %%writefile {_trainer_module_file}  import neural_structured_learning as nsl  import tensorflow as tf  import tensorflow_model_analysis as tfma import tensorflow_transform as tft from tensorflow_transform.tf_metadata import schema_utils   NBR_FEATURE_PREFIX = 'NL_nbr_' NBR_WEIGHT_SUFFIX = '_weight' LABEL_KEY = 'label' ID_FEATURE_KEY = 'id'  def _transformed_name(key):   return key + '_xf'   def _transformed_names(keys):   return [_transformed_name(key) for key in keys]   # Hyperparameters: # # We will use an instance of `HParams` to inclue various hyperparameters and # constants used for training and evaluation. We briefly describe each of them # below: # # -   max_seq_length: This is the maximum number of words considered from each #                     movie review in this example. # -   vocab_size: This is the size of the vocabulary considered for this #                 example. # -   oov_size: This is the out-of-vocabulary size considered for this example. # -   distance_type: This is the distance metric used to regularize the sample #                    with its neighbors. # -   graph_regularization_multiplier: This controls the relative weight of the #                                      graph regularization term in the overall #                                      loss function. # -   num_neighbors: The number of neighbors used for graph regularization. This #                    value has to be less than or equal to the `num_neighbors` #                    argument used above in the GraphAugmentation component when #                    invoking `nsl.tools.pack_nbrs`. # -   num_fc_units: The number of units in the fully connected layer of the #                   neural network. class HParams(object):   \"\"\"Hyperparameters used for training.\"\"\"   def __init__(self):     ### dataset parameters     # The following 3 values should match those defined in the Transform     # Component.     self.max_seq_length = 100     self.vocab_size = 10000     self.oov_size = 100     ### Neural Graph Learning parameters     self.distance_type = nsl.configs.DistanceType.L2     self.graph_regularization_multiplier = 0.1     # The following value has to be at most the value of 'num_neighbors' used     # in the GraphAugmentation component.     self.num_neighbors = 1     ### Model Architecture     self.num_embedding_dims = 16     self.num_fc_units = 64  HPARAMS = HParams()   def optimizer_fn():   \"\"\"Returns an instance of `tf.Optimizer`.\"\"\"   return tf.compat.v1.train.RMSPropOptimizer(     learning_rate=0.0001, decay=1e-6)   def build_train_op(loss, global_step):   \"\"\"Builds a train op to optimize the given loss using gradient descent.\"\"\"   with tf.name_scope('train'):     optimizer = optimizer_fn()     train_op = optimizer.minimize(loss=loss, global_step=global_step)   return train_op   # Building the model: # # A neural network is created by stacking layers\u2014this requires two main # architectural decisions: # * How many layers to use in the model? # * How many *hidden units* to use for each layer? # # In this example, the input data consists of an array of word-indices. The # labels to predict are either 0 or 1. We will use a feed-forward neural network # as our base model in this tutorial. def feed_forward_model(features, is_training, reuse=tf.compat.v1.AUTO_REUSE):   \"\"\"Builds a simple 2 layer feed forward neural network.    The layers are effectively stacked sequentially to build the classifier. The   first layer is an Embedding layer, which takes the integer-encoded vocabulary   and looks up the embedding vector for each word-index. These vectors are   learned as the model trains. The vectors add a dimension to the output array.   The resulting dimensions are: (batch, sequence, embedding). Next is a global   average pooling 1D layer, which reduces the dimensionality of its inputs from   3D to 2D. This fixed-length output vector is piped through a fully-connected   (Dense) layer with 16 hidden units. The last layer is densely connected with a   single output node. Using the sigmoid activation function, this value is a   float between 0 and 1, representing a probability, or confidence level.    Args:     features: A dictionary containing batch features returned from the       `input_fn`, that include sample features, corresponding neighbor features,       and neighbor weights.     is_training: a Python Boolean value or a Boolean scalar Tensor, indicating       whether to apply dropout.     reuse: a Python Boolean value for reusing variable scope.    Returns:     logits: Tensor of shape [batch_size, 1].     representations: Tensor of shape [batch_size, _] for graph regularization.       This is the representation of each example at the graph regularization       layer.   \"\"\"    with tf.compat.v1.variable_scope('ff', reuse=reuse):     inputs = features[_transformed_name('text')]     embeddings = tf.compat.v1.get_variable(         'embeddings',         shape=[             HPARAMS.vocab_size + HPARAMS.oov_size, HPARAMS.num_embedding_dims         ])     embedding_layer = tf.nn.embedding_lookup(embeddings, inputs)      pooling_layer = tf.compat.v1.layers.AveragePooling1D(         pool_size=HPARAMS.max_seq_length, strides=HPARAMS.max_seq_length)(             embedding_layer)     # Shape of pooling_layer is now [batch_size, 1, HPARAMS.num_embedding_dims]     pooling_layer = tf.reshape(pooling_layer, [-1, HPARAMS.num_embedding_dims])      dense_layer = tf.compat.v1.layers.Dense(         16, activation='relu')(             pooling_layer)      output_layer = tf.compat.v1.layers.Dense(         1, activation='sigmoid')(             dense_layer)      # Graph regularization will be done on the penultimate (dense) layer     # because the output layer is a single floating point number.     return output_layer, dense_layer   # A note on hidden units: # # The above model has two intermediate or \"hidden\" layers, between the input and # output, and excluding the Embedding layer. The number of outputs (units, # nodes, or neurons) is the dimension of the representational space for the # layer. In other words, the amount of freedom the network is allowed when # learning an internal representation. If a model has more hidden units # (a higher-dimensional representation space), and/or more layers, then the # network can learn more complex representations. However, it makes the network # more computationally expensive and may lead to learning unwanted # patterns\u2014patterns that improve performance on training data but not on the # test data. This is called overfitting.   # This function will be used to generate the embeddings for samples and their # corresponding neighbors, which will then be used for graph regularization. def embedding_fn(features, mode, **params):   \"\"\"Returns the embedding corresponding to the given features.    Args:     features: A dictionary containing batch features returned from the       `input_fn`, that include sample features, corresponding neighbor features,       and neighbor weights.     mode: Specifies if this is training, evaluation, or prediction. See       tf.estimator.ModeKeys.    Returns:     The embedding that will be used for graph regularization.   \"\"\"   is_training = (mode == tf.estimator.ModeKeys.TRAIN)   _, embedding = feed_forward_model(features, is_training)   return embedding   def feed_forward_model_fn(features, labels, mode, params, config):   \"\"\"Implementation of the model_fn for the base feed-forward model.    Args:     features: This is the first item returned from the `input_fn` passed to       `train`, `evaluate`, and `predict`. This should be a single `Tensor` or       `dict` of same.     labels: This is the second item returned from the `input_fn` passed to       `train`, `evaluate`, and `predict`. This should be a single `Tensor` or       `dict` of same (for multi-head models). If mode is `ModeKeys.PREDICT`,       `labels=None` will be passed. If the `model_fn`'s signature does not       accept `mode`, the `model_fn` must still be able to handle `labels=None`.     mode: Optional. Specifies if this training, evaluation or prediction. See       `ModeKeys`.     params: An HParams instance as returned by get_hyper_parameters().     config: Optional configuration object. Will receive what is passed to       Estimator in `config` parameter, or the default `config`. Allows updating       things in your model_fn based on configuration such as `num_ps_replicas`,       or `model_dir`. Unused currently.    Returns:      A `tf.estimator.EstimatorSpec` for the base feed-forward model. This does      not include graph-based regularization.   \"\"\"    is_training = mode == tf.estimator.ModeKeys.TRAIN    # Build the computation graph.   probabilities, _ = feed_forward_model(features, is_training)   predictions = tf.round(probabilities)    if mode == tf.estimator.ModeKeys.PREDICT:     # labels will be None, and no loss to compute.     cross_entropy_loss = None     eval_metric_ops = None   else:     # Loss is required in train and eval modes.     # Flatten 'probabilities' to 1-D.     probabilities = tf.reshape(probabilities, shape=[-1])     cross_entropy_loss = tf.compat.v1.keras.losses.binary_crossentropy(         labels, probabilities)     eval_metric_ops = {         'accuracy': tf.compat.v1.metrics.accuracy(labels, predictions)     }    if is_training:     global_step = tf.compat.v1.train.get_or_create_global_step()     train_op = build_train_op(cross_entropy_loss, global_step)   else:     train_op = None    return tf.estimator.EstimatorSpec(       mode=mode,       predictions={           'probabilities': probabilities,           'predictions': predictions       },       loss=cross_entropy_loss,       train_op=train_op,       eval_metric_ops=eval_metric_ops)   # Tf.Transform considers these features as \"raw\" def _get_raw_feature_spec(schema):   return schema_utils.schema_as_feature_spec(schema).feature_spec   def _gzip_reader_fn(filenames):   \"\"\"Small utility returning a record reader that can read gzip'ed files.\"\"\"   return tf.data.TFRecordDataset(       filenames,       compression_type='GZIP')   def _example_serving_receiver_fn(tf_transform_output, schema):   \"\"\"Build the serving in inputs.    Args:     tf_transform_output: A TFTransformOutput.     schema: the schema of the input data.    Returns:     Tensorflow graph which parses examples, applying tf-transform to them.   \"\"\"   raw_feature_spec = _get_raw_feature_spec(schema)   raw_feature_spec.pop(LABEL_KEY)    # We don't need the ID feature for serving.   raw_feature_spec.pop(ID_FEATURE_KEY)    raw_input_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(       raw_feature_spec, default_batch_size=None)   serving_input_receiver = raw_input_fn()    transformed_features = tf_transform_output.transform_raw_features(       serving_input_receiver.features)    # Even though, LABEL_KEY was removed from 'raw_feature_spec', the transform   # operation would have injected the transformed LABEL_KEY feature with a   # default value.   transformed_features.pop(_transformed_name(LABEL_KEY))   return tf.estimator.export.ServingInputReceiver(       transformed_features, serving_input_receiver.receiver_tensors)   def _eval_input_receiver_fn(tf_transform_output, schema):   \"\"\"Build everything needed for the tf-model-analysis to run the model.    Args:     tf_transform_output: A TFTransformOutput.     schema: the schema of the input data.    Returns:     EvalInputReceiver function, which contains:       - Tensorflow graph which parses raw untransformed features, applies the         tf-transform preprocessing operators.       - Set of raw, untransformed features.       - Label against which predictions will be compared.   \"\"\"   # Notice that the inputs are raw features, not transformed features here.   raw_feature_spec = _get_raw_feature_spec(schema)    # We don't need the ID feature for TFMA.   raw_feature_spec.pop(ID_FEATURE_KEY)    raw_input_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(       raw_feature_spec, default_batch_size=None)   serving_input_receiver = raw_input_fn()    transformed_features = tf_transform_output.transform_raw_features(       serving_input_receiver.features)    labels = transformed_features.pop(_transformed_name(LABEL_KEY))   return tfma.export.EvalInputReceiver(       features=transformed_features,       receiver_tensors=serving_input_receiver.receiver_tensors,       labels=labels)   def _augment_feature_spec(feature_spec, num_neighbors):   \"\"\"Augments `feature_spec` to include neighbor features.     Args:       feature_spec: Dictionary of feature keys mapping to TF feature types.       num_neighbors: Number of neighbors to use for feature key augmentation.     Returns:       An augmented `feature_spec` that includes neighbor feature keys.   \"\"\"   for i in range(num_neighbors):     feature_spec['{}{}_{}'.format(NBR_FEATURE_PREFIX, i, 'id')] = \\         tf.io.VarLenFeature(dtype=tf.string)     # We don't care about the neighbor features corresponding to     # _transformed_name(LABEL_KEY) because the LABEL_KEY feature will be     # removed from the feature spec during training/evaluation.     feature_spec['{}{}_{}'.format(NBR_FEATURE_PREFIX, i, 'text_xf')] = \\         tf.io.FixedLenFeature(shape=[HPARAMS.max_seq_length], dtype=tf.int64,                               default_value=tf.constant(0, dtype=tf.int64,                                                         shape=[HPARAMS.max_seq_length]))     # The 'NL_num_nbrs' features is currently not used.    # Set the neighbor weight feature keys.   for i in range(num_neighbors):     feature_spec['{}{}{}'.format(NBR_FEATURE_PREFIX, i, NBR_WEIGHT_SUFFIX)] = \\         tf.io.FixedLenFeature(shape=[1], dtype=tf.float32, default_value=[0.0])    return feature_spec   def _input_fn(filenames, tf_transform_output, is_training, batch_size=200):   \"\"\"Generates features and labels for training or evaluation.    Args:     filenames: [str] list of CSV files to read data from.     tf_transform_output: A TFTransformOutput.     is_training: Boolean indicating if we are in training mode.     batch_size: int First dimension size of the Tensors returned by input_fn    Returns:     A (features, indices) tuple where features is a dictionary of       Tensors, and indices is a single Tensor of label indices.   \"\"\"   transformed_feature_spec = (       tf_transform_output.transformed_feature_spec().copy())    # During training, NSL uses augmented training data (which includes features   # from graph neighbors). So, update the feature spec accordingly. This needs   # to be done because we are using different schemas for NSL training and eval,   # but the Trainer Component only accepts a single schema.   if is_training:     transformed_feature_spec =_augment_feature_spec(transformed_feature_spec,                                                     HPARAMS.num_neighbors)    dataset = tf.data.experimental.make_batched_features_dataset(       filenames, batch_size, transformed_feature_spec, reader=_gzip_reader_fn)    transformed_features = tf.compat.v1.data.make_one_shot_iterator(       dataset).get_next()   # We pop the label because we do not want to use it as a feature while we're   # training.   return transformed_features, transformed_features.pop(       _transformed_name(LABEL_KEY))   # TFX will call this function def trainer_fn(hparams, schema):   \"\"\"Build the estimator using the high level API.   Args:     hparams: Holds hyperparameters used to train the model as name/value pairs.     schema: Holds the schema of the training examples.   Returns:     A dict of the following:       - estimator: The estimator that will be used for training and eval.       - train_spec: Spec for training.       - eval_spec: Spec for eval.       - eval_input_receiver_fn: Input function for eval.   \"\"\"   train_batch_size = 40   eval_batch_size = 40    tf_transform_output = tft.TFTransformOutput(hparams.transform_output)    train_input_fn = lambda: _input_fn(       hparams.train_files,       tf_transform_output,       is_training=True,       batch_size=train_batch_size)    eval_input_fn = lambda: _input_fn(       hparams.eval_files,       tf_transform_output,       is_training=False,       batch_size=eval_batch_size)    train_spec = tf.estimator.TrainSpec(       train_input_fn,       max_steps=hparams.train_steps)    serving_receiver_fn = lambda: _example_serving_receiver_fn(       tf_transform_output, schema)    exporter = tf.estimator.FinalExporter('imdb', serving_receiver_fn)   eval_spec = tf.estimator.EvalSpec(       eval_input_fn,       steps=hparams.eval_steps,       exporters=[exporter],       name='imdb-eval')    run_config = tf.estimator.RunConfig(       save_checkpoints_steps=999, keep_checkpoint_max=1)    run_config = run_config.replace(model_dir=hparams.serving_model_dir)    estimator = tf.estimator.Estimator(       model_fn=feed_forward_model_fn, config=run_config, params=HPARAMS)    # Create a graph regularization config.   graph_reg_config = nsl.configs.make_graph_reg_config(       max_neighbors=HPARAMS.num_neighbors,       multiplier=HPARAMS.graph_regularization_multiplier,       distance_type=HPARAMS.distance_type,       sum_over_axis=-1)    # Invoke the Graph Regularization Estimator wrapper to incorporate   # graph-based regularization for training.   graph_nsl_estimator = nsl.estimator.add_graph_regularization(       estimator,       embedding_fn,       optimizer_fn=optimizer_fn,       graph_reg_config=graph_reg_config)    # Create an input receiver for TFMA processing   receiver_fn = lambda: _eval_input_receiver_fn(       tf_transform_output, schema)    return {       'estimator': graph_nsl_estimator,       'train_spec': train_spec,       'eval_spec': eval_spec,       'eval_input_receiver_fn': receiver_fn   } <p>Create and run the <code>Trainer</code> component, passing it the file that we created above.</p> In\u00a0[\u00a0]: Copied! <pre># Uses user-provided Python function that implements a model using TensorFlow's\n# Estimators API.\ntrainer = Trainer(\n    module_file=_trainer_module_file,\n    custom_executor_spec=executor_spec.ExecutorClassSpec(\n        trainer_executor.Executor),\n    transformed_examples=graph_augmentation.outputs['augmented_examples'],\n    schema=schema_gen.outputs['schema'],\n    transform_graph=transform.outputs['transform_graph'],\n    train_args=trainer_pb2.TrainArgs(num_steps=10000),\n    eval_args=trainer_pb2.EvalArgs(num_steps=5000))\ncontext.run(trainer)\n</pre> # Uses user-provided Python function that implements a model using TensorFlow's # Estimators API. trainer = Trainer(     module_file=_trainer_module_file,     custom_executor_spec=executor_spec.ExecutorClassSpec(         trainer_executor.Executor),     transformed_examples=graph_augmentation.outputs['augmented_examples'],     schema=schema_gen.outputs['schema'],     transform_graph=transform.outputs['transform_graph'],     train_args=trainer_pb2.TrainArgs(num_steps=10000),     eval_args=trainer_pb2.EvalArgs(num_steps=5000)) context.run(trainer) <p>Take a peek at the trained model which was exported from <code>Trainer</code>.</p> In\u00a0[\u00a0]: Copied! <pre>train_uri = trainer.outputs['model'].get()[0].uri\nserving_model_path = os.path.join(train_uri, 'Format-Serving')\nexported_model = tf.saved_model.load(serving_model_path)\n</pre> train_uri = trainer.outputs['model'].get()[0].uri serving_model_path = os.path.join(train_uri, 'Format-Serving') exported_model = tf.saved_model.load(serving_model_path) In\u00a0[\u00a0]: Copied! <pre>exported_model.graph.get_operations()[:10] + [\"...\"]\n</pre> exported_model.graph.get_operations()[:10] + [\"...\"] <p>Let's visualize the model's metrics using Tensorboard.</p> In\u00a0[\u00a0]: Copied! <pre>#docs_infra: no_execute\n\n# Get the URI of the output artifact representing the training logs,\n# which is a directory\nmodel_run_dir = trainer.outputs['model_run'].get()[0].uri\n\n%load_ext tensorboard\n%tensorboard --logdir {model_run_dir}\n</pre> #docs_infra: no_execute  # Get the URI of the output artifact representing the training logs, # which is a directory model_run_dir = trainer.outputs['model_run'].get()[0].uri  %load_ext tensorboard %tensorboard --logdir {model_run_dir}"},{"location":"tutorials/tfx/neural_structured_learning/#copyright-2020-the-tensorflow-authors","title":"Copyright 2020 The TensorFlow Authors.\u00b6","text":""},{"location":"tutorials/tfx/neural_structured_learning/#graph-based-neural-structured-learning-in-tfx","title":"Graph-based Neural Structured Learning in TFX\u00b6","text":"<p>This tutorial describes graph regularization from the Neural Structured Learning framework and demonstrates an end-to-end workflow for sentiment classification in a TFX pipeline.</p>"},{"location":"tutorials/tfx/neural_structured_learning/#overview","title":"Overview\u00b6","text":""},{"location":"tutorials/tfx/neural_structured_learning/#upgrade-pip","title":"Upgrade Pip\u00b6","text":"<p>To avoid upgrading Pip in a system when running locally, check to make sure that we're running in Colab.  Local systems can of course be upgraded separately.</p>"},{"location":"tutorials/tfx/neural_structured_learning/#install-required-packages","title":"Install Required Packages\u00b6","text":""},{"location":"tutorials/tfx/neural_structured_learning/#did-you-restart-the-runtime","title":"Did you restart the runtime?\u00b6","text":"<p>If you are using Google Colab, the first time that you run the cell above, you must restart the runtime (Runtime &gt; Restart runtime ...). This is because of the way that Colab loads packages.</p>"},{"location":"tutorials/tfx/neural_structured_learning/#dependencies-and-imports","title":"Dependencies and imports\u00b6","text":""},{"location":"tutorials/tfx/neural_structured_learning/#imdb-dataset","title":"IMDB dataset\u00b6","text":"<p>The IMDB dataset contains the text of 50,000 movie reviews from the Internet Movie Database. These are split into 25,000 reviews for training and 25,000 reviews for testing. The training and testing sets are balanced, meaning they contain an equal number of positive and negative reviews. Moreover, there are 50,000 additional unlabeled movie reviews.</p>"},{"location":"tutorials/tfx/neural_structured_learning/#download-preprocessed-imdb-dataset","title":"Download preprocessed IMDB dataset\u00b6","text":"<p>The following code downloads the IMDB dataset (or uses a cached copy if it has already been downloaded) using TFDS. To speed up this notebook we will use only 10,000 labeled reviews and 10,000 unlabeled reviews for training, and 10,000 test reviews for evaluation.</p>"},{"location":"tutorials/tfx/neural_structured_learning/#run-tfx-components-interactively","title":"Run TFX Components Interactively\u00b6","text":"<p>In the cells that follow you will construct TFX components and run each one interactively within the InteractiveContext to obtain <code>ExecutionResult</code> objects.  This mirrors the process of an orchestrator running components in a TFX DAG based on when the dependencies for each component are met.</p>"},{"location":"tutorials/tfx/neural_structured_learning/#the-examplegen-component","title":"The ExampleGen Component\u00b6","text":"<p>In any ML development process the first step when starting code development is to ingest the training and test datasets.  The <code>ExampleGen</code> component brings data into the TFX pipeline.</p> <p>Create an ExampleGen component and run it.</p>"},{"location":"tutorials/tfx/neural_structured_learning/#the-identifyexamples-custom-component","title":"The IdentifyExamples Custom Component\u00b6","text":"<p>To use NSL, we will need each instance to have a unique ID. We create a custom component that adds such a unique ID to all instances across all splits. We leverage Apache Beam to be able to easily scale to large datasets if needed.</p>"},{"location":"tutorials/tfx/neural_structured_learning/#the-statisticsgen-component","title":"The StatisticsGen Component\u00b6","text":"<p>The <code>StatisticsGen</code> component computes descriptive statistics for your dataset.  The statistics that it generates can be visualized for review, and are used for example validation and to infer a schema.</p> <p>Create a StatisticsGen component and run it.</p>"},{"location":"tutorials/tfx/neural_structured_learning/#the-schemagen-component","title":"The SchemaGen Component\u00b6","text":"<p>The <code>SchemaGen</code> component generates a schema for your data based on the statistics from StatisticsGen.  It tries to infer the data types of each of your features, and the ranges of legal values for categorical features.</p> <p>Create a SchemaGen component and run it.</p>"},{"location":"tutorials/tfx/neural_structured_learning/#the-examplevalidator-component","title":"The ExampleValidator Component\u00b6","text":"<p>The <code>ExampleValidator</code> performs anomaly detection, based on the statistics from StatisticsGen and the schema from SchemaGen.  It looks for problems such as missing values, values of the wrong type, or categorical values outside of the domain of acceptable values.</p> <p>Create an ExampleValidator component and run it.</p>"},{"location":"tutorials/tfx/neural_structured_learning/#the-synthesizegraph-component","title":"The SynthesizeGraph Component\u00b6","text":""},{"location":"tutorials/tfx/neural_structured_learning/#the-transform-component","title":"The Transform Component\u00b6","text":"<p>The <code>Transform</code> component performs data transformations and feature engineering.  The results include an input TensorFlow graph which is used during both training and serving to preprocess the data before training or inference.  This graph becomes part of the SavedModel that is the result of model training.  Since the same input graph is used for both training and serving, the preprocessing will always be the same, and only needs to be written once.</p> <p>The Transform component requires more code than many other components because of the arbitrary complexity of the feature engineering that you may need for the data and/or model that you're working with.  It requires code files to be available which define the processing needed.</p>"},{"location":"tutorials/tfx/neural_structured_learning/#the-graphaugmentation-component","title":"The GraphAugmentation Component\u00b6","text":"<p>Since we have the sample features and the synthesized graph, we can generate the augmented training data for Neural Structured Learning. The NSL framework provides a library to combine the graph and the sample features to produce the final training data for graph regularization. The resulting training data will include original sample features as well as features of their corresponding neighbors.</p> <p>In this tutorial, we consider undirected edges and use a maximum of 3 neighbors per sample to augment training data with graph neighbors.</p>"},{"location":"tutorials/tfx/neural_structured_learning/#the-trainer-component","title":"The Trainer Component\u00b6","text":"<p>The <code>Trainer</code> component trains models using TensorFlow.</p> <p>Create a Python module containing a <code>trainer_fn</code> function, which must return an estimator.  If you prefer creating a Keras model, you can do so and then convert it to an estimator using <code>keras.model_to_estimator()</code>.</p>"},{"location":"tutorials/tfx/neural_structured_learning/#model-serving","title":"Model Serving\u00b6","text":"<p>Graph regularization only affects the training workflow by adding a regularization term to  the loss function. As a result, the model evaluation and serving workflows remain unchanged. It is for the same reason that we've also omitted downstream TFX components that typically come after the Trainer component like the Evaluator, Pusher, etc.</p>"},{"location":"tutorials/tfx/neural_structured_learning/#conclusion","title":"Conclusion\u00b6","text":"<p>We have demonstrated the use of graph regularization using the Neural Structured Learning (NSL) framework in a TFX pipeline even when the input does not contain an explicit graph. We considered the task of sentiment classification of IMDB movie reviews for which we synthesized a similarity graph based on review embeddings. We encourage users to experiment further by using different embeddings for graph construction, varying hyperparameters, changing the amount of supervision, and by defining different model architectures.</p>"},{"location":"tutorials/tfx/penguin_simple/","title":"Simple TFX Pipeline Tutorial using Penguin dataset","text":"In\u00a0[\u00a0]: Copied! <pre>#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n</pre> #@title Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # https://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. <p>Note: We recommend running this tutorial in a Colab notebook, with no setup required!  Just click \"Run in Google Colab\".</p>        View on TensorFlow.org             Run in Google Colab             View source on GitHub             Download notebook      <p>In this notebook-based tutorial, we will create and run a TFX pipeline for a simple classification model. The pipeline will consist of three essential TFX components: ExampleGen, Trainer and Pusher. The pipeline includes the most minimal ML workflow like importing data, training a model and exporting the trained model.</p> <p>Please see Understanding TFX Pipelines to learn more about various concepts in TFX.</p> In\u00a0[\u00a0]: Copied! <pre>try:\n  import colab\n  !pip install --upgrade pip\nexcept:\n  pass\n</pre> try:   import colab   !pip install --upgrade pip except:   pass In\u00a0[\u00a0]: Copied! <pre>!pip install -U tfx\n</pre> !pip install -U tfx <p>Check the TensorFlow and TFX versions.</p> In\u00a0[\u00a0]: Copied! <pre>import tensorflow as tf\nprint('TensorFlow version: {}'.format(tf.__version__))\nfrom tfx import v1 as tfx\nprint('TFX version: {}'.format(tfx.__version__))\n</pre> import tensorflow as tf print('TensorFlow version: {}'.format(tf.__version__)) from tfx import v1 as tfx print('TFX version: {}'.format(tfx.__version__)) In\u00a0[\u00a0]: Copied! <pre>import os\n\nPIPELINE_NAME = \"penguin-simple\"\n\n# Output directory to store artifacts generated from the pipeline.\nPIPELINE_ROOT = os.path.join('pipelines', PIPELINE_NAME)\n# Path to a SQLite DB file to use as an MLMD storage.\nMETADATA_PATH = os.path.join('metadata', PIPELINE_NAME, 'metadata.db')\n# Output directory where created models from the pipeline will be exported.\nSERVING_MODEL_DIR = os.path.join('serving_model', PIPELINE_NAME)\n\nfrom absl import logging\nlogging.set_verbosity(logging.INFO)  # Set default logging level.\n</pre> import os  PIPELINE_NAME = \"penguin-simple\"  # Output directory to store artifacts generated from the pipeline. PIPELINE_ROOT = os.path.join('pipelines', PIPELINE_NAME) # Path to a SQLite DB file to use as an MLMD storage. METADATA_PATH = os.path.join('metadata', PIPELINE_NAME, 'metadata.db') # Output directory where created models from the pipeline will be exported. SERVING_MODEL_DIR = os.path.join('serving_model', PIPELINE_NAME)  from absl import logging logging.set_verbosity(logging.INFO)  # Set default logging level. <p>Because TFX ExampleGen reads inputs from a directory, we need to create a directory and copy dataset to it.</p> In\u00a0[\u00a0]: Copied! <pre>import urllib.request\nimport tempfile\n\nDATA_ROOT = tempfile.mkdtemp(prefix='tfx-data')  # Create a temporary directory.\n_data_url = 'https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/penguin/data/labelled/penguins_processed.csv'\n_data_filepath = os.path.join(DATA_ROOT, \"data.csv\")\nurllib.request.urlretrieve(_data_url, _data_filepath)\n</pre> import urllib.request import tempfile  DATA_ROOT = tempfile.mkdtemp(prefix='tfx-data')  # Create a temporary directory. _data_url = 'https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/penguin/data/labelled/penguins_processed.csv' _data_filepath = os.path.join(DATA_ROOT, \"data.csv\") urllib.request.urlretrieve(_data_url, _data_filepath) <p>Take a quick look at the CSV file.</p> In\u00a0[\u00a0]: Copied! <pre>!head {_data_filepath}\n</pre> !head {_data_filepath} <p>You should be able to see five values. <code>species</code> is one of 0, 1 or 2, and all other features should have values between 0 and 1.</p> In\u00a0[\u00a0]: Copied! <pre>_trainer_module_file = 'penguin_trainer.py'\n</pre> _trainer_module_file = 'penguin_trainer.py' In\u00a0[\u00a0]: Copied! <pre>%%writefile {_trainer_module_file}\n\nfrom typing import List\nfrom absl import logging\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow_transform.tf_metadata import schema_utils\n\nfrom tfx import v1 as tfx\nfrom tfx_bsl.public import tfxio\nfrom tensorflow_metadata.proto.v0 import schema_pb2\n\n_FEATURE_KEYS = [\n    'culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g'\n]\n_LABEL_KEY = 'species'\n\n_TRAIN_BATCH_SIZE = 20\n_EVAL_BATCH_SIZE = 10\n\n# Since we're not generating or creating a schema, we will instead create\n# a feature spec.  Since there are a fairly small number of features this is\n# manageable for this dataset.\n_FEATURE_SPEC = {\n    **{\n        feature: tf.io.FixedLenFeature(shape=[1], dtype=tf.float32)\n           for feature in _FEATURE_KEYS\n       },\n    _LABEL_KEY: tf.io.FixedLenFeature(shape=[1], dtype=tf.int64)\n}\n\n\ndef _input_fn(file_pattern: List[str],\n              data_accessor: tfx.components.DataAccessor,\n              schema: schema_pb2.Schema,\n              batch_size: int = 200) -&gt; tf.data.Dataset:\n  \"\"\"Generates features and label for training.\n\n  Args:\n    file_pattern: List of paths or patterns of input tfrecord files.\n    data_accessor: DataAccessor for converting input to RecordBatch.\n    schema: schema of the input data.\n    batch_size: representing the number of consecutive elements of returned\n      dataset to combine in a single batch\n\n  Returns:\n    A dataset that contains (features, indices) tuple where features is a\n      dictionary of Tensors, and indices is a single Tensor of label indices.\n  \"\"\"\n  return data_accessor.tf_dataset_factory(\n      file_pattern,\n      tfxio.TensorFlowDatasetOptions(\n          batch_size=batch_size, label_key=_LABEL_KEY),\n      schema=schema).repeat()\n\n\ndef _build_keras_model() -&gt; tf.keras.Model:\n  \"\"\"Creates a DNN Keras model for classifying penguin data.\n\n  Returns:\n    A Keras Model.\n  \"\"\"\n  # The model below is built with Functional API, please refer to\n  # https://www.tensorflow.org/guide/keras/overview for all API options.\n  inputs = [keras.layers.Input(shape=(1,), name=f) for f in _FEATURE_KEYS]\n  d = keras.layers.concatenate(inputs)\n  for _ in range(2):\n    d = keras.layers.Dense(8, activation='relu')(d)\n  outputs = keras.layers.Dense(3)(d)\n\n  model = keras.Model(inputs=inputs, outputs=outputs)\n  model.compile(\n      optimizer=keras.optimizers.Adam(1e-2),\n      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n      metrics=[keras.metrics.SparseCategoricalAccuracy()])\n\n  model.summary(print_fn=logging.info)\n  return model\n\n\n# TFX Trainer will call this function.\ndef run_fn(fn_args: tfx.components.FnArgs):\n  \"\"\"Train the model based on given args.\n\n  Args:\n    fn_args: Holds args used to train the model as name/value pairs.\n  \"\"\"\n\n  # This schema is usually either an output of SchemaGen or a manually-curated\n  # version provided by pipeline author. A schema can also derived from TFT\n  # graph if a Transform component is used. In the case when either is missing,\n  # `schema_from_feature_spec` could be used to generate schema from very simple\n  # feature_spec, but the schema returned would be very primitive.\n  schema = schema_utils.schema_from_feature_spec(_FEATURE_SPEC)\n\n  train_dataset = _input_fn(\n      fn_args.train_files,\n      fn_args.data_accessor,\n      schema,\n      batch_size=_TRAIN_BATCH_SIZE)\n  eval_dataset = _input_fn(\n      fn_args.eval_files,\n      fn_args.data_accessor,\n      schema,\n      batch_size=_EVAL_BATCH_SIZE)\n\n  model = _build_keras_model()\n  model.fit(\n      train_dataset,\n      steps_per_epoch=fn_args.train_steps,\n      validation_data=eval_dataset,\n      validation_steps=fn_args.eval_steps)\n\n  # The result of the training should be saved in `fn_args.serving_model_dir`\n  # directory.\n  model.save(fn_args.serving_model_dir, save_format='tf')\n</pre> %%writefile {_trainer_module_file}  from typing import List from absl import logging import tensorflow as tf from tensorflow import keras from tensorflow_transform.tf_metadata import schema_utils  from tfx import v1 as tfx from tfx_bsl.public import tfxio from tensorflow_metadata.proto.v0 import schema_pb2  _FEATURE_KEYS = [     'culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g' ] _LABEL_KEY = 'species'  _TRAIN_BATCH_SIZE = 20 _EVAL_BATCH_SIZE = 10  # Since we're not generating or creating a schema, we will instead create # a feature spec.  Since there are a fairly small number of features this is # manageable for this dataset. _FEATURE_SPEC = {     **{         feature: tf.io.FixedLenFeature(shape=[1], dtype=tf.float32)            for feature in _FEATURE_KEYS        },     _LABEL_KEY: tf.io.FixedLenFeature(shape=[1], dtype=tf.int64) }   def _input_fn(file_pattern: List[str],               data_accessor: tfx.components.DataAccessor,               schema: schema_pb2.Schema,               batch_size: int = 200) -&gt; tf.data.Dataset:   \"\"\"Generates features and label for training.    Args:     file_pattern: List of paths or patterns of input tfrecord files.     data_accessor: DataAccessor for converting input to RecordBatch.     schema: schema of the input data.     batch_size: representing the number of consecutive elements of returned       dataset to combine in a single batch    Returns:     A dataset that contains (features, indices) tuple where features is a       dictionary of Tensors, and indices is a single Tensor of label indices.   \"\"\"   return data_accessor.tf_dataset_factory(       file_pattern,       tfxio.TensorFlowDatasetOptions(           batch_size=batch_size, label_key=_LABEL_KEY),       schema=schema).repeat()   def _build_keras_model() -&gt; tf.keras.Model:   \"\"\"Creates a DNN Keras model for classifying penguin data.    Returns:     A Keras Model.   \"\"\"   # The model below is built with Functional API, please refer to   # https://www.tensorflow.org/guide/keras/overview for all API options.   inputs = [keras.layers.Input(shape=(1,), name=f) for f in _FEATURE_KEYS]   d = keras.layers.concatenate(inputs)   for _ in range(2):     d = keras.layers.Dense(8, activation='relu')(d)   outputs = keras.layers.Dense(3)(d)    model = keras.Model(inputs=inputs, outputs=outputs)   model.compile(       optimizer=keras.optimizers.Adam(1e-2),       loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),       metrics=[keras.metrics.SparseCategoricalAccuracy()])    model.summary(print_fn=logging.info)   return model   # TFX Trainer will call this function. def run_fn(fn_args: tfx.components.FnArgs):   \"\"\"Train the model based on given args.    Args:     fn_args: Holds args used to train the model as name/value pairs.   \"\"\"    # This schema is usually either an output of SchemaGen or a manually-curated   # version provided by pipeline author. A schema can also derived from TFT   # graph if a Transform component is used. In the case when either is missing,   # `schema_from_feature_spec` could be used to generate schema from very simple   # feature_spec, but the schema returned would be very primitive.   schema = schema_utils.schema_from_feature_spec(_FEATURE_SPEC)    train_dataset = _input_fn(       fn_args.train_files,       fn_args.data_accessor,       schema,       batch_size=_TRAIN_BATCH_SIZE)   eval_dataset = _input_fn(       fn_args.eval_files,       fn_args.data_accessor,       schema,       batch_size=_EVAL_BATCH_SIZE)    model = _build_keras_model()   model.fit(       train_dataset,       steps_per_epoch=fn_args.train_steps,       validation_data=eval_dataset,       validation_steps=fn_args.eval_steps)    # The result of the training should be saved in `fn_args.serving_model_dir`   # directory.   model.save(fn_args.serving_model_dir, save_format='tf') <p>Now you have completed all preparation steps to build a TFX pipeline.</p> In\u00a0[\u00a0]: Copied! <pre>def _create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str,\n                     module_file: str, serving_model_dir: str,\n                     metadata_path: str) -&gt; tfx.dsl.Pipeline:\n  \"\"\"Creates a three component penguin pipeline with TFX.\"\"\"\n  # Brings data into the pipeline.\n  example_gen = tfx.components.CsvExampleGen(input_base=data_root)\n\n  # Uses user-provided Python function that trains a model.\n  trainer = tfx.components.Trainer(\n      module_file=module_file,\n      examples=example_gen.outputs['examples'],\n      train_args=tfx.proto.TrainArgs(num_steps=100),\n      eval_args=tfx.proto.EvalArgs(num_steps=5))\n\n  # Pushes the model to a filesystem destination.\n  pusher = tfx.components.Pusher(\n      model=trainer.outputs['model'],\n      push_destination=tfx.proto.PushDestination(\n          filesystem=tfx.proto.PushDestination.Filesystem(\n              base_directory=serving_model_dir)))\n\n  # Following three components will be included in the pipeline.\n  components = [\n      example_gen,\n      trainer,\n      pusher,\n  ]\n\n  return tfx.dsl.Pipeline(\n      pipeline_name=pipeline_name,\n      pipeline_root=pipeline_root,\n      metadata_connection_config=tfx.orchestration.metadata\n      .sqlite_metadata_connection_config(metadata_path),\n      components=components)\n</pre> def _create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str,                      module_file: str, serving_model_dir: str,                      metadata_path: str) -&gt; tfx.dsl.Pipeline:   \"\"\"Creates a three component penguin pipeline with TFX.\"\"\"   # Brings data into the pipeline.   example_gen = tfx.components.CsvExampleGen(input_base=data_root)    # Uses user-provided Python function that trains a model.   trainer = tfx.components.Trainer(       module_file=module_file,       examples=example_gen.outputs['examples'],       train_args=tfx.proto.TrainArgs(num_steps=100),       eval_args=tfx.proto.EvalArgs(num_steps=5))    # Pushes the model to a filesystem destination.   pusher = tfx.components.Pusher(       model=trainer.outputs['model'],       push_destination=tfx.proto.PushDestination(           filesystem=tfx.proto.PushDestination.Filesystem(               base_directory=serving_model_dir)))    # Following three components will be included in the pipeline.   components = [       example_gen,       trainer,       pusher,   ]    return tfx.dsl.Pipeline(       pipeline_name=pipeline_name,       pipeline_root=pipeline_root,       metadata_connection_config=tfx.orchestration.metadata       .sqlite_metadata_connection_config(metadata_path),       components=components) <p>Now we create a <code>LocalDagRunner</code> and pass a <code>Pipeline</code> object created from the function we already defined.</p> <p>The pipeline runs directly and you can see logs for the progress of the pipeline including ML model training.</p> In\u00a0[\u00a0]: Copied! <pre>tfx.orchestration.LocalDagRunner().run(\n  _create_pipeline(\n      pipeline_name=PIPELINE_NAME,\n      pipeline_root=PIPELINE_ROOT,\n      data_root=DATA_ROOT,\n      module_file=_trainer_module_file,\n      serving_model_dir=SERVING_MODEL_DIR,\n      metadata_path=METADATA_PATH))\n</pre> tfx.orchestration.LocalDagRunner().run(   _create_pipeline(       pipeline_name=PIPELINE_NAME,       pipeline_root=PIPELINE_ROOT,       data_root=DATA_ROOT,       module_file=_trainer_module_file,       serving_model_dir=SERVING_MODEL_DIR,       metadata_path=METADATA_PATH)) <p>You should see \"INFO:absl:Component Pusher is finished.\" at the end of the logs if the pipeline finished successfully. Because <code>Pusher</code> component is the last component of the pipeline.</p> <p>The pusher component pushes the trained model to the <code>SERVING_MODEL_DIR</code> which is the <code>serving_model/penguin-simple</code> directory if you did not change the variables in the previous steps. You can see the result from the file browser in the left-side panel in Colab, or using the following command:</p> In\u00a0[\u00a0]: Copied! <pre># List files in created model directory.\n!find {SERVING_MODEL_DIR}\n</pre> # List files in created model directory. !find {SERVING_MODEL_DIR}"},{"location":"tutorials/tfx/penguin_simple/#copyright-2021-the-tensorflow-authors","title":"Copyright 2021 The TensorFlow Authors.\u00b6","text":""},{"location":"tutorials/tfx/penguin_simple/#simple-tfx-pipeline-tutorial-using-penguin-dataset","title":"Simple TFX Pipeline Tutorial using Penguin dataset\u00b6","text":"<p>A Short tutorial to run a simple TFX pipeline.</p>"},{"location":"tutorials/tfx/penguin_simple/#set-up","title":"Set Up\u00b6","text":"<p>We first need to install the TFX Python package and download the dataset which we will use for our model.</p>"},{"location":"tutorials/tfx/penguin_simple/#upgrade-pip","title":"Upgrade Pip\u00b6","text":"<p>To avoid upgrading Pip in a system when running locally, check to make sure that we are running in Colab. Local systems can of course be upgraded separately.</p>"},{"location":"tutorials/tfx/penguin_simple/#install-tfx","title":"Install TFX\u00b6","text":""},{"location":"tutorials/tfx/penguin_simple/#did-you-restart-the-runtime","title":"Did you restart the runtime?\u00b6","text":"<p>If you are using Google Colab, the first time that you run the cell above, you must restart the runtime by clicking above \"RESTART RUNTIME\" button or using \"Runtime &gt; Restart runtime ...\" menu. This is because of the way that Colab loads packages.</p>"},{"location":"tutorials/tfx/penguin_simple/#set-up-variables","title":"Set up variables\u00b6","text":"<p>There are some variables used to define a pipeline. You can customize these variables as you want. By default all output from the pipeline will be generated under the current directory.</p>"},{"location":"tutorials/tfx/penguin_simple/#prepare-example-data","title":"Prepare example data\u00b6","text":"<p>We will download the example dataset for use in our TFX pipeline. The dataset we are using is Palmer Penguins dataset which is also used in other TFX examples.</p> <p>There are four numeric features in this dataset:</p> <ul> <li>culmen_length_mm</li> <li>culmen_depth_mm</li> <li>flipper_length_mm</li> <li>body_mass_g</li> </ul> <p>All features were already normalized to have range [0,1]. We will build a classification model which predicts the <code>species</code> of penguins.</p>"},{"location":"tutorials/tfx/penguin_simple/#create-a-pipeline","title":"Create a pipeline\u00b6","text":"<p>TFX pipelines are defined using Python APIs. We will define a pipeline which consists of following three components.</p> <ul> <li>CsvExampleGen: Reads in data files and convert them to TFX internal format for further processing. There are multiple ExampleGens for various formats. In this tutorial, we will use CsvExampleGen which takes CSV file input.</li> <li>Trainer: Trains an ML model. Trainer component requires a model definition code from users. You can use TensorFlow APIs to specify how to train a model and save it in a saved_model format.</li> <li>Pusher: Copies the trained model outside of the TFX pipeline. Pusher component can be thought of as a deployment process of the trained ML model.</li> </ul> <p>Before actually define the pipeline, we need to write a model code for the Trainer component first.</p>"},{"location":"tutorials/tfx/penguin_simple/#write-model-training-code","title":"Write model training code\u00b6","text":"<p>We will create a simple DNN model for classification using TensorFlow Keras API. This model training code will be saved to a separate file.</p> <p>In this tutorial we will use Generic Trainer of TFX which support Keras-based models. You need to write a Python file containing <code>run_fn</code> function, which is the entrypoint for the <code>Trainer</code> component.</p>"},{"location":"tutorials/tfx/penguin_simple/#write-a-pipeline-definition","title":"Write a pipeline definition\u00b6","text":"<p>We define a function to create a TFX pipeline. A <code>Pipeline</code> object represents a TFX pipeline which can be run using one of the pipeline orchestration systems that TFX supports.</p>"},{"location":"tutorials/tfx/penguin_simple/#run-the-pipeline","title":"Run the pipeline\u00b6","text":"<p>TFX supports multiple orchestrators to run pipelines. In this tutorial we will use <code>LocalDagRunner</code> which is included in the TFX Python package and runs pipelines on local environment. We often call TFX pipelines \"DAGs\" which stands for directed acyclic graph.</p> <p><code>LocalDagRunner</code> provides fast iterations for development and debugging. TFX also supports other orchestrators including Kubeflow Pipelines and Apache Airflow which are suitable for production use cases.</p> <p>See TFX on Cloud AI Platform Pipelines or TFX Airflow Tutorial to learn more about other orchestration systems.</p>"},{"location":"tutorials/tfx/penguin_simple/#next-steps","title":"Next steps\u00b6","text":"<p>You can find more resources on https://www.tensorflow.org/tfx/tutorials.</p> <p>Please see Understanding TFX Pipelines to learn more about various concepts in TFX.</p>"},{"location":"tutorials/tfx/penguin_template/","title":"Create a TFX pipeline for your data with Penguin template","text":"In\u00a0[\u00a0]: Copied! <pre>#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n</pre> #@title Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # https://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. <p>Note: We recommend running this tutorial in a Colab notebook, with no setup required!  Just click \"Run in Google Colab\".</p>        View on TensorFlow.org             Run in Google Colab             View source on GitHub             Download notebook      In\u00a0[\u00a0]: Copied! <pre>import sys\nif 'google.colab' in sys.modules:\n  !pip install --upgrade pip\n</pre> import sys if 'google.colab' in sys.modules:   !pip install --upgrade pip In\u00a0[\u00a0]: Copied! <pre>!pip install -U tfx tensorflow-model-analysis\n</pre> !pip install -U tfx tensorflow-model-analysis <p>Let's check the versions of TFX.</p> In\u00a0[\u00a0]: Copied! <pre>import tensorflow as tf\nimport tensorflow_model_analysis as tfma\nimport tfx\n\nprint('TF version: {}'.format(tf.__version__))\nprint('TFMA version: {}'.format(tfma.__version__))\nprint('TFX version: {}'.format(tfx.__version__))\n</pre> import tensorflow as tf import tensorflow_model_analysis as tfma import tfx  print('TF version: {}'.format(tf.__version__)) print('TFMA version: {}'.format(tfma.__version__)) print('TFX version: {}'.format(tfx.__version__)) <p>We are ready to create a pipeline.</p> <p>Set <code>PROJECT_DIR</code> to appropriate destination for your environment. Default value is <code>~/imported/${PIPELINE_NAME}</code> which is appropriate for Google Cloud AI Platform Notebook environment.</p> <p>You may give your pipeline a different name by changing the <code>PIPELINE_NAME</code> below. This will also become the name of the project directory where your files will be put.</p> In\u00a0[\u00a0]: Copied! <pre>PIPELINE_NAME=\"my_pipeline\"\nimport os\n# Set this project directory to your new tfx pipeline project.\nPROJECT_DIR=os.path.join(os.path.expanduser(\"~\"), \"imported\", PIPELINE_NAME)\n</pre> PIPELINE_NAME=\"my_pipeline\" import os # Set this project directory to your new tfx pipeline project. PROJECT_DIR=os.path.join(os.path.expanduser(\"~\"), \"imported\", PIPELINE_NAME) In\u00a0[\u00a0]: Copied! <pre># Set `PATH` to include user python binary directory and a directory containing `skaffold`.\nPATH=%env PATH\n%env PATH={PATH}:/home/jupyter/.local/bin\n\n!tfx template copy \\\n  --pipeline-name={PIPELINE_NAME} \\\n  --destination-path={PROJECT_DIR} \\\n  --model=penguin\n</pre> # Set `PATH` to include user python binary directory and a directory containing `skaffold`. PATH=%env PATH %env PATH={PATH}:/home/jupyter/.local/bin  !tfx template copy \\   --pipeline-name={PIPELINE_NAME} \\   --destination-path={PROJECT_DIR} \\   --model=penguin <p>Change the working directory context in this notebook to the project directory.</p> In\u00a0[\u00a0]: Copied! <pre>%cd {PROJECT_DIR}\n</pre> %cd {PROJECT_DIR} <p>NOTE: If you are using JupyterLab or Google Cloud AI Platform Notebook, don't forget to change directory in <code>File Browser</code> on the left by clicking into the project directory once it is created.</p> <p>By default, the template only includes standard TFX components. If you need some customized actions, you can create custom components for your pipeline. Please see TFX custom component guide for the detail.</p> In\u00a0[\u00a0]: Copied! <pre>import sys\n!{sys.executable} -m models.features_test\n</pre> import sys !{sys.executable} -m models.features_test <p>We will use <code>local_runner.py</code> to run your pipeline using local orchestrator. You have to create a pipeline before running it. You can create a pipeline with <code>pipeline create</code> command.</p> In\u00a0[\u00a0]: Copied! <pre>!tfx pipeline create --engine=local --pipeline_path=local_runner.py\n</pre> !tfx pipeline create --engine=local --pipeline_path=local_runner.py <p><code>pipeline create</code> command registers your pipeline defined in <code>local_runner.py</code> without actually running it.</p> <p>You will run the created pipeline with <code>run create</code> command in following steps.</p> <p>The initial pipeline is consist of four components, <code>ExampleGen</code>, <code>StatisticsGen</code>, <code>SchemaGen</code> and <code>ExampleValidator</code>. We don't need to change anything for <code>StatisticsGen</code>, <code>SchemaGen</code> and <code>ExampleValidator</code>. Let's run the pipeline for the first time.</p> In\u00a0[\u00a0]: Copied! <pre># Update and run the pipeline.\n!tfx pipeline update --engine=local --pipeline_path=local_runner.py \\\n &amp;&amp; tfx run create --engine=local --pipeline_name={PIPELINE_NAME}\n</pre> # Update and run the pipeline. !tfx pipeline update --engine=local --pipeline_path=local_runner.py \\  &amp;&amp; tfx run create --engine=local --pipeline_name={PIPELINE_NAME} <p>You should see \"Component ExampleValidator is finished.\" if the pipeline ran successfully.</p> <p>TFX pipeline produces two kinds of output, artifacts and a metadata DB(MLMD) which contains metadata of artifacts and pipeline executions. The location to the output is defined in <code>local_runner.py</code>. By default, artifacts are stored under <code>tfx_pipeline_output</code> directory and metadata is stored as an sqlite database under <code>tfx_metadata</code> directory.</p> <p>You can use MLMD APIs to examine these outputs. First, we will define some utility functions to search output artifacts that were just produced.</p> In\u00a0[\u00a0]: Copied! <pre>import tensorflow as tf\nimport tfx\nfrom ml_metadata import errors\nfrom ml_metadata.proto import metadata_store_pb2\nfrom tfx.types import artifact_utils\n\n# TODO(b/171447278): Move these functions into TFX library.\n\ndef get_latest_executions(store, pipeline_name, component_id = None):\n  \"\"\"Fetch all pipeline runs.\"\"\"\n  if component_id is None:  # Find entire pipeline runs.\n    run_contexts = [\n        c for c in store.get_contexts_by_type('run')\n        if c.properties['pipeline_name'].string_value == pipeline_name\n    ]\n  else:  # Find specific component runs.\n    run_contexts = [\n        c for c in store.get_contexts_by_type('component_run')\n        if c.properties['pipeline_name'].string_value == pipeline_name and\n           c.properties['component_id'].string_value == component_id\n    ]\n  if not run_contexts:\n    return []\n  # Pick the latest run context.\n  latest_context = max(run_contexts,\n                       key=lambda c: c.last_update_time_since_epoch)\n  return store.get_executions_by_context(latest_context.id)\n\ndef get_latest_artifacts(store, pipeline_name, component_id = None):\n  \"\"\"Fetch all artifacts from latest pipeline execution.\"\"\"\n  executions = get_latest_executions(store, pipeline_name, component_id)\n\n  # Fetch all artifacts produced from the given executions.\n  execution_ids = [e.id for e in executions]\n  events = store.get_events_by_execution_ids(execution_ids)\n  artifact_ids = [\n      event.artifact_id for event in events\n      if event.type == metadata_store_pb2.Event.OUTPUT\n  ]\n  return store.get_artifacts_by_id(artifact_ids)\n\ndef find_latest_artifacts_by_type(store, artifacts, artifact_type):\n  \"\"\"Get the latest artifacts of a specified type.\"\"\"\n  # Get type information from MLMD\n  try:\n    artifact_type = store.get_artifact_type(artifact_type)\n  except errors.NotFoundError:\n    return []\n  # Filter artifacts with type.\n  filtered_artifacts = [aritfact for aritfact in artifacts\n                        if aritfact.type_id == artifact_type.id]\n  # Convert MLMD artifact data into TFX Artifact instances.\n  return [artifact_utils.deserialize_artifact(artifact_type, artifact)\n      for artifact in filtered_artifacts]\n\n\nfrom tfx.orchestration.experimental.interactive import visualizations\n\ndef visualize_artifacts(artifacts):\n  \"\"\"Visualizes artifacts using standard visualization modules.\"\"\"\n  for artifact in artifacts:\n    visualization = visualizations.get_registry().get_visualization(\n        artifact.type_name)\n    if visualization:\n      visualization.display(artifact)\n\nfrom tfx.orchestration.experimental.interactive import standard_visualizations\nstandard_visualizations.register_standard_visualizations()\n\nimport pprint\n\nfrom tfx.orchestration import metadata\nfrom tfx.types import artifact_utils\nfrom tfx.types import standard_artifacts\n\ndef preview_examples(artifacts):\n  \"\"\"Preview a few records from Examples artifacts.\"\"\"\n  pp = pprint.PrettyPrinter()\n  for artifact in artifacts:\n    print(\"==== Examples artifact:{}({})\".format(artifact.name, artifact.uri))\n    for split in artifact_utils.decode_split_names(artifact.split_names):\n      print(\"==== Reading from split:{}\".format(split))\n      split_uri = artifact_utils.get_split_uri([artifact], split)\n\n      # Get the list of files in this directory (all compressed TFRecord files)\n      tfrecord_filenames = [os.path.join(split_uri, name)\n                            for name in os.listdir(split_uri)]\n      # Create a `TFRecordDataset` to read these files\n      dataset = tf.data.TFRecordDataset(tfrecord_filenames,\n                                        compression_type=\"GZIP\")\n      # Iterate over the first 2 records and decode them.\n      for tfrecord in dataset.take(2):\n        serialized_example = tfrecord.numpy()\n        example = tf.train.Example()\n        example.ParseFromString(serialized_example)\n        pp.pprint(example)\n\nimport local_runner\n\nmetadata_connection_config = metadata.sqlite_metadata_connection_config(\n              local_runner.METADATA_PATH)\n</pre> import tensorflow as tf import tfx from ml_metadata import errors from ml_metadata.proto import metadata_store_pb2 from tfx.types import artifact_utils  # TODO(b/171447278): Move these functions into TFX library.  def get_latest_executions(store, pipeline_name, component_id = None):   \"\"\"Fetch all pipeline runs.\"\"\"   if component_id is None:  # Find entire pipeline runs.     run_contexts = [         c for c in store.get_contexts_by_type('run')         if c.properties['pipeline_name'].string_value == pipeline_name     ]   else:  # Find specific component runs.     run_contexts = [         c for c in store.get_contexts_by_type('component_run')         if c.properties['pipeline_name'].string_value == pipeline_name and            c.properties['component_id'].string_value == component_id     ]   if not run_contexts:     return []   # Pick the latest run context.   latest_context = max(run_contexts,                        key=lambda c: c.last_update_time_since_epoch)   return store.get_executions_by_context(latest_context.id)  def get_latest_artifacts(store, pipeline_name, component_id = None):   \"\"\"Fetch all artifacts from latest pipeline execution.\"\"\"   executions = get_latest_executions(store, pipeline_name, component_id)    # Fetch all artifacts produced from the given executions.   execution_ids = [e.id for e in executions]   events = store.get_events_by_execution_ids(execution_ids)   artifact_ids = [       event.artifact_id for event in events       if event.type == metadata_store_pb2.Event.OUTPUT   ]   return store.get_artifacts_by_id(artifact_ids)  def find_latest_artifacts_by_type(store, artifacts, artifact_type):   \"\"\"Get the latest artifacts of a specified type.\"\"\"   # Get type information from MLMD   try:     artifact_type = store.get_artifact_type(artifact_type)   except errors.NotFoundError:     return []   # Filter artifacts with type.   filtered_artifacts = [aritfact for aritfact in artifacts                         if aritfact.type_id == artifact_type.id]   # Convert MLMD artifact data into TFX Artifact instances.   return [artifact_utils.deserialize_artifact(artifact_type, artifact)       for artifact in filtered_artifacts]   from tfx.orchestration.experimental.interactive import visualizations  def visualize_artifacts(artifacts):   \"\"\"Visualizes artifacts using standard visualization modules.\"\"\"   for artifact in artifacts:     visualization = visualizations.get_registry().get_visualization(         artifact.type_name)     if visualization:       visualization.display(artifact)  from tfx.orchestration.experimental.interactive import standard_visualizations standard_visualizations.register_standard_visualizations()  import pprint  from tfx.orchestration import metadata from tfx.types import artifact_utils from tfx.types import standard_artifacts  def preview_examples(artifacts):   \"\"\"Preview a few records from Examples artifacts.\"\"\"   pp = pprint.PrettyPrinter()   for artifact in artifacts:     print(\"==== Examples artifact:{}({})\".format(artifact.name, artifact.uri))     for split in artifact_utils.decode_split_names(artifact.split_names):       print(\"==== Reading from split:{}\".format(split))       split_uri = artifact_utils.get_split_uri([artifact], split)        # Get the list of files in this directory (all compressed TFRecord files)       tfrecord_filenames = [os.path.join(split_uri, name)                             for name in os.listdir(split_uri)]       # Create a `TFRecordDataset` to read these files       dataset = tf.data.TFRecordDataset(tfrecord_filenames,                                         compression_type=\"GZIP\")       # Iterate over the first 2 records and decode them.       for tfrecord in dataset.take(2):         serialized_example = tfrecord.numpy()         example = tf.train.Example()         example.ParseFromString(serialized_example)         pp.pprint(example)  import local_runner  metadata_connection_config = metadata.sqlite_metadata_connection_config(               local_runner.METADATA_PATH) <p>Now we can read metadata of output artifacts from MLMD.</p> In\u00a0[\u00a0]: Copied! <pre>with metadata.Metadata(metadata_connection_config) as metadata_handler:\n    # Search all aritfacts from the previous pipeline run.\n    artifacts = get_latest_artifacts(metadata_handler.store, PIPELINE_NAME)\n    # Find artifacts of Examples type.\n    examples_artifacts = find_latest_artifacts_by_type(\n        metadata_handler.store, artifacts,\n        standard_artifacts.Examples.TYPE_NAME)\n    # Find artifacts generated from StatisticsGen.\n    stats_artifacts = find_latest_artifacts_by_type(\n        metadata_handler.store, artifacts,\n        standard_artifacts.ExampleStatistics.TYPE_NAME)\n    # Find artifacts generated from SchemaGen.\n    schema_artifacts = find_latest_artifacts_by_type(\n        metadata_handler.store, artifacts,\n        standard_artifacts.Schema.TYPE_NAME)\n    # Find artifacts generated from ExampleValidator.\n    anomalies_artifacts = find_latest_artifacts_by_type(\n        metadata_handler.store, artifacts,\n        standard_artifacts.ExampleAnomalies.TYPE_NAME)\n</pre> with metadata.Metadata(metadata_connection_config) as metadata_handler:     # Search all aritfacts from the previous pipeline run.     artifacts = get_latest_artifacts(metadata_handler.store, PIPELINE_NAME)     # Find artifacts of Examples type.     examples_artifacts = find_latest_artifacts_by_type(         metadata_handler.store, artifacts,         standard_artifacts.Examples.TYPE_NAME)     # Find artifacts generated from StatisticsGen.     stats_artifacts = find_latest_artifacts_by_type(         metadata_handler.store, artifacts,         standard_artifacts.ExampleStatistics.TYPE_NAME)     # Find artifacts generated from SchemaGen.     schema_artifacts = find_latest_artifacts_by_type(         metadata_handler.store, artifacts,         standard_artifacts.Schema.TYPE_NAME)     # Find artifacts generated from ExampleValidator.     anomalies_artifacts = find_latest_artifacts_by_type(         metadata_handler.store, artifacts,         standard_artifacts.ExampleAnomalies.TYPE_NAME) <p>Now we can examine outputs from each component. Tensorflow Data Validation(TFDV) is used in <code>StatisticsGen</code>, <code>SchemaGen</code> and <code>ExampleValidator</code>, and TFDV can be used to visualize outputs from these components.</p> <p>In this tutorial, we will use visualzation helper methods in TFX which use TFDV internally to show the visualization. Please see TFX components tutorial to learn more about each component.</p> In\u00a0[\u00a0]: Copied! <pre>preview_examples(examples_artifacts)\n</pre> preview_examples(examples_artifacts) <p>By default, TFX ExampleGen divides examples into two splits, train and eval, but you can adjust your split configuration.</p> In\u00a0[\u00a0]: Copied! <pre>visualize_artifacts(stats_artifacts)\n</pre> visualize_artifacts(stats_artifacts) <p>These statistics are supplied to SchemaGen to construct a schema of data automatically.</p> In\u00a0[\u00a0]: Copied! <pre>visualize_artifacts(schema_artifacts)\n</pre> visualize_artifacts(schema_artifacts) <p>This schema is automatically inferred from the output of StatisticsGen. We will use this generated schema in this tutorial, but you also can modify and customize the schema.</p> In\u00a0[\u00a0]: Copied! <pre>visualize_artifacts(anomalies_artifacts)\n</pre> visualize_artifacts(anomalies_artifacts) <p>If any anomalies were found, you may review your data that all examples follow your assumptions. Outputs from other components like StatistcsGen might be useful. Found anomalies don't block the pipeline execution.</p> <p>You can see the available features from the outputs of the <code>SchemaGen</code>. If your features can be used to construct ML model in <code>Trainer</code> directly, you can skip the next step and go to Step 4. Otherwise you can do some feature engineering work in the next step. <code>Transform</code> component is needed when full-pass operations like calculating averages are required, especially when you need to scale.</p> In\u00a0[\u00a0]: Copied! <pre>!tfx pipeline update --engine=local --pipeline_path=local_runner.py \\\n &amp;&amp; tfx run create --engine=local --pipeline_name={PIPELINE_NAME}\n</pre> !tfx pipeline update --engine=local --pipeline_path=local_runner.py \\  &amp;&amp; tfx run create --engine=local --pipeline_name={PIPELINE_NAME} <p>If the pipeline ran successfully, you should see \"Component Transform is finished.\" somewhere in the log. Because <code>Transform</code> component and <code>ExampleValidator</code> component are not dependent to each other, the order of executions is not fixed. That said, either of <code>Transform</code> and <code>ExampleValidator</code> can be the last component in the pipeline execution.</p> In\u00a0[\u00a0]: Copied! <pre>with metadata.Metadata(metadata_connection_config) as metadata_handler:\n    # Search all aritfacts from the previous run of Transform component.\n    artifacts = get_latest_artifacts(metadata_handler.store,\n                                     PIPELINE_NAME, \"Transform\")\n    # Find artifacts of Examples type.\n    transformed_examples_artifacts = find_latest_artifacts_by_type(\n        metadata_handler.store, artifacts,\n        standard_artifacts.Examples.TYPE_NAME)\n</pre> with metadata.Metadata(metadata_connection_config) as metadata_handler:     # Search all aritfacts from the previous run of Transform component.     artifacts = get_latest_artifacts(metadata_handler.store,                                      PIPELINE_NAME, \"Transform\")     # Find artifacts of Examples type.     transformed_examples_artifacts = find_latest_artifacts_by_type(         metadata_handler.store, artifacts,         standard_artifacts.Examples.TYPE_NAME) In\u00a0[\u00a0]: Copied! <pre>preview_examples(transformed_examples_artifacts)\n</pre> preview_examples(transformed_examples_artifacts) In\u00a0[\u00a0]: Copied! <pre>!tfx pipeline update --engine=local --pipeline_path=local_runner.py \\\n &amp;&amp; tfx run create --engine=local --pipeline_name={PIPELINE_NAME}\n</pre> !tfx pipeline update --engine=local --pipeline_path=local_runner.py \\  &amp;&amp; tfx run create --engine=local --pipeline_name={PIPELINE_NAME} <p>When this execution runs successfully, you have now created and run your first TFX pipeline for your model. Congratulations!</p> <p>Your new model will be located in some place under the output directory, but it would be better to have a model in fixed location or service outside of the TFX pipeline which holds many interim results. Even better with continuous evaluation of the built model which is critical in ML production systems. We will see how continuous evaluation and deployments work in TFX in the next step.</p> <p><code>Evaluator</code> component continuously evaluate every built model from <code>Trainer</code>, and <code>Pusher</code> copies the model to a predefined location in the file system or even to Google Cloud AI Platform Models.</p> In\u00a0[\u00a0]: Copied! <pre># Update and run the pipeline.\n!tfx pipeline update --engine=local --pipeline_path=local_runner.py \\\n &amp;&amp; tfx run create --engine=local --pipeline_name={PIPELINE_NAME}\n</pre> # Update and run the pipeline. !tfx pipeline update --engine=local --pipeline_path=local_runner.py \\  &amp;&amp; tfx run create --engine=local --pipeline_name={PIPELINE_NAME} In\u00a0[\u00a0]: Copied! <pre># Install TFMA notebook extension.\n!jupyter labextension install tensorflow_model_analysis@{tfma.__version__}\n</pre> # Install TFMA notebook extension. !jupyter labextension install tensorflow_model_analysis@{tfma.__version__} <p>If installation is completed, please reload your browser to make the extension take effect.</p> In\u00a0[\u00a0]: Copied! <pre>with metadata.Metadata(metadata_connection_config) as metadata_handler:\n  # Search all aritfacts from the previous pipeline run.\n  artifacts = get_latest_artifacts(metadata_handler.store, PIPELINE_NAME)\n  model_evaluation_artifacts = find_latest_artifacts_by_type(\n      metadata_handler.store, artifacts,\n      standard_artifacts.ModelEvaluation.TYPE_NAME)\n</pre> with metadata.Metadata(metadata_connection_config) as metadata_handler:   # Search all aritfacts from the previous pipeline run.   artifacts = get_latest_artifacts(metadata_handler.store, PIPELINE_NAME)   model_evaluation_artifacts = find_latest_artifacts_by_type(       metadata_handler.store, artifacts,       standard_artifacts.ModelEvaluation.TYPE_NAME) In\u00a0[\u00a0]: Copied! <pre>if model_evaluation_artifacts:\n  tfma_result = tfma.load_eval_result(model_evaluation_artifacts[0].uri)\n  tfma.view.render_slicing_metrics(tfma_result)\n</pre> if model_evaluation_artifacts:   tfma_result = tfma.load_eval_result(model_evaluation_artifacts[0].uri)   tfma.view.render_slicing_metrics(tfma_result) In\u00a0[\u00a0]: Copied! <pre># Update and run the pipeline.\n!tfx pipeline update --engine=local --pipeline_path=local_runner.py \\\n &amp;&amp; tfx run create --engine=local --pipeline_name={PIPELINE_NAME}\n</pre> # Update and run the pipeline. !tfx pipeline update --engine=local --pipeline_path=local_runner.py \\  &amp;&amp; tfx run create --engine=local --pipeline_name={PIPELINE_NAME} <p>You should be able to find your new model at <code>SERVING_MODEL_DIR</code>.</p> <p>As mentioned earlier, <code>local_runner.py</code> is good for debugging or development purpose but not a best solution for production workloads. In this step, we will deploy the pipeline to Kubeflow Pipelines on Google Cloud.</p> In\u00a0[\u00a0]: Copied! <pre>!pip install --upgrade -q kfp\n\n# Download skaffold and set it executable.\n!curl -Lo skaffold https://storage.googleapis.com/skaffold/releases/latest/skaffold-linux-amd64 &amp;&amp; chmod +x skaffold\n</pre> !pip install --upgrade -q kfp  # Download skaffold and set it executable. !curl -Lo skaffold https://storage.googleapis.com/skaffold/releases/latest/skaffold-linux-amd64 &amp;&amp; chmod +x skaffold <p>You need to move <code>skaffold</code> binary to the place where your shell can find it. Or you can specify the path to skaffold when you run <code>tfx</code> binary with <code>--skaffold-cmd</code> flag.</p> In\u00a0[\u00a0]: Copied! <pre># Move skaffold binary into your path\n!mv skaffold /home/jupyter/.local/bin/\n</pre> # Move skaffold binary into your path !mv skaffold /home/jupyter/.local/bin/ <p>You also need a Kubeflow Pipelines cluster to run the pipeline. Please follow Step 1 and 2 in TFX on Cloud AI Platform Pipelines tutorial.</p> <p>When your cluster is ready, open the pipeline dashboard by clicking Open Pipelines Dashboard in the <code>Pipelines</code> page of the Google cloud console. The URL of this page is <code>ENDPOINT</code> to request a pipeline run. The endpoint value is everything in the URL after the https://, up to, and including, googleusercontent.com. Put your endpoint to following code block.</p> In\u00a0[\u00a0]: Copied! <pre>ENDPOINT='' # Enter your ENDPOINT here.\n</pre> ENDPOINT='' # Enter your ENDPOINT here. <p>To run our code in a Kubeflow Pipelines cluster, we need to pack our code into a container image. The image will be built automatically while deploying our pipeline, and you only need to set a name and an container registry for your image. In our example, we will use Google Container registry, and name it <code>tfx-pipeline</code>.</p> In\u00a0[\u00a0]: Copied! <pre># Read GCP project id from env.\nshell_output=!gcloud config list --format 'value(core.project)' 2&gt;/dev/null\nGOOGLE_CLOUD_PROJECT=shell_output[0]\n\n# Docker image name for the pipeline image.\nCUSTOM_TFX_IMAGE='gcr.io/' + GOOGLE_CLOUD_PROJECT + '/tfx-pipeline'\n</pre> # Read GCP project id from env. shell_output=!gcloud config list --format 'value(core.project)' 2&gt;/dev/null GOOGLE_CLOUD_PROJECT=shell_output[0]  # Docker image name for the pipeline image. CUSTOM_TFX_IMAGE='gcr.io/' + GOOGLE_CLOUD_PROJECT + '/tfx-pipeline' In\u00a0[\u00a0]: Copied! <pre>!gsutil cp data/data.csv gs://{GOOGLE_CLOUD_PROJECT}-kubeflowpipelines-default/tfx-template/data/penguin/\n</pre> !gsutil cp data/data.csv gs://{GOOGLE_CLOUD_PROJECT}-kubeflowpipelines-default/tfx-template/data/penguin/ <p>Update the data location stored at <code>DATA_PATH</code> in <code>kubeflow_runner.py</code>.</p> <p>If you are using BigQueryExampleGen, there is no need to upload the data file, but please make sure that <code>kubeflow_runner.py</code> uses the same <code>query</code> and <code>beam_pipeline_args</code> argument for <code>pipeline.create_pipeline()</code> function.</p> In\u00a0[\u00a0]: Copied! <pre>!tfx pipeline create  \\\n--engine=kubeflow \\\n--pipeline-path=kubeflow_runner.py \\\n--endpoint={ENDPOINT} \\\n--build-target-image={CUSTOM_TFX_IMAGE}\n</pre> !tfx pipeline create  \\ --engine=kubeflow \\ --pipeline-path=kubeflow_runner.py \\ --endpoint={ENDPOINT} \\ --build-target-image={CUSTOM_TFX_IMAGE} <p>Now start an execution run with the newly created pipeline using the <code>tfx run create</code> command.</p> In\u00a0[\u00a0]: Copied! <pre>!tfx run create --engine=kubeflow --pipeline-name={PIPELINE_NAME} --endpoint={ENDPOINT}\n</pre> !tfx run create --engine=kubeflow --pipeline-name={PIPELINE_NAME} --endpoint={ENDPOINT} <p>Or, you can also run the pipeline in the Kubeflow Pipelines dashboard. The new run will be listed under <code>Experiments</code> in the Kubeflow Pipelines dashboard. Clicking into the experiment will allow you to monitor progress and visualize the artifacts created during the execution run.</p> <p>If you are interested in running your pipeline on Kubeflow Pipelines, find more instructions in TFX on Cloud AI Platform Pipelines tutorial.</p>"},{"location":"tutorials/tfx/penguin_template/#copyright-2020-the-tensorflow-authors","title":"Copyright 2020 The TensorFlow Authors.\u00b6","text":""},{"location":"tutorials/tfx/penguin_template/#create-a-tfx-pipeline-for-your-data-with-penguin-template","title":"Create a TFX pipeline for your data with Penguin template\u00b6","text":""},{"location":"tutorials/tfx/penguin_template/#introduction","title":"Introduction\u00b6","text":"<p>This document will provide instructions to create a TensorFlow Extended (TFX) pipeline for your own dataset using penguin template which is provided with TFX Python package. Created pipeline will be using Palmer Penguins dataset initially, but we will transform the pipeline for your dataset.</p>"},{"location":"tutorials/tfx/penguin_template/#prerequisites","title":"Prerequisites\u00b6","text":"<ul> <li>Linux / MacOS</li> <li>Python 3.6-3.8</li> <li>Jupyter notebook</li> </ul>"},{"location":"tutorials/tfx/penguin_template/#step-1-copy-the-predefined-template-to-your-project-directory","title":"Step 1. Copy the predefined template to your project directory.\u00b6","text":"<p>In this step, we will create a working pipeline project directory and files by copying files from penguin template in TFX. You can think of this as a scaffold for your TFX pipeline project.</p>"},{"location":"tutorials/tfx/penguin_template/#update-pip","title":"Update Pip\u00b6","text":"<p>If we're running in Colab then we should make sure that we have the latest version of Pip.  Local systems can of course be updated separately.</p> <p>Note: Updating is probably also a good idea if you are running in Vertex AI Workbench.</p>"},{"location":"tutorials/tfx/penguin_template/#install-required-package","title":"Install required package\u00b6","text":"<p>First, install TFX and TensorFlow Model Analysis (TFMA).</p>"},{"location":"tutorials/tfx/penguin_template/#copy-template-files","title":"Copy template files.\u00b6","text":"<p>TFX includes the <code>penguin</code> template with the TFX python package. <code>penguin</code> template contains many instructions to bring your dataset into the pipeline which is the purpose of this tutorial.</p> <p>The <code>tfx template copy</code> CLI command copies predefined template files into your project directory.</p>"},{"location":"tutorials/tfx/penguin_template/#browse-your-copied-source-files","title":"Browse your copied source files\u00b6","text":"<p>The TFX template provides basic scaffold files to build a pipeline, including Python source code and sample data. The <code>penguin</code> template uses the same Palmer Penguins dataset and ML model as the Penguin example.</p> <p>Here is brief introduction to each of the Python files.</p> <ul> <li><code>pipeline</code> - This directory contains the definition of the pipeline<ul> <li><code>configs.py</code> \u2014 defines common constants for pipeline runners</li> <li><code>pipeline.py</code> \u2014 defines TFX components and a pipeline</li> </ul> </li> <li><code>models</code> - This directory contains ML model definitions<ul> <li><code>features.py</code>, <code>features_test.py</code> \u2014 defines features for the model</li> <li><code>preprocessing.py</code>, <code>preprocessing_test.py</code> \u2014 defines preprocessing routines for data</li> <li><code>constants.py</code> \u2014 defines constants of the model</li> <li><code>model.py</code>, <code>model_test.py</code> \u2014 defines ML model using ML frameworks like TensorFlow</li> </ul> </li> <li><code>local_runner.py</code> \u2014 define a runner for local environment which uses local orchestration engine</li> <li><code>kubeflow_runner.py</code> \u2014 define a runner for Kubeflow Pipelines orchestration engine</li> </ul>"},{"location":"tutorials/tfx/penguin_template/#unit-test-files","title":"Unit-test files.\u00b6","text":"<p>You might notice that there are some files with <code>_test.py</code> in their name. These are unit tests of the pipeline and it is recommended to add more unit tests as you implement your own pipelines. You can run unit tests by supplying the module name of test files with <code>-m</code> flag. You can usually get a module name by deleting <code>.py</code> extension and replacing <code>/</code> with <code>.</code>.  For example:</p>"},{"location":"tutorials/tfx/penguin_template/#create-a-tfx-pipeline-in-local-environment","title":"Create a TFX pipeline in local environment.\u00b6","text":"<p>TFX supports several orchestration engines to run pipelines. We will use local orchestration engine. Local orchestration engine runs without any further dependencies, and it is suitable for development and debugging because it runs on local environment rather than depends on remote computing clusters.</p>"},{"location":"tutorials/tfx/penguin_template/#step-2-ingest-your-data-to-the-pipeline","title":"Step 2. Ingest YOUR data to the pipeline.\u00b6","text":"<p>The initial pipeline ingests the penguin dataset which is included in the template. You need to put your data into the pipeline, and most TFX pipelines start with ExampleGen component.</p>"},{"location":"tutorials/tfx/penguin_template/#choose-an-examplegen","title":"Choose an ExampleGen\u00b6","text":"<p>Your data can be stored anywhere your pipeline can access, on either a local or distributed filesystem, or a query-able system. TFX provides various <code>ExampleGen</code> components to bring your data into a TFX pipeline. You can choose one from following example generating components.</p> <ul> <li>CsvExampleGen: Reads CSV files in a directory. Used in penguin example and Chicago taxi example.</li> <li>ImportExampleGen: Takes TFRecord files with TF Example data format. Used in MNIST examples.</li> <li>FileBasedExampleGen for Avro or Parquet format.</li> <li>BigQueryExampleGen: Reads data in Google Cloud BigQuery directly. Used in Chicago taxi examples.</li> </ul> <p>You can also create your own ExampleGen, for example, tfx includes a custom ExecampleGen which uses Presto as a data source. See the guide for more information on how to use and develop custom executors.</p> <p>Once you decide which ExampleGen to use, you will need to modify the pipeline definition to use your data.</p> <ol> <li>Modify the <code>DATA_PATH</code> in <code>local_runner.py</code> and set it to the location of your files.</li> </ol> <ul> <li>If you have files in local environment, specify the path. This is the best option for developing or debugging a pipeline.</li> <li>If the files are stored in GCS, you can use a path starting with <code>gs://{bucket_name}/...</code>. Please make sure that you can access GCS from your terminal, for example, using <code>gsutil</code>. Please follow authorization guide in Google Cloud if needed.</li> <li>If you want to use a Query-based ExampleGen like BigQueryExampleGen, you need a Query statement to select data from the data source. There are a few more things you need to set to use Google Cloud BigQuery as a data source.<ul> <li>In <code>pipeline/configs.py</code>:<ul> <li>Change <code>GOOGLE_CLOUD_PROJECT</code> and <code>GCS_BUCKET_NAME</code> to your GCP project and bucket name. The bucket should exist before we run the pipeline.</li> <li>Uncomment <code>BIG_QUERY_WITH_DIRECT_RUNNER_BEAM_PIPELINE_ARGS</code> variable.</li> <li>Uncomment and set <code>BIG_QUERY_QUERY</code> variable to your query statement.</li> </ul> </li> <li>In <code>local_runner.py</code>:<ul> <li>Comment out <code>data_path</code> argument and uncomment <code>query</code> argument instead in <code>pipeline.create_pipeline()</code>.</li> </ul> </li> <li>In <code>pipeline/pipeline.py</code>:<ul> <li>Comment out <code>data_path</code> argument and uncomment <code>query</code> argument in <code>create_pipeline()</code>.</li> <li>Use BigQueryExampleGen instead of CsvExampleGen.</li> </ul> </li> </ul> </li> </ul> <ol> <li>Replace existing CsvExampleGen to your ExampleGen class in <code>pipeline/pipeline.py</code>. Each ExampleGen class has different signature. Please see ExampleGen component guide for more detail. Don't forget to import required modules with <code>import</code> statements in <code>pipeline/pipeline.py</code>.</li> </ol>"},{"location":"tutorials/tfx/penguin_template/#examine-output-of-the-pipeline","title":"Examine output of the pipeline.\u00b6","text":""},{"location":"tutorials/tfx/penguin_template/#examine-output-form-examplegen","title":"Examine output form ExampleGen\u00b6","text":"<p>Let's examine output from ExampleGen. Take a look at the first two examples for each split:</p>"},{"location":"tutorials/tfx/penguin_template/#examine-output-from-statisticsgen","title":"Examine output from StatisticsGen\u00b6","text":""},{"location":"tutorials/tfx/penguin_template/#examine-output-from-schemagen","title":"Examine output from SchemaGen\u00b6","text":""},{"location":"tutorials/tfx/penguin_template/#examine-output-from-examplevalidator","title":"Examine output from ExampleValidator\u00b6","text":""},{"location":"tutorials/tfx/penguin_template/#step-3-optional-feature-engineering-with-transform-component","title":"Step 3. (Optional) Feature engineering with Transform component.\u00b6","text":"<p>In this step, you will define various feature engineering job which will be used by <code>Transform</code> component in the pipeline. See Transform component guide for more information.</p> <p>This is only necessary if you training code requires additional feature(s) which is not available in the output of ExampleGen. Otherwise, feel free to fast forward to next step of using Trainer.</p>"},{"location":"tutorials/tfx/penguin_template/#define-features-of-the-model","title":"Define features of the model\u00b6","text":"<p><code>models/features.py</code> contains constants to define features for the model including feature names, size of vocabulariy and so on. By default <code>penguin</code> template has two costants, <code>FEATURE_KEYS</code> and <code>LABEL_KEY</code>, because our <code>penguin</code> model solves a classification problem using supervised learning and all features are continuous numeric features. See feature definitions from the chicago taxi example for another example.</p>"},{"location":"tutorials/tfx/penguin_template/#implement-preprocessing-for-training-serving-in-preprocessing_fn","title":"Implement preprocessing for training / serving in preprocessing_fn().\u00b6","text":"<p>Actual feature engineering happens in <code>preprocessing_fn()</code> function in <code>models/preprocessing.py</code>.</p> <p>In <code>preprocessing_fn</code> you can define a series of functions that manipulate the input dict of tensors to produce the output dict of tensors. There are helper functions like <code>scale_to_0_1</code> and <code>compute_and_apply_vocabulary</code> in the TensorFlow Transform API or you can simply use regular TensorFlow functions. By default <code>penguin</code> template includes example usages of tft.scale_to_z_score function to normalize feature values.</p> <p>See Tensflow Transform guide for more information about authoring <code>preprocessing_fn</code>.</p>"},{"location":"tutorials/tfx/penguin_template/#add-transform-component-to-the-pipeline","title":"Add Transform component to the pipeline.\u00b6","text":"<p>If your preprocessing_fn is ready, add <code>Transform</code> component to the pipeline.</p> <ol> <li>In <code>pipeline/pipeline.py</code> file, uncomment <code># components.append(transform)</code> to add the component to the pipeline.</li> </ol> <p>You can update the pipeline and run again.</p>"},{"location":"tutorials/tfx/penguin_template/#examine-output-from-transform","title":"Examine output from Transform\u00b6","text":"<p>Transform component creates two kinds of outputs, a Tensorflow graph and transformed examples. The transformed examples are Examples artifact type which is also produced by ExampleGen, but this one contains transformed feature values instead.</p> <p>You can examine them as we did in the previous step.</p>"},{"location":"tutorials/tfx/penguin_template/#step-4-train-your-model-with-trainer-component","title":"Step 4. Train your model with Trainer component.\u00b6","text":"<p>We will build a ML model using <code>Trainer</code> component. See Trainer component guide for more information. You need to provide your model code to the Trainer component.</p>"},{"location":"tutorials/tfx/penguin_template/#define-your-model","title":"Define your model.\u00b6","text":"<p>In penguin template, <code>models.model.run_fn</code> is used as <code>run_fn</code> argument for <code>Trainer</code> component. It means that <code>run_fn()</code> function in <code>models/model.py</code> will be called when <code>Trainer</code> component runs. You can see the code to construct a simple DNN model using <code>keras</code> API in given code. See TensorFlow 2.x in TFX guide for more information about using keras API in TFX.</p> <p>In this <code>run_fn</code>, you should build a model and save it to a directory pointed by <code>fn_args.serving_model_dir</code> which is specified by the component. You can use other arguments in <code>fn_args</code> which is passed into the <code>run_fn</code>. See related codes for the full list of arguments in <code>fn_args</code>.</p> <p>Define your features in <code>models/features.py</code> and use them as needed. If you have transformed your features in Step 3, you should use transformed features as inputs to your model.</p>"},{"location":"tutorials/tfx/penguin_template/#add-trainer-component-to-the-pipeline","title":"Add Trainer component to the pipeline.\u00b6","text":"<p>If your run_fn is ready, add <code>Trainer</code> component to the pipeline.</p> <ol> <li>In <code>pipeline/pipeline.py</code> file, uncomment <code># components.append(trainer)</code> to add the component to the pipeline.</li> </ol> <p>Arguments for the trainer component might depends on whether you use Transform component or not.</p> <ul> <li>If you do NOT use <code>Transform</code> component, you don't need to change the arguments.</li> <li>If you use <code>Transform</code> component, you need to change arguments when creating a <code>Trainer</code> component instance.<ul> <li>Change <code>examples</code> argument to <code>examples=transform.outputs['transformed_examples'],</code>. We need to use transformed examples for training.</li> <li>Add <code>transform_graph</code> argument like <code>transform_graph=transform.outputs['transform_graph'],</code>. This graph contains TensorFlow graph for the transform operations.</li> <li>After above changes, the code for Trainer component creation will look like following.</li> </ul> <pre># If you use a Transform component.\ntrainer = Trainer(\n    run_fn=run_fn,\n    examples=transform.outputs['transformed_examples'],\n    transform_graph=transform.outputs['transform_graph'],\n    schema=schema_gen.outputs['schema'],\n    ...\n</pre> </li> </ul> <p>You can update the pipeline and run again.</p>"},{"location":"tutorials/tfx/penguin_template/#step-5-optional-evaluate-the-model-with-evaluator-and-publish-with-pusher","title":"Step 5.  (Optional) Evaluate the model with Evaluator and publish with pusher.\u00b6","text":""},{"location":"tutorials/tfx/penguin_template/#adds-evaluator-component-to-the-pipeline","title":"Adds Evaluator component to the pipeline.\u00b6","text":"<p>In <code>pipeline/pipeline.py</code> file:</p> <ol> <li>Uncomment <code># components.append(model_resolver)</code> to add latest model resolver to the pipeline. Evaluator can be used to compare a model with old baseline model which passed Evaluator in last pipeline run. <code>LatestBlessedModelResolver</code> finds the latest model which passed Evaluator.</li> <li>Set proper <code>tfma.MetricsSpec</code> for your model. Evaluation might be different for every ML model. In the penguin template, <code>SparseCategoricalAccuracy</code> was used because we are solving a multi category classification problem. You also need to specify <code>tfma.SliceSpec</code> to analyze your model for specific slices. For more detail, see Evaluator component guide.</li> <li>Uncomment <code># components.append(evaluator)</code> to add the component to the pipeline.</li> </ol> <p>You can update the pipeline and run again.</p>"},{"location":"tutorials/tfx/penguin_template/#examine-output-of-evaluator","title":"Examine output of Evaluator\u00b6","text":"<p>This step requires TensorFlow Model Analysis(TFMA) Jupyter notebook extension. Note that the version of the TFMA notebook extension should be identical to the version of TFMA python package.</p> <p>Following command will install TFMA notebook extension from NPM registry. It might take several minutes to complete.</p>"},{"location":"tutorials/tfx/penguin_template/#adds-pusher-component-to-the-pipeline","title":"Adds Pusher component to the pipeline.\u00b6","text":"<p>If the model looks promising, we need to publish the model. Pusher component can publish the model to a location in the filesystem or to GCP AI Platform Models using a custom executor.</p> <p><code>Evaluator</code> component continuously evaluate every built model from <code>Trainer</code>, and <code>Pusher</code> copies the model to a predefined location in the file system or even to Google Cloud AI Platform Models.</p> <ol> <li>In <code>local_runner.py</code>, set <code>SERVING_MODEL_DIR</code> to a directory to publish.</li> <li>In <code>pipeline/pipeline.py</code> file, uncomment <code># components.append(pusher)</code> to add Pusher to the pipeline.</li> </ol> <p>You can update the pipeline and run again.</p>"},{"location":"tutorials/tfx/penguin_template/#step-6-optional-deploy-your-pipeline-to-kubeflow-pipelines-on-gcp","title":"Step 6.  (Optional) Deploy your pipeline to Kubeflow Pipelines on GCP.\u00b6","text":""},{"location":"tutorials/tfx/penguin_template/#preparation","title":"Preparation\u00b6","text":"<p>We need <code>kfp</code> python package and <code>skaffold</code> program to deploy a pipeline to a Kubeflow Pipelines cluster.</p>"},{"location":"tutorials/tfx/penguin_template/#set-data-location","title":"Set data location.\u00b6","text":"<p>Your data should be accessible from the Kubeflow Pipelines cluster. If you have used data in your local environment, you might need to upload it to remote storage like Google Cloud Storage. For example, we can upload penguin data to a default bucket which is created automatically when a Kubeflow Pipelines cluster is deployed like following.</p>"},{"location":"tutorials/tfx/penguin_template/#deploy-the-pipeline","title":"Deploy the pipeline.\u00b6","text":"<p>If everything is ready, you can create a pipeline using <code>tfx pipeline create</code> command.</p> <p>Note: When creating a pipeline for Kubeflow Pipelines, we need a container image which will be used to run our pipeline. And <code>skaffold</code> will build the image for us. Because <code>skaffold</code> pulls base images from the docker hub, it will take 5~10 minutes when we build the image for the first time, but it will take much less time from the second build.</p>"},{"location":"tutorials/tfx/penguin_template/#cleaning-up","title":"Cleaning up\u00b6","text":"<p>To clean up all Google Cloud resources used in this step, you can delete the Google Cloud project you used for the tutorial.</p> <p>Alternatively, you can clean up individual resources by visiting each consoles:</p> <ul> <li>Google Cloud Storage</li> <li>Google Container Registry</li> <li>Google Kubernetes Engine</li> </ul>"},{"location":"tutorials/tfx/penguin_tfdv/","title":"Data validation using TFX Pipeline and TensorFlow Data Validation","text":"In\u00a0[\u00a0]: Copied! <pre>#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n</pre> #@title Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # https://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. <p>Note: We recommend running this tutorial in a Colab notebook, with no setup required!  Just click \"Run in Google Colab\".</p>        View on TensorFlow.org             Run in Google Colab             View source on GitHub             Download notebook      <p>In this notebook-based tutorial, we will create and run TFX pipelines to validate input data and create an ML model. This notebook is based on the TFX pipeline we built in Simple TFX Pipeline Tutorial. If you have not read that tutorial yet, you should read it before proceeding with this notebook.</p> <p>The first task in any data science or ML project is to understand and clean the data, which includes:</p> <ul> <li>Understanding the data types, distributions, and other information (e.g., mean value, or number of uniques) about each feature</li> <li>Generating a preliminary schema that describes the data</li> <li>Identifying anomalies and missing values in the data with respect to given schema</li> </ul> <p>In this tutorial, we will create two TFX pipelines.</p> <p>First, we will create a pipeline to analyze the dataset and generate a preliminary schema of the given dataset. This pipeline will include two new components, <code>StatisticsGen</code> and <code>SchemaGen</code>.</p> <p>Once we have a proper schema of the data, we will create a pipeline to train an ML classification model based on the pipeline from the previous tutorial. In this pipeline, we will use the schema from the first pipeline and a new component, <code>ExampleValidator</code>, to validate the input data.</p> <p>The three new components, StatisticsGen, SchemaGen and ExampleValidator, are TFX components for data analysis and validation, and they are implemented using the TensorFlow Data Validation library.</p> <p>Please see Understanding TFX Pipelines to learn more about various concepts in TFX.</p> In\u00a0[\u00a0]: Copied! <pre>try:\n  import colab\n  !pip install --upgrade pip\nexcept:\n  pass\n</pre> try:   import colab   !pip install --upgrade pip except:   pass In\u00a0[\u00a0]: Copied! <pre>!pip install -U tfx\n</pre> !pip install -U tfx <p>Check the TensorFlow and TFX versions.</p> In\u00a0[\u00a0]: Copied! <pre>import tensorflow as tf\nprint('TensorFlow version: {}'.format(tf.__version__))\nfrom tfx import v1 as tfx\nprint('TFX version: {}'.format(tfx.__version__))\n</pre> import tensorflow as tf print('TensorFlow version: {}'.format(tf.__version__)) from tfx import v1 as tfx print('TFX version: {}'.format(tfx.__version__)) In\u00a0[\u00a0]: Copied! <pre>import os\n\n# We will create two pipelines. One for schema generation and one for training.\nSCHEMA_PIPELINE_NAME = \"penguin-tfdv-schema\"\nPIPELINE_NAME = \"penguin-tfdv\"\n\n# Output directory to store artifacts generated from the pipeline.\nSCHEMA_PIPELINE_ROOT = os.path.join('pipelines', SCHEMA_PIPELINE_NAME)\nPIPELINE_ROOT = os.path.join('pipelines', PIPELINE_NAME)\n# Path to a SQLite DB file to use as an MLMD storage.\nSCHEMA_METADATA_PATH = os.path.join('metadata', SCHEMA_PIPELINE_NAME,\n                                    'metadata.db')\nMETADATA_PATH = os.path.join('metadata', PIPELINE_NAME, 'metadata.db')\n\n# Output directory where created models from the pipeline will be exported.\nSERVING_MODEL_DIR = os.path.join('serving_model', PIPELINE_NAME)\n\nfrom absl import logging\nlogging.set_verbosity(logging.INFO)  # Set default logging level.\n</pre> import os  # We will create two pipelines. One for schema generation and one for training. SCHEMA_PIPELINE_NAME = \"penguin-tfdv-schema\" PIPELINE_NAME = \"penguin-tfdv\"  # Output directory to store artifacts generated from the pipeline. SCHEMA_PIPELINE_ROOT = os.path.join('pipelines', SCHEMA_PIPELINE_NAME) PIPELINE_ROOT = os.path.join('pipelines', PIPELINE_NAME) # Path to a SQLite DB file to use as an MLMD storage. SCHEMA_METADATA_PATH = os.path.join('metadata', SCHEMA_PIPELINE_NAME,                                     'metadata.db') METADATA_PATH = os.path.join('metadata', PIPELINE_NAME, 'metadata.db')  # Output directory where created models from the pipeline will be exported. SERVING_MODEL_DIR = os.path.join('serving_model', PIPELINE_NAME)  from absl import logging logging.set_verbosity(logging.INFO)  # Set default logging level. <p>Because the TFX ExampleGen component reads inputs from a directory, we need to create a directory and copy the dataset to it.</p> In\u00a0[\u00a0]: Copied! <pre>import urllib.request\nimport tempfile\n\nDATA_ROOT = tempfile.mkdtemp(prefix='tfx-data')  # Create a temporary directory.\n_data_url = 'https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/penguin/data/labelled/penguins_processed.csv'\n_data_filepath = os.path.join(DATA_ROOT, \"data.csv\")\nurllib.request.urlretrieve(_data_url, _data_filepath)\n</pre> import urllib.request import tempfile  DATA_ROOT = tempfile.mkdtemp(prefix='tfx-data')  # Create a temporary directory. _data_url = 'https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/penguin/data/labelled/penguins_processed.csv' _data_filepath = os.path.join(DATA_ROOT, \"data.csv\") urllib.request.urlretrieve(_data_url, _data_filepath) <p>Take a quick look at the CSV file.</p> In\u00a0[\u00a0]: Copied! <pre>!head {_data_filepath}\n</pre> !head {_data_filepath} <p>You should be able to see five feature columns. <code>species</code> is one of 0, 1 or 2, and all other features should have values between 0 and 1. We will create a TFX pipeline to analyze this dataset.</p> In\u00a0[\u00a0]: Copied! <pre>def _create_schema_pipeline(pipeline_name: str,\n                            pipeline_root: str,\n                            data_root: str,\n                            metadata_path: str) -&gt; tfx.dsl.Pipeline:\n  \"\"\"Creates a pipeline for schema generation.\"\"\"\n  # Brings data into the pipeline.\n  example_gen = tfx.components.CsvExampleGen(input_base=data_root)\n\n  # NEW: Computes statistics over data for visualization and schema generation.\n  statistics_gen = tfx.components.StatisticsGen(\n      examples=example_gen.outputs['examples'])\n\n  # NEW: Generates schema based on the generated statistics.\n  schema_gen = tfx.components.SchemaGen(\n      statistics=statistics_gen.outputs['statistics'], infer_feature_shape=True)\n\n  components = [\n      example_gen,\n      statistics_gen,\n      schema_gen,\n  ]\n\n  return tfx.dsl.Pipeline(\n      pipeline_name=pipeline_name,\n      pipeline_root=pipeline_root,\n      metadata_connection_config=tfx.orchestration.metadata\n      .sqlite_metadata_connection_config(metadata_path),\n      components=components)\n</pre> def _create_schema_pipeline(pipeline_name: str,                             pipeline_root: str,                             data_root: str,                             metadata_path: str) -&gt; tfx.dsl.Pipeline:   \"\"\"Creates a pipeline for schema generation.\"\"\"   # Brings data into the pipeline.   example_gen = tfx.components.CsvExampleGen(input_base=data_root)    # NEW: Computes statistics over data for visualization and schema generation.   statistics_gen = tfx.components.StatisticsGen(       examples=example_gen.outputs['examples'])    # NEW: Generates schema based on the generated statistics.   schema_gen = tfx.components.SchemaGen(       statistics=statistics_gen.outputs['statistics'], infer_feature_shape=True)    components = [       example_gen,       statistics_gen,       schema_gen,   ]    return tfx.dsl.Pipeline(       pipeline_name=pipeline_name,       pipeline_root=pipeline_root,       metadata_connection_config=tfx.orchestration.metadata       .sqlite_metadata_connection_config(metadata_path),       components=components) In\u00a0[\u00a0]: Copied! <pre>tfx.orchestration.LocalDagRunner().run(\n  _create_schema_pipeline(\n      pipeline_name=SCHEMA_PIPELINE_NAME,\n      pipeline_root=SCHEMA_PIPELINE_ROOT,\n      data_root=DATA_ROOT,\n      metadata_path=SCHEMA_METADATA_PATH))\n</pre> tfx.orchestration.LocalDagRunner().run(   _create_schema_pipeline(       pipeline_name=SCHEMA_PIPELINE_NAME,       pipeline_root=SCHEMA_PIPELINE_ROOT,       data_root=DATA_ROOT,       metadata_path=SCHEMA_METADATA_PATH)) <p>You should see \"INFO:absl:Component SchemaGen is finished.\" if the pipeline finished successfully.</p> <p>We will examine the output of the pipeline to understand our dataset.</p> <p>As explained in the previous tutorial, a TFX pipeline produces two kinds of outputs, artifacts and a metadata DB(MLMD) which contains metadata of artifacts and pipeline executions. We defined the location of these outputs in the above cells. By default, artifacts are stored under the <code>pipelines</code> directory and metadata is stored as a sqlite database under the <code>metadata</code> directory.</p> <p>You can use MLMD APIs to locate these outputs programatically. First, we will define some utility functions to search for the output artifacts that were just produced.</p> In\u00a0[\u00a0]: Copied! <pre>from ml_metadata.proto import metadata_store_pb2\n# Non-public APIs, just for showcase.\nfrom tfx.orchestration.portable.mlmd import execution_lib\n\n# TODO(b/171447278): Move these functions into the TFX library.\n\ndef get_latest_artifacts(metadata, pipeline_name, component_id):\n  \"\"\"Output artifacts of the latest run of the component.\"\"\"\n  context = metadata.store.get_context_by_type_and_name(\n      'node', f'{pipeline_name}.{component_id}')\n  executions = metadata.store.get_executions_by_context(context.id)\n  latest_execution = max(executions,\n                         key=lambda e:e.last_update_time_since_epoch)\n  return execution_lib.get_output_artifacts(metadata, latest_execution.id)\n\n# Non-public APIs, just for showcase.\nfrom tfx.orchestration.experimental.interactive import visualizations\n\ndef visualize_artifacts(artifacts):\n  \"\"\"Visualizes artifacts using standard visualization modules.\"\"\"\n  for artifact in artifacts:\n    visualization = visualizations.get_registry().get_visualization(\n        artifact.type_name)\n    if visualization:\n      visualization.display(artifact)\n\nfrom tfx.orchestration.experimental.interactive import standard_visualizations\nstandard_visualizations.register_standard_visualizations()\n</pre> from ml_metadata.proto import metadata_store_pb2 # Non-public APIs, just for showcase. from tfx.orchestration.portable.mlmd import execution_lib  # TODO(b/171447278): Move these functions into the TFX library.  def get_latest_artifacts(metadata, pipeline_name, component_id):   \"\"\"Output artifacts of the latest run of the component.\"\"\"   context = metadata.store.get_context_by_type_and_name(       'node', f'{pipeline_name}.{component_id}')   executions = metadata.store.get_executions_by_context(context.id)   latest_execution = max(executions,                          key=lambda e:e.last_update_time_since_epoch)   return execution_lib.get_output_artifacts(metadata, latest_execution.id)  # Non-public APIs, just for showcase. from tfx.orchestration.experimental.interactive import visualizations  def visualize_artifacts(artifacts):   \"\"\"Visualizes artifacts using standard visualization modules.\"\"\"   for artifact in artifacts:     visualization = visualizations.get_registry().get_visualization(         artifact.type_name)     if visualization:       visualization.display(artifact)  from tfx.orchestration.experimental.interactive import standard_visualizations standard_visualizations.register_standard_visualizations() <p>Now we can examine the outputs from the pipeline execution.</p> In\u00a0[\u00a0]: Copied! <pre># Non-public APIs, just for showcase.\nfrom tfx.orchestration.metadata import Metadata\nfrom tfx.types import standard_component_specs\n\nmetadata_connection_config = tfx.orchestration.metadata.sqlite_metadata_connection_config(\n    SCHEMA_METADATA_PATH)\n\nwith Metadata(metadata_connection_config) as metadata_handler:\n  # Find output artifacts from MLMD.\n  stat_gen_output = get_latest_artifacts(metadata_handler, SCHEMA_PIPELINE_NAME,\n                                         'StatisticsGen')\n  stats_artifacts = stat_gen_output[standard_component_specs.STATISTICS_KEY]\n\n  schema_gen_output = get_latest_artifacts(metadata_handler,\n                                           SCHEMA_PIPELINE_NAME, 'SchemaGen')\n  schema_artifacts = schema_gen_output[standard_component_specs.SCHEMA_KEY]\n</pre> # Non-public APIs, just for showcase. from tfx.orchestration.metadata import Metadata from tfx.types import standard_component_specs  metadata_connection_config = tfx.orchestration.metadata.sqlite_metadata_connection_config(     SCHEMA_METADATA_PATH)  with Metadata(metadata_connection_config) as metadata_handler:   # Find output artifacts from MLMD.   stat_gen_output = get_latest_artifacts(metadata_handler, SCHEMA_PIPELINE_NAME,                                          'StatisticsGen')   stats_artifacts = stat_gen_output[standard_component_specs.STATISTICS_KEY]    schema_gen_output = get_latest_artifacts(metadata_handler,                                            SCHEMA_PIPELINE_NAME, 'SchemaGen')   schema_artifacts = schema_gen_output[standard_component_specs.SCHEMA_KEY] <p>It is time to examine the outputs from each component. As described above, Tensorflow Data Validation(TFDV) is used in <code>StatisticsGen</code> and <code>SchemaGen</code>, and TFDV also provides visualization of the outputs from these components.</p> <p>In this tutorial, we will use the visualization helper methods in TFX which use TFDV internally to show the visualization.</p> In\u00a0[\u00a0]: Copied! <pre># docs-infra: no-execute\nvisualize_artifacts(stats_artifacts)\n</pre> # docs-infra: no-execute visualize_artifacts(stats_artifacts) <p>You can see various stats for the input data. These statistics are supplied to <code>SchemaGen</code> to construct an initial schema of data automatically.</p> In\u00a0[\u00a0]: Copied! <pre>visualize_artifacts(schema_artifacts)\n</pre> visualize_artifacts(schema_artifacts) <p>This schema is automatically inferred from the output of StatisticsGen. You should be able to see 4 FLOAT features and 1 INT feature.</p> In\u00a0[\u00a0]: Copied! <pre>import shutil\n\n_schema_filename = 'schema.pbtxt'\nSCHEMA_PATH = 'schema'\n\nos.makedirs(SCHEMA_PATH, exist_ok=True)\n_generated_path = os.path.join(schema_artifacts[0].uri, _schema_filename)\n\n# Copy the 'schema.pbtxt' file from the artifact uri to a predefined path.\nshutil.copy(_generated_path, SCHEMA_PATH)\n</pre> import shutil  _schema_filename = 'schema.pbtxt' SCHEMA_PATH = 'schema'  os.makedirs(SCHEMA_PATH, exist_ok=True) _generated_path = os.path.join(schema_artifacts[0].uri, _schema_filename)  # Copy the 'schema.pbtxt' file from the artifact uri to a predefined path. shutil.copy(_generated_path, SCHEMA_PATH) <p>The schema file uses Protocol Buffer text format and an instance of TensorFlow Metadata Schema proto.</p> In\u00a0[\u00a0]: Copied! <pre>print(f'Schema at {SCHEMA_PATH}-----')\n!cat {SCHEMA_PATH}/*\n</pre> print(f'Schema at {SCHEMA_PATH}-----') !cat {SCHEMA_PATH}/* <p>You should be sure to review and possibly edit the schema definition as needed. In this tutorial, we will just use the generated schema unchanged.</p> In\u00a0[\u00a0]: Copied! <pre>_trainer_module_file = 'penguin_trainer.py'\n</pre> _trainer_module_file = 'penguin_trainer.py' In\u00a0[\u00a0]: Copied! <pre>%%writefile {_trainer_module_file}\n\nfrom typing import List\nfrom absl import logging\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow_transform.tf_metadata import schema_utils\n\nfrom tfx import v1 as tfx\nfrom tfx_bsl.public import tfxio\nfrom tensorflow_metadata.proto.v0 import schema_pb2\n\n# We don't need to specify _FEATURE_KEYS and _FEATURE_SPEC any more.\n# Those information can be read from the given schema file.\n\n_LABEL_KEY = 'species'\n\n_TRAIN_BATCH_SIZE = 20\n_EVAL_BATCH_SIZE = 10\n\ndef _input_fn(file_pattern: List[str],\n              data_accessor: tfx.components.DataAccessor,\n              schema: schema_pb2.Schema,\n              batch_size: int = 200) -&gt; tf.data.Dataset:\n  \"\"\"Generates features and label for training.\n\n  Args:\n    file_pattern: List of paths or patterns of input tfrecord files.\n    data_accessor: DataAccessor for converting input to RecordBatch.\n    schema: schema of the input data.\n    batch_size: representing the number of consecutive elements of returned\n      dataset to combine in a single batch\n\n  Returns:\n    A dataset that contains (features, indices) tuple where features is a\n      dictionary of Tensors, and indices is a single Tensor of label indices.\n  \"\"\"\n  return data_accessor.tf_dataset_factory(\n      file_pattern,\n      tfxio.TensorFlowDatasetOptions(\n          batch_size=batch_size, label_key=_LABEL_KEY),\n      schema=schema).repeat()\n\n\ndef _build_keras_model(schema: schema_pb2.Schema) -&gt; tf.keras.Model:\n  \"\"\"Creates a DNN Keras model for classifying penguin data.\n\n  Returns:\n    A Keras Model.\n  \"\"\"\n  # The model below is built with Functional API, please refer to\n  # https://www.tensorflow.org/guide/keras/overview for all API options.\n\n  # ++ Changed code: Uses all features in the schema except the label.\n  feature_keys = [f.name for f in schema.feature if f.name != _LABEL_KEY]\n  inputs = [keras.layers.Input(shape=(1,), name=f) for f in feature_keys]\n  # ++ End of the changed code.\n\n  d = keras.layers.concatenate(inputs)\n  for _ in range(2):\n    d = keras.layers.Dense(8, activation='relu')(d)\n  outputs = keras.layers.Dense(3)(d)\n\n  model = keras.Model(inputs=inputs, outputs=outputs)\n  model.compile(\n      optimizer=keras.optimizers.Adam(1e-2),\n      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n      metrics=[keras.metrics.SparseCategoricalAccuracy()])\n\n  model.summary(print_fn=logging.info)\n  return model\n\n\n# TFX Trainer will call this function.\ndef run_fn(fn_args: tfx.components.FnArgs):\n  \"\"\"Train the model based on given args.\n\n  Args:\n    fn_args: Holds args used to train the model as name/value pairs.\n  \"\"\"\n\n  # ++ Changed code: Reads in schema file passed to the Trainer component.\n  schema = tfx.utils.parse_pbtxt_file(fn_args.schema_path, schema_pb2.Schema())\n  # ++ End of the changed code.\n\n  train_dataset = _input_fn(\n      fn_args.train_files,\n      fn_args.data_accessor,\n      schema,\n      batch_size=_TRAIN_BATCH_SIZE)\n  eval_dataset = _input_fn(\n      fn_args.eval_files,\n      fn_args.data_accessor,\n      schema,\n      batch_size=_EVAL_BATCH_SIZE)\n\n  model = _build_keras_model(schema)\n  model.fit(\n      train_dataset,\n      steps_per_epoch=fn_args.train_steps,\n      validation_data=eval_dataset,\n      validation_steps=fn_args.eval_steps)\n\n  # The result of the training should be saved in `fn_args.serving_model_dir`\n  # directory.\n  model.save(fn_args.serving_model_dir, save_format='tf')\n</pre> %%writefile {_trainer_module_file}  from typing import List from absl import logging import tensorflow as tf from tensorflow import keras from tensorflow_transform.tf_metadata import schema_utils  from tfx import v1 as tfx from tfx_bsl.public import tfxio from tensorflow_metadata.proto.v0 import schema_pb2  # We don't need to specify _FEATURE_KEYS and _FEATURE_SPEC any more. # Those information can be read from the given schema file.  _LABEL_KEY = 'species'  _TRAIN_BATCH_SIZE = 20 _EVAL_BATCH_SIZE = 10  def _input_fn(file_pattern: List[str],               data_accessor: tfx.components.DataAccessor,               schema: schema_pb2.Schema,               batch_size: int = 200) -&gt; tf.data.Dataset:   \"\"\"Generates features and label for training.    Args:     file_pattern: List of paths or patterns of input tfrecord files.     data_accessor: DataAccessor for converting input to RecordBatch.     schema: schema of the input data.     batch_size: representing the number of consecutive elements of returned       dataset to combine in a single batch    Returns:     A dataset that contains (features, indices) tuple where features is a       dictionary of Tensors, and indices is a single Tensor of label indices.   \"\"\"   return data_accessor.tf_dataset_factory(       file_pattern,       tfxio.TensorFlowDatasetOptions(           batch_size=batch_size, label_key=_LABEL_KEY),       schema=schema).repeat()   def _build_keras_model(schema: schema_pb2.Schema) -&gt; tf.keras.Model:   \"\"\"Creates a DNN Keras model for classifying penguin data.    Returns:     A Keras Model.   \"\"\"   # The model below is built with Functional API, please refer to   # https://www.tensorflow.org/guide/keras/overview for all API options.    # ++ Changed code: Uses all features in the schema except the label.   feature_keys = [f.name for f in schema.feature if f.name != _LABEL_KEY]   inputs = [keras.layers.Input(shape=(1,), name=f) for f in feature_keys]   # ++ End of the changed code.    d = keras.layers.concatenate(inputs)   for _ in range(2):     d = keras.layers.Dense(8, activation='relu')(d)   outputs = keras.layers.Dense(3)(d)    model = keras.Model(inputs=inputs, outputs=outputs)   model.compile(       optimizer=keras.optimizers.Adam(1e-2),       loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),       metrics=[keras.metrics.SparseCategoricalAccuracy()])    model.summary(print_fn=logging.info)   return model   # TFX Trainer will call this function. def run_fn(fn_args: tfx.components.FnArgs):   \"\"\"Train the model based on given args.    Args:     fn_args: Holds args used to train the model as name/value pairs.   \"\"\"    # ++ Changed code: Reads in schema file passed to the Trainer component.   schema = tfx.utils.parse_pbtxt_file(fn_args.schema_path, schema_pb2.Schema())   # ++ End of the changed code.    train_dataset = _input_fn(       fn_args.train_files,       fn_args.data_accessor,       schema,       batch_size=_TRAIN_BATCH_SIZE)   eval_dataset = _input_fn(       fn_args.eval_files,       fn_args.data_accessor,       schema,       batch_size=_EVAL_BATCH_SIZE)    model = _build_keras_model(schema)   model.fit(       train_dataset,       steps_per_epoch=fn_args.train_steps,       validation_data=eval_dataset,       validation_steps=fn_args.eval_steps)    # The result of the training should be saved in `fn_args.serving_model_dir`   # directory.   model.save(fn_args.serving_model_dir, save_format='tf') <p>Now you have completed all preparation steps to build a TFX pipeline for model training.</p> In\u00a0[\u00a0]: Copied! <pre>def _create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str,\n                     schema_path: str, module_file: str, serving_model_dir: str,\n                     metadata_path: str) -&gt; tfx.dsl.Pipeline:\n  \"\"\"Creates a pipeline using predefined schema with TFX.\"\"\"\n  # Brings data into the pipeline.\n  example_gen = tfx.components.CsvExampleGen(input_base=data_root)\n\n  # Computes statistics over data for visualization and example validation.\n  statistics_gen = tfx.components.StatisticsGen(\n      examples=example_gen.outputs['examples'])\n\n  # NEW: Import the schema.\n  schema_importer = tfx.dsl.Importer(\n      source_uri=schema_path,\n      artifact_type=tfx.types.standard_artifacts.Schema).with_id(\n          'schema_importer')\n\n  # NEW: Performs anomaly detection based on statistics and data schema.\n  example_validator = tfx.components.ExampleValidator(\n      statistics=statistics_gen.outputs['statistics'],\n      schema=schema_importer.outputs['result'])\n\n  # Uses user-provided Python function that trains a model.\n  trainer = tfx.components.Trainer(\n      module_file=module_file,\n      examples=example_gen.outputs['examples'],\n      schema=schema_importer.outputs['result'],  # Pass the imported schema.\n      train_args=tfx.proto.TrainArgs(num_steps=100),\n      eval_args=tfx.proto.EvalArgs(num_steps=5))\n\n  # Pushes the model to a filesystem destination.\n  pusher = tfx.components.Pusher(\n      model=trainer.outputs['model'],\n      push_destination=tfx.proto.PushDestination(\n          filesystem=tfx.proto.PushDestination.Filesystem(\n              base_directory=serving_model_dir)))\n\n  components = [\n      example_gen,\n\n      # NEW: Following three components were added to the pipeline.\n      statistics_gen,\n      schema_importer,\n      example_validator,\n\n      trainer,\n      pusher,\n  ]\n\n  return tfx.dsl.Pipeline(\n      pipeline_name=pipeline_name,\n      pipeline_root=pipeline_root,\n      metadata_connection_config=tfx.orchestration.metadata\n      .sqlite_metadata_connection_config(metadata_path),\n      components=components)\n</pre> def _create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str,                      schema_path: str, module_file: str, serving_model_dir: str,                      metadata_path: str) -&gt; tfx.dsl.Pipeline:   \"\"\"Creates a pipeline using predefined schema with TFX.\"\"\"   # Brings data into the pipeline.   example_gen = tfx.components.CsvExampleGen(input_base=data_root)    # Computes statistics over data for visualization and example validation.   statistics_gen = tfx.components.StatisticsGen(       examples=example_gen.outputs['examples'])    # NEW: Import the schema.   schema_importer = tfx.dsl.Importer(       source_uri=schema_path,       artifact_type=tfx.types.standard_artifacts.Schema).with_id(           'schema_importer')    # NEW: Performs anomaly detection based on statistics and data schema.   example_validator = tfx.components.ExampleValidator(       statistics=statistics_gen.outputs['statistics'],       schema=schema_importer.outputs['result'])    # Uses user-provided Python function that trains a model.   trainer = tfx.components.Trainer(       module_file=module_file,       examples=example_gen.outputs['examples'],       schema=schema_importer.outputs['result'],  # Pass the imported schema.       train_args=tfx.proto.TrainArgs(num_steps=100),       eval_args=tfx.proto.EvalArgs(num_steps=5))    # Pushes the model to a filesystem destination.   pusher = tfx.components.Pusher(       model=trainer.outputs['model'],       push_destination=tfx.proto.PushDestination(           filesystem=tfx.proto.PushDestination.Filesystem(               base_directory=serving_model_dir)))    components = [       example_gen,        # NEW: Following three components were added to the pipeline.       statistics_gen,       schema_importer,       example_validator,        trainer,       pusher,   ]    return tfx.dsl.Pipeline(       pipeline_name=pipeline_name,       pipeline_root=pipeline_root,       metadata_connection_config=tfx.orchestration.metadata       .sqlite_metadata_connection_config(metadata_path),       components=components) In\u00a0[\u00a0]: Copied! <pre>tfx.orchestration.LocalDagRunner().run(\n  _create_pipeline(\n      pipeline_name=PIPELINE_NAME,\n      pipeline_root=PIPELINE_ROOT,\n      data_root=DATA_ROOT,\n      schema_path=SCHEMA_PATH,\n      module_file=_trainer_module_file,\n      serving_model_dir=SERVING_MODEL_DIR,\n      metadata_path=METADATA_PATH))\n</pre> tfx.orchestration.LocalDagRunner().run(   _create_pipeline(       pipeline_name=PIPELINE_NAME,       pipeline_root=PIPELINE_ROOT,       data_root=DATA_ROOT,       schema_path=SCHEMA_PATH,       module_file=_trainer_module_file,       serving_model_dir=SERVING_MODEL_DIR,       metadata_path=METADATA_PATH)) <p>You should see \"INFO:absl:Component Pusher is finished.\" if the pipeline finished successfully.</p> In\u00a0[\u00a0]: Copied! <pre>metadata_connection_config = tfx.orchestration.metadata.sqlite_metadata_connection_config(\n    METADATA_PATH)\n\nwith Metadata(metadata_connection_config) as metadata_handler:\n  ev_output = get_latest_artifacts(metadata_handler, PIPELINE_NAME,\n                                   'ExampleValidator')\n  anomalies_artifacts = ev_output[standard_component_specs.ANOMALIES_KEY]\n</pre> metadata_connection_config = tfx.orchestration.metadata.sqlite_metadata_connection_config(     METADATA_PATH)  with Metadata(metadata_connection_config) as metadata_handler:   ev_output = get_latest_artifacts(metadata_handler, PIPELINE_NAME,                                    'ExampleValidator')   anomalies_artifacts = ev_output[standard_component_specs.ANOMALIES_KEY] <p>ExampleAnomalies from the ExampleValidator can be visualized as well.</p> In\u00a0[\u00a0]: Copied! <pre>visualize_artifacts(anomalies_artifacts)\n</pre> visualize_artifacts(anomalies_artifacts) <p>You should see \"No anomalies found\" for each split of examples. Because we used the same data which was used for the schema generation in this pipeline, no anomaly is expected here. If you run this pipeline repeatedly with new incoming data, ExampleValidator should be able to find any discrepancies between the new data and the existing schema.</p> <p>If any anomalies were found, you may review your data to check to see if any examples do not follow your assumptions. Outputs from other components like StatisticsGen might be useful. However, any anomalies which are found will NOT block further pipeline executions.</p>"},{"location":"tutorials/tfx/penguin_tfdv/#copyright-2021-the-tensorflow-authors","title":"Copyright 2021 The TensorFlow Authors.\u00b6","text":""},{"location":"tutorials/tfx/penguin_tfdv/#data-validation-using-tfx-pipeline-and-tensorflow-data-validation","title":"Data validation using TFX Pipeline and TensorFlow Data Validation\u00b6","text":""},{"location":"tutorials/tfx/penguin_tfdv/#set-up","title":"Set Up\u00b6","text":"<p>We first need to install the TFX Python package and download the dataset which we will use for our model.</p>"},{"location":"tutorials/tfx/penguin_tfdv/#upgrade-pip","title":"Upgrade Pip\u00b6","text":"<p>To avoid upgrading Pip in a system when running locally, check to make sure that we are running in Colab. Local systems can of course be upgraded separately.</p>"},{"location":"tutorials/tfx/penguin_tfdv/#install-tfx","title":"Install TFX\u00b6","text":""},{"location":"tutorials/tfx/penguin_tfdv/#did-you-restart-the-runtime","title":"Did you restart the runtime?\u00b6","text":"<p>If you are using Google Colab, the first time that you run the cell above, you must restart the runtime by clicking above \"RESTART RUNTIME\" button or using \"Runtime &gt; Restart runtime ...\" menu. This is because of the way that Colab loads packages.</p>"},{"location":"tutorials/tfx/penguin_tfdv/#set-up-variables","title":"Set up variables\u00b6","text":"<p>There are some variables used to define a pipeline. You can customize these variables as you want. By default all output from the pipeline will be generated under the current directory.</p>"},{"location":"tutorials/tfx/penguin_tfdv/#prepare-example-data","title":"Prepare example data\u00b6","text":"<p>We will download the example dataset for use in our TFX pipeline. The dataset we are using is Palmer Penguins dataset which is also used in other TFX examples.</p> <p>There are four numeric features in this dataset:</p> <ul> <li>culmen_length_mm</li> <li>culmen_depth_mm</li> <li>flipper_length_mm</li> <li>body_mass_g</li> </ul> <p>All features were already normalized to have range [0,1]. We will build a classification model which predicts the <code>species</code> of penguins.</p>"},{"location":"tutorials/tfx/penguin_tfdv/#generate-a-preliminary-schema","title":"Generate a preliminary schema\u00b6","text":"<p>TFX pipelines are defined using Python APIs. We will create a pipeline to generate a schema from the input examples automatically. This schema can be reviewed by a human and adjusted as needed. Once the schema is finalized it can be used for training and example validation in later tasks.</p> <p>In addition to <code>CsvExampleGen</code> which is used in Simple TFX Pipeline Tutorial, we will use <code>StatisticsGen</code> and <code>SchemaGen</code>:</p> <ul> <li>StatisticsGen calculates statistics for the dataset.</li> <li>SchemaGen examines the statistics and creates an initial data schema.</li> </ul> <p>See the guides for each component or TFX components tutorial to learn more on these components.</p>"},{"location":"tutorials/tfx/penguin_tfdv/#write-a-pipeline-definition","title":"Write a pipeline definition\u00b6","text":"<p>We define a function to create a TFX pipeline. A <code>Pipeline</code> object represents a TFX pipeline which can be run using one of pipeline orchestration systems that TFX supports.</p>"},{"location":"tutorials/tfx/penguin_tfdv/#run-the-pipeline","title":"Run the pipeline\u00b6","text":"<p>We will use <code>LocalDagRunner</code> as in the previous tutorial.</p>"},{"location":"tutorials/tfx/penguin_tfdv/#review-outputs-of-the-pipeline","title":"Review outputs of the pipeline\u00b6","text":""},{"location":"tutorials/tfx/penguin_tfdv/#examine-the-output-from-statisticsgen","title":"Examine the output from StatisticsGen\u00b6","text":""},{"location":"tutorials/tfx/penguin_tfdv/#examine-the-output-from-schemagen","title":"Examine the output from SchemaGen\u00b6","text":""},{"location":"tutorials/tfx/penguin_tfdv/#export-the-schema-for-future-use","title":"Export the schema for future use\u00b6","text":"<p>We need to review and refine the generated schema. The reviewed schema needs to be persisted to be used in subsequent pipelines for ML model training. In other words, you might want to add the schema file to your version control system for actual use cases. In this tutorial, we will just copy the schema to a predefined filesystem path for simplicity.</p>"},{"location":"tutorials/tfx/penguin_tfdv/#validate-input-examples-and-train-an-ml-model","title":"Validate input examples and train an ML model\u00b6","text":"<p>We will go back to the pipeline that we created in Simple TFX Pipeline Tutorial, to train an ML model and use the generated schema for writing the model training code.</p> <p>We will also add an ExampleValidator component which will look for anomalies and missing values in the incoming dataset with respect to the schema.</p>"},{"location":"tutorials/tfx/penguin_tfdv/#write-model-training-code","title":"Write model training code\u00b6","text":"<p>We need to write the model code as we did in Simple TFX Pipeline Tutorial.</p> <p>The model itself is the same as in the previous tutorial, but this time we will use the schema generated from the previous pipeline instead of specifying features manually. Most of the code was not changed. The only difference is that we do not need to specify the names and types of features in this file. Instead, we read them from the schema file.</p>"},{"location":"tutorials/tfx/penguin_tfdv/#write-a-pipeline-definition","title":"Write a pipeline definition\u00b6","text":"<p>We will add two new components, <code>Importer</code> and <code>ExampleValidator</code>. Importer brings an external file into the TFX pipeline. In this case, it is a file containing schema definition. ExampleValidator will examine the input data and validate whether all input data conforms the data schema we provided.</p>"},{"location":"tutorials/tfx/penguin_tfdv/#run-the-pipeline","title":"Run the pipeline\u00b6","text":""},{"location":"tutorials/tfx/penguin_tfdv/#examine-outputs-of-the-pipeline","title":"Examine outputs of the pipeline\u00b6","text":"<p>We have trained the classification model for penguins, and we also have validated the input examples in the ExampleValidator component. We can analyze the output from ExampleValidator as we did with the previous pipeline.</p>"},{"location":"tutorials/tfx/penguin_tfdv/#next-steps","title":"Next steps\u00b6","text":"<p>You can find more resources on https://www.tensorflow.org/tfx/tutorials.</p> <p>Please see Understanding TFX Pipelines to learn more about various concepts in TFX.</p>"},{"location":"tutorials/tfx/penguin_tfma/","title":"Model analysis using TFX Pipeline and TensorFlow Model Analysis","text":"In\u00a0[\u00a0]: Copied! <pre>#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n</pre> #@title Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # https://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. <p>Note: We recommend running this tutorial in a Colab notebook, with no setup required!  Just click \"Run in Google Colab\".</p>        View on TensorFlow.org             Run in Google Colab             View source on GitHub             Download notebook      <p>In this notebook-based tutorial, we will create and run a TFX pipeline which creates a simple classification model and analyzes its performance across multiple runs. This notebook is based on the TFX pipeline we built in Simple TFX Pipeline Tutorial. If you have not read that tutorial yet, you should read it before proceeding with this notebook.</p> <p>As you tweak your model or train it with a new dataset, you need to check whether your model has improved or become worse. Just checking top-level metrics like accuracy might not be enough. Every trained model should be evaluated before it is pushed to production.</p> <p>We will add an <code>Evaluator</code> component to the pipeline created in the previous tutorial. The Evaluator component performs deep analysis for your models and compare the new model against a baseline to determine they are \"good enough\". It is implemented using the TensorFlow Model Analysis library.</p> <p>Please see Understanding TFX Pipelines to learn more about various concepts in TFX.</p> In\u00a0[\u00a0]: Copied! <pre>try:\n  import colab\n  !pip install --upgrade pip\nexcept:\n  pass\n</pre> try:   import colab   !pip install --upgrade pip except:   pass In\u00a0[\u00a0]: Copied! <pre>!pip install -U tfx\n</pre> !pip install -U tfx <p>Check the TensorFlow and TFX versions.</p> In\u00a0[\u00a0]: Copied! <pre>import tensorflow as tf\nprint('TensorFlow version: {}'.format(tf.__version__))\nfrom tfx import v1 as tfx\nprint('TFX version: {}'.format(tfx.__version__))\n</pre> import tensorflow as tf print('TensorFlow version: {}'.format(tf.__version__)) from tfx import v1 as tfx print('TFX version: {}'.format(tfx.__version__)) In\u00a0[\u00a0]: Copied! <pre>import os\n\nPIPELINE_NAME = \"penguin-tfma\"\n\n# Output directory to store artifacts generated from the pipeline.\nPIPELINE_ROOT = os.path.join('pipelines', PIPELINE_NAME)\n# Path to a SQLite DB file to use as an MLMD storage.\nMETADATA_PATH = os.path.join('metadata', PIPELINE_NAME, 'metadata.db')\n# Output directory where created models from the pipeline will be exported.\nSERVING_MODEL_DIR = os.path.join('serving_model', PIPELINE_NAME)\n\nfrom absl import logging\nlogging.set_verbosity(logging.INFO)  # Set default logging level.\n</pre> import os  PIPELINE_NAME = \"penguin-tfma\"  # Output directory to store artifacts generated from the pipeline. PIPELINE_ROOT = os.path.join('pipelines', PIPELINE_NAME) # Path to a SQLite DB file to use as an MLMD storage. METADATA_PATH = os.path.join('metadata', PIPELINE_NAME, 'metadata.db') # Output directory where created models from the pipeline will be exported. SERVING_MODEL_DIR = os.path.join('serving_model', PIPELINE_NAME)  from absl import logging logging.set_verbosity(logging.INFO)  # Set default logging level. <p>Because TFX ExampleGen reads inputs from a directory, we need to create a directory and copy dataset to it.</p> In\u00a0[\u00a0]: Copied! <pre>import urllib.request\nimport tempfile\n\nDATA_ROOT = tempfile.mkdtemp(prefix='tfx-data')  # Create a temporary directory.\n_data_url = 'https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/penguin/data/labelled/penguins_processed.csv'\n_data_filepath = os.path.join(DATA_ROOT, \"data.csv\")\nurllib.request.urlretrieve(_data_url, _data_filepath)\n</pre> import urllib.request import tempfile  DATA_ROOT = tempfile.mkdtemp(prefix='tfx-data')  # Create a temporary directory. _data_url = 'https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/penguin/data/labelled/penguins_processed.csv' _data_filepath = os.path.join(DATA_ROOT, \"data.csv\") urllib.request.urlretrieve(_data_url, _data_filepath) In\u00a0[\u00a0]: Copied! <pre>_trainer_module_file = 'penguin_trainer.py'\n</pre> _trainer_module_file = 'penguin_trainer.py' In\u00a0[\u00a0]: Copied! <pre>%%writefile {_trainer_module_file}\n\n# Copied from https://www.tensorflow.org/tfx/tutorials/tfx/penguin_simple\n\nfrom typing import List\nfrom absl import logging\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow_transform.tf_metadata import schema_utils\n\nfrom tfx.components.trainer.executor import TrainerFnArgs\nfrom tfx.components.trainer.fn_args_utils import DataAccessor\nfrom tfx_bsl.tfxio import dataset_options\nfrom tensorflow_metadata.proto.v0 import schema_pb2\n\n_FEATURE_KEYS = [\n    'culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g'\n]\n_LABEL_KEY = 'species'\n\n_TRAIN_BATCH_SIZE = 20\n_EVAL_BATCH_SIZE = 10\n\n# Since we're not generating or creating a schema, we will instead create\n# a feature spec.  Since there are a fairly small number of features this is\n# manageable for this dataset.\n_FEATURE_SPEC = {\n    **{\n        feature: tf.io.FixedLenFeature(shape=[1], dtype=tf.float32)\n           for feature in _FEATURE_KEYS\n       },\n    _LABEL_KEY: tf.io.FixedLenFeature(shape=[1], dtype=tf.int64)\n}\n\n\ndef _input_fn(file_pattern: List[str],\n              data_accessor: DataAccessor,\n              schema: schema_pb2.Schema,\n              batch_size: int = 200) -&gt; tf.data.Dataset:\n  \"\"\"Generates features and label for training.\n\n  Args:\n    file_pattern: List of paths or patterns of input tfrecord files.\n    data_accessor: DataAccessor for converting input to RecordBatch.\n    schema: schema of the input data.\n    batch_size: representing the number of consecutive elements of returned\n      dataset to combine in a single batch\n\n  Returns:\n    A dataset that contains (features, indices) tuple where features is a\n      dictionary of Tensors, and indices is a single Tensor of label indices.\n  \"\"\"\n  return data_accessor.tf_dataset_factory(\n      file_pattern,\n      dataset_options.TensorFlowDatasetOptions(\n          batch_size=batch_size, label_key=_LABEL_KEY),\n      schema=schema).repeat()\n\n\ndef _build_keras_model() -&gt; tf.keras.Model:\n  \"\"\"Creates a DNN Keras model for classifying penguin data.\n\n  Returns:\n    A Keras Model.\n  \"\"\"\n  # The model below is built with Functional API, please refer to\n  # https://www.tensorflow.org/guide/keras/overview for all API options.\n  inputs = [keras.layers.Input(shape=(1,), name=f) for f in _FEATURE_KEYS]\n  d = keras.layers.concatenate(inputs)\n  for _ in range(2):\n    d = keras.layers.Dense(8, activation='relu')(d)\n  outputs = keras.layers.Dense(3)(d)\n\n  model = keras.Model(inputs=inputs, outputs=outputs)\n  model.compile(\n      optimizer=keras.optimizers.Adam(1e-2),\n      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n      metrics=[keras.metrics.SparseCategoricalAccuracy()])\n\n  model.summary(print_fn=logging.info)\n  return model\n\n\n# TFX Trainer will call this function.\ndef run_fn(fn_args: TrainerFnArgs):\n  \"\"\"Train the model based on given args.\n\n  Args:\n    fn_args: Holds args used to train the model as name/value pairs.\n  \"\"\"\n\n  # This schema is usually either an output of SchemaGen or a manually-curated\n  # version provided by pipeline author. A schema can also derived from TFT\n  # graph if a Transform component is used. In the case when either is missing,\n  # `schema_from_feature_spec` could be used to generate schema from very simple\n  # feature_spec, but the schema returned would be very primitive.\n  schema = schema_utils.schema_from_feature_spec(_FEATURE_SPEC)\n\n  train_dataset = _input_fn(\n      fn_args.train_files,\n      fn_args.data_accessor,\n      schema,\n      batch_size=_TRAIN_BATCH_SIZE)\n  eval_dataset = _input_fn(\n      fn_args.eval_files,\n      fn_args.data_accessor,\n      schema,\n      batch_size=_EVAL_BATCH_SIZE)\n\n  model = _build_keras_model()\n  model.fit(\n      train_dataset,\n      steps_per_epoch=fn_args.train_steps,\n      validation_data=eval_dataset,\n      validation_steps=fn_args.eval_steps)\n\n  # The result of the training should be saved in `fn_args.serving_model_dir`\n  # directory.\n  model.save(fn_args.serving_model_dir, save_format='tf')\n</pre> %%writefile {_trainer_module_file}  # Copied from https://www.tensorflow.org/tfx/tutorials/tfx/penguin_simple  from typing import List from absl import logging import tensorflow as tf from tensorflow import keras from tensorflow_transform.tf_metadata import schema_utils  from tfx.components.trainer.executor import TrainerFnArgs from tfx.components.trainer.fn_args_utils import DataAccessor from tfx_bsl.tfxio import dataset_options from tensorflow_metadata.proto.v0 import schema_pb2  _FEATURE_KEYS = [     'culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g' ] _LABEL_KEY = 'species'  _TRAIN_BATCH_SIZE = 20 _EVAL_BATCH_SIZE = 10  # Since we're not generating or creating a schema, we will instead create # a feature spec.  Since there are a fairly small number of features this is # manageable for this dataset. _FEATURE_SPEC = {     **{         feature: tf.io.FixedLenFeature(shape=[1], dtype=tf.float32)            for feature in _FEATURE_KEYS        },     _LABEL_KEY: tf.io.FixedLenFeature(shape=[1], dtype=tf.int64) }   def _input_fn(file_pattern: List[str],               data_accessor: DataAccessor,               schema: schema_pb2.Schema,               batch_size: int = 200) -&gt; tf.data.Dataset:   \"\"\"Generates features and label for training.    Args:     file_pattern: List of paths or patterns of input tfrecord files.     data_accessor: DataAccessor for converting input to RecordBatch.     schema: schema of the input data.     batch_size: representing the number of consecutive elements of returned       dataset to combine in a single batch    Returns:     A dataset that contains (features, indices) tuple where features is a       dictionary of Tensors, and indices is a single Tensor of label indices.   \"\"\"   return data_accessor.tf_dataset_factory(       file_pattern,       dataset_options.TensorFlowDatasetOptions(           batch_size=batch_size, label_key=_LABEL_KEY),       schema=schema).repeat()   def _build_keras_model() -&gt; tf.keras.Model:   \"\"\"Creates a DNN Keras model for classifying penguin data.    Returns:     A Keras Model.   \"\"\"   # The model below is built with Functional API, please refer to   # https://www.tensorflow.org/guide/keras/overview for all API options.   inputs = [keras.layers.Input(shape=(1,), name=f) for f in _FEATURE_KEYS]   d = keras.layers.concatenate(inputs)   for _ in range(2):     d = keras.layers.Dense(8, activation='relu')(d)   outputs = keras.layers.Dense(3)(d)    model = keras.Model(inputs=inputs, outputs=outputs)   model.compile(       optimizer=keras.optimizers.Adam(1e-2),       loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),       metrics=[keras.metrics.SparseCategoricalAccuracy()])    model.summary(print_fn=logging.info)   return model   # TFX Trainer will call this function. def run_fn(fn_args: TrainerFnArgs):   \"\"\"Train the model based on given args.    Args:     fn_args: Holds args used to train the model as name/value pairs.   \"\"\"    # This schema is usually either an output of SchemaGen or a manually-curated   # version provided by pipeline author. A schema can also derived from TFT   # graph if a Transform component is used. In the case when either is missing,   # `schema_from_feature_spec` could be used to generate schema from very simple   # feature_spec, but the schema returned would be very primitive.   schema = schema_utils.schema_from_feature_spec(_FEATURE_SPEC)    train_dataset = _input_fn(       fn_args.train_files,       fn_args.data_accessor,       schema,       batch_size=_TRAIN_BATCH_SIZE)   eval_dataset = _input_fn(       fn_args.eval_files,       fn_args.data_accessor,       schema,       batch_size=_EVAL_BATCH_SIZE)    model = _build_keras_model()   model.fit(       train_dataset,       steps_per_epoch=fn_args.train_steps,       validation_data=eval_dataset,       validation_steps=fn_args.eval_steps)    # The result of the training should be saved in `fn_args.serving_model_dir`   # directory.   model.save(fn_args.serving_model_dir, save_format='tf') In\u00a0[\u00a0]: Copied! <pre>import tensorflow_model_analysis as tfma\n\ndef _create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str,\n                     module_file: str, serving_model_dir: str,\n                     metadata_path: str) -&gt; tfx.dsl.Pipeline:\n  \"\"\"Creates a three component penguin pipeline with TFX.\"\"\"\n  # Brings data into the pipeline.\n  example_gen = tfx.components.CsvExampleGen(input_base=data_root)\n\n  # Uses user-provided Python function that trains a model.\n  trainer = tfx.components.Trainer(\n      module_file=module_file,\n      examples=example_gen.outputs['examples'],\n      train_args=tfx.proto.TrainArgs(num_steps=100),\n      eval_args=tfx.proto.EvalArgs(num_steps=5))\n\n  # NEW: Get the latest blessed model for Evaluator.\n  model_resolver = tfx.dsl.Resolver(\n      strategy_class=tfx.dsl.experimental.LatestBlessedModelStrategy,\n      model=tfx.dsl.Channel(type=tfx.types.standard_artifacts.Model),\n      model_blessing=tfx.dsl.Channel(\n          type=tfx.types.standard_artifacts.ModelBlessing)).with_id(\n              'latest_blessed_model_resolver')\n\n  # NEW: Uses TFMA to compute evaluation statistics over features of a model and\n  #   perform quality validation of a candidate model (compared to a baseline).\n\n  eval_config = tfma.EvalConfig(\n      model_specs=[tfma.ModelSpec(label_key='species')],\n      slicing_specs=[\n          # An empty slice spec means the overall slice, i.e. the whole dataset.\n          tfma.SlicingSpec(),\n          # Calculate metrics for each penguin species.\n          tfma.SlicingSpec(feature_keys=['species']),\n          ],\n      metrics_specs=[\n          tfma.MetricsSpec(per_slice_thresholds={\n              'sparse_categorical_accuracy':\n                  tfma.PerSliceMetricThresholds(thresholds=[\n                      tfma.PerSliceMetricThreshold(\n                          slicing_specs=[tfma.SlicingSpec()],\n                          threshold=tfma.MetricThreshold(\n                              value_threshold=tfma.GenericValueThreshold(\n                                   lower_bound={'value': 0.6}),\n                              # Change threshold will be ignored if there is no\n                              # baseline model resolved from MLMD (first run).\n                              change_threshold=tfma.GenericChangeThreshold(\n                                  direction=tfma.MetricDirection.HIGHER_IS_BETTER,\n                                  absolute={'value': -1e-10}))\n                       )]),\n          })],\n      )\n  evaluator = tfx.components.Evaluator(\n      examples=example_gen.outputs['examples'],\n      model=trainer.outputs['model'],\n      baseline_model=model_resolver.outputs['model'],\n      eval_config=eval_config)\n\n  # Checks whether the model passed the validation steps and pushes the model\n  # to a file destination if check passed.\n  pusher = tfx.components.Pusher(\n      model=trainer.outputs['model'],\n      model_blessing=evaluator.outputs['blessing'], # Pass an evaluation result.\n      push_destination=tfx.proto.PushDestination(\n          filesystem=tfx.proto.PushDestination.Filesystem(\n              base_directory=serving_model_dir)))\n\n  components = [\n      example_gen,\n      trainer,\n\n      # Following two components were added to the pipeline.\n      model_resolver,\n      evaluator,\n\n      pusher,\n  ]\n\n  return tfx.dsl.Pipeline(\n      pipeline_name=pipeline_name,\n      pipeline_root=pipeline_root,\n      metadata_connection_config=tfx.orchestration.metadata\n      .sqlite_metadata_connection_config(metadata_path),\n      components=components)\n</pre> import tensorflow_model_analysis as tfma  def _create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str,                      module_file: str, serving_model_dir: str,                      metadata_path: str) -&gt; tfx.dsl.Pipeline:   \"\"\"Creates a three component penguin pipeline with TFX.\"\"\"   # Brings data into the pipeline.   example_gen = tfx.components.CsvExampleGen(input_base=data_root)    # Uses user-provided Python function that trains a model.   trainer = tfx.components.Trainer(       module_file=module_file,       examples=example_gen.outputs['examples'],       train_args=tfx.proto.TrainArgs(num_steps=100),       eval_args=tfx.proto.EvalArgs(num_steps=5))    # NEW: Get the latest blessed model for Evaluator.   model_resolver = tfx.dsl.Resolver(       strategy_class=tfx.dsl.experimental.LatestBlessedModelStrategy,       model=tfx.dsl.Channel(type=tfx.types.standard_artifacts.Model),       model_blessing=tfx.dsl.Channel(           type=tfx.types.standard_artifacts.ModelBlessing)).with_id(               'latest_blessed_model_resolver')    # NEW: Uses TFMA to compute evaluation statistics over features of a model and   #   perform quality validation of a candidate model (compared to a baseline).    eval_config = tfma.EvalConfig(       model_specs=[tfma.ModelSpec(label_key='species')],       slicing_specs=[           # An empty slice spec means the overall slice, i.e. the whole dataset.           tfma.SlicingSpec(),           # Calculate metrics for each penguin species.           tfma.SlicingSpec(feature_keys=['species']),           ],       metrics_specs=[           tfma.MetricsSpec(per_slice_thresholds={               'sparse_categorical_accuracy':                   tfma.PerSliceMetricThresholds(thresholds=[                       tfma.PerSliceMetricThreshold(                           slicing_specs=[tfma.SlicingSpec()],                           threshold=tfma.MetricThreshold(                               value_threshold=tfma.GenericValueThreshold(                                    lower_bound={'value': 0.6}),                               # Change threshold will be ignored if there is no                               # baseline model resolved from MLMD (first run).                               change_threshold=tfma.GenericChangeThreshold(                                   direction=tfma.MetricDirection.HIGHER_IS_BETTER,                                   absolute={'value': -1e-10}))                        )]),           })],       )   evaluator = tfx.components.Evaluator(       examples=example_gen.outputs['examples'],       model=trainer.outputs['model'],       baseline_model=model_resolver.outputs['model'],       eval_config=eval_config)    # Checks whether the model passed the validation steps and pushes the model   # to a file destination if check passed.   pusher = tfx.components.Pusher(       model=trainer.outputs['model'],       model_blessing=evaluator.outputs['blessing'], # Pass an evaluation result.       push_destination=tfx.proto.PushDestination(           filesystem=tfx.proto.PushDestination.Filesystem(               base_directory=serving_model_dir)))    components = [       example_gen,       trainer,        # Following two components were added to the pipeline.       model_resolver,       evaluator,        pusher,   ]    return tfx.dsl.Pipeline(       pipeline_name=pipeline_name,       pipeline_root=pipeline_root,       metadata_connection_config=tfx.orchestration.metadata       .sqlite_metadata_connection_config(metadata_path),       components=components) <p>We need to supply the following information to the Evaluator via <code>eval_config</code>:</p> <ul> <li>Additional metrics to configure (if want more metrics than defined in model).</li> <li>Slices to configure</li> <li>Model validations thresholds to verify if validation to be included</li> </ul> <p>Because <code>SparseCategoricalAccuracy</code> was already included in the <code>model.compile()</code> call, it will be included in the analysis automatically. So we do not add any additional metrics here. <code>SparseCategoricalAccuracy</code> will be used to decide whether the model is good enough, too.</p> <p>We compute the metrics for the whole dataset and for each penguin species. <code>SlicingSpec</code> specifies how we aggregate the declared metrics.</p> <p>There are two thresholds that a new model should pass, one is an absolute threshold of 0.6 and the other is a relative threshold that it should be higher than the baseline model. When you run the pipeline for the first time, the <code>change_threshold</code> will be ignored and only the value_threshold will be checked. If you run the pipeline more than once, the <code>Resolver</code> will find a model from the previous run and it will be used as a baseline model for the comparison.</p> <p>See Evaluator component guide for more information.</p> <p>We will use <code>LocalDagRunner</code> as in the previous tutorial.</p> In\u00a0[\u00a0]: Copied! <pre>tfx.orchestration.LocalDagRunner().run(\n  _create_pipeline(\n      pipeline_name=PIPELINE_NAME,\n      pipeline_root=PIPELINE_ROOT,\n      data_root=DATA_ROOT,\n      module_file=_trainer_module_file,\n      serving_model_dir=SERVING_MODEL_DIR,\n      metadata_path=METADATA_PATH))\n</pre> tfx.orchestration.LocalDagRunner().run(   _create_pipeline(       pipeline_name=PIPELINE_NAME,       pipeline_root=PIPELINE_ROOT,       data_root=DATA_ROOT,       module_file=_trainer_module_file,       serving_model_dir=SERVING_MODEL_DIR,       metadata_path=METADATA_PATH)) <p>When the pipeline completed, you should be able to see something like following:</p> <pre><code>INFO:absl:Blessing result True written to pipelines/penguin-tfma/Evaluator/blessing/4.\n</code></pre> <p>Or you can also check manually the output directory where the generated artifacts are stored. If you visit <code>pipelines/penguin-tfma/Evaluator/blessing/</code> with a file browser, you can see a file with a name <code>BLESSED</code> or <code>NOT_BLESSED</code> according to the evaluation result.</p> <p>If the blessing result is <code>False</code>, Pusher will refuse to push the model to the <code>serving_model_dir</code>, because the model is not good enough to be used in production.</p> <p>You can run the pipeline again possibly with different evaluation configs. Even if you run the pipeline with the exact same config and dataset, the trained model might be slightly different due to the inherent randomness of the model training which can lead to a <code>NOT_BLESSED</code> model.</p> <p>NOTE: If you are not on Colab, Install Jupyter Extensions. You need an TensorFlow Model Analysis extension to see the visualization from TFMA. This extension is already installed on Google Colab, but you might need to install it if you are running this notebook on other environments. See installation direction of Jupyter extension in the Install guide.</p> In\u00a0[\u00a0]: Copied! <pre>from ml_metadata.proto import metadata_store_pb2\n# Non-public APIs, just for showcase.\nfrom tfx.orchestration.portable.mlmd import execution_lib\n\n# TODO(b/171447278): Move these functions into the TFX library.\n\ndef get_latest_artifacts(metadata, pipeline_name, component_id):\n  \"\"\"Output artifacts of the latest run of the component.\"\"\"\n  context = metadata.store.get_context_by_type_and_name(\n      'node', f'{pipeline_name}.{component_id}')\n  executions = metadata.store.get_executions_by_context(context.id)\n  latest_execution = max(executions,\n                         key=lambda e:e.last_update_time_since_epoch)\n  return execution_lib.get_output_artifacts(metadata, latest_execution.id)\n</pre> from ml_metadata.proto import metadata_store_pb2 # Non-public APIs, just for showcase. from tfx.orchestration.portable.mlmd import execution_lib  # TODO(b/171447278): Move these functions into the TFX library.  def get_latest_artifacts(metadata, pipeline_name, component_id):   \"\"\"Output artifacts of the latest run of the component.\"\"\"   context = metadata.store.get_context_by_type_and_name(       'node', f'{pipeline_name}.{component_id}')   executions = metadata.store.get_executions_by_context(context.id)   latest_execution = max(executions,                          key=lambda e:e.last_update_time_since_epoch)   return execution_lib.get_output_artifacts(metadata, latest_execution.id)  <p>We can find the latest execution of the <code>Evaluator</code> component and get output artifacts of it.</p> In\u00a0[\u00a0]: Copied! <pre># Non-public APIs, just for showcase.\nfrom tfx.orchestration.metadata import Metadata\nfrom tfx.types import standard_component_specs\n\nmetadata_connection_config = tfx.orchestration.metadata.sqlite_metadata_connection_config(\n    METADATA_PATH)\n\nwith Metadata(metadata_connection_config) as metadata_handler:\n  # Find output artifacts from MLMD.\n  evaluator_output = get_latest_artifacts(metadata_handler, PIPELINE_NAME,\n                                          'Evaluator')\n  eval_artifact = evaluator_output[standard_component_specs.EVALUATION_KEY][0]\n</pre> # Non-public APIs, just for showcase. from tfx.orchestration.metadata import Metadata from tfx.types import standard_component_specs  metadata_connection_config = tfx.orchestration.metadata.sqlite_metadata_connection_config(     METADATA_PATH)  with Metadata(metadata_connection_config) as metadata_handler:   # Find output artifacts from MLMD.   evaluator_output = get_latest_artifacts(metadata_handler, PIPELINE_NAME,                                           'Evaluator')   eval_artifact = evaluator_output[standard_component_specs.EVALUATION_KEY][0] <p><code>Evaluator</code> always returns one evaluation artifact, and we can visualize it using TensorFlow Model Analysis library. For example, following code will render the accuracy metrics for each penguin species.</p> In\u00a0[\u00a0]: Copied! <pre>import tensorflow_model_analysis as tfma\n\neval_result = tfma.load_eval_result(eval_artifact.uri)\ntfma.view.render_slicing_metrics(eval_result, slicing_column='species')\n</pre> import tensorflow_model_analysis as tfma  eval_result = tfma.load_eval_result(eval_artifact.uri) tfma.view.render_slicing_metrics(eval_result, slicing_column='species') <p>If you choose 'sparse_categorical_accuracy' in <code>Show</code> drop-down list, you can see the accuracy values per species. You might want to add more slices and check whether your model is good for all distribution and if there is any possible bias.</p>"},{"location":"tutorials/tfx/penguin_tfma/#copyright-2021-the-tensorflow-authors","title":"Copyright 2021 The TensorFlow Authors.\u00b6","text":""},{"location":"tutorials/tfx/penguin_tfma/#model-analysis-using-tfx-pipeline-and-tensorflow-model-analysis","title":"Model analysis using TFX Pipeline and TensorFlow Model Analysis\u00b6","text":""},{"location":"tutorials/tfx/penguin_tfma/#set-up","title":"Set Up\u00b6","text":"<p>The Set up process is the same as the previous tutorial.</p> <p>We first need to install the TFX Python package and download the dataset which we will use for our model.</p>"},{"location":"tutorials/tfx/penguin_tfma/#upgrade-pip","title":"Upgrade Pip\u00b6","text":"<p>To avoid upgrading Pip in a system when running locally, check to make sure that we are running in Colab. Local systems can of course be upgraded separately.</p>"},{"location":"tutorials/tfx/penguin_tfma/#install-tfx","title":"Install TFX\u00b6","text":""},{"location":"tutorials/tfx/penguin_tfma/#did-you-restart-the-runtime","title":"Did you restart the runtime?\u00b6","text":"<p>If you are using Google Colab, the first time that you run the cell above, you must restart the runtime by clicking above \"RESTART RUNTIME\" button or using \"Runtime &gt; Restart runtime ...\" menu. This is because of the way that Colab loads packages.</p>"},{"location":"tutorials/tfx/penguin_tfma/#set-up-variables","title":"Set up variables\u00b6","text":"<p>There are some variables used to define a pipeline. You can customize these variables as you want. By default all output from the pipeline will be generated under the current directory.</p>"},{"location":"tutorials/tfx/penguin_tfma/#prepare-example-data","title":"Prepare example data\u00b6","text":"<p>We will use the same Palmer Penguins dataset.</p> <p>There are four numeric features in this dataset which were already normalized to have range [0,1]. We will build a classification model which predicts the <code>species</code> of penguins.</p>"},{"location":"tutorials/tfx/penguin_tfma/#create-a-pipeline","title":"Create a pipeline\u00b6","text":"<p>We will add an <code>Evaluator</code> component to the pipeline we created in the Simple TFX Pipeline Tutorial.</p> <p>An Evaluator component requires input data from an <code>ExampleGen</code> component and a model from a <code>Trainer</code> component and a <code>tfma.EvalConfig</code> object. We can optionally supply a baseline model which can be used to compare metrics with the newly trained model.</p> <p>An evaluator creates two kinds of output artifacts, <code>ModelEvaluation</code> and <code>ModelBlessing</code>. ModelEvaluation contains the detailed evaluation result which can be investigated and visualized further with TFMA library. ModelBlessing contains a boolean result whether the model passed given criteria and can be used in later components like a Pusher as a signal.</p>"},{"location":"tutorials/tfx/penguin_tfma/#write-model-training-code","title":"Write model training code\u00b6","text":"<p>We will use the same model code as in the Simple TFX Pipeline Tutorial.</p>"},{"location":"tutorials/tfx/penguin_tfma/#write-a-pipeline-definition","title":"Write a pipeline definition\u00b6","text":"<p>We will define a function to create a TFX pipeline. In addition to the Evaluator component we mentioned above, we will add one more node called <code>Resolver</code>. To check a new model is getting better than previous model, we need to compare it against a previous published model, called baseline. ML Metadata(MLMD) tracks all previous artifacts of the pipeline and <code>Resolver</code> can find what was the latest blessed model -- a model passed Evaluator successfully -- from MLMD using a strategy class called <code>LatestBlessedModelStrategy</code>.</p>"},{"location":"tutorials/tfx/penguin_tfma/#run-the-pipeline","title":"Run the pipeline\u00b6","text":""},{"location":"tutorials/tfx/penguin_tfma/#examine-outputs-of-the-pipeline","title":"Examine outputs of the pipeline\u00b6","text":"<p>You can use TFMA to investigate and visualize the evaluation result in ModelEvaluation artifact.</p>"},{"location":"tutorials/tfx/penguin_tfma/#get-analysis-result-from-output-artifacts","title":"Get analysis result from output artifacts\u00b6","text":"<p>You can use MLMD APIs to locate these outputs programmatically. First, we will define some utility functions to search for the output artifacts that were just produced.</p>"},{"location":"tutorials/tfx/penguin_tfma/#next-steps","title":"Next steps\u00b6","text":"<p>Learn more on model analysis at TensorFlow Model Analysis library tutorial.</p> <p>You can find more resources on https://www.tensorflow.org/tfx/tutorials.</p> <p>Please see Understanding TFX Pipelines to learn more about various concepts in TFX.</p>"},{"location":"tutorials/tfx/penguin_tft/","title":"Feature Engineering using TFX Pipeline and TensorFlow Transform","text":"In\u00a0[\u00a0]: Copied! <pre>#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n</pre> #@title Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # https://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. <p>Note: We recommend running this tutorial in a Colab notebook, with no setup required!  Just click \"Run in Google Colab\".</p>        View on TensorFlow.org             Run in Google Colab             View source on GitHub             Download notebook      <p>In this notebook-based tutorial, we will create and run a TFX pipeline to ingest raw input data and preprocess it appropriately for ML training. This notebook is based on the TFX pipeline we built in Data validation using TFX Pipeline and TensorFlow Data Validation Tutorial. If you have not read that one yet, you should read it before proceeding with this notebook.</p> <p>You can increase the predictive quality of your data and/or reduce dimensionality with feature engineering. One of the benefits of using TFX is that you will write your transformation code once, and the resulting transforms will be consistent between training and serving in order to avoid training/serving skew.</p> <p>We will add a <code>Transform</code> component to the pipeline. The Transform component is implemented using the tf.transform library.</p> <p>Please see Understanding TFX Pipelines to learn more about various concepts in TFX.</p> In\u00a0[\u00a0]: Copied! <pre>try:\n  import colab\n  !pip install --upgrade pip\nexcept:\n  pass\n</pre> try:   import colab   !pip install --upgrade pip except:   pass In\u00a0[\u00a0]: Copied! <pre>!pip install -U tfx\n</pre> !pip install -U tfx <p>Check the TensorFlow and TFX versions.</p> In\u00a0[\u00a0]: Copied! <pre>import tensorflow as tf\nprint('TensorFlow version: {}'.format(tf.__version__))\nfrom tfx import v1 as tfx\nprint('TFX version: {}'.format(tfx.__version__))\n</pre> import tensorflow as tf print('TensorFlow version: {}'.format(tf.__version__)) from tfx import v1 as tfx print('TFX version: {}'.format(tfx.__version__)) In\u00a0[\u00a0]: Copied! <pre>import os\n\nPIPELINE_NAME = \"penguin-transform\"\n\n# Output directory to store artifacts generated from the pipeline.\nPIPELINE_ROOT = os.path.join('pipelines', PIPELINE_NAME)\n# Path to a SQLite DB file to use as an MLMD storage.\nMETADATA_PATH = os.path.join('metadata', PIPELINE_NAME, 'metadata.db')\n# Output directory where created models from the pipeline will be exported.\nSERVING_MODEL_DIR = os.path.join('serving_model', PIPELINE_NAME)\n\nfrom absl import logging\nlogging.set_verbosity(logging.INFO)  # Set default logging level.\n</pre> import os  PIPELINE_NAME = \"penguin-transform\"  # Output directory to store artifacts generated from the pipeline. PIPELINE_ROOT = os.path.join('pipelines', PIPELINE_NAME) # Path to a SQLite DB file to use as an MLMD storage. METADATA_PATH = os.path.join('metadata', PIPELINE_NAME, 'metadata.db') # Output directory where created models from the pipeline will be exported. SERVING_MODEL_DIR = os.path.join('serving_model', PIPELINE_NAME)  from absl import logging logging.set_verbosity(logging.INFO)  # Set default logging level. <p>Because the TFX ExampleGen component reads inputs from a directory, we need to create a directory and copy the dataset to it.</p> In\u00a0[\u00a0]: Copied! <pre>import urllib.request\nimport tempfile\n\nDATA_ROOT = tempfile.mkdtemp(prefix='tfx-data')  # Create a temporary directory.\n_data_path = 'https://storage.googleapis.com/download.tensorflow.org/data/palmer_penguins/penguins_size.csv'\n_data_filepath = os.path.join(DATA_ROOT, \"data.csv\")\nurllib.request.urlretrieve(_data_path, _data_filepath)\n</pre> import urllib.request import tempfile  DATA_ROOT = tempfile.mkdtemp(prefix='tfx-data')  # Create a temporary directory. _data_path = 'https://storage.googleapis.com/download.tensorflow.org/data/palmer_penguins/penguins_size.csv' _data_filepath = os.path.join(DATA_ROOT, \"data.csv\") urllib.request.urlretrieve(_data_path, _data_filepath) <p>Take a quick look at what the raw data looks like.</p> In\u00a0[\u00a0]: Copied! <pre>!head {_data_filepath}\n</pre> !head {_data_filepath} <p>There are some entries with missing values which are represented as <code>NA</code>. We will just delete those entries in this tutorial.</p> In\u00a0[\u00a0]: Copied! <pre>!sed -i '/\\bNA\\b/d' {_data_filepath}\n!head {_data_filepath}\n</pre> !sed -i '/\\bNA\\b/d' {_data_filepath} !head {_data_filepath} <p>You should be able to see seven features which describe penguins. We will use the same set of features as the previous tutorials - 'culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g' - and will predict the 'species' of a penguin.</p> <p>The only difference will be that the input data is not preprocessed. Note that we will not use other features like 'island' or 'sex' in this tutorial.</p> In\u00a0[\u00a0]: Copied! <pre>import shutil\n\nSCHEMA_PATH = 'schema'\n\n_schema_uri = 'https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/penguin/schema/raw/schema.pbtxt'\n_schema_filename = 'schema.pbtxt'\n_schema_filepath = os.path.join(SCHEMA_PATH, _schema_filename)\n\nos.makedirs(SCHEMA_PATH, exist_ok=True)\nurllib.request.urlretrieve(_schema_uri, _schema_filepath)\n</pre> import shutil  SCHEMA_PATH = 'schema'  _schema_uri = 'https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/penguin/schema/raw/schema.pbtxt' _schema_filename = 'schema.pbtxt' _schema_filepath = os.path.join(SCHEMA_PATH, _schema_filename)  os.makedirs(SCHEMA_PATH, exist_ok=True) urllib.request.urlretrieve(_schema_uri, _schema_filepath) <p>This schema file was created with the same pipeline as in the previous tutorial without any manual changes.</p> In\u00a0[\u00a0]: Copied! <pre>_module_file = 'penguin_utils.py'\n</pre> _module_file = 'penguin_utils.py' In\u00a0[\u00a0]: Copied! <pre>%%writefile {_module_file}\n\n\nfrom typing import List, Text\nfrom absl import logging\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow_metadata.proto.v0 import schema_pb2\nimport tensorflow_transform as tft\nfrom tensorflow_transform.tf_metadata import schema_utils\n\nfrom tfx import v1 as tfx\nfrom tfx_bsl.public import tfxio\n\n# Specify features that we will use.\n_FEATURE_KEYS = [\n    'culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g'\n]\n_LABEL_KEY = 'species'\n\n_TRAIN_BATCH_SIZE = 20\n_EVAL_BATCH_SIZE = 10\n\n\n# NEW: TFX Transform will call this function.\ndef preprocessing_fn(inputs):\n  \"\"\"tf.transform's callback function for preprocessing inputs.\n\n  Args:\n    inputs: map from feature keys to raw not-yet-transformed features.\n\n  Returns:\n    Map from string feature key to transformed feature.\n  \"\"\"\n  outputs = {}\n\n  # Uses features defined in _FEATURE_KEYS only.\n  for key in _FEATURE_KEYS:\n    # tft.scale_to_z_score computes the mean and variance of the given feature\n    # and scales the output based on the result.\n    outputs[key] = tft.scale_to_z_score(inputs[key])\n\n  # For the label column we provide the mapping from string to index.\n  # We could instead use `tft.compute_and_apply_vocabulary()` in order to\n  # compute the vocabulary dynamically and perform a lookup.\n  # Since in this example there are only 3 possible values, we use a hard-coded\n  # table for simplicity.\n  table_keys = ['Adelie', 'Chinstrap', 'Gentoo']\n  initializer = tf.lookup.KeyValueTensorInitializer(\n      keys=table_keys,\n      values=tf.cast(tf.range(len(table_keys)), tf.int64),\n      key_dtype=tf.string,\n      value_dtype=tf.int64)\n  table = tf.lookup.StaticHashTable(initializer, default_value=-1)\n  outputs[_LABEL_KEY] = table.lookup(inputs[_LABEL_KEY])\n\n  return outputs\n\n\n# NEW: This function will apply the same transform operation to training data\n#      and serving requests.\ndef _apply_preprocessing(raw_features, tft_layer):\n  transformed_features = tft_layer(raw_features)\n  if _LABEL_KEY in raw_features:\n    transformed_label = transformed_features.pop(_LABEL_KEY)\n    return transformed_features, transformed_label\n  else:\n    return transformed_features, None\n\n\n# NEW: This function will create a handler function which gets a serialized\n#      tf.example, preprocess and run an inference with it.\ndef _get_serve_tf_examples_fn(model, tf_transform_output):\n  # We must save the tft_layer to the model to ensure its assets are kept and\n  # tracked.\n  model.tft_layer = tf_transform_output.transform_features_layer()\n\n  @tf.function(input_signature=[\n      tf.TensorSpec(shape=[None], dtype=tf.string, name='examples')\n  ])\n  def serve_tf_examples_fn(serialized_tf_examples):\n    # Expected input is a string which is serialized tf.Example format.\n    feature_spec = tf_transform_output.raw_feature_spec()\n    # Because input schema includes unnecessary fields like 'species' and\n    # 'island', we filter feature_spec to include required keys only.\n    required_feature_spec = {\n        k: v for k, v in feature_spec.items() if k in _FEATURE_KEYS\n    }\n    parsed_features = tf.io.parse_example(serialized_tf_examples,\n                                          required_feature_spec)\n\n    # Preprocess parsed input with transform operation defined in\n    # preprocessing_fn().\n    transformed_features, _ = _apply_preprocessing(parsed_features,\n                                                   model.tft_layer)\n    # Run inference with ML model.\n    return model(transformed_features)\n\n  return serve_tf_examples_fn\n\n\ndef _input_fn(file_pattern: List[Text],\n              data_accessor: tfx.components.DataAccessor,\n              tf_transform_output: tft.TFTransformOutput,\n              batch_size: int = 200) -&gt; tf.data.Dataset:\n  \"\"\"Generates features and label for tuning/training.\n\n  Args:\n    file_pattern: List of paths or patterns of input tfrecord files.\n    data_accessor: DataAccessor for converting input to RecordBatch.\n    tf_transform_output: A TFTransformOutput.\n    batch_size: representing the number of consecutive elements of returned\n      dataset to combine in a single batch\n\n  Returns:\n    A dataset that contains (features, indices) tuple where features is a\n      dictionary of Tensors, and indices is a single Tensor of label indices.\n  \"\"\"\n  dataset = data_accessor.tf_dataset_factory(\n      file_pattern,\n      tfxio.TensorFlowDatasetOptions(batch_size=batch_size),\n      schema=tf_transform_output.raw_metadata.schema)\n\n  transform_layer = tf_transform_output.transform_features_layer()\n  def apply_transform(raw_features):\n    return _apply_preprocessing(raw_features, transform_layer)\n\n  return dataset.map(apply_transform).repeat()\n\n\ndef _build_keras_model() -&gt; tf.keras.Model:\n  \"\"\"Creates a DNN Keras model for classifying penguin data.\n\n  Returns:\n    A Keras Model.\n  \"\"\"\n  # The model below is built with Functional API, please refer to\n  # https://www.tensorflow.org/guide/keras/overview for all API options.\n  inputs = [\n      keras.layers.Input(shape=(1,), name=key)\n      for key in _FEATURE_KEYS\n  ]\n  d = keras.layers.concatenate(inputs)\n  for _ in range(2):\n    d = keras.layers.Dense(8, activation='relu')(d)\n  outputs = keras.layers.Dense(3)(d)\n\n  model = keras.Model(inputs=inputs, outputs=outputs)\n  model.compile(\n      optimizer=keras.optimizers.Adam(1e-2),\n      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n      metrics=[keras.metrics.SparseCategoricalAccuracy()])\n\n  model.summary(print_fn=logging.info)\n  return model\n\n\n# TFX Trainer will call this function.\ndef run_fn(fn_args: tfx.components.FnArgs):\n  \"\"\"Train the model based on given args.\n\n  Args:\n    fn_args: Holds args used to train the model as name/value pairs.\n  \"\"\"\n  tf_transform_output = tft.TFTransformOutput(fn_args.transform_output)\n\n  train_dataset = _input_fn(\n      fn_args.train_files,\n      fn_args.data_accessor,\n      tf_transform_output,\n      batch_size=_TRAIN_BATCH_SIZE)\n  eval_dataset = _input_fn(\n      fn_args.eval_files,\n      fn_args.data_accessor,\n      tf_transform_output,\n      batch_size=_EVAL_BATCH_SIZE)\n\n  model = _build_keras_model()\n  model.fit(\n      train_dataset,\n      steps_per_epoch=fn_args.train_steps,\n      validation_data=eval_dataset,\n      validation_steps=fn_args.eval_steps)\n\n  # NEW: Save a computation graph including transform layer.\n  signatures = {\n      'serving_default': _get_serve_tf_examples_fn(model, tf_transform_output),\n  }\n  model.save(fn_args.serving_model_dir, save_format='tf', signatures=signatures)\n</pre> %%writefile {_module_file}   from typing import List, Text from absl import logging import tensorflow as tf from tensorflow import keras from tensorflow_metadata.proto.v0 import schema_pb2 import tensorflow_transform as tft from tensorflow_transform.tf_metadata import schema_utils  from tfx import v1 as tfx from tfx_bsl.public import tfxio  # Specify features that we will use. _FEATURE_KEYS = [     'culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g' ] _LABEL_KEY = 'species'  _TRAIN_BATCH_SIZE = 20 _EVAL_BATCH_SIZE = 10   # NEW: TFX Transform will call this function. def preprocessing_fn(inputs):   \"\"\"tf.transform's callback function for preprocessing inputs.    Args:     inputs: map from feature keys to raw not-yet-transformed features.    Returns:     Map from string feature key to transformed feature.   \"\"\"   outputs = {}    # Uses features defined in _FEATURE_KEYS only.   for key in _FEATURE_KEYS:     # tft.scale_to_z_score computes the mean and variance of the given feature     # and scales the output based on the result.     outputs[key] = tft.scale_to_z_score(inputs[key])    # For the label column we provide the mapping from string to index.   # We could instead use `tft.compute_and_apply_vocabulary()` in order to   # compute the vocabulary dynamically and perform a lookup.   # Since in this example there are only 3 possible values, we use a hard-coded   # table for simplicity.   table_keys = ['Adelie', 'Chinstrap', 'Gentoo']   initializer = tf.lookup.KeyValueTensorInitializer(       keys=table_keys,       values=tf.cast(tf.range(len(table_keys)), tf.int64),       key_dtype=tf.string,       value_dtype=tf.int64)   table = tf.lookup.StaticHashTable(initializer, default_value=-1)   outputs[_LABEL_KEY] = table.lookup(inputs[_LABEL_KEY])    return outputs   # NEW: This function will apply the same transform operation to training data #      and serving requests. def _apply_preprocessing(raw_features, tft_layer):   transformed_features = tft_layer(raw_features)   if _LABEL_KEY in raw_features:     transformed_label = transformed_features.pop(_LABEL_KEY)     return transformed_features, transformed_label   else:     return transformed_features, None   # NEW: This function will create a handler function which gets a serialized #      tf.example, preprocess and run an inference with it. def _get_serve_tf_examples_fn(model, tf_transform_output):   # We must save the tft_layer to the model to ensure its assets are kept and   # tracked.   model.tft_layer = tf_transform_output.transform_features_layer()    @tf.function(input_signature=[       tf.TensorSpec(shape=[None], dtype=tf.string, name='examples')   ])   def serve_tf_examples_fn(serialized_tf_examples):     # Expected input is a string which is serialized tf.Example format.     feature_spec = tf_transform_output.raw_feature_spec()     # Because input schema includes unnecessary fields like 'species' and     # 'island', we filter feature_spec to include required keys only.     required_feature_spec = {         k: v for k, v in feature_spec.items() if k in _FEATURE_KEYS     }     parsed_features = tf.io.parse_example(serialized_tf_examples,                                           required_feature_spec)      # Preprocess parsed input with transform operation defined in     # preprocessing_fn().     transformed_features, _ = _apply_preprocessing(parsed_features,                                                    model.tft_layer)     # Run inference with ML model.     return model(transformed_features)    return serve_tf_examples_fn   def _input_fn(file_pattern: List[Text],               data_accessor: tfx.components.DataAccessor,               tf_transform_output: tft.TFTransformOutput,               batch_size: int = 200) -&gt; tf.data.Dataset:   \"\"\"Generates features and label for tuning/training.    Args:     file_pattern: List of paths or patterns of input tfrecord files.     data_accessor: DataAccessor for converting input to RecordBatch.     tf_transform_output: A TFTransformOutput.     batch_size: representing the number of consecutive elements of returned       dataset to combine in a single batch    Returns:     A dataset that contains (features, indices) tuple where features is a       dictionary of Tensors, and indices is a single Tensor of label indices.   \"\"\"   dataset = data_accessor.tf_dataset_factory(       file_pattern,       tfxio.TensorFlowDatasetOptions(batch_size=batch_size),       schema=tf_transform_output.raw_metadata.schema)    transform_layer = tf_transform_output.transform_features_layer()   def apply_transform(raw_features):     return _apply_preprocessing(raw_features, transform_layer)    return dataset.map(apply_transform).repeat()   def _build_keras_model() -&gt; tf.keras.Model:   \"\"\"Creates a DNN Keras model for classifying penguin data.    Returns:     A Keras Model.   \"\"\"   # The model below is built with Functional API, please refer to   # https://www.tensorflow.org/guide/keras/overview for all API options.   inputs = [       keras.layers.Input(shape=(1,), name=key)       for key in _FEATURE_KEYS   ]   d = keras.layers.concatenate(inputs)   for _ in range(2):     d = keras.layers.Dense(8, activation='relu')(d)   outputs = keras.layers.Dense(3)(d)    model = keras.Model(inputs=inputs, outputs=outputs)   model.compile(       optimizer=keras.optimizers.Adam(1e-2),       loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),       metrics=[keras.metrics.SparseCategoricalAccuracy()])    model.summary(print_fn=logging.info)   return model   # TFX Trainer will call this function. def run_fn(fn_args: tfx.components.FnArgs):   \"\"\"Train the model based on given args.    Args:     fn_args: Holds args used to train the model as name/value pairs.   \"\"\"   tf_transform_output = tft.TFTransformOutput(fn_args.transform_output)    train_dataset = _input_fn(       fn_args.train_files,       fn_args.data_accessor,       tf_transform_output,       batch_size=_TRAIN_BATCH_SIZE)   eval_dataset = _input_fn(       fn_args.eval_files,       fn_args.data_accessor,       tf_transform_output,       batch_size=_EVAL_BATCH_SIZE)    model = _build_keras_model()   model.fit(       train_dataset,       steps_per_epoch=fn_args.train_steps,       validation_data=eval_dataset,       validation_steps=fn_args.eval_steps)    # NEW: Save a computation graph including transform layer.   signatures = {       'serving_default': _get_serve_tf_examples_fn(model, tf_transform_output),   }   model.save(fn_args.serving_model_dir, save_format='tf', signatures=signatures) <p>Now you have completed all of the preparation steps to build a TFX pipeline.</p> In\u00a0[\u00a0]: Copied! <pre>def _create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str,\n                     schema_path: str, module_file: str, serving_model_dir: str,\n                     metadata_path: str) -&gt; tfx.dsl.Pipeline:\n  \"\"\"Implements the penguin pipeline with TFX.\"\"\"\n  # Brings data into the pipeline or otherwise joins/converts training data.\n  example_gen = tfx.components.CsvExampleGen(input_base=data_root)\n\n  # Computes statistics over data for visualization and example validation.\n  statistics_gen = tfx.components.StatisticsGen(\n      examples=example_gen.outputs['examples'])\n\n  # Import the schema.\n  schema_importer = tfx.dsl.Importer(\n      source_uri=schema_path,\n      artifact_type=tfx.types.standard_artifacts.Schema).with_id(\n          'schema_importer')\n\n  # Performs anomaly detection based on statistics and data schema.\n  example_validator = tfx.components.ExampleValidator(\n      statistics=statistics_gen.outputs['statistics'],\n      schema=schema_importer.outputs['result'])\n\n  # NEW: Transforms input data using preprocessing_fn in the 'module_file'.\n  transform = tfx.components.Transform(\n      examples=example_gen.outputs['examples'],\n      schema=schema_importer.outputs['result'],\n      materialize=False,\n      module_file=module_file)\n\n  # Uses user-provided Python function that trains a model.\n  trainer = tfx.components.Trainer(\n      module_file=module_file,\n      examples=example_gen.outputs['examples'],\n\n      # NEW: Pass transform_graph to the trainer.\n      transform_graph=transform.outputs['transform_graph'],\n\n      train_args=tfx.proto.TrainArgs(num_steps=100),\n      eval_args=tfx.proto.EvalArgs(num_steps=5))\n\n  # Pushes the model to a filesystem destination.\n  pusher = tfx.components.Pusher(\n      model=trainer.outputs['model'],\n      push_destination=tfx.proto.PushDestination(\n          filesystem=tfx.proto.PushDestination.Filesystem(\n              base_directory=serving_model_dir)))\n\n  components = [\n      example_gen,\n      statistics_gen,\n      schema_importer,\n      example_validator,\n\n      transform,  # NEW: Transform component was added to the pipeline.\n\n      trainer,\n      pusher,\n  ]\n\n  return tfx.dsl.Pipeline(\n      pipeline_name=pipeline_name,\n      pipeline_root=pipeline_root,\n      metadata_connection_config=tfx.orchestration.metadata\n      .sqlite_metadata_connection_config(metadata_path),\n      components=components)\n</pre> def _create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str,                      schema_path: str, module_file: str, serving_model_dir: str,                      metadata_path: str) -&gt; tfx.dsl.Pipeline:   \"\"\"Implements the penguin pipeline with TFX.\"\"\"   # Brings data into the pipeline or otherwise joins/converts training data.   example_gen = tfx.components.CsvExampleGen(input_base=data_root)    # Computes statistics over data for visualization and example validation.   statistics_gen = tfx.components.StatisticsGen(       examples=example_gen.outputs['examples'])    # Import the schema.   schema_importer = tfx.dsl.Importer(       source_uri=schema_path,       artifact_type=tfx.types.standard_artifacts.Schema).with_id(           'schema_importer')    # Performs anomaly detection based on statistics and data schema.   example_validator = tfx.components.ExampleValidator(       statistics=statistics_gen.outputs['statistics'],       schema=schema_importer.outputs['result'])    # NEW: Transforms input data using preprocessing_fn in the 'module_file'.   transform = tfx.components.Transform(       examples=example_gen.outputs['examples'],       schema=schema_importer.outputs['result'],       materialize=False,       module_file=module_file)    # Uses user-provided Python function that trains a model.   trainer = tfx.components.Trainer(       module_file=module_file,       examples=example_gen.outputs['examples'],        # NEW: Pass transform_graph to the trainer.       transform_graph=transform.outputs['transform_graph'],        train_args=tfx.proto.TrainArgs(num_steps=100),       eval_args=tfx.proto.EvalArgs(num_steps=5))    # Pushes the model to a filesystem destination.   pusher = tfx.components.Pusher(       model=trainer.outputs['model'],       push_destination=tfx.proto.PushDestination(           filesystem=tfx.proto.PushDestination.Filesystem(               base_directory=serving_model_dir)))    components = [       example_gen,       statistics_gen,       schema_importer,       example_validator,        transform,  # NEW: Transform component was added to the pipeline.        trainer,       pusher,   ]    return tfx.dsl.Pipeline(       pipeline_name=pipeline_name,       pipeline_root=pipeline_root,       metadata_connection_config=tfx.orchestration.metadata       .sqlite_metadata_connection_config(metadata_path),       components=components) In\u00a0[\u00a0]: Copied! <pre>tfx.orchestration.LocalDagRunner().run(\n  _create_pipeline(\n      pipeline_name=PIPELINE_NAME,\n      pipeline_root=PIPELINE_ROOT,\n      data_root=DATA_ROOT,\n      schema_path=SCHEMA_PATH,\n      module_file=_module_file,\n      serving_model_dir=SERVING_MODEL_DIR,\n      metadata_path=METADATA_PATH))\n</pre> tfx.orchestration.LocalDagRunner().run(   _create_pipeline(       pipeline_name=PIPELINE_NAME,       pipeline_root=PIPELINE_ROOT,       data_root=DATA_ROOT,       schema_path=SCHEMA_PATH,       module_file=_module_file,       serving_model_dir=SERVING_MODEL_DIR,       metadata_path=METADATA_PATH)) <p>You should see \"INFO:absl:Component Pusher is finished.\" if the pipeline finished successfully.</p> <p>The pusher component pushes the trained model to the <code>SERVING_MODEL_DIR</code> which is the <code>serving_model/penguin-transform</code> directory if you did not change the variables in the previous steps. You can see the result from the file browser in the left-side panel in Colab, or using the following command:</p> In\u00a0[\u00a0]: Copied! <pre># List files in created model directory.\n!find {SERVING_MODEL_DIR}\n</pre> # List files in created model directory. !find {SERVING_MODEL_DIR} <p>You can also check the signature of the generated model using the <code>saved_model_cli</code> tool.</p> In\u00a0[\u00a0]: Copied! <pre>!saved_model_cli show --dir {SERVING_MODEL_DIR}/$(ls -1 {SERVING_MODEL_DIR} | sort -nr | head -1) --tag_set serve --signature_def serving_default\n</pre> !saved_model_cli show --dir {SERVING_MODEL_DIR}/$(ls -1 {SERVING_MODEL_DIR} | sort -nr | head -1) --tag_set serve --signature_def serving_default <p>Because we defined <code>serving_default</code> with our own <code>serve_tf_examples_fn</code> function, the signature shows that it takes a single string. This string is a serialized string of tf.Examples and will be parsed with the tf.io.parse_example() function as we defined earlier (learn more about tf.Examples here).</p> <p>We can load the exported model and try some inferences with a few examples.</p> In\u00a0[\u00a0]: Copied! <pre># Find a model with the latest timestamp.\nmodel_dirs = (item for item in os.scandir(SERVING_MODEL_DIR) if item.is_dir())\nmodel_path = max(model_dirs, key=lambda i: int(i.name)).path\n\nloaded_model = tf.keras.models.load_model(model_path)\ninference_fn = loaded_model.signatures['serving_default']\n</pre> # Find a model with the latest timestamp. model_dirs = (item for item in os.scandir(SERVING_MODEL_DIR) if item.is_dir()) model_path = max(model_dirs, key=lambda i: int(i.name)).path  loaded_model = tf.keras.models.load_model(model_path) inference_fn = loaded_model.signatures['serving_default'] In\u00a0[\u00a0]: Copied! <pre># Prepare an example and run inference.\nfeatures = {\n  'culmen_length_mm': tf.train.Feature(float_list=tf.train.FloatList(value=[49.9])),\n  'culmen_depth_mm': tf.train.Feature(float_list=tf.train.FloatList(value=[16.1])),\n  'flipper_length_mm': tf.train.Feature(int64_list=tf.train.Int64List(value=[213])),\n  'body_mass_g': tf.train.Feature(int64_list=tf.train.Int64List(value=[5400])),\n}\nexample_proto = tf.train.Example(features=tf.train.Features(feature=features))\nexamples = example_proto.SerializeToString()\n\nresult = inference_fn(examples=tf.constant([examples]))\nprint(result['output_0'].numpy())\n</pre> # Prepare an example and run inference. features = {   'culmen_length_mm': tf.train.Feature(float_list=tf.train.FloatList(value=[49.9])),   'culmen_depth_mm': tf.train.Feature(float_list=tf.train.FloatList(value=[16.1])),   'flipper_length_mm': tf.train.Feature(int64_list=tf.train.Int64List(value=[213])),   'body_mass_g': tf.train.Feature(int64_list=tf.train.Int64List(value=[5400])), } example_proto = tf.train.Example(features=tf.train.Features(feature=features)) examples = example_proto.SerializeToString()  result = inference_fn(examples=tf.constant([examples])) print(result['output_0'].numpy()) <p>The third element, which corresponds to 'Gentoo' species, is expected to be the largest among three.</p>"},{"location":"tutorials/tfx/penguin_tft/#copyright-2021-the-tensorflow-authors","title":"Copyright 2021 The TensorFlow Authors.\u00b6","text":""},{"location":"tutorials/tfx/penguin_tft/#feature-engineering-using-tfx-pipeline-and-tensorflow-transform","title":"Feature Engineering using TFX Pipeline and TensorFlow Transform\u00b6","text":"<p>Transform input data and train a model with a TFX pipeline.</p>"},{"location":"tutorials/tfx/penguin_tft/#set-up","title":"Set Up\u00b6","text":"<p>We first need to install the TFX Python package and download the dataset which we will use for our model.</p>"},{"location":"tutorials/tfx/penguin_tft/#upgrade-pip","title":"Upgrade Pip\u00b6","text":"<p>To avoid upgrading Pip in a system when running locally, check to make sure that we are running in Colab. Local systems can of course be upgraded separately.</p>"},{"location":"tutorials/tfx/penguin_tft/#install-tfx","title":"Install TFX\u00b6","text":""},{"location":"tutorials/tfx/penguin_tft/#did-you-restart-the-runtime","title":"Did you restart the runtime?\u00b6","text":"<p>If you are using Google Colab, the first time that you run the cell above, you must restart the runtime by clicking above \"RESTART RUNTIME\" button or using \"Runtime &gt; Restart runtime ...\" menu. This is because of the way that Colab loads packages.</p>"},{"location":"tutorials/tfx/penguin_tft/#set-up-variables","title":"Set up variables\u00b6","text":"<p>There are some variables used to define a pipeline. You can customize these variables as you want. By default all output from the pipeline will be generated under the current directory.</p>"},{"location":"tutorials/tfx/penguin_tft/#prepare-example-data","title":"Prepare example data\u00b6","text":"<p>We will download the example dataset for use in our TFX pipeline. The dataset we are using is Palmer Penguins dataset.</p> <p>However, unlike previous tutorials which used an already preprocessed dataset, we will use the raw Palmer Penguins dataset.</p>"},{"location":"tutorials/tfx/penguin_tft/#prepare-a-schema-file","title":"Prepare a schema file\u00b6","text":"<p>As described in Data validation using TFX Pipeline and TensorFlow Data Validation Tutorial, we need a schema file for the dataset. Because the dataset is different from the previous tutorial we need to generate it again. In this tutorial, we will skip those steps and just use a prepared schema file.</p>"},{"location":"tutorials/tfx/penguin_tft/#create-a-pipeline","title":"Create a pipeline\u00b6","text":"<p>TFX pipelines are defined using Python APIs. We will add <code>Transform</code> component to the pipeline we created in the Data Validation tutorial.</p> <p>A Transform component requires input data from an <code>ExampleGen</code> component and a schema from a <code>SchemaGen</code> component, and produces a \"transform graph\". The output will be used in a <code>Trainer</code> component. Transform can optionally produce \"transformed data\" in addition, which is the materialized data after transformation. However, we will transform data during training in this tutorial without materialization of the intermediate transformed data.</p> <p>One thing to note is that we need to define a Python function, <code>preprocessing_fn</code> to describe how input data should be transformed. This is similar to a Trainer component which also requires user code for model definition.</p>"},{"location":"tutorials/tfx/penguin_tft/#write-preprocessing-and-training-code","title":"Write preprocessing and training code\u00b6","text":"<p>We need to define two Python functions. One for Transform and one for Trainer.</p>"},{"location":"tutorials/tfx/penguin_tft/#preprocessing_fn","title":"preprocessing_fn\u00b6","text":"<p>The Transform component will find a function named <code>preprocessing_fn</code> in the given module file as we did for <code>Trainer</code> component. You can also specify a specific function using the <code>preprocessing_fn</code> parameter of the Transform component.</p> <p>In this example, we will do two kinds of transformation. For continuous numeric features like <code>culmen_length_mm</code> and <code>body_mass_g</code>, we will normalize these values using the tft.scale_to_z_score function. For the label feature, we need to convert string labels into numeric index values. We will use <code>tf.lookup.StaticHashTable</code> for conversion.</p>"},{"location":"tutorials/tfx/penguin_tft/#run_fn","title":"run_fn\u00b6","text":"<p>The model itself is almost the same as in the previous tutorials, but this time we will transform the input data using the transform graph from the Transform component.</p> <p>One more important difference compared to the previous tutorial is that now we export a model for serving which includes not only the computation graph of the model, but also the transform graph for preprocessing, which is generated in Transform component. We need to define a separate function which will be used for serving incoming requests. You can see that the same function <code>_apply_preprocessing</code> was used for both of the training data and the serving request.</p>"},{"location":"tutorials/tfx/penguin_tft/#write-a-pipeline-definition","title":"Write a pipeline definition\u00b6","text":"<p>We define a function to create a TFX pipeline. A <code>Pipeline</code> object represents a TFX pipeline, which can be run using one of the pipeline orchestration systems that TFX supports.</p>"},{"location":"tutorials/tfx/penguin_tft/#run-the-pipeline","title":"Run the pipeline\u00b6","text":"<p>We will use <code>LocalDagRunner</code> as in the previous tutorial.</p>"},{"location":"tutorials/tfx/penguin_tft/#next-steps","title":"Next steps\u00b6","text":"<p>If you want to learn more about Transform component, see Transform Component guide. You can find more resources on https://www.tensorflow.org/tfx/tutorials.</p> <p>Please see Understanding TFX Pipelines to learn more about various concepts in TFX.</p>"},{"location":"tutorials/tfx/python_function_component/","title":"TFX Python function component tutorial","text":"In\u00a0[\u00a0]: Copied! <pre>#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n</pre> #@title Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # https://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. <p>Note: We recommend running this tutorial in a Colab notebook, with no setup required!  Just click \"Run in Google Colab\".</p>        View on TensorFlow.org             Run in Google Colab             View source on GitHub             Download notebook      <p>This notebook contains an examples on how to author and run Python function components within the TFX InteractiveContext and in a locally-orchestrated TFX pipeline.</p> <p>For more context and information, see the Custom Python function components page on the TFX documentation site.</p> In\u00a0[\u00a0]: Copied! <pre>import sys\nsys.version\n</pre> import sys sys.version In\u00a0[\u00a0]: Copied! <pre>try:\n  import colab\n  !pip install --upgrade pip\nexcept:\n  pass\n</pre> try:   import colab   !pip install --upgrade pip except:   pass In\u00a0[\u00a0]: Copied! <pre>!pip install tfx\n</pre> !pip install tfx In\u00a0[\u00a0]: Copied! <pre># Check version\nfrom tfx import v1 as tfx\ntfx.__version__\n</pre> # Check version from tfx import v1 as tfx tfx.__version__ In\u00a0[\u00a0]: Copied! <pre>%%writefile my_generator.py\n\nimport os\nimport tensorflow as tf  # Used for writing files.\n\nfrom tfx import v1 as tfx\n\n# Non-public APIs, just for showcase.\nfrom tfx.types.experimental.simple_artifacts import Dataset\n\n@tfx.dsl.components.component\ndef MyGenerator(data: tfx.dsl.components.OutputArtifact[Dataset]):\n  \"\"\"Create a file with dummy data in the output artifact.\"\"\"\n  with tf.io.gfile.GFile(os.path.join(data.uri, 'data_file.txt'), 'w') as f:\n    f.write('Dummy data')\n\n  # Set metadata and ensure that it gets passed to downstream components.\n  data.set_string_custom_property('my_custom_field', 'my_custom_value')\n</pre> %%writefile my_generator.py  import os import tensorflow as tf  # Used for writing files.  from tfx import v1 as tfx  # Non-public APIs, just for showcase. from tfx.types.experimental.simple_artifacts import Dataset  @tfx.dsl.components.component def MyGenerator(data: tfx.dsl.components.OutputArtifact[Dataset]):   \"\"\"Create a file with dummy data in the output artifact.\"\"\"   with tf.io.gfile.GFile(os.path.join(data.uri, 'data_file.txt'), 'w') as f:     f.write('Dummy data')    # Set metadata and ensure that it gets passed to downstream components.   data.set_string_custom_property('my_custom_field', 'my_custom_value') <p>Next, we write a second component that uses the dummy data produced. We will just calculate hash of the data and return it.</p> In\u00a0[\u00a0]: Copied! <pre>%%writefile my_consumer.py\n\nimport hashlib\nimport os\nimport tensorflow as tf\n\nfrom tfx import v1 as tfx\n\n# Non-public APIs, just for showcase.\nfrom tfx.types.experimental.simple_artifacts import Dataset\nfrom tfx.types.standard_artifacts import String\n\n@tfx.dsl.components.component\ndef MyConsumer(data: tfx.dsl.components.InputArtifact[Dataset],\n               hash: tfx.dsl.components.OutputArtifact[String],\n               algorithm: tfx.dsl.components.Parameter[str] = 'sha256'):\n  \"\"\"Reads the contents of data and calculate.\"\"\"\n  with tf.io.gfile.GFile(\n      os.path.join(data.uri, 'data_file.txt'), 'r') as f:\n    contents = f.read()\n  h = hashlib.new(algorithm)\n  h.update(tf.compat.as_bytes(contents))\n  hash.value = h.hexdigest()\n\n  # Read a custom property from the input artifact and set to the output.\n  custom_value = data.get_string_custom_property('my_custom_field')\n  hash.set_string_custom_property('input_custom_field', custom_value)\n</pre> %%writefile my_consumer.py  import hashlib import os import tensorflow as tf  from tfx import v1 as tfx  # Non-public APIs, just for showcase. from tfx.types.experimental.simple_artifacts import Dataset from tfx.types.standard_artifacts import String  @tfx.dsl.components.component def MyConsumer(data: tfx.dsl.components.InputArtifact[Dataset],                hash: tfx.dsl.components.OutputArtifact[String],                algorithm: tfx.dsl.components.Parameter[str] = 'sha256'):   \"\"\"Reads the contents of data and calculate.\"\"\"   with tf.io.gfile.GFile(       os.path.join(data.uri, 'data_file.txt'), 'r') as f:     contents = f.read()   h = hashlib.new(algorithm)   h.update(tf.compat.as_bytes(contents))   hash.value = h.hexdigest()    # Read a custom property from the input artifact and set to the output.   custom_value = data.get_string_custom_property('my_custom_field')   hash.set_string_custom_property('input_custom_field', custom_value) In\u00a0[\u00a0]: Copied! <pre>from my_generator import MyGenerator\nfrom my_consumer import MyConsumer\n</pre> from my_generator import MyGenerator from my_consumer import MyConsumer In\u00a0[\u00a0]: Copied! <pre># Here, we create an InteractiveContext using default parameters. This will\n# use a temporary directory with an ephemeral ML Metadata database instance.\n# To use your own pipeline root or database, the optional properties\n# `pipeline_root` and `metadata_connection_config` may be passed to\n# InteractiveContext. Calls to InteractiveContext are no-ops outside of the\n# notebook.\nfrom tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\ncontext = InteractiveContext()\n</pre> # Here, we create an InteractiveContext using default parameters. This will # use a temporary directory with an ephemeral ML Metadata database instance. # To use your own pipeline root or database, the optional properties # `pipeline_root` and `metadata_connection_config` may be passed to # InteractiveContext. Calls to InteractiveContext are no-ops outside of the # notebook. from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext context = InteractiveContext() In\u00a0[\u00a0]: Copied! <pre>generator = MyGenerator()\ncontext.run(generator)\n</pre> generator = MyGenerator() context.run(generator) In\u00a0[\u00a0]: Copied! <pre>consumer = MyConsumer(\n    data=generator.outputs['data'],\n    algorithm='md5')\ncontext.run(consumer)\n</pre> consumer = MyConsumer(     data=generator.outputs['data'],     algorithm='md5') context.run(consumer) <p>After execution, we can inspect the contents of the \"hash\" output artifact of the consumer component on disk.</p> In\u00a0[\u00a0]: Copied! <pre>!tail -v {consumer.outputs['hash'].get()[0].uri}\n</pre> !tail -v {consumer.outputs['hash'].get()[0].uri} <p>That's it, and you've now written and executed your own custom components!</p> In\u00a0[\u00a0]: Copied! <pre>import os\nimport tempfile\nfrom tfx import v1 as tfx\n\n# Select a persistent TFX root directory to store your output artifacts.\n# For demonstration purposes only, we use a temporary directory.\nPIPELINE_ROOT = tempfile.mkdtemp()\n# Select a pipeline name so that multiple runs of the same logical pipeline\n# can be grouped.\nPIPELINE_NAME = \"function-based-pipeline\"\n# We use a ML Metadata configuration that uses a local SQLite database in\n# the pipeline root directory. Other backends for ML Metadata are available\n# for production usage.\nMETADATA_CONNECTION_CONFIG = tfx.orchestration.metadata.sqlite_metadata_connection_config(\n    os.path.join(PIPELINE_ROOT, 'metadata.sqlite'))\n\ndef function_based_pipeline():\n  # Here, we construct our generator and consumer components in the same way.\n  generator = MyGenerator()\n  consumer = MyConsumer(\n      data=generator.outputs['data'],\n      algorithm='md5')\n\n  return tfx.dsl.Pipeline(\n      pipeline_name=PIPELINE_NAME,\n      pipeline_root=PIPELINE_ROOT,\n      components=[generator, consumer],\n      metadata_connection_config=METADATA_CONNECTION_CONFIG)\n\nmy_pipeline = function_based_pipeline()\n</pre> import os import tempfile from tfx import v1 as tfx  # Select a persistent TFX root directory to store your output artifacts. # For demonstration purposes only, we use a temporary directory. PIPELINE_ROOT = tempfile.mkdtemp() # Select a pipeline name so that multiple runs of the same logical pipeline # can be grouped. PIPELINE_NAME = \"function-based-pipeline\" # We use a ML Metadata configuration that uses a local SQLite database in # the pipeline root directory. Other backends for ML Metadata are available # for production usage. METADATA_CONNECTION_CONFIG = tfx.orchestration.metadata.sqlite_metadata_connection_config(     os.path.join(PIPELINE_ROOT, 'metadata.sqlite'))  def function_based_pipeline():   # Here, we construct our generator and consumer components in the same way.   generator = MyGenerator()   consumer = MyConsumer(       data=generator.outputs['data'],       algorithm='md5')    return tfx.dsl.Pipeline(       pipeline_name=PIPELINE_NAME,       pipeline_root=PIPELINE_ROOT,       components=[generator, consumer],       metadata_connection_config=METADATA_CONNECTION_CONFIG)  my_pipeline = function_based_pipeline() In\u00a0[\u00a0]: Copied! <pre>tfx.orchestration.LocalDagRunner().run(my_pipeline)\n</pre> tfx.orchestration.LocalDagRunner().run(my_pipeline) <p>We can inspect the output artifacts generated by this pipeline execution.</p> In\u00a0[\u00a0]: Copied! <pre>!find {PIPELINE_ROOT}\n</pre> !find {PIPELINE_ROOT} <p>You have now written your own custom components and orchestrated their execution on the LocalDagRunner! For next steps, check out additional tutorials and guides on the TFX website.</p>"},{"location":"tutorials/tfx/python_function_component/#copyright-2021-the-tensorflow-authors","title":"Copyright 2021 The TensorFlow Authors.\u00b6","text":""},{"location":"tutorials/tfx/python_function_component/#tfx-python-function-component-tutorial","title":"TFX Python function component tutorial\u00b6","text":""},{"location":"tutorials/tfx/python_function_component/#setup","title":"Setup\u00b6","text":"<p>We will first install TFX and import necessary modules. TFX requires Python 3.</p>"},{"location":"tutorials/tfx/python_function_component/#check-the-system-python-version","title":"Check the system Python version\u00b6","text":""},{"location":"tutorials/tfx/python_function_component/#upgrade-pip","title":"Upgrade Pip\u00b6","text":"<p>To avoid upgrading Pip in a system when running locally, check to make sure that we're running in Colab.  Local systems can of course be upgraded separately.</p>"},{"location":"tutorials/tfx/python_function_component/#install-tfx","title":"Install TFX\u00b6","text":"<p>Note: In Google Colab, because of package updates, the first time you run this cell you must restart the runtime (Runtime &gt; Restart runtime ...).</p>"},{"location":"tutorials/tfx/python_function_component/#did-you-restart-the-runtime","title":"Did you restart the runtime?\u00b6","text":"<p>If you are using Google Colab, the first time that you run the cell above, you must restart the runtime (Runtime &gt; Restart runtime ...). This is because of the way that Colab loads packages.</p>"},{"location":"tutorials/tfx/python_function_component/#import-packages","title":"Import packages\u00b6","text":"<p>We import TFX and check its version.</p>"},{"location":"tutorials/tfx/python_function_component/#custom-python-function-components","title":"Custom Python function components\u00b6","text":"<p>In this section, we will create components from Python functions. We will not be doing any real ML problem \u2014 these simple functions are just used to illustrate the Python function component development process.</p> <p>See Python function based component guide for more documentation.</p>"},{"location":"tutorials/tfx/python_function_component/#create-python-custom-components","title":"Create Python custom components\u00b6","text":"<p>We begin by writing a function that generate some dummy data. This is written to its own Python module file.</p>"},{"location":"tutorials/tfx/python_function_component/#run-in-notebook-with-the-interactivecontext","title":"Run in-notebook with the InteractiveContext\u00b6","text":"<p>Now, we will demonstrate usage of our new components in the TFX InteractiveContext.</p> <p>For more information on what you can do with the TFX notebook InteractiveContext, see the in-notebook TFX Keras Component Tutorial.</p>"},{"location":"tutorials/tfx/python_function_component/#construct-the-interactivecontext","title":"Construct the InteractiveContext\u00b6","text":""},{"location":"tutorials/tfx/python_function_component/#run-your-component-interactively-with-contextrun","title":"Run your component interactively with <code>context.run()</code>\u00b6","text":"<p>Next, we run our components interactively within the notebook with <code>context.run()</code>. Our consumer component uses the outputs of the generator component.</p>"},{"location":"tutorials/tfx/python_function_component/#write-a-pipeline-definition","title":"Write a pipeline definition\u00b6","text":"<p>Next, we will author a pipeline using these same components. While using the <code>InteractiveContext</code> within a notebook works well for experimentation, defining a pipeline lets you deploy your pipeline on local or remote runners for production usage.</p> <p>Here, we will demonstrate usage of the LocalDagRunner running locally on your machine. For production execution, the Airflow or Kubeflow runners may be more suitable.</p>"},{"location":"tutorials/tfx/python_function_component/#construct-a-pipeline","title":"Construct a pipeline\u00b6","text":""},{"location":"tutorials/tfx/python_function_component/#run-your-pipeline-with-the-localdagrunner","title":"Run your pipeline with the <code>LocalDagRunner</code>\u00b6","text":""},{"location":"tutorials/tfx/recommenders/","title":"Recommending Movies: Recommender Models in TFX","text":"In\u00a0[\u00a0]: Copied! <pre>#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n</pre> #@title Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # https://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. <p>Note: We recommend running this tutorial in a Colab notebook, with no setup required!  Just click \"Run in Google Colab\".</p>        View on TensorFlow.org             Run in Google Colab             View source on GitHub             Download notebook      In\u00a0[\u00a0]: Copied! <pre>!pip install -Uq tfx\n!pip install -Uq tensorflow-recommenders\n!pip install -Uq tensorflow-datasets\n</pre> !pip install -Uq tfx !pip install -Uq tensorflow-recommenders !pip install -Uq tensorflow-datasets In\u00a0[\u00a0]: Copied! <pre>import os\nimport absl\nimport json\nimport pprint\nimport tempfile\n\nfrom typing import Any, Dict, List, Text\n\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nimport tensorflow_recommenders as tfrs\nimport apache_beam as beam\n\nfrom absl import logging\n\nfrom tfx.components.example_gen.base_example_gen_executor import BaseExampleGenExecutor\nfrom tfx.components.example_gen.component import FileBasedExampleGen\nfrom tfx.components.example_gen import utils\nfrom tfx.dsl.components.base import executor_spec\n\nfrom tfx.types import artifact\nfrom tfx.types import artifact_utils\nfrom tfx.types import channel\nfrom tfx.types import standard_artifacts\nfrom tfx.types.standard_artifacts import Examples\n\nfrom tfx.dsl.component.experimental.annotations import InputArtifact\nfrom tfx.dsl.component.experimental.annotations import OutputArtifact\nfrom tfx.dsl.component.experimental.annotations import Parameter\nfrom tfx.dsl.component.experimental.decorators import component\nfrom tfx.types.experimental.simple_artifacts import Dataset\n\nfrom tfx import v1 as tfx\nfrom tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n\n# Set up logging.\ntf.get_logger().propagate = False\nabsl.logging.set_verbosity(absl.logging.INFO)\npp = pprint.PrettyPrinter()\n\nprint(f\"TensorFlow version: {tf.__version__}\")\nprint(f\"TFX version: {tfx.__version__}\")\nprint(f\"TensorFlow Recommenders version: {tfrs.__version__}\")\n\n%load_ext tfx.orchestration.experimental.interactive.notebook_extensions.skip\n</pre> import os import absl import json import pprint import tempfile  from typing import Any, Dict, List, Text  import numpy as np import tensorflow as tf import tensorflow_datasets as tfds import tensorflow_recommenders as tfrs import apache_beam as beam  from absl import logging  from tfx.components.example_gen.base_example_gen_executor import BaseExampleGenExecutor from tfx.components.example_gen.component import FileBasedExampleGen from tfx.components.example_gen import utils from tfx.dsl.components.base import executor_spec  from tfx.types import artifact from tfx.types import artifact_utils from tfx.types import channel from tfx.types import standard_artifacts from tfx.types.standard_artifacts import Examples  from tfx.dsl.component.experimental.annotations import InputArtifact from tfx.dsl.component.experimental.annotations import OutputArtifact from tfx.dsl.component.experimental.annotations import Parameter from tfx.dsl.component.experimental.decorators import component from tfx.types.experimental.simple_artifacts import Dataset  from tfx import v1 as tfx from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext  # Set up logging. tf.get_logger().propagate = False absl.logging.set_verbosity(absl.logging.INFO) pp = pprint.PrettyPrinter()  print(f\"TensorFlow version: {tf.__version__}\") print(f\"TFX version: {tfx.__version__}\") print(f\"TensorFlow Recommenders version: {tfrs.__version__}\")  %load_ext tfx.orchestration.experimental.interactive.notebook_extensions.skip In\u00a0[\u00a0]: Copied! <pre>@beam.ptransform_fn\n@beam.typehints.with_input_types(beam.Pipeline)\n@beam.typehints.with_output_types(tf.train.Example)\ndef _TFDatasetToExample(  # pylint: disable=invalid-name\n    pipeline: beam.Pipeline,\n    exec_properties: Dict[str, Any],\n    split_pattern: str\n    ) -&gt; beam.pvalue.PCollection:\n    \"\"\"Read a TensorFlow Dataset and create tf.Examples\"\"\"\n    custom_config = json.loads(exec_properties['custom_config'])\n    dataset_name = custom_config['dataset']\n    split_name = custom_config['split']\n\n    builder = tfds.builder(dataset_name)\n    builder.download_and_prepare()\n\n    return (pipeline\n            | 'MakeExamples' &gt;&gt; tfds.beam.ReadFromTFDS(builder, split=split_name)\n            | 'AsNumpy' &gt;&gt; beam.Map(tfds.as_numpy)\n            | 'ToDict' &gt;&gt; beam.Map(dict)\n            | 'ToTFExample' &gt;&gt; beam.Map(utils.dict_to_example)\n            )\n\nclass TFDSExecutor(BaseExampleGenExecutor):\n  def GetInputSourceToExamplePTransform(self) -&gt; beam.PTransform:\n    \"\"\"Returns PTransform for TF Dataset to TF examples.\"\"\"\n    return _TFDatasetToExample\n</pre> @beam.ptransform_fn @beam.typehints.with_input_types(beam.Pipeline) @beam.typehints.with_output_types(tf.train.Example) def _TFDatasetToExample(  # pylint: disable=invalid-name     pipeline: beam.Pipeline,     exec_properties: Dict[str, Any],     split_pattern: str     ) -&gt; beam.pvalue.PCollection:     \"\"\"Read a TensorFlow Dataset and create tf.Examples\"\"\"     custom_config = json.loads(exec_properties['custom_config'])     dataset_name = custom_config['dataset']     split_name = custom_config['split']      builder = tfds.builder(dataset_name)     builder.download_and_prepare()      return (pipeline             | 'MakeExamples' &gt;&gt; tfds.beam.ReadFromTFDS(builder, split=split_name)             | 'AsNumpy' &gt;&gt; beam.Map(tfds.as_numpy)             | 'ToDict' &gt;&gt; beam.Map(dict)             | 'ToTFExample' &gt;&gt; beam.Map(utils.dict_to_example)             )  class TFDSExecutor(BaseExampleGenExecutor):   def GetInputSourceToExamplePTransform(self) -&gt; beam.PTransform:     \"\"\"Returns PTransform for TF Dataset to TF examples.\"\"\"     return _TFDatasetToExample In\u00a0[\u00a0]: Copied! <pre>context = InteractiveContext()\n</pre> context = InteractiveContext() In\u00a0[\u00a0]: Copied! <pre># Ratings data.\nratings_example_gen = FileBasedExampleGen(\n    input_base='dummy',\n    custom_config={'dataset':'movielens/100k-ratings', 'split':'train'},\n    custom_executor_spec=executor_spec.ExecutorClassSpec(TFDSExecutor))\ncontext.run(ratings_example_gen, enable_cache=True)\n</pre> # Ratings data. ratings_example_gen = FileBasedExampleGen(     input_base='dummy',     custom_config={'dataset':'movielens/100k-ratings', 'split':'train'},     custom_executor_spec=executor_spec.ExecutorClassSpec(TFDSExecutor)) context.run(ratings_example_gen, enable_cache=True) In\u00a0[\u00a0]: Copied! <pre># Features of all the available movies.\nmovies_example_gen = FileBasedExampleGen(\n    input_base='dummy',\n    custom_config={'dataset':'movielens/100k-movies', 'split':'train'},\n    custom_executor_spec=executor_spec.ExecutorClassSpec(TFDSExecutor))\ncontext.run(movies_example_gen, enable_cache=True)\n</pre> # Features of all the available movies. movies_example_gen = FileBasedExampleGen(     input_base='dummy',     custom_config={'dataset':'movielens/100k-movies', 'split':'train'},     custom_executor_spec=executor_spec.ExecutorClassSpec(TFDSExecutor)) context.run(movies_example_gen, enable_cache=True) In\u00a0[\u00a0]: Copied! <pre>def inspect_examples(component,\n                     channel_name='examples',\n                     split_name='train',\n                     num_examples=1):\n  # Get the URI of the output artifact, which is a directory\n  full_split_name = 'Split-{}'.format(split_name)\n  print('channel_name: {}, split_name: {} (\\\"{}\\\"), num_examples: {}\\n'.format(\n      channel_name, split_name, full_split_name, num_examples))\n  train_uri = os.path.join(\n      component.outputs[channel_name].get()[0].uri, full_split_name)\n\n  # Get the list of files in this directory (all compressed TFRecord files)\n  tfrecord_filenames = [os.path.join(train_uri, name)\n                        for name in os.listdir(train_uri)]\n\n  # Create a `TFRecordDataset` to read these files\n  dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")\n\n  # Iterate over the records and print them\n  for tfrecord in dataset.take(num_examples):\n    serialized_example = tfrecord.numpy()\n    example = tf.train.Example()\n    example.ParseFromString(serialized_example)\n    pp.pprint(example)\n\ninspect_examples(ratings_example_gen)\n</pre> def inspect_examples(component,                      channel_name='examples',                      split_name='train',                      num_examples=1):   # Get the URI of the output artifact, which is a directory   full_split_name = 'Split-{}'.format(split_name)   print('channel_name: {}, split_name: {} (\\\"{}\\\"), num_examples: {}\\n'.format(       channel_name, split_name, full_split_name, num_examples))   train_uri = os.path.join(       component.outputs[channel_name].get()[0].uri, full_split_name)    # Get the list of files in this directory (all compressed TFRecord files)   tfrecord_filenames = [os.path.join(train_uri, name)                         for name in os.listdir(train_uri)]    # Create a `TFRecordDataset` to read these files   dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")    # Iterate over the records and print them   for tfrecord in dataset.take(num_examples):     serialized_example = tfrecord.numpy()     example = tf.train.Example()     example.ParseFromString(serialized_example)     pp.pprint(example)  inspect_examples(ratings_example_gen) <p>The movies dataset contains the movie id, movie title, and data on what genres it belongs to. Note that the genres are encoded with integer labels.</p> In\u00a0[\u00a0]: Copied! <pre>inspect_examples(movies_example_gen)\n</pre> inspect_examples(movies_example_gen) In\u00a0[\u00a0]: Copied! <pre>movies_stats_gen = tfx.components.StatisticsGen(\n    examples=movies_example_gen.outputs['examples'])\ncontext.run(movies_stats_gen, enable_cache=True)\n</pre> movies_stats_gen = tfx.components.StatisticsGen(     examples=movies_example_gen.outputs['examples']) context.run(movies_stats_gen, enable_cache=True) In\u00a0[\u00a0]: Copied! <pre>context.show(movies_stats_gen.outputs['statistics'])\n</pre> context.show(movies_stats_gen.outputs['statistics']) In\u00a0[\u00a0]: Copied! <pre>ratings_stats_gen = tfx.components.StatisticsGen(\n    examples=ratings_example_gen.outputs['examples'])\ncontext.run(ratings_stats_gen, enable_cache=True)\n</pre> ratings_stats_gen = tfx.components.StatisticsGen(     examples=ratings_example_gen.outputs['examples']) context.run(ratings_stats_gen, enable_cache=True) In\u00a0[\u00a0]: Copied! <pre>context.show(ratings_stats_gen.outputs['statistics'])\n</pre> context.show(ratings_stats_gen.outputs['statistics']) In\u00a0[\u00a0]: Copied! <pre>movies_schema_gen = tfx.components.SchemaGen(\n    statistics=movies_stats_gen.outputs['statistics'],\n    infer_feature_shape=False)\ncontext.run(movies_schema_gen, enable_cache=True)\n</pre> movies_schema_gen = tfx.components.SchemaGen(     statistics=movies_stats_gen.outputs['statistics'],     infer_feature_shape=False) context.run(movies_schema_gen, enable_cache=True) In\u00a0[\u00a0]: Copied! <pre>context.show(movies_schema_gen.outputs['schema'])\n</pre> context.show(movies_schema_gen.outputs['schema']) In\u00a0[\u00a0]: Copied! <pre>ratings_schema_gen = tfx.components.SchemaGen(\n    statistics=ratings_stats_gen.outputs['statistics'],\n    infer_feature_shape=False)\ncontext.run(ratings_schema_gen, enable_cache=True)\n</pre> ratings_schema_gen = tfx.components.SchemaGen(     statistics=ratings_stats_gen.outputs['statistics'],     infer_feature_shape=False) context.run(ratings_schema_gen, enable_cache=True) In\u00a0[\u00a0]: Copied! <pre>context.show(ratings_schema_gen.outputs['schema'])\n</pre> context.show(ratings_schema_gen.outputs['schema']) In\u00a0[\u00a0]: Copied! <pre>_movies_transform_module_file = 'movies_transform_module.py'\n</pre> _movies_transform_module_file = 'movies_transform_module.py' In\u00a0[\u00a0]: Copied! <pre>%%writefile {_movies_transform_module_file}\n\nimport tensorflow as tf\nimport tensorflow_transform as tft\n\ndef preprocessing_fn(inputs):\n  # We only want the movie title\n  return {'movie_title':inputs['movie_title']}\n</pre> %%writefile {_movies_transform_module_file}  import tensorflow as tf import tensorflow_transform as tft  def preprocessing_fn(inputs):   # We only want the movie title   return {'movie_title':inputs['movie_title']} In\u00a0[\u00a0]: Copied! <pre>movies_transform = tfx.components.Transform(\n    examples=movies_example_gen.outputs['examples'],\n    schema=movies_schema_gen.outputs['schema'],\n    module_file=os.path.abspath(_movies_transform_module_file))\ncontext.run(movies_transform, enable_cache=True)\n</pre> movies_transform = tfx.components.Transform(     examples=movies_example_gen.outputs['examples'],     schema=movies_schema_gen.outputs['schema'],     module_file=os.path.abspath(_movies_transform_module_file)) context.run(movies_transform, enable_cache=True) In\u00a0[\u00a0]: Copied! <pre>context.show(movies_transform.outputs['post_transform_schema'])\n</pre> context.show(movies_transform.outputs['post_transform_schema']) In\u00a0[\u00a0]: Copied! <pre>inspect_examples(movies_transform, channel_name='transformed_examples')\n</pre> inspect_examples(movies_transform, channel_name='transformed_examples') In\u00a0[\u00a0]: Copied! <pre>_ratings_transform_module_file = 'ratings_transform_module.py'\n</pre> _ratings_transform_module_file = 'ratings_transform_module.py' In\u00a0[\u00a0]: Copied! <pre>%%writefile {_ratings_transform_module_file}\n\nimport tensorflow as tf\nimport tensorflow_transform as tft\nimport pdb\n\nNUM_OOV_BUCKETS = 1\n\ndef preprocessing_fn(inputs):\n  # We only want the user ID and the movie title, but we also need vocabularies\n  # for both of them.  The vocabularies aren't features, they're only used by\n  # the lookup.\n  outputs = {}\n  outputs['user_id'] = tft.sparse_tensor_to_dense_with_shape(inputs['user_id'], [None, 1], '-1')\n  outputs['movie_title'] = tft.sparse_tensor_to_dense_with_shape(inputs['movie_title'], [None, 1], '-1')\n\n  tft.compute_and_apply_vocabulary(\n      inputs['user_id'],\n      num_oov_buckets=NUM_OOV_BUCKETS,\n      vocab_filename='user_id_vocab')\n\n  tft.compute_and_apply_vocabulary(\n      inputs['movie_title'],\n      num_oov_buckets=NUM_OOV_BUCKETS,\n      vocab_filename='movie_title_vocab')\n\n  return outputs\n</pre> %%writefile {_ratings_transform_module_file}  import tensorflow as tf import tensorflow_transform as tft import pdb  NUM_OOV_BUCKETS = 1  def preprocessing_fn(inputs):   # We only want the user ID and the movie title, but we also need vocabularies   # for both of them.  The vocabularies aren't features, they're only used by   # the lookup.   outputs = {}   outputs['user_id'] = tft.sparse_tensor_to_dense_with_shape(inputs['user_id'], [None, 1], '-1')   outputs['movie_title'] = tft.sparse_tensor_to_dense_with_shape(inputs['movie_title'], [None, 1], '-1')    tft.compute_and_apply_vocabulary(       inputs['user_id'],       num_oov_buckets=NUM_OOV_BUCKETS,       vocab_filename='user_id_vocab')    tft.compute_and_apply_vocabulary(       inputs['movie_title'],       num_oov_buckets=NUM_OOV_BUCKETS,       vocab_filename='movie_title_vocab')    return outputs In\u00a0[\u00a0]: Copied! <pre>ratings_transform = tfx.components.Transform(\n    examples=ratings_example_gen.outputs['examples'],\n    schema=ratings_schema_gen.outputs['schema'],\n    module_file=os.path.abspath(_ratings_transform_module_file))\ncontext.run(ratings_transform, enable_cache=True)\n</pre> ratings_transform = tfx.components.Transform(     examples=ratings_example_gen.outputs['examples'],     schema=ratings_schema_gen.outputs['schema'],     module_file=os.path.abspath(_ratings_transform_module_file)) context.run(ratings_transform, enable_cache=True) In\u00a0[\u00a0]: Copied! <pre>context.show(ratings_transform.outputs['post_transform_schema'])\n</pre> context.show(ratings_transform.outputs['post_transform_schema']) In\u00a0[\u00a0]: Copied! <pre>inspect_examples(ratings_transform, channel_name='transformed_examples')\n</pre> inspect_examples(ratings_transform, channel_name='transformed_examples') In\u00a0[\u00a0]: Copied! <pre># We're now going to create the module file for Trainer, which will include the\n# code above with some modifications for TFX.\n\n_trainer_module_file = 'trainer_module.py'\n</pre> # We're now going to create the module file for Trainer, which will include the # code above with some modifications for TFX.  _trainer_module_file = 'trainer_module.py' In\u00a0[\u00a0]: Copied! <pre>%%writefile {_trainer_module_file}\n\nfrom typing import Dict, List, Text\n\nimport pdb\n\nimport os\nimport absl\nimport datetime\nimport glob\nimport tensorflow as tf\nimport tensorflow_transform as tft\nimport tensorflow_recommenders as tfrs\n\nfrom absl import logging\nfrom tfx.types import artifact_utils\n\nfrom tfx import v1 as tfx\nfrom tfx_bsl.coders import example_coder\nfrom tfx_bsl.public import tfxio\n\nabsl.logging.set_verbosity(absl.logging.INFO)\n\nEMBEDDING_DIMENSION = 32\nINPUT_FN_BATCH_SIZE = 1\n\n\ndef extract_str_feature(dataset, feature_name):\n  np_dataset = []\n  for example in dataset:\n    np_example = example_coder.ExampleToNumpyDict(example.numpy())\n    np_dataset.append(np_example[feature_name][0].decode())\n  return tf.data.Dataset.from_tensor_slices(np_dataset)\n\n\nclass MovielensModel(tfrs.Model):\n\n  def __init__(self, user_model, movie_model, tf_transform_output, movies_uri):\n    super().__init__()\n    self.movie_model: tf.keras.Model = movie_model\n    self.user_model: tf.keras.Model = user_model\n\n    movies_artifact = movies_uri.get()[0]\n    input_dir = artifact_utils.get_split_uri([movies_artifact], 'train')\n    movie_files = glob.glob(os.path.join(input_dir, '*'))\n    movies = tf.data.TFRecordDataset(movie_files, compression_type=\"GZIP\")\n    movies_dataset = extract_str_feature(movies, 'movie_title')\n\n    loss_metrics = tfrs.metrics.FactorizedTopK(\n        candidates=movies_dataset.batch(128).map(movie_model)\n        )\n\n    self.task: tf.keras.layers.Layer = tfrs.tasks.Retrieval(\n        metrics=loss_metrics\n        )\n\n\n  def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -&gt; tf.Tensor:\n    # We pick out the user features and pass them into the user model.\n    try:\n      user_embeddings = tf.squeeze(self.user_model(features['user_id']), axis=1)\n      # And pick out the movie features and pass them into the movie model,\n      # getting embeddings back.\n      positive_movie_embeddings = self.movie_model(features['movie_title'])\n\n      # The task computes the loss and the metrics.\n      _task = self.task(user_embeddings, positive_movie_embeddings)\n    except BaseException as err:\n      logging.error('######## ERROR IN compute_loss:\\n{}\\n###############'.format(err))\n\n    return _task\n\n\n# This function will apply the same transform operation to training data\n# and serving requests.\ndef _apply_preprocessing(raw_features, tft_layer):\n  try:\n    transformed_features = tft_layer(raw_features)\n  except BaseException as err:\n    logging.error('######## ERROR IN _apply_preprocessing:\\n{}\\n###############'.format(err))\n\n  return transformed_features\n\n\ndef _input_fn(file_pattern: List[Text],\n              data_accessor: tfx.components.DataAccessor,\n              tf_transform_output: tft.TFTransformOutput,\n              batch_size: int = 200) -&gt; tf.data.Dataset:\n  \"\"\"Generates features and label for tuning/training.\n\n  Args:\n    file_pattern: List of paths or patterns of input tfrecord files.\n    data_accessor: DataAccessor for converting input to RecordBatch.\n    tf_transform_output: A TFTransformOutput.\n    batch_size: representing the number of consecutive elements of returned\n      dataset to combine in a single batch\n\n  Returns:\n    A dataset that contains (features, indices) tuple where features is a\n      dictionary of Tensors, and indices is a single Tensor of label indices.\n  \"\"\"\n  try:\n    return data_accessor.tf_dataset_factory(\n      file_pattern,\n      tfxio.TensorFlowDatasetOptions(\n          batch_size=batch_size),\n      tf_transform_output.transformed_metadata.schema)\n  except BaseException as err:\n    logging.error('######## ERROR IN _input_fn:\\n{}\\n###############'.format(err))\n\n  return None\n\n\ndef _get_serve_tf_examples_fn(model, tf_transform_output):\n  \"\"\"Returns a function that parses a serialized tf.Example and applies TFT.\"\"\"\n  try:\n    model.tft_layer = tf_transform_output.transform_features_layer()\n\n    @tf.function\n    def serve_tf_examples_fn(serialized_tf_examples):\n      \"\"\"Returns the output to be used in the serving signature.\"\"\"\n      try:\n        feature_spec = tf_transform_output.raw_feature_spec()\n        parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)\n        transformed_features = model.tft_layer(parsed_features)\n        result = model(transformed_features)\n      except BaseException as err:\n        logging.error('######## ERROR IN serve_tf_examples_fn:\\n{}\\n###############'.format(err))\n      return result\n  except BaseException as err:\n      logging.error('######## ERROR IN _get_serve_tf_examples_fn:\\n{}\\n###############'.format(err))\n\n  return serve_tf_examples_fn\n\n\ndef _build_user_model(\n    tf_transform_output: tft.TFTransformOutput, # Specific to ratings\n    embedding_dimension: int = 32) -&gt; tf.keras.Model:\n  \"\"\"Creates a Keras model for the query tower.\n\n  Args:\n    tf_transform_output: [tft.TFTransformOutput], the results of Transform\n    embedding_dimension: [int], the dimensionality of the embedding space\n\n  Returns:\n    A keras Model.\n  \"\"\"\n  try:\n    unique_user_ids = tf_transform_output.vocabulary_by_name('user_id_vocab')\n    users_vocab_str = [b.decode() for b in unique_user_ids]\n\n    model = tf.keras.Sequential(\n        [\n         tf.keras.layers.StringLookup(\n             vocabulary=users_vocab_str, mask_token=None),\n         # We add an additional embedding to account for unknown tokens.\n         tf.keras.layers.Embedding(len(users_vocab_str) + 1, embedding_dimension)\n         ])\n  except BaseException as err:\n    logging.error('######## ERROR IN _build_user_model:\\n{}\\n###############'.format(err))\n\n  return model\n\n\ndef _build_movie_model(\n    tf_transform_output: tft.TFTransformOutput, # Specific to movies\n    embedding_dimension: int = 32) -&gt; tf.keras.Model:\n  \"\"\"Creates a Keras model for the candidate tower.\n\n  Args:\n    tf_transform_output: [tft.TFTransformOutput], the results of Transform\n    embedding_dimension: [int], the dimensionality of the embedding space\n\n  Returns:\n    A keras Model.\n  \"\"\"\n  try:\n    unique_movie_titles = tf_transform_output.vocabulary_by_name('movie_title_vocab')\n    titles_vocab_str = [b.decode() for b in unique_movie_titles]\n\n    model = tf.keras.Sequential(\n        [\n         tf.keras.layers.StringLookup(\n             vocabulary=titles_vocab_str, mask_token=None),\n         # We add an additional embedding to account for unknown tokens.\n         tf.keras.layers.Embedding(len(titles_vocab_str) + 1, embedding_dimension)\n        ])\n  except BaseException as err:\n      logging.error('######## ERROR IN _build_movie_model:\\n{}\\n###############'.format(err))\n  return model\n\n\n# TFX Trainer will call this function.\ndef run_fn(fn_args: tfx.components.FnArgs):\n  \"\"\"Train the model based on given args.\n\n  Args:\n    fn_args: Holds args used to train the model as name/value pairs.\n  \"\"\"\n  try:\n    tf_transform_output = tft.TFTransformOutput(fn_args.transform_output)\n\n    train_dataset = _input_fn(fn_args.train_files, fn_args.data_accessor,\n                              tf_transform_output, INPUT_FN_BATCH_SIZE)\n    eval_dataset = _input_fn(fn_args.eval_files, fn_args.data_accessor,\n                            tf_transform_output, INPUT_FN_BATCH_SIZE)\n\n    model = MovielensModel(\n        _build_user_model(tf_transform_output, EMBEDDING_DIMENSION),\n        _build_movie_model(tf_transform_output, EMBEDDING_DIMENSION),\n        tf_transform_output,\n        fn_args.custom_config['movies']\n        )\n\n    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n        log_dir=fn_args.model_run_dir, update_freq='batch')\n\n    model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1))\n  except BaseException as err:\n    logging.error('######## ERROR IN run_fn before fit:\\n{}\\n###############'.format(err))\n\n  try:\n    model.fit(\n        train_dataset,\n        epochs=fn_args.custom_config['epochs'],\n        steps_per_epoch=fn_args.train_steps,\n        validation_data=eval_dataset,\n        validation_steps=fn_args.eval_steps,\n        callbacks=[tensorboard_callback])\n  except BaseException as err:\n      logging.error('######## ERROR IN run_fn during fit:\\n{}\\n###############'.format(err))\n\n  try:\n    index = tfrs.layers.factorized_top_k.BruteForce(model.user_model)\n\n    movies_artifact = fn_args.custom_config['movies'].get()[0]\n    input_dir = artifact_utils.get_split_uri([movies_artifact], 'eval')\n    movie_files = glob.glob(os.path.join(input_dir, '*'))\n    movies = tf.data.TFRecordDataset(movie_files, compression_type=\"GZIP\")\n\n    movies_dataset = extract_str_feature(movies, 'movie_title')\n\n    index.index_from_dataset(\n      tf.data.Dataset.zip((\n          movies_dataset.batch(100),\n          movies_dataset.batch(100).map(model.movie_model))\n      )\n    )\n\n    # Run once so that we can get the right signatures into SavedModel\n    _, titles = index(tf.constant([\"42\"]))\n    print(f\"Recommendations for user 42: {titles[0, :3]}\")\n\n    signatures = {\n        'serving_default':\n            _get_serve_tf_examples_fn(index,\n                                      tf_transform_output).get_concrete_function(\n                                          tf.TensorSpec(\n                                              shape=[None],\n                                              dtype=tf.string,\n                                              name='examples')),\n    }\n    index.save(fn_args.serving_model_dir, save_format='tf', signatures=signatures)\n\n  except BaseException as err:\n      logging.error('######## ERROR IN run_fn during export:\\n{}\\n###############'.format(err))\n</pre> %%writefile {_trainer_module_file}  from typing import Dict, List, Text  import pdb  import os import absl import datetime import glob import tensorflow as tf import tensorflow_transform as tft import tensorflow_recommenders as tfrs  from absl import logging from tfx.types import artifact_utils  from tfx import v1 as tfx from tfx_bsl.coders import example_coder from tfx_bsl.public import tfxio  absl.logging.set_verbosity(absl.logging.INFO)  EMBEDDING_DIMENSION = 32 INPUT_FN_BATCH_SIZE = 1   def extract_str_feature(dataset, feature_name):   np_dataset = []   for example in dataset:     np_example = example_coder.ExampleToNumpyDict(example.numpy())     np_dataset.append(np_example[feature_name][0].decode())   return tf.data.Dataset.from_tensor_slices(np_dataset)   class MovielensModel(tfrs.Model):    def __init__(self, user_model, movie_model, tf_transform_output, movies_uri):     super().__init__()     self.movie_model: tf.keras.Model = movie_model     self.user_model: tf.keras.Model = user_model      movies_artifact = movies_uri.get()[0]     input_dir = artifact_utils.get_split_uri([movies_artifact], 'train')     movie_files = glob.glob(os.path.join(input_dir, '*'))     movies = tf.data.TFRecordDataset(movie_files, compression_type=\"GZIP\")     movies_dataset = extract_str_feature(movies, 'movie_title')      loss_metrics = tfrs.metrics.FactorizedTopK(         candidates=movies_dataset.batch(128).map(movie_model)         )      self.task: tf.keras.layers.Layer = tfrs.tasks.Retrieval(         metrics=loss_metrics         )     def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -&gt; tf.Tensor:     # We pick out the user features and pass them into the user model.     try:       user_embeddings = tf.squeeze(self.user_model(features['user_id']), axis=1)       # And pick out the movie features and pass them into the movie model,       # getting embeddings back.       positive_movie_embeddings = self.movie_model(features['movie_title'])        # The task computes the loss and the metrics.       _task = self.task(user_embeddings, positive_movie_embeddings)     except BaseException as err:       logging.error('######## ERROR IN compute_loss:\\n{}\\n###############'.format(err))      return _task   # This function will apply the same transform operation to training data # and serving requests. def _apply_preprocessing(raw_features, tft_layer):   try:     transformed_features = tft_layer(raw_features)   except BaseException as err:     logging.error('######## ERROR IN _apply_preprocessing:\\n{}\\n###############'.format(err))    return transformed_features   def _input_fn(file_pattern: List[Text],               data_accessor: tfx.components.DataAccessor,               tf_transform_output: tft.TFTransformOutput,               batch_size: int = 200) -&gt; tf.data.Dataset:   \"\"\"Generates features and label for tuning/training.    Args:     file_pattern: List of paths or patterns of input tfrecord files.     data_accessor: DataAccessor for converting input to RecordBatch.     tf_transform_output: A TFTransformOutput.     batch_size: representing the number of consecutive elements of returned       dataset to combine in a single batch    Returns:     A dataset that contains (features, indices) tuple where features is a       dictionary of Tensors, and indices is a single Tensor of label indices.   \"\"\"   try:     return data_accessor.tf_dataset_factory(       file_pattern,       tfxio.TensorFlowDatasetOptions(           batch_size=batch_size),       tf_transform_output.transformed_metadata.schema)   except BaseException as err:     logging.error('######## ERROR IN _input_fn:\\n{}\\n###############'.format(err))    return None   def _get_serve_tf_examples_fn(model, tf_transform_output):   \"\"\"Returns a function that parses a serialized tf.Example and applies TFT.\"\"\"   try:     model.tft_layer = tf_transform_output.transform_features_layer()      @tf.function     def serve_tf_examples_fn(serialized_tf_examples):       \"\"\"Returns the output to be used in the serving signature.\"\"\"       try:         feature_spec = tf_transform_output.raw_feature_spec()         parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)         transformed_features = model.tft_layer(parsed_features)         result = model(transformed_features)       except BaseException as err:         logging.error('######## ERROR IN serve_tf_examples_fn:\\n{}\\n###############'.format(err))       return result   except BaseException as err:       logging.error('######## ERROR IN _get_serve_tf_examples_fn:\\n{}\\n###############'.format(err))    return serve_tf_examples_fn   def _build_user_model(     tf_transform_output: tft.TFTransformOutput, # Specific to ratings     embedding_dimension: int = 32) -&gt; tf.keras.Model:   \"\"\"Creates a Keras model for the query tower.    Args:     tf_transform_output: [tft.TFTransformOutput], the results of Transform     embedding_dimension: [int], the dimensionality of the embedding space    Returns:     A keras Model.   \"\"\"   try:     unique_user_ids = tf_transform_output.vocabulary_by_name('user_id_vocab')     users_vocab_str = [b.decode() for b in unique_user_ids]      model = tf.keras.Sequential(         [          tf.keras.layers.StringLookup(              vocabulary=users_vocab_str, mask_token=None),          # We add an additional embedding to account for unknown tokens.          tf.keras.layers.Embedding(len(users_vocab_str) + 1, embedding_dimension)          ])   except BaseException as err:     logging.error('######## ERROR IN _build_user_model:\\n{}\\n###############'.format(err))    return model   def _build_movie_model(     tf_transform_output: tft.TFTransformOutput, # Specific to movies     embedding_dimension: int = 32) -&gt; tf.keras.Model:   \"\"\"Creates a Keras model for the candidate tower.    Args:     tf_transform_output: [tft.TFTransformOutput], the results of Transform     embedding_dimension: [int], the dimensionality of the embedding space    Returns:     A keras Model.   \"\"\"   try:     unique_movie_titles = tf_transform_output.vocabulary_by_name('movie_title_vocab')     titles_vocab_str = [b.decode() for b in unique_movie_titles]      model = tf.keras.Sequential(         [          tf.keras.layers.StringLookup(              vocabulary=titles_vocab_str, mask_token=None),          # We add an additional embedding to account for unknown tokens.          tf.keras.layers.Embedding(len(titles_vocab_str) + 1, embedding_dimension)         ])   except BaseException as err:       logging.error('######## ERROR IN _build_movie_model:\\n{}\\n###############'.format(err))   return model   # TFX Trainer will call this function. def run_fn(fn_args: tfx.components.FnArgs):   \"\"\"Train the model based on given args.    Args:     fn_args: Holds args used to train the model as name/value pairs.   \"\"\"   try:     tf_transform_output = tft.TFTransformOutput(fn_args.transform_output)      train_dataset = _input_fn(fn_args.train_files, fn_args.data_accessor,                               tf_transform_output, INPUT_FN_BATCH_SIZE)     eval_dataset = _input_fn(fn_args.eval_files, fn_args.data_accessor,                             tf_transform_output, INPUT_FN_BATCH_SIZE)      model = MovielensModel(         _build_user_model(tf_transform_output, EMBEDDING_DIMENSION),         _build_movie_model(tf_transform_output, EMBEDDING_DIMENSION),         tf_transform_output,         fn_args.custom_config['movies']         )      tensorboard_callback = tf.keras.callbacks.TensorBoard(         log_dir=fn_args.model_run_dir, update_freq='batch')      model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1))   except BaseException as err:     logging.error('######## ERROR IN run_fn before fit:\\n{}\\n###############'.format(err))    try:     model.fit(         train_dataset,         epochs=fn_args.custom_config['epochs'],         steps_per_epoch=fn_args.train_steps,         validation_data=eval_dataset,         validation_steps=fn_args.eval_steps,         callbacks=[tensorboard_callback])   except BaseException as err:       logging.error('######## ERROR IN run_fn during fit:\\n{}\\n###############'.format(err))    try:     index = tfrs.layers.factorized_top_k.BruteForce(model.user_model)      movies_artifact = fn_args.custom_config['movies'].get()[0]     input_dir = artifact_utils.get_split_uri([movies_artifact], 'eval')     movie_files = glob.glob(os.path.join(input_dir, '*'))     movies = tf.data.TFRecordDataset(movie_files, compression_type=\"GZIP\")      movies_dataset = extract_str_feature(movies, 'movie_title')      index.index_from_dataset(       tf.data.Dataset.zip((           movies_dataset.batch(100),           movies_dataset.batch(100).map(model.movie_model))       )     )      # Run once so that we can get the right signatures into SavedModel     _, titles = index(tf.constant([\"42\"]))     print(f\"Recommendations for user 42: {titles[0, :3]}\")      signatures = {         'serving_default':             _get_serve_tf_examples_fn(index,                                       tf_transform_output).get_concrete_function(                                           tf.TensorSpec(                                               shape=[None],                                               dtype=tf.string,                                               name='examples')),     }     index.save(fn_args.serving_model_dir, save_format='tf', signatures=signatures)    except BaseException as err:       logging.error('######## ERROR IN run_fn during export:\\n{}\\n###############'.format(err)) In\u00a0[\u00a0]: Copied! <pre>trainer = tfx.components.Trainer(\n    module_file=os.path.abspath(_trainer_module_file),\n    examples=ratings_transform.outputs['transformed_examples'],\n    transform_graph=ratings_transform.outputs['transform_graph'],\n    schema=ratings_transform.outputs['post_transform_schema'],\n    train_args=tfx.proto.TrainArgs(num_steps=500),\n    eval_args=tfx.proto.EvalArgs(num_steps=10),\n    custom_config={\n        'epochs':5,\n        'movies':movies_transform.outputs['transformed_examples'],\n        'movie_schema':movies_transform.outputs['post_transform_schema'],\n        'ratings':ratings_transform.outputs['transformed_examples'],\n        'ratings_schema':ratings_transform.outputs['post_transform_schema']\n        })\n\ncontext.run(trainer, enable_cache=False)\n</pre> trainer = tfx.components.Trainer(     module_file=os.path.abspath(_trainer_module_file),     examples=ratings_transform.outputs['transformed_examples'],     transform_graph=ratings_transform.outputs['transform_graph'],     schema=ratings_transform.outputs['post_transform_schema'],     train_args=tfx.proto.TrainArgs(num_steps=500),     eval_args=tfx.proto.EvalArgs(num_steps=10),     custom_config={         'epochs':5,         'movies':movies_transform.outputs['transformed_examples'],         'movie_schema':movies_transform.outputs['post_transform_schema'],         'ratings':ratings_transform.outputs['transformed_examples'],         'ratings_schema':ratings_transform.outputs['post_transform_schema']         })  context.run(trainer, enable_cache=False) In\u00a0[\u00a0]: Copied! <pre>_serving_model_dir = os.path.join(tempfile.mkdtemp(), 'serving_model/tfrs_retrieval')\n\npusher = tfx.components.Pusher(\n    model=trainer.outputs['model'],\n    push_destination=tfx.proto.PushDestination(\n        filesystem=tfx.proto.PushDestination.Filesystem(\n            base_directory=_serving_model_dir)))\ncontext.run(pusher, enable_cache=True)\n</pre> _serving_model_dir = os.path.join(tempfile.mkdtemp(), 'serving_model/tfrs_retrieval')  pusher = tfx.components.Pusher(     model=trainer.outputs['model'],     push_destination=tfx.proto.PushDestination(         filesystem=tfx.proto.PushDestination.Filesystem(             base_directory=_serving_model_dir))) context.run(pusher, enable_cache=True) In\u00a0[\u00a0]: Copied! <pre>loaded = tf.saved_model.load(pusher.outputs['pushed_model'].get()[0].uri)\nscores, titles = loaded([\"42\"])\n\nprint(f\"Recommendations: {titles[0][:3]}\")\n</pre> loaded = tf.saved_model.load(pusher.outputs['pushed_model'].get()[0].uri) scores, titles = loaded([\"42\"])  print(f\"Recommendations: {titles[0][:3]}\")"},{"location":"tutorials/tfx/recommenders/#copyright-2022-the-tensorflow-authors","title":"Copyright 2022 The TensorFlow Authors.\u00b6","text":""},{"location":"tutorials/tfx/recommenders/#recommending-movies-recommender-models-in-tfx","title":"Recommending Movies: Recommender Models in TFX\u00b6","text":""},{"location":"tutorials/tfx/recommenders/#tfrs-tutorial-ported-to-tfx","title":"TFRS Tutorial Ported to TFX\u00b6","text":"<p>This is a port of a basic TensorFlow Recommenders (TFRS) tutorial to TFX, which is designed to demonstrate how to use TFRS in a TFX pipeline.  It mirrors the basic tutorial.</p> <p>For context, real-world recommender systems are often composed of two stages:</p> <ol> <li>The retrieval stage is responsible for selecting an initial set of hundreds of candidates from all possible candidates. The main objective of this model is to efficiently weed out all candidates that the user is not interested in. Because the retrieval model may be dealing with millions of candidates, it has to be computationally efficient.</li> <li>The ranking stage takes the outputs of the retrieval model and fine-tunes them to select the best possible handful of recommendations. Its task is to narrow down the set of items the user may be interested in to a shortlist of likely candidates.</li> </ol> <p>In this tutorial, we're going to focus on the first stage, retrieval. Retrieval models are often composed of two sub-models:</p> <ol> <li>A query model computing the query representation (normally a fixed-dimensionality embedding vector) using query features.</li> <li>A candidate model computing the candidate representation (an equally-sized vector) using the candidate features</li> </ol> <p>The outputs of the two models are then multiplied together to give a query-candidate affinity score, with higher scores expressing a better match between the candidate and the query.</p> <p>In this tutorial, we're going to build and train such a two-tower model using the Movielens dataset.</p> <p>We're going to:</p> <ol> <li>Ingest and inspect the MovieLens dataset.</li> <li>Implement a retrieval model.</li> <li>Train and export the model.</li> <li>Make predictions</li> </ol>"},{"location":"tutorials/tfx/recommenders/#the-dataset","title":"The dataset\u00b6","text":"<p>The Movielens dataset is a classic dataset from the GroupLens research group at the University of Minnesota. It contains a set of ratings given to movies by a set of users, and is a workhorse of recommender system research.</p> <p>The data can be treated in two ways:</p> <ol> <li>It can be interpreted as expressesing which movies the users watched (and rated), and which they did not. This is a form of implicit feedback, where users' watches tell us which things they prefer to see and which they'd rather not see.</li> <li>It can also be seen as expressesing how much the users liked the movies they did watch. This is a form of explicit feedback: given that a user watched a movie, we can tell roughly how much they liked by looking at the rating they have given.</li> </ol> <p>In this tutorial, we are focusing on a retrieval system: a model that predicts a set of movies from the catalogue that the user is likely to watch. Often, implicit data is more useful here, and so we are going to treat Movielens as an implicit system. This means that every movie a user watched is a positive example, and every movie they have not seen is an implicit negative example.</p>"},{"location":"tutorials/tfx/recommenders/#imports","title":"Imports\u00b6","text":"<p>Let's first get our imports out of the way.</p>"},{"location":"tutorials/tfx/recommenders/#did-you-restart-the-runtime","title":"Did you restart the runtime?\u00b6","text":"<p>If you are using Google Colab, the first time that you run the cell above, you must restart the runtime (Runtime &gt; Restart runtime ...). This is because of the way that Colab loads packages.</p>"},{"location":"tutorials/tfx/recommenders/#create-a-tfds-examplegen","title":"Create a TFDS ExampleGen\u00b6","text":"<p>We create a custom ExampleGen component which we use to load a TensorFlow Datasets (TFDS) dataset.  This uses a custom executor in a FileBasedExampleGen.</p>"},{"location":"tutorials/tfx/recommenders/#init-tfx-pipeline-context","title":"Init TFX Pipeline Context\u00b6","text":""},{"location":"tutorials/tfx/recommenders/#preparing-the-dataset","title":"Preparing the dataset\u00b6","text":"<p>We will use our custom executor in a <code>FileBasedExampleGen</code> to load our datasets from TFDS.  Since we have two datasets, we will create two <code>ExampleGen</code> components.</p>"},{"location":"tutorials/tfx/recommenders/#create-inspect_examples-utility","title":"Create <code>inspect_examples</code> utility\u00b6","text":"<p>We create a convenience utility to inspect datasets of TF.Examples. The ratings dataset returns a dictionary of movie id, user id, the assigned rating, timestamp, movie information, and user information:</p>"},{"location":"tutorials/tfx/recommenders/#examplegen-did-the-split","title":"ExampleGen did the split\u00b6","text":"<p>When we ingested the movie lens dataset, our <code>ExampleGen</code> component split the data into <code>train</code> and <code>eval</code> splits.  They are actually named <code>Split-train</code> and <code>Split-eval</code>. By default the split is 66% training, 34% evaluation.</p>"},{"location":"tutorials/tfx/recommenders/#generate-statistics-for-movies-and-ratings","title":"Generate statistics for movies and ratings\u00b6","text":"<p>For a TFX pipeline we need to generate statistics for the dataset.  We do that by using a StatisticsGen component. These will be used by the SchemaGen component below when we generate a schema for our dataset.  This is good practice anyway, because it's important to examine and analyze your data on an ongoing basis.  Since we have two datasets we will create two StatisticsGen components.</p>"},{"location":"tutorials/tfx/recommenders/#create-schemas-for-movies-and-ratings","title":"Create schemas for movies and ratings\u00b6","text":"<p>For a TFX pipeline we need to generate a data schema from our dataset.  We do that by using a SchemaGen component. This will be used by the Transform component below to do our feature engineering in a way that is highly scalable to large datasets, and avoids training/serving skew.  Since we have two datasets we will create two SchemaGen components.</p>"},{"location":"tutorials/tfx/recommenders/#feature-engineering-using-transform","title":"Feature Engineering using Transform\u00b6","text":"<p>For a structured and repeatable design of a TFX pipeline we will need a scalable approach to feature engineering.  This allows us to handle the large datasets which are usually part of many recommender systems, and it also avoids training/serving skew.  We will do that using the Transform component.</p> <p>The Transform component uses a module file to supply user code for the feature engineering what we want to do, so our first step is to create that module file. Since we have two datasets, we will create two of these module files and two Transform components.</p> <p>One of the things that our recommender needs is vocabularies for the <code>user_id</code> and <code>movie_title</code> fields.  In the basic_retrieval tutorial those are created with inline Numpy, but here we will use Transform.</p> <p>Note: The <code>%%writefile {_movies_transform_module_file}</code> cell magic below creates and writes the contents of that cell to a file on the notebook server where this notebook is running (for example, the Colab VM).  When doing this outside of a notebook you would just create a Python file.</p>"},{"location":"tutorials/tfx/recommenders/#implementing-a-model-in-tfx","title":"Implementing a model in TFX\u00b6","text":"<p>In the basic_retrieval tutorial the model was created inline in the Python runtime. In a TFX pipeline, the model, metric, and loss are defined and trained in the module file for a pipeline component called Trainer. This makes the model, metric, and loss part of a repeatable process which can be automated and monitored.</p>"},{"location":"tutorials/tfx/recommenders/#tensorflow-recommenders-model-architecture","title":"TensorFlow Recommenders model architecture\u00b6","text":"<p>We are going to build a two-tower retrieval model. The concept of two-tower means we will have a query tower computing the user representation using user features, and another item tower computing the movie representation using the movie features. We can build each tower separately (in the <code>_build_user_model()</code> and <code>_build_movie_model()</code> methods below) and then combine them in the final model (as in the <code>MobieLensModel</code> class). <code>MovieLensModel</code> is a subclass of <code>tfrs.Model</code> base class, which streamlines building models: all we need to do is to set up the components in the <code>__init__</code> method, and implement the <code>compute_loss</code> method, taking in the raw features and returning a loss value.</p>"},{"location":"tutorials/tfx/recommenders/#training-the-model","title":"Training the model\u00b6","text":"<p>After defining the model, we can run the Trainer component to do the model training.</p>"},{"location":"tutorials/tfx/recommenders/#exporting-the-model","title":"Exporting the model\u00b6","text":"<p>After training the model, we can use the Pusher component to export the model.</p>"},{"location":"tutorials/tfx/recommenders/#make-predictions","title":"Make predictions\u00b6","text":"<p>Now that we have a model, we load it back and make predictions.</p>"},{"location":"tutorials/tfx/recommenders/#next-step","title":"Next step\u00b6","text":"<p>In this tutorial, you have learned how to implement a retrieval model with TensorFlow Recommenders and TFX. To expand on what is presented here, have a look at the TFRS ranking with TFX tutorial.</p>"},{"location":"tutorials/tfx/stub_template/","title":"Stub template","text":""},{"location":"tutorials/tfx/stub_template/#testing-the-pipeline-using-stub-executors","title":"Testing the pipeline using Stub Executors","text":""},{"location":"tutorials/tfx/stub_template/#introduction","title":"Introduction","text":"<p>You should complete template.ipynb tutorial up to Step 6 in order to proceed this tutorial.</p> <p>This document will provide instructions to test a TensorFlow Extended (TFX) pipeline using <code>BaseStubExecuctor</code>, which generates fake artifacts using the golden test data. This is intended for users to replace executors they don't want to test so that they could save time from running the actual executors. Stub executor is provided with TFX Python package under <code>tfx.experimental.pipeline_testing.base_stub_executor</code>.</p> <p>This tutorial serves as an extension to <code>template.ipynb</code> tutorial, thus you will also use Taxi Trips dataset released by the City of Chicago. We strongly encourage you to try modifying the components prior to utilizing stub executors.</p>"},{"location":"tutorials/tfx/stub_template/#1-record-the-pipeline-outputs-in-google-cloud-storage","title":"1. Record the pipeline outputs in Google Cloud Storage","text":"<p>We first need to record the pipeline outputs so that the stub executors can copy over the artifacts from the recorded outputs.</p> <p>Since this tutorial assumes that you have completed <code>template.ipynb</code> up to step 6, a successful pipeline run must have been saved in the MLMD. The execution information in MLMD can be accessed using gRPC server.</p> <p>Open a Terminal and run the following commands:</p> <ol> <li> <p>Generate a kubeconfig file with appropriate credentials: <code>bash gcloud     container clusters get-credentials $cluster_name --zone $compute_zone     --project $gcp_project_id</code> <code>$compute_zone</code> is region for gcp engine and     <code>$gcp_project_id</code> is project id of your GCP project.</p> </li> <li> <p>Set up port-forwarding for connecting to MLMD: <code>bash nohup kubectl     port-forward deployment/metadata-grpc-deployment -n $namespace $port:8080 &amp;</code> <code>$namespace</code> is the cluster namespace and <code>$port</code> is any unused port that     will be used for port-forwarding.</p> </li> <li> <p>Clone the tfx GitHub repository. Inside the tfx directory, run the following     command:</p> </li> </ol> <pre><code>python tfx/experimental/pipeline_testing/pipeline_recorder.py \\\n--output_dir=gs://&lt;gcp_project_id&gt;-kubeflowpipelines-default/testdata \\\n--host=$host \\\n--port=$port \\\n--pipeline_name=$pipeline_name\n</code></pre> <p><code>$output_dir</code> should be set to a path in Google Cloud Storage where the pipeline outputs are to be recorded, so make sure to replace <code>&lt;gcp_project_id&gt;</code> with GCP project id.</p> <p><code>$host</code> and <code>$port</code> are hostname and port of the metadata grpc server to connect to MLMD. <code>$port</code> should be set to the port number you used for port-forwarding and you may set \"localhost\" for the hostname.</p> <p>In <code>template.ipynb</code> tutorial, the pipeline name is set as \"my_pipeline\" by default, so set <code>pipeline_name=\"my_pipeline\"</code>. If you have modified the pipeline name when running the template tutorial, you should modify the <code>--pipeline_name</code> accordingly.</p>"},{"location":"tutorials/tfx/stub_template/#2-enable-stub-executors-in-kubeflow-dag-runner","title":"2. Enable Stub Executors in Kubeflow DAG Runner","text":"<p>First, make sure that the predefined template has been copied to your project directory using <code>tfx template copy</code> CLI command. It is necessary to edit the following two files in the copied source files.</p> <ol> <li> <p>Create a file called <code>stub_component_launcher.py</code> in the directory where     kubeflow_dag_runner.py is located, and put following content to it.</p> <pre><code>from tfx.experimental.pipeline_testing import base_stub_component_launcher\nfrom pipeline import configs\n\nclass StubComponentLauncher(\n    base_stub_component_launcher.BaseStubComponentLauncher):\n  pass\n\n# GCS directory where KFP outputs are recorded\ntest_data_dir = \"gs://{}/testdata\".format(configs.GCS_BUCKET_NAME)\n# TODO: customize self.test_component_ids to test components, replacing other\n# component executors with a BaseStubExecutor.\ntest_component_ids = ['Trainer']\nStubComponentLauncher.initialize(\n    test_data_dir=test_data_dir,\n    test_component_ids=test_component_ids)\n</code></pre> <p>Note</p> <p>This stub component launcher cannot be defined within <code>kubeflow_dag_runner.py</code> because launcher class is imported by the module path.</p> </li> <li> <p>Set component ids to be list of component ids that are to be tested (in     other words, other components' executors are replaced with BaseStubExecutor)     .</p> </li> <li> <p>Open <code>kubeflow_dag_runner.py</code>. Add following import statement at the top to     use <code>StubComponentLauncher</code> class we just added.</p> <pre><code>import stub_component_launcher\n</code></pre> </li> <li> <p>In <code>kubeflow_dag_runner.py</code>, add <code>StubComponentLauncher</code> class to     <code>KubeflowDagRunnerConfig</code>'s <code>supported_launcher_class</code> to enable launch of     stub executors:</p> <pre><code>runner_config = kubeflow_dag_runner.KubeflowDagRunnerConfig(\n    supported_launcher_classes=[\n        stub_component_launcher.StubComponentLauncher\n    ],\n</code></pre> </li> </ol>"},{"location":"tutorials/tfx/stub_template/#3-update-and-run-the-pipeline-with-stub-executors","title":"3. Update and run the pipeline with stub executors","text":"<p>Update the existing pipeline with modified pipeline definition with stub executors.</p> <pre><code>tfx pipeline update --pipeline-path=kubeflow_dag_runner.py \\\n  --endpoint=$endpoint --engine=kubeflow\n</code></pre> <p><code>$endpoint</code> should be set to your KFP cluster endpoint.</p> <p>Run the following command to create a new execution run of your updated pipeline.</p> <pre><code>tfx run create --pipeline-name $pipeline_name --endpoint=$endpoint \\\n  --engine=kubeflow\n</code></pre>"},{"location":"tutorials/tfx/stub_template/#cleaning-up","title":"Cleaning up","text":"<p>Use command <code>fg</code> to access the port-forwarding in the background then ctrl-C to terminate. You can delete the directory with recorded pipeline outputs using <code>gsutil -m rm -R $output_dir</code>.</p> <p>To clean up all Google Cloud resources used in this project, you can delete the Google Cloud project you used for the tutorial.</p> <p>Alternatively, you can clean up individual resources by visiting each consoles: - Google Cloud Storage - Google Container Registry - Google Kubernetes Engine</p>"},{"location":"tutorials/tfx/template/","title":"Create a TFX pipeline using templates","text":"In\u00a0[\u00a0]: Copied! <pre>#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n</pre> #@title Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # https://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. <p>Note: We recommend running this tutorial in a Colab notebook, with no setup required!  Just click \"Run in Google Colab\".</p>        View on TensorFlow.org             Run in Google Colab             View source on GitHub             Download notebook      <p>Warning: Estimators are not recommended for new code.  Estimators run &lt;a href=\"https://www.tensorflow.org/api_docs/python/tf/compat/v1/Session\\\"&gt;<code>v1.Session</code>-style code which is more difficult to write correctly, and can behave unexpectedly, especially when combined with TF 2 code. Estimators do fall under our compatibility guarantees, but will receive no fixes other than security vulnerabilities. See the migration guide for details.</p> <p>Install <code>tfx</code> python package with <code>kfp</code> extra requirement.</p> In\u00a0[\u00a0]: Copied! <pre>import sys\n# Use the latest version of pip.\n!pip install --upgrade pip\n# Install tfx and kfp Python packages.\n# TFX has a constraint of 1.16 due to the removal of tf.estimator support.\n!pip install --upgrade \"tfx[kfp]&lt;1.16\"\n</pre> import sys # Use the latest version of pip. !pip install --upgrade pip # Install tfx and kfp Python packages. # TFX has a constraint of 1.16 due to the removal of tf.estimator support. !pip install --upgrade \"tfx[kfp]&lt;1.16\" <p>Let's check the versions of TFX.</p> In\u00a0[\u00a0]: Copied! <pre>!python3 -c \"from tfx import version ; print('TFX version: {}'.format(version.__version__))\"\n</pre> !python3 -c \"from tfx import version ; print('TFX version: {}'.format(version.__version__))\" <p>In AI Platform Pipelines, TFX is running in a hosted Kubernetes environment using Kubeflow Pipelines.</p> <p>Let's set some environment variables to use Kubeflow Pipelines.</p> <p>First, get your GCP project ID.</p> In\u00a0[\u00a0]: Copied! <pre># Read GCP project id from env.\nshell_output=!gcloud config list --format 'value(core.project)' 2&gt;/dev/null\nGOOGLE_CLOUD_PROJECT=shell_output[0]\n%env GOOGLE_CLOUD_PROJECT={GOOGLE_CLOUD_PROJECT}\nprint(\"GCP project ID:\" + GOOGLE_CLOUD_PROJECT)\n</pre> # Read GCP project id from env. shell_output=!gcloud config list --format 'value(core.project)' 2&gt;/dev/null GOOGLE_CLOUD_PROJECT=shell_output[0] %env GOOGLE_CLOUD_PROJECT={GOOGLE_CLOUD_PROJECT} print(\"GCP project ID:\" + GOOGLE_CLOUD_PROJECT) <p>We also need to access your KFP cluster. You can access it in your Google Cloud Console under \"AI Platform &gt; Pipeline\" menu. The \"endpoint\" of the KFP cluster can be found from the URL of the Pipelines dashboard, or you can get it from the URL of the Getting Started page where you launched this notebook. Let's create an <code>ENDPOINT</code> environment variable and set it to the KFP cluster endpoint. ENDPOINT should contain only the hostname part of the URL. For example, if the URL of the KFP dashboard is <code>https://1e9deb537390ca22-dot-asia-east1.pipelines.googleusercontent.com/#/start</code>, ENDPOINT value becomes <code>1e9deb537390ca22-dot-asia-east1.pipelines.googleusercontent.com</code>.</p> <p>NOTE: You MUST set your ENDPOINT value below.</p> In\u00a0[\u00a0]: Copied! <pre># This refers to the KFP cluster endpoint\nENDPOINT='' # Enter your ENDPOINT here.\nif not ENDPOINT:\n    from absl import logging\n    logging.error('Set your ENDPOINT in this cell.')\n</pre> # This refers to the KFP cluster endpoint ENDPOINT='' # Enter your ENDPOINT here. if not ENDPOINT:     from absl import logging     logging.error('Set your ENDPOINT in this cell.') <p>Set the image name as <code>tfx-pipeline</code> under the current GCP project.</p> In\u00a0[\u00a0]: Copied! <pre># Docker image name for the pipeline image.\nCUSTOM_TFX_IMAGE='gcr.io/' + GOOGLE_CLOUD_PROJECT + '/tfx-pipeline'\n</pre> # Docker image name for the pipeline image. CUSTOM_TFX_IMAGE='gcr.io/' + GOOGLE_CLOUD_PROJECT + '/tfx-pipeline' <p>And, it's done. We are ready to create a pipeline.</p> In\u00a0[\u00a0]: Copied! <pre>PIPELINE_NAME=\"my_pipeline\"\nimport os\nPROJECT_DIR=os.path.join(os.path.expanduser(\"~\"),\"imported\",PIPELINE_NAME)\n</pre> PIPELINE_NAME=\"my_pipeline\" import os PROJECT_DIR=os.path.join(os.path.expanduser(\"~\"),\"imported\",PIPELINE_NAME) <p>TFX includes the <code>taxi</code> template with the TFX python package. If you are planning to solve a point-wise prediction problem, including classification and regresssion, this template could be used as a starting point.</p> <p>The <code>tfx template copy</code> CLI command copies predefined template files into your project directory.</p> In\u00a0[\u00a0]: Copied! <pre>!tfx template copy \\\n  --pipeline-name={PIPELINE_NAME} \\\n  --destination-path={PROJECT_DIR} \\\n  --model=taxi\n</pre> !tfx template copy \\   --pipeline-name={PIPELINE_NAME} \\   --destination-path={PROJECT_DIR} \\   --model=taxi <p>Change the working directory context in this notebook to the project directory.</p> In\u00a0[\u00a0]: Copied! <pre>%cd {PROJECT_DIR}\n</pre> %cd {PROJECT_DIR} <p>NOTE: Don't forget to change directory in <code>File Browser</code> on the left by clicking into the project directory once it is created.</p> <p>You might notice that there are some files with <code>_test.py</code> in their name. These are unit tests of the pipeline and it is recommended to add more unit tests as you implement your own pipelines. You can run unit tests by supplying the module name of test files with <code>-m</code> flag. You can usually get a module name by deleting <code>.py</code> extension and replacing <code>/</code> with <code>.</code>.  For example:</p> In\u00a0[\u00a0]: Copied! <pre>!{sys.executable} -m models.features_test\n!{sys.executable} -m models.keras_model.model_test\n</pre> !{sys.executable} -m models.features_test !{sys.executable} -m models.keras_model.model_test  <p>Let's upload our sample data to GCS bucket so that we can use it in our pipeline later.</p> In\u00a0[\u00a0]: Copied! <pre>!gsutil cp data/data.csv gs://{GOOGLE_CLOUD_PROJECT}-kubeflowpipelines-default/tfx-template/data/taxi/data.csv\n</pre> !gsutil cp data/data.csv gs://{GOOGLE_CLOUD_PROJECT}-kubeflowpipelines-default/tfx-template/data/taxi/data.csv <p>Let's create a TFX pipeline using the <code>tfx pipeline create</code> command.</p> <p>Note: When creating a pipeline for KFP, we need a container image which will be used to run our pipeline. And <code>skaffold</code> will build the image for us. Because skaffold pulls base images from the docker hub, it will take 5~10 minutes when we build the image for the first time, but it will take much less time from the second build.</p> In\u00a0[\u00a0]: Copied! <pre>!tfx pipeline create  --pipeline-path=kubeflow_runner.py --endpoint={ENDPOINT} \\\n--build-image\n</pre> !tfx pipeline create  --pipeline-path=kubeflow_runner.py --endpoint={ENDPOINT} \\ --build-image <p>While creating a pipeline, <code>Dockerfile</code> will be generated to build a Docker image. Don't forget to add it to the source control system (for example, git) along with other source files.</p> <p>NOTE: <code>kubeflow</code> will be automatically selected as an orchestration engine if <code>airflow</code> is not installed and <code>--engine</code> is not specified.</p> <p>Now start an execution run with the newly created pipeline using the <code>tfx run create</code> command.</p> In\u00a0[\u00a0]: Copied! <pre>!tfx run create --pipeline-name={PIPELINE_NAME} --endpoint={ENDPOINT}\n</pre> !tfx run create --pipeline-name={PIPELINE_NAME} --endpoint={ENDPOINT} <p>Or, you can also run the pipeline in the KFP Dashboard.  The new execution run will be listed under Experiments in the KFP Dashboard.  Clicking into the experiment will allow you to monitor progress and visualize the artifacts created during the execution run.</p> <p>However, we recommend visiting the KFP Dashboard. You can access the KFP Dashboard from the Cloud AI Platform Pipelines menu in Google Cloud Console. Once you visit the dashboard, you will be able to find the pipeline, and access a wealth of information about the pipeline. For example, you can find your runs under the Experiments menu, and when you open your execution run under Experiments you can find all your artifacts from the pipeline under Artifacts menu.</p> <p>Note: If your pipeline run fails, you can see detailed logs for each TFX component in the Experiments tab in the KFP Dashboard.</p> <p>One of the major sources of failure is permission related problems. Please make sure your KFP cluster has permissions to access Google Cloud APIs. This can be configured when you create a KFP cluster in GCP, or see Troubleshooting document in GCP.</p> In\u00a0[\u00a0]: Copied! <pre># Update the pipeline\n!tfx pipeline update \\\n--pipeline-path=kubeflow_runner.py \\\n--endpoint={ENDPOINT}\n# You can run the pipeline the same way.\n!tfx run create --pipeline-name {PIPELINE_NAME} --endpoint={ENDPOINT}\n</pre> # Update the pipeline !tfx pipeline update \\ --pipeline-path=kubeflow_runner.py \\ --endpoint={ENDPOINT} # You can run the pipeline the same way. !tfx run create --pipeline-name {PIPELINE_NAME} --endpoint={ENDPOINT} In\u00a0[\u00a0]: Copied! <pre>!tfx pipeline update \\\n--pipeline-path=kubeflow_runner.py \\\n--endpoint={ENDPOINT}\n!tfx run create --pipeline-name {PIPELINE_NAME} --endpoint={ENDPOINT}\n</pre> !tfx pipeline update \\ --pipeline-path=kubeflow_runner.py \\ --endpoint={ENDPOINT} !tfx run create --pipeline-name {PIPELINE_NAME} --endpoint={ENDPOINT} <p>When this execution run finishes successfully, you have now created and run your first TFX pipeline in AI Platform Pipelines!</p> <p>NOTE: If we changed anything in the model code, we have to rebuild the container image, too. We can trigger rebuild using <code>--build-image</code> flag in the <code>pipeline update</code> command.</p> <p>NOTE: You might have noticed that every time we create a pipeline run, every component runs again and again even though the input and the parameters were not changed. It is waste of time and resources, and you can skip those executions with pipeline caching. You can enable caching by specifying <code>enable_cache=True</code> for the <code>Pipeline</code> object in <code>pipeline.py</code>.</p> In\u00a0[\u00a0]: Copied! <pre>!tfx pipeline update \\\n--pipeline-path=kubeflow_runner.py \\\n--endpoint={ENDPOINT}\n!tfx run create --pipeline-name {PIPELINE_NAME} --endpoint={ENDPOINT}\n</pre> !tfx pipeline update \\ --pipeline-path=kubeflow_runner.py \\ --endpoint={ENDPOINT} !tfx run create --pipeline-name {PIPELINE_NAME} --endpoint={ENDPOINT} In\u00a0[\u00a0]: Copied! <pre>!tfx pipeline update \\\n--pipeline-path=kubeflow_runner.py \\\n--endpoint={ENDPOINT}\n!tfx run create --pipeline-name {PIPELINE_NAME} --endpoint={ENDPOINT}\n</pre> !tfx pipeline update \\ --pipeline-path=kubeflow_runner.py \\ --endpoint={ENDPOINT} !tfx run create --pipeline-name {PIPELINE_NAME} --endpoint={ENDPOINT} <p>You can find your Dataflow jobs in Dataflow in Cloud Console.</p> In\u00a0[\u00a0]: Copied! <pre>!tfx pipeline update \\\n--pipeline-path=kubeflow_runner.py \\\n--endpoint={ENDPOINT}\n!tfx run create --pipeline-name {PIPELINE_NAME} --endpoint={ENDPOINT}\n</pre> !tfx pipeline update \\ --pipeline-path=kubeflow_runner.py \\ --endpoint={ENDPOINT} !tfx run create --pipeline-name {PIPELINE_NAME} --endpoint={ENDPOINT} <p>You can find your training jobs in Cloud AI Platform Jobs. If your pipeline completed successfully, you can find your model in Cloud AI Platform Models.</p>"},{"location":"tutorials/tfx/template/#copyright-2020-the-tensorflow-authors","title":"Copyright 2020 The TensorFlow Authors.\u00b6","text":""},{"location":"tutorials/tfx/template/#create-a-tfx-pipeline-using-templates","title":"Create a TFX pipeline using templates\u00b6","text":""},{"location":"tutorials/tfx/template/#introduction","title":"Introduction\u00b6","text":"<p>This document will provide instructions to create a TensorFlow Extended (TFX) pipeline using templates which are provided with TFX Python package. Many of the instructions are Linux shell commands, which will run on an AI Platform Notebooks instance. Corresponding Jupyter Notebook code cells which invoke those commands using <code>!</code> are provided.</p> <p>You will build a pipeline using Taxi Trips dataset released by the City of Chicago. We strongly encourage you to try building your own pipeline using your dataset by utilizing this pipeline as a baseline.</p>"},{"location":"tutorials/tfx/template/#step-1-set-up-your-environment","title":"Step 1. Set up your environment.\u00b6","text":"<p>AI Platform Pipelines will prepare a development environment to build a pipeline, and a Kubeflow Pipeline cluster to run the newly built pipeline.</p> <p>NOTE: To select a particular TensorFlow version, or select a GPU instance, create a TensorFlow pre-installed instance in AI Platform Notebooks.</p>"},{"location":"tutorials/tfx/template/#step-2-copy-the-predefined-template-to-your-project-directory","title":"Step 2. Copy the predefined template to your project directory.\u00b6","text":"<p>In this step, we will create a working pipeline project directory and files by copying additional files from a predefined template.</p> <p>You may give your pipeline a different name by changing the <code>PIPELINE_NAME</code> below. This will also become the name of the project directory where your files will be put.</p>"},{"location":"tutorials/tfx/template/#step-3-browse-your-copied-source-files","title":"Step 3. Browse your copied source files\u00b6","text":"<p>The TFX template provides basic scaffold files to build a pipeline, including Python source code, sample data, and Jupyter Notebooks to analyse the output of the pipeline. The <code>taxi</code> template uses the same Chicago Taxi dataset and ML model as the Airflow Tutorial.</p> <p>Here is brief introduction to each of the Python files.</p> <ul> <li><code>pipeline</code> - This directory contains the definition of the pipeline<ul> <li><code>configs.py</code> \u2014 defines common constants for pipeline runners</li> <li><code>pipeline.py</code> \u2014 defines TFX components and a pipeline</li> </ul> </li> <li><code>models</code> - This directory contains ML model definitions.<ul> <li><code>features.py</code>, <code>features_test.py</code> \u2014 defines features for the model</li> <li><code>preprocessing.py</code>, <code>preprocessing_test.py</code> \u2014 defines preprocessing jobs using <code>tf::Transform</code></li> <li><code>estimator</code> - This directory contains an Estimator based model.<ul> <li><code>constants.py</code> \u2014 defines constants of the model</li> <li><code>model.py</code>, <code>model_test.py</code> \u2014 defines DNN model using TF estimator</li> </ul> </li> <li><code>keras</code> - This directory contains a Keras based model.<ul> <li><code>constants.py</code> \u2014 defines constants of the model</li> <li><code>model.py</code>, <code>model_test.py</code> \u2014 defines DNN model using Keras</li> </ul> </li> </ul> </li> <li><code>local_runner.py</code>, <code>kubeflow_runner.py</code> \u2014 define runners for each orchestration engine</li> </ul>"},{"location":"tutorials/tfx/template/#step-4-run-your-first-tfx-pipeline","title":"Step 4. Run your first TFX pipeline\u00b6","text":"<p>Components in the TFX pipeline will generate outputs for each run as ML Metadata Artifacts, and they need to be stored somewhere. You can use any storage which the KFP cluster can access, and for this example we will use Google Cloud Storage (GCS). A default GCS bucket should have been created automatically. Its name will be <code>&lt;your-project-id&gt;-kubeflowpipelines-default</code>.</p>"},{"location":"tutorials/tfx/template/#step-5-add-components-for-data-validation","title":"Step 5. Add components for data validation.\u00b6","text":"<p>In this step, you will add components for data validation including <code>StatisticsGen</code>, <code>SchemaGen</code>, and <code>ExampleValidator</code>. If you are interested in data validation, please see Get started with Tensorflow Data Validation.</p> <p>Double-click to change directory to <code>pipeline</code> and double-click again to open <code>pipeline.py</code>. Find and uncomment the 3 lines which add <code>StatisticsGen</code>, <code>SchemaGen</code>, and <code>ExampleValidator</code> to the pipeline. (Tip: search for comments containing <code>TODO(step 5):</code>).  Make sure to save <code>pipeline.py</code> after you edit it.</p> <p>You now need to update the existing pipeline with modified pipeline definition. Use the <code>tfx pipeline update</code> command to update your pipeline, followed by the <code>tfx run create</code> command to create a new execution run of your updated pipeline.</p>"},{"location":"tutorials/tfx/template/#check-pipeline-outputs","title":"Check pipeline outputs\u00b6","text":"<p>Visit the KFP dashboard to find pipeline outputs in the page for your pipeline run. Click the Experiments tab on the left, and All runs in the Experiments page. You should be able to find the latest run under the name of your pipeline.</p>"},{"location":"tutorials/tfx/template/#step-6-add-components-for-training","title":"Step 6. Add components for training.\u00b6","text":"<p>In this step, you will add components for training and model validation including <code>Transform</code>, <code>Trainer</code>, <code>Resolver</code>, <code>Evaluator</code>, and <code>Pusher</code>.</p> <p>Double-click to open <code>pipeline.py</code>. Find and uncomment the 5 lines which add <code>Transform</code>, <code>Trainer</code>, <code>Resolver</code>, <code>Evaluator</code> and <code>Pusher</code> to the pipeline. (Tip: search for <code>TODO(step 6):</code>)</p> <p>As you did before, you now need to update the existing pipeline with the modified pipeline definition. The instructions are the same as Step 5. Update the pipeline using <code>tfx pipeline update</code>, and create an execution run using <code>tfx run create</code>.</p>"},{"location":"tutorials/tfx/template/#step-7-optional-try-bigqueryexamplegen","title":"Step 7. (Optional) Try BigQueryExampleGen\u00b6","text":"<p>BigQuery is a serverless, highly scalable, and cost-effective cloud data warehouse. BigQuery can be used as a source for training examples in TFX. In this step, we will add <code>BigQueryExampleGen</code> to the pipeline.</p> <p>Double-click to open <code>pipeline.py</code>. Comment out <code>CsvExampleGen</code> and uncomment the line which creates an instance of <code>BigQueryExampleGen</code>. You also need to uncomment the <code>query</code> argument of the <code>create_pipeline</code> function.</p> <p>We need to specify which GCP project to use for BigQuery, and this is done by setting <code>--project</code> in <code>beam_pipeline_args</code> when creating a pipeline.</p> <p>Double-click to open <code>configs.py</code>. Uncomment the definition of <code>GOOGLE_CLOUD_REGION</code>, <code>BIG_QUERY_WITH_DIRECT_RUNNER_BEAM_PIPELINE_ARGS</code> and <code>BIG_QUERY_QUERY</code>. You should replace the region value in this file with the correct values for your GCP project.</p> <p>Note: You MUST set your GCP region in the <code>configs.py</code> file before proceeding.</p> <p>Change directory one level up. Click the name of the directory above the file list. The name of the directory is the name of the pipeline which is <code>my_pipeline</code> if you didn't change.</p> <p>Double-click to open <code>kubeflow_runner.py</code>. Uncomment two arguments, <code>query</code> and <code>beam_pipeline_args</code>, for the <code>create_pipeline</code> function.</p> <p>Now the pipeline is ready to use BigQuery as an example source. Update the pipeline as before and create a new execution run as we did in step 5 and 6.</p>"},{"location":"tutorials/tfx/template/#step-8-optional-try-dataflow-with-kfp","title":"Step 8. (Optional) Try Dataflow with KFP\u00b6","text":"<p>Several TFX Components uses Apache Beam to implement data-parallel pipelines, and it means that you can distribute data processing workloads using Google Cloud Dataflow. In this step, we will set the Kubeflow orchestrator to use dataflow as the data processing back-end for Apache Beam.</p> <p>Double-click <code>pipeline</code> to change directory, and double-click to open <code>configs.py</code>. Uncomment the definition of <code>GOOGLE_CLOUD_REGION</code>, and <code>DATAFLOW_BEAM_PIPELINE_ARGS</code>.</p> <p>Change directory one level up. Click the name of the directory above the file list. The name of the directory is the name of the pipeline which is <code>my_pipeline</code> if you didn't change.</p> <p>Double-click to open <code>kubeflow_runner.py</code>. Uncomment <code>beam_pipeline_args</code>. (Also make sure to comment out current <code>beam_pipeline_args</code> that you added in Step 7.)</p> <p>Now the pipeline is ready to use Dataflow. Update the pipeline and create an execution run as we did in step 5 and 6.</p>"},{"location":"tutorials/tfx/template/#step-9-optional-try-cloud-ai-platform-training-and-prediction-with-kfp","title":"Step 9. (Optional) Try Cloud AI Platform Training and Prediction with KFP\u00b6","text":"<p>TFX interoperates with several managed GCP services, such as Cloud AI Platform for Training and Prediction. You can set your <code>Trainer</code> component to use Cloud AI Platform Training, a managed service for training ML models. Moreover, when your model is built and ready to be served, you can push your model to Cloud AI Platform Prediction for serving. In this step, we will set our <code>Trainer</code> and <code>Pusher</code> component to use Cloud AI Platform services.</p> <p>Before editing files, you might first have to enable AI Platform Training &amp; Prediction API.</p> <p>Double-click <code>pipeline</code> to change directory, and double-click to open <code>configs.py</code>. Uncomment the definition of <code>GOOGLE_CLOUD_REGION</code>, <code>GCP_AI_PLATFORM_TRAINING_ARGS</code> and <code>GCP_AI_PLATFORM_SERVING_ARGS</code>. We will use our custom built container image to train a model in Cloud AI Platform Training, so we should set <code>masterConfig.imageUri</code> in <code>GCP_AI_PLATFORM_TRAINING_ARGS</code> to the same value as <code>CUSTOM_TFX_IMAGE</code> above.</p> <p>Change directory one level up, and double-click to open <code>kubeflow_runner.py</code>. Uncomment <code>ai_platform_training_args</code> and <code>ai_platform_serving_args</code>.</p> <p>Update the pipeline and create an execution run as we did in step 5 and 6.</p>"},{"location":"tutorials/tfx/template/#step-10-ingest-your-data-to-the-pipeline","title":"Step 10. Ingest YOUR data to the pipeline\u00b6","text":"<p>We made a pipeline for a model using the Chicago Taxi dataset. Now it's time to put your data into the pipeline.</p> <p>Your data can be stored anywhere your pipeline can access, including GCS, or BigQuery. You will need to modify the pipeline definition to access your data.</p> <ol> <li>If your data is stored in files, modify the <code>DATA_PATH</code> in <code>kubeflow_runner.py</code> or <code>local_runner.py</code> and set it to the location of your files. If your data is stored in BigQuery, modify <code>BIG_QUERY_QUERY</code> in <code>pipeline/configs.py</code> to correctly query for your data.</li> <li>Add features in <code>models/features.py</code>.</li> <li>Modify <code>models/preprocessing.py</code> to transform input data for training.</li> <li>Modify <code>models/keras/model.py</code> and <code>models/keras/constants.py</code> to describe your ML model.</li> </ol> <ul> <li>You can use an estimator based model, too. Change <code>RUN_FN</code> constant to <code>models.estimator.model.run_fn</code> in <code>pipeline/configs.py</code>.</li> </ul> <p>Please see Trainer component guide for more introduction.</p>"},{"location":"tutorials/tfx/template/#cleaning-up","title":"Cleaning up\u00b6","text":"<p>To clean up all Google Cloud resources used in this project, you can delete the Google Cloud project you used for the tutorial.</p> <p>Alternatively, you can clean up individual resources by visiting each consoles:</p> <ul> <li>Google Cloud Storage</li> <li>Google Container Registry</li> <li>Google Kubernetes Engine</li> </ul>"},{"location":"tutorials/tfx/template_local/","title":"Create a TFX pipeline using templates with Local orchestrator","text":"In\u00a0[\u00a0]: Copied! <pre>#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n</pre> #@title Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # https://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. <p>Note: We recommend running this tutorial in a Colab notebook, with no setup required!  Just click \"Run in Google Colab\".</p>        View on TensorFlow.org             Run in Google Colab             View source on GitHub             Download notebook      <p>Warning: Estimators are not recommended for new code.  Estimators run &lt;a href=\"https://www.tensorflow.org/api_docs/python/tf/compat/v1/Session\\\"&gt;<code>v1.Session</code>-style code which is more difficult to write correctly, and can behave unexpectedly, especially when combined with TF 2 code. Estimators do fall under our compatibility guarantees, but will receive no fixes other than security vulnerabilities. See the migration guide for details.</p> In\u00a0[\u00a0]: Copied! <pre>import sys\n# TFX has a constraint of 1.16 due to the removal of tf.estimator support.\n!{sys.executable} -m pip install --upgrade \"tfx&lt;1.16\"\n</pre> import sys # TFX has a constraint of 1.16 due to the removal of tf.estimator support. !{sys.executable} -m pip install --upgrade \"tfx&lt;1.16\" <p>NOTE: There might be some errors during package installation. For example,</p> <p>ERROR: some-package 0.some_version.1 has requirement other-package!=2.0.,&lt;3,&gt;=1.15, but you'll have other-package 2.0.0 which is incompatible.</p> <p>Please ignore these errors at this moment.</p> In\u00a0[\u00a0]: Copied! <pre># Set `PATH` to include user python binary directory.\nHOME=%env HOME\nPATH=%env PATH\n%env PATH={PATH}:{HOME}/.local/bin\n</pre> # Set `PATH` to include user python binary directory. HOME=%env HOME PATH=%env PATH %env PATH={PATH}:{HOME}/.local/bin <p>Let's check the version of TFX.</p> <pre>python -c \"from tfx import version ; print('TFX version: {}'.format(version.__version__))\"\n</pre> In\u00a0[\u00a0]: Copied! <pre>!python3 -c \"from tfx import version ; print('TFX version: {}'.format(version.__version__))\"\n</pre> !python3 -c \"from tfx import version ; print('TFX version: {}'.format(version.__version__))\" <p>And, it's done. We are ready to create a pipeline.</p> In\u00a0[\u00a0]: Copied! <pre>PIPELINE_NAME=\"my_pipeline\"\nimport os\n# Create a project directory under Colab content directory.\nPROJECT_DIR=os.path.join(os.sep,\"content\",PIPELINE_NAME)\n</pre> PIPELINE_NAME=\"my_pipeline\" import os # Create a project directory under Colab content directory. PROJECT_DIR=os.path.join(os.sep,\"content\",PIPELINE_NAME) <p>TFX includes the <code>taxi</code> template with the TFX python package. If you are planning to solve a point-wise prediction problem, including classification and regresssion, this template could be used as a starting point.</p> <p>The <code>tfx template copy</code> CLI command copies predefined template files into your project directory.</p> <pre>tfx template copy \\\n   --pipeline_name=\"${PIPELINE_NAME}\" \\\n   --destination_path=\"${PROJECT_DIR}\" \\\n   --model=taxi\n</pre> In\u00a0[\u00a0]: Copied! <pre>!tfx template copy \\\n  --pipeline_name={PIPELINE_NAME} \\\n  --destination_path={PROJECT_DIR} \\\n  --model=taxi\n</pre> !tfx template copy \\   --pipeline_name={PIPELINE_NAME} \\   --destination_path={PROJECT_DIR} \\   --model=taxi <p>Change the working directory context in this notebook to the project directory.</p> <pre>cd ${PROJECT_DIR}\n</pre> In\u00a0[\u00a0]: Copied! <pre>%cd {PROJECT_DIR}\n</pre> %cd {PROJECT_DIR} <p>The TFX template provides basic scaffold files to build a pipeline, including Python source code, sample data, and Jupyter Notebooks to analyse the output of the pipeline. The <code>taxi</code> template uses the same Chicago Taxi dataset and ML model as the Airflow Tutorial.</p> <p>In Google Colab, you can browse files by clicking a folder icon on the left. Files should be copied under the project directoy, whose name is <code>my_pipeline</code> in this case. You can click directory names to see the content of the directory, and double-click file names to open them.</p> <p>Here is brief introduction to each of the Python files.</p> <ul> <li><code>pipeline</code> - This directory contains the definition of the pipeline<ul> <li><code>configs.py</code> \u2014 defines common constants for pipeline runners</li> <li><code>pipeline.py</code> \u2014 defines TFX components and a pipeline</li> </ul> </li> <li><code>models</code> - This directory contains ML model definitions.<ul> <li><code>features.py</code>, <code>features_test.py</code> \u2014 defines features for the model</li> <li><code>preprocessing.py</code>, <code>preprocessing_test.py</code> \u2014 defines preprocessing jobs using <code>tf::Transform</code></li> <li><code>estimator</code> - This directory contains an Estimator based model.<ul> <li><code>constants.py</code> \u2014 defines constants of the model</li> <li><code>model.py</code>, <code>model_test.py</code> \u2014 defines DNN model using TF estimator</li> </ul> </li> <li><code>keras</code> - This directory contains a Keras based model.<ul> <li><code>constants.py</code> \u2014 defines constants of the model</li> <li><code>model.py</code>, <code>model_test.py</code> \u2014 defines DNN model using Keras</li> </ul> </li> </ul> </li> <li><code>local_runner.py</code>, <code>kubeflow_runner.py</code> \u2014 define runners for each orchestration engine</li> </ul> <p>You might notice that there are some files with <code>_test.py</code> in their name. These are unit tests of the pipeline and it is recommended to add more unit tests as you implement your own pipelines. You can run unit tests by supplying the module name of test files with <code>-m</code> flag. You can usually get a module name by deleting <code>.py</code> extension and replacing <code>/</code> with <code>.</code>.  For example:</p> <pre>python -m models.features_test\n</pre> In\u00a0[\u00a0]: Copied! <pre>!{sys.executable} -m models.features_test\n!{sys.executable} -m models.keras_model.model_test\n</pre> !{sys.executable} -m models.features_test !{sys.executable} -m models.keras_model.model_test  In\u00a0[\u00a0]: Copied! <pre>!tfx pipeline create --engine=local --pipeline_path=local_runner.py\n</pre> !tfx pipeline create --engine=local --pipeline_path=local_runner.py <p>Then, you can run the created pipeline using <code>run create</code> command.</p> <pre>tfx run create --engine=local --pipeline_name=\"${PIPELINE_NAME}\"\n</pre> In\u00a0[\u00a0]: Copied! <pre>!tfx run create --engine=local --pipeline_name={PIPELINE_NAME}\n</pre> !tfx run create --engine=local --pipeline_name={PIPELINE_NAME} <p>If successful, you'll see <code>Component CsvExampleGen is finished.</code> When you copy the template, only one component, CsvExampleGen, is included in the pipeline.</p> In\u00a0[\u00a0]: Copied! <pre># Update the pipeline\n!tfx pipeline update --engine=local --pipeline_path=local_runner.py\n# You can run the pipeline the same way.\n!tfx run create --engine local --pipeline_name {PIPELINE_NAME}\n</pre> # Update the pipeline !tfx pipeline update --engine=local --pipeline_path=local_runner.py # You can run the pipeline the same way. !tfx run create --engine local --pipeline_name {PIPELINE_NAME} <p>You should be able to see the output log from the added components. Our pipeline creates output artifacts in <code>tfx_pipeline_output/my_pipeline</code> directory.</p> In\u00a0[\u00a0]: Copied! <pre>!tfx pipeline update --engine=local --pipeline_path=local_runner.py\n!tfx run create --engine local --pipeline_name {PIPELINE_NAME}\n</pre> !tfx pipeline update --engine=local --pipeline_path=local_runner.py !tfx run create --engine local --pipeline_name {PIPELINE_NAME} <p>When this execution run finishes successfully, you have now created and run your first TFX pipeline using Local orchestrator!</p> <p>NOTE: You might have noticed that every time we create a pipeline run, every component runs again and again even though the input and the parameters were not changed. It is waste of time and resources, and you can skip those executions with pipeline caching. You can enable caching by specifying <code>enable_cache=True</code> for the <code>Pipeline</code> object in <code>pipeline.py</code>.</p> In\u00a0[\u00a0]: Copied! <pre>if 'google.colab' in sys.modules:\n  from google.colab import auth\n  auth.authenticate_user()\n  print('Authenticated')\n</pre> if 'google.colab' in sys.modules:   from google.colab import auth   auth.authenticate_user()   print('Authenticated')  <p>You should specify your GCP project name to access BigQuery resources using TFX. Set <code>GOOGLE_CLOUD_PROJECT</code> environment variable to your project name.</p> <pre>export GOOGLE_CLOUD_PROJECT=YOUR_PROJECT_NAME_HERE\n</pre> In\u00a0[\u00a0]: Copied! <pre># Set your project name below.\n# WARNING! ENTER your project name before running this cell.\n%env GOOGLE_CLOUD_PROJECT=YOUR_PROJECT_NAME_HERE\n</pre> # Set your project name below. # WARNING! ENTER your project name before running this cell. %env GOOGLE_CLOUD_PROJECT=YOUR_PROJECT_NAME_HERE <p>Open <code>pipeline/pipeline.py</code>. Comment out <code>CsvExampleGen</code> and uncomment the line which create an instance of <code>BigQueryExampleGen</code>. You also need to uncomment <code>query</code> argument of the <code>create_pipeline</code> function.</p> <p>We need to specify which GCP project to use for BigQuery again, and this is done by setting <code>--project</code> in <code>beam_pipeline_args</code> when creating a pipeline.</p> <p>Open <code>pipeline/configs.py</code>. Uncomment the definition of <code>BIG_QUERY__WITH_DIRECT_RUNNER_BEAM_PIPELINE_ARGS</code> and <code>BIG_QUERY_QUERY</code>. You should replace the project id and the region value in this file with the correct values for your GCP project.</p> <p>Open <code>local_runner.py</code>. Uncomment two arguments, <code>query</code> and <code>beam_pipeline_args</code>, for create_pipeline() method.</p> <p>Now the pipeline is ready to use BigQuery as an example source. Update the pipeline and create a run as we did in step 5 and 6.</p> In\u00a0[\u00a0]: Copied! <pre>!tfx pipeline update --engine=local --pipeline_path=local_runner.py\n!tfx run create --engine local --pipeline_name {PIPELINE_NAME}\n</pre> !tfx pipeline update --engine=local --pipeline_path=local_runner.py !tfx run create --engine local --pipeline_name {PIPELINE_NAME}"},{"location":"tutorials/tfx/template_local/#copyright-2020-the-tensorflow-authors","title":"Copyright 2020 The TensorFlow Authors.\u00b6","text":""},{"location":"tutorials/tfx/template_local/#create-a-tfx-pipeline-using-templates-with-local-orchestrator","title":"Create a TFX pipeline using templates with Local orchestrator\u00b6","text":""},{"location":"tutorials/tfx/template_local/#introduction","title":"Introduction\u00b6","text":"<p>This document will provide instructions to create a TensorFlow Extended (TFX) pipeline using templates which are provided with TFX Python package. Most of instructions are Linux shell commands, and corresponding Jupyter Notebook code cells which invoke those commands using <code>!</code> are provided.</p> <p>You will build a pipeline using Taxi Trips dataset released by the City of Chicago. We strongly encourage you to try to build your own pipeline using your dataset by utilizing this pipeline as a baseline.</p> <p>We will build a pipeline which runs on local environment. If you are interested in using Kubeflow orchestrator on Google Cloud, please see TFX on Cloud AI Platform Pipelines tutorial.</p>"},{"location":"tutorials/tfx/template_local/#prerequisites","title":"Prerequisites\u00b6","text":"<ul> <li>Linux / MacOS</li> <li>Python &gt;= 3.5.3</li> </ul> <p>You can get all prerequisites easily by running this notebook on Google Colab.</p>"},{"location":"tutorials/tfx/template_local/#step-1-set-up-your-environment","title":"Step 1. Set up your environment.\u00b6","text":"<p>Throughout this document, we will present commands twice. Once as a copy-and-paste-ready shell command, once as a jupyter notebook cell. If you are using Colab, just skip shell script block and execute notebook cells.</p> <p>You should prepare a development environment to build a pipeline.</p> <p>Install <code>tfx</code> python package. We recommend use of <code>virtualenv</code> in the local environment. You can use following shell script snippet to set up your environment.</p> <pre># Create a virtualenv for tfx.\nvirtualenv -p python3 venv\nsource venv/bin/activate\n# Install python packages.\npython -m pip install --upgrade \"tfx&lt;2\"\n</pre> <p>If you are using colab:</p>"},{"location":"tutorials/tfx/template_local/#step-2-copy-predefined-template-to-your-project-directory","title":"Step 2. Copy predefined template to your project directory.\u00b6","text":"<p>In this step, we will create a working pipeline project directory and files by copying additional files from a predefined template.</p> <p>You may give your pipeline a different name by changing the <code>PIPELINE_NAME</code> below. This will also become the name of the project directory where your files will be put.</p> <pre>export PIPELINE_NAME=\"my_pipeline\"\nexport PROJECT_DIR=~/tfx/${PIPELINE_NAME}\n</pre>"},{"location":"tutorials/tfx/template_local/#step-3-browse-your-copied-source-files","title":"Step 3. Browse your copied source files.\u00b6","text":""},{"location":"tutorials/tfx/template_local/#step-4-run-your-first-tfx-pipeline","title":"Step 4. Run your first TFX pipeline\u00b6","text":"<p>You can create a pipeline using <code>pipeline create</code> command.</p> <pre>tfx pipeline create --engine=local --pipeline_path=local_runner.py\n</pre>"},{"location":"tutorials/tfx/template_local/#step-5-add-components-for-data-validation","title":"Step 5. Add components for data validation.\u00b6","text":"<p>In this step, you will add components for data validation including <code>StatisticsGen</code>, <code>SchemaGen</code>, and <code>ExampleValidator</code>. If you are interested in data validation, please see Get started with Tensorflow Data Validation.</p> <p>We will modify copied pipeline definition in <code>pipeline/pipeline.py</code>. If you are working on your local environment, use your favorite editor to edit the file. If you are working on Google Colab,</p> <p>Click folder icon on the left to open <code>Files</code> view.</p> <p>Click <code>my_pipeline</code> to open the directory and click <code>pipeline</code> directory to open and double-click <code>pipeline.py</code> to open the file.</p> <p>Find and uncomment the 3 lines which add <code>StatisticsGen</code>, <code>SchemaGen</code>, and <code>ExampleValidator</code> to the pipeline. (Tip: find comments containing <code>TODO(step 5):</code>).</p> <p>Your change will be saved automatically in a few seconds. Make sure that the <code>*</code> mark in front of the <code>pipeline.py</code> disappeared in the tab title. There is no save button or shortcut for the file editor in Colab. Python files in file editor can be saved to the runtime environment even in <code>playground</code> mode.</p> <p>You now need to update the existing pipeline with modified pipeline definition. Use the <code>tfx pipeline update</code> command to update your pipeline, followed by the <code>tfx run create</code> command to create a new execution run of your updated pipeline.</p> <pre># Update the pipeline\ntfx pipeline update --engine=local --pipeline_path=local_runner.py\n# You can run the pipeline the same way.\ntfx run create --engine local --pipeline_name \"${PIPELINE_NAME}\"\n</pre>"},{"location":"tutorials/tfx/template_local/#step-6-add-components-for-training","title":"Step 6. Add components for training.\u00b6","text":"<p>In this step, you will add components for training and model validation including <code>Transform</code>, <code>Trainer</code>, <code>Resolver</code>, <code>Evaluator</code>, and <code>Pusher</code>.</p> <p>Open <code>pipeline/pipeline.py</code>. Find and uncomment 5 lines which add <code>Transform</code>, <code>Trainer</code>, <code>Resolver</code>, <code>Evaluator</code> and <code>Pusher</code> to the pipeline. (Tip: find <code>TODO(step 6):</code>)</p> <p>As you did before, you now need to update the existing pipeline with the modified pipeline definition. The instructions are the same as Step 5. Update the pipeline using <code>tfx pipeline update</code>, and create an execution run using <code>tfx run create</code>.</p> <pre>tfx pipeline update --engine=local --pipeline_path=local_runner.py\ntfx run create --engine local --pipeline_name \"${PIPELINE_NAME}\"\n</pre>"},{"location":"tutorials/tfx/template_local/#step-7-optional-try-bigqueryexamplegen","title":"Step 7. (Optional) Try BigQueryExampleGen.\u00b6","text":"<p>[BigQuery] is a serverless, highly scalable, and cost-effective cloud data warehouse. BigQuery can be used as a source for training examples in TFX. In this step, we will add <code>BigQueryExampleGen</code> to the pipeline.</p> <p>You need a Google Cloud Platform account to use BigQuery. Please prepare a GCP project.</p> <p>Login to your project using colab auth library or <code>gcloud</code> utility.</p> <pre># You need `gcloud` tool to login in local shell environment.\ngcloud auth login\n</pre>"},{"location":"tutorials/tfx/template_local/#whats-next-ingest-your-data-to-the-pipeline","title":"What's next: Ingest YOUR data to the pipeline.\u00b6","text":"<p>We made a pipeline for a model using the Chicago Taxi dataset. Now it's time to put your data into the pipeline.</p> <p>Your data can be stored anywhere your pipeline can access, including GCS, or BigQuery. You will need to modify the pipeline definition to access your data.</p> <ol> <li>If your data is stored in files, modify the <code>DATA_PATH</code> in <code>kubeflow_runner.py</code> or <code>local_runner.py</code> and set it to the location of your files. If your data is stored in BigQuery, modify <code>BIG_QUERY_QUERY</code> in <code>pipeline/configs.py</code> to correctly query for your data.</li> <li>Add features in <code>models/features.py</code>.</li> <li>Modify <code>models/preprocessing.py</code> to transform input data for training.</li> <li>Modify <code>models/keras/model.py</code> and <code>models/keras/constants.py</code> to describe your ML model.</li> </ol> <ul> <li>You can use an estimator based model, too. Change <code>RUN_FN</code> constant to <code>models.estimator.model.run_fn</code> in <code>pipeline/configs.py</code>.</li> </ul> <p>Please see Trainer component guide for more introduction.</p>"},{"location":"tutorials/tfx/tfx_for_mobile/","title":"TFX for Mobile","text":""},{"location":"tutorials/tfx/tfx_for_mobile/#introduction","title":"Introduction","text":"<p>This guide demonstrates how Tensorflow Extended (TFX) can create and evaluate machine learning models that will be deployed on-device. TFX now provides native support for TFLite, which makes it possible to perform highly efficient inference on mobile devices.</p> <p>This guide walks you through the changes that can be made to any pipeline to generate and evaluate TFLite models. We provide a complete example here, demonstrating how TFX can train and evaluate TFLite models that are trained off of the MNIST dataset. Further, we show how the same pipeline can be used to simulataneously export both the standard Keras-based SavedModel as well as the TFLite one, allowing users to compare the quality of the two.</p> <p>We assume you are familiar with TFX, our components, and our pipelines. If not, then please see this tutorial.</p>"},{"location":"tutorials/tfx/tfx_for_mobile/#steps","title":"Steps","text":"<p>Only two steps are required to create and evaluate a TFLite model in TFX. The first step is invoking the TFLite rewriter within the context of the TFX Trainer to convert the trained TensorFlow model into a TFLite one. The second step is configuring the Evaluator to evaluate TFLite models. We now discuss each in turn.</p>"},{"location":"tutorials/tfx/tfx_for_mobile/#invoking-the-tflite-rewriter-within-the-trainer","title":"Invoking the TFLite rewriter within the Trainer.","text":"<p>The TFX Trainer expects a user-defined <code>run_fn</code> to be specified in a module file. This <code>run_fn</code> defines the model to be trained, trains it for the specified number of iterations, and exports the trained model.</p> <p>As shown in the code below, we must first create a signature that takes a <code>Tensor</code> for every feature as input. Note that this is a departure from most existing models in TFX, which take serialized tf.Example protos as input.</p> <pre><code> signatures = {\n      'serving_default':\n          _get_serve_tf_examples_fn(\n              model, tf_transform_output).get_concrete_function(\n                  tf.TensorSpec(\n                      shape=[None, 784],\n                      dtype=tf.float32,\n                      name='image_floats'))\n  }\n</code></pre> <p>Then the Keras model is saved as a SavedModel in the same way that it normally is.</p> <pre><code>  temp_saving_model_dir = os.path.join(fn_args.serving_model_dir, 'temp')\n  model.save(temp_saving_model_dir, save_format='tf', signatures=signatures)\n</code></pre> <p>Finally, we create an instance of the TFLite rewriter (<code>tfrw</code>), and invoke it on the SavedModel to obtain the TFLite model. We store this TFLite model in the <code>serving_model_dir</code> provided by the caller of the <code>run_fn</code>. This way, the TFLite model is stored in the location where all downstream TFX components will be expecting to find the model.</p> <pre><code>  tfrw = rewriter_factory.create_rewriter(\n      rewriter_factory.TFLITE_REWRITER, name='tflite_rewriter')\n  converters.rewrite_saved_model(temp_saving_model_dir,\n                                 fn_args.serving_model_dir,\n                                 tfrw,\n                                 rewriter.ModelType.TFLITE_MODEL)\n</code></pre>"},{"location":"tutorials/tfx/tfx_for_mobile/#evaluating-the-tflite-model","title":"Evaluating the TFLite model.","text":"<p>The TFX Evaluator provides the ability to analyze trained models to understand their quality across a wide range of metrics. In addition to analyzing SavedModels, the TFX Evaluator is now able to analyze TFLite models as well.</p> <p>The following code snippet (reproduced from the MNIST pipeline), shows how to configure an Evaluator that analyzes a TFLite model.</p> <pre><code>  # Informs the evaluator that the model is a TFLite model.\n  eval_config_lite.model_specs[0].model_type = 'tf_lite'\n\n  ...\n\n  # Uses TFMA to compute the evaluation statistics over features of a TFLite\n  # model.\n  model_analyzer_lite = Evaluator(\n      examples=example_gen.outputs['examples'],\n      model=trainer_lite.outputs['model'],\n      eval_config=eval_config_lite,\n  ).with_id('mnist_lite')\n</code></pre> <p>As shown above, the only change that we need to make is to set the <code>model_type</code> field to <code>tf_lite</code>. No other configuration changes are required to analyze the TFLite model. Regardless of whether a TFLite model or the a SavedModel is analyzed, the output of the <code>Evaluator</code> will have exactly the same structure.</p> <p>However, please note that the Evaluator assumes that the TFLite model is saved in a file named <code>tflite</code> within trainer_lite.outputs['model'].</p>"},{"location":"tutorials/tfx/gcp/vertex_pipelines_bq/","title":"Reading data from BigQuery with TFX and Vertex Pipelines","text":"In\u00a0[\u00a0]: Copied! <pre>#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n</pre> #@title Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # https://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. <p>Note: We recommend running this tutorial in a Colab notebook, with no setup required!  Just click \"Run in Google Colab\".</p>        View on TensorFlow.org             Run in Google Colab             View source on GitHub             Download notebook      <p>This notebook-based tutorial will use Google Cloud BigQuery as a data source to train an ML model. The ML pipeline will be constructed using TFX and run on Google Cloud Vertex Pipelines.</p> <p>This notebook is based on the TFX pipeline we built in Simple TFX Pipeline for Vertex Pipelines Tutorial. If you have not read that tutorial yet, you should read it before proceeding with this notebook.</p> <p>BigQuery is serverless, highly scalable, and cost-effective multi-cloud data warehouse designed for business agility. TFX can be used to read training data from BigQuery and to publish the trained model to BigQuery.</p> <p>In this tutorial, we will use the <code>BigQueryExampleGen</code> component which reads data from BigQuery to TFX pipelines.</p> <p>This notebook is intended to be run on Google Colab or on AI Platform Notebooks. If you are not using one of these, you can simply click \"Run in Google Colab\" button above.</p> <p>We will install required Python packages including TFX and KFP to author ML pipelines and submit jobs to Vertex Pipelines.</p> In\u00a0[\u00a0]: Copied! <pre># Use the latest version of pip.\n!pip install --upgrade pip\n!pip install --upgrade \"tfx[kfp]&lt;2\"\n</pre> # Use the latest version of pip. !pip install --upgrade pip !pip install --upgrade \"tfx[kfp]&lt;2\" <p>If you are not on Colab, you can restart runtime with following cell.</p> In\u00a0[\u00a0]: Copied! <pre># docs_infra: no_execute\nimport sys\nif not 'google.colab' in sys.modules:\n  # Automatically restart kernel after installs\n  import IPython\n  app = IPython.Application.instance()\n  app.kernel.do_shutdown(True)\n</pre> # docs_infra: no_execute import sys if not 'google.colab' in sys.modules:   # Automatically restart kernel after installs   import IPython   app = IPython.Application.instance()   app.kernel.do_shutdown(True) In\u00a0[\u00a0]: Copied! <pre>import sys\nif 'google.colab' in sys.modules:\n  from google.colab import auth\n  auth.authenticate_user()\n</pre> import sys if 'google.colab' in sys.modules:   from google.colab import auth   auth.authenticate_user() <p>If you are on AI Platform Notebooks, authenticate with Google Cloud before running the next section, by running</p> <pre>gcloud auth login\n</pre> <p>in the Terminal window (which you can open via File &gt; New in the menu). You only need to do this once per notebook instance.</p> <p>Check the package versions.</p> In\u00a0[\u00a0]: Copied! <pre>import tensorflow as tf\nprint('TensorFlow version: {}'.format(tf.__version__))\nfrom tfx import v1 as tfx\nprint('TFX version: {}'.format(tfx.__version__))\nimport kfp\nprint('KFP version: {}'.format(kfp.__version__))\n</pre> import tensorflow as tf print('TensorFlow version: {}'.format(tf.__version__)) from tfx import v1 as tfx print('TFX version: {}'.format(tfx.__version__)) import kfp print('KFP version: {}'.format(kfp.__version__)) In\u00a0[\u00a0]: Copied! <pre>GOOGLE_CLOUD_PROJECT = ''         # &lt;--- ENTER THIS\nGOOGLE_CLOUD_PROJECT_NUMBER = ''  # &lt;--- ENTER THIS\nGOOGLE_CLOUD_REGION = ''          # &lt;--- ENTER THIS\nGCS_BUCKET_NAME = ''              # &lt;--- ENTER THIS\n\nif not (GOOGLE_CLOUD_PROJECT and  GOOGLE_CLOUD_PROJECT_NUMBER and GOOGLE_CLOUD_REGION and GCS_BUCKET_NAME):\n    from absl import logging\n    logging.error('Please set all required parameters.')\n</pre> GOOGLE_CLOUD_PROJECT = ''         # &lt;--- ENTER THIS GOOGLE_CLOUD_PROJECT_NUMBER = ''  # &lt;--- ENTER THIS GOOGLE_CLOUD_REGION = ''          # &lt;--- ENTER THIS GCS_BUCKET_NAME = ''              # &lt;--- ENTER THIS  if not (GOOGLE_CLOUD_PROJECT and  GOOGLE_CLOUD_PROJECT_NUMBER and GOOGLE_CLOUD_REGION and GCS_BUCKET_NAME):     from absl import logging     logging.error('Please set all required parameters.') <p>Set <code>gcloud</code> to use your project.</p> In\u00a0[\u00a0]: Copied! <pre>!gcloud config set project {GOOGLE_CLOUD_PROJECT}\n</pre> !gcloud config set project {GOOGLE_CLOUD_PROJECT} In\u00a0[\u00a0]: Copied! <pre>PIPELINE_NAME = 'penguin-bigquery'\n\n# Path to various pipeline artifact.\nPIPELINE_ROOT = 'gs://{}/pipeline_root/{}'.format(\n    GCS_BUCKET_NAME, PIPELINE_NAME)\n\n# Paths for users' Python module.\nMODULE_ROOT = 'gs://{}/pipeline_module/{}'.format(\n    GCS_BUCKET_NAME, PIPELINE_NAME)\n\n# Paths for users' data.\nDATA_ROOT = 'gs://{}/data/{}'.format(GCS_BUCKET_NAME, PIPELINE_NAME)\n\n# This is the path where your model will be pushed for serving.\nSERVING_MODEL_DIR = 'gs://{}/serving_model/{}'.format(\n    GCS_BUCKET_NAME, PIPELINE_NAME)\n\nprint('PIPELINE_ROOT: {}'.format(PIPELINE_ROOT))\n</pre> PIPELINE_NAME = 'penguin-bigquery'  # Path to various pipeline artifact. PIPELINE_ROOT = 'gs://{}/pipeline_root/{}'.format(     GCS_BUCKET_NAME, PIPELINE_NAME)  # Paths for users' Python module. MODULE_ROOT = 'gs://{}/pipeline_module/{}'.format(     GCS_BUCKET_NAME, PIPELINE_NAME)  # Paths for users' data. DATA_ROOT = 'gs://{}/data/{}'.format(GCS_BUCKET_NAME, PIPELINE_NAME)  # This is the path where your model will be pushed for serving. SERVING_MODEL_DIR = 'gs://{}/serving_model/{}'.format(     GCS_BUCKET_NAME, PIPELINE_NAME)  print('PIPELINE_ROOT: {}'.format(PIPELINE_ROOT)) <p>By default the Vertex Pipelines uses the default GCE VM service account of format <code>[project-number]-compute@developer.gserviceaccount.com</code>. We need to give a permission to use BigQuery to this account to access BigQuery in the pipeline. We will add 'BigQuery User' role to the account.</p> In\u00a0[\u00a0]: Copied! <pre>!gcloud projects add-iam-policy-binding {GOOGLE_CLOUD_PROJECT} \\\n  --member=serviceAccount:{GOOGLE_CLOUD_PROJECT_NUMBER}-compute@developer.gserviceaccount.com \\\n  --role=roles/bigquery.user\n</pre> !gcloud projects add-iam-policy-binding {GOOGLE_CLOUD_PROJECT} \\   --member=serviceAccount:{GOOGLE_CLOUD_PROJECT_NUMBER}-compute@developer.gserviceaccount.com \\   --role=roles/bigquery.user <p>Please see Vertex documentation to learn more about service accounts and IAM configuration.</p> In\u00a0[\u00a0]: Copied! <pre># docs_infra: no_execute\n%%bigquery --project {GOOGLE_CLOUD_PROJECT}\nSELECT *\nFROM `tfx-oss-public.palmer_penguins.palmer_penguins`\nLIMIT 5\n</pre> # docs_infra: no_execute %%bigquery --project {GOOGLE_CLOUD_PROJECT} SELECT * FROM `tfx-oss-public.palmer_penguins.palmer_penguins` LIMIT 5 <p>All features were already normalized to 0~1 except <code>species</code> which is the label. We will build a classification model which predicts the <code>species</code> of penguins.</p> <p><code>BigQueryExampleGen</code> requires a query to specify which data to fetch. Because we will use all the fields of all rows in the table, the query is quite simple. You can also specify field names and add <code>WHERE</code> conditions as needed according to the BigQuery Standard SQL syntax.</p> In\u00a0[\u00a0]: Copied! <pre>QUERY = \"SELECT * FROM `tfx-oss-public.palmer_penguins.palmer_penguins`\"\n</pre> QUERY = \"SELECT * FROM `tfx-oss-public.palmer_penguins.palmer_penguins`\" In\u00a0[\u00a0]: Copied! <pre>_trainer_module_file = 'penguin_trainer.py'\n</pre> _trainer_module_file = 'penguin_trainer.py' In\u00a0[\u00a0]: Copied! <pre>%%writefile {_trainer_module_file}\n\n# Copied from https://www.tensorflow.org/tfx/tutorials/tfx/penguin_simple\n\nfrom typing import List\nfrom absl import logging\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow_transform.tf_metadata import schema_utils\n\nfrom tfx import v1 as tfx\nfrom tfx_bsl.public import tfxio\n\nfrom tensorflow_metadata.proto.v0 import schema_pb2\n\n_FEATURE_KEYS = [\n    'culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g'\n]\n_LABEL_KEY = 'species'\n\n_TRAIN_BATCH_SIZE = 20\n_EVAL_BATCH_SIZE = 10\n\n# Since we're not generating or creating a schema, we will instead create\n# a feature spec.  Since there are a fairly small number of features this is\n# manageable for this dataset.\n_FEATURE_SPEC = {\n    **{\n        feature: tf.io.FixedLenFeature(shape=[1], dtype=tf.float32)\n           for feature in _FEATURE_KEYS\n       },\n    _LABEL_KEY: tf.io.FixedLenFeature(shape=[1], dtype=tf.int64)\n}\n\n\ndef _input_fn(file_pattern: List[str],\n              data_accessor: tfx.components.DataAccessor,\n              schema: schema_pb2.Schema,\n              batch_size: int) -&gt; tf.data.Dataset:\n  \"\"\"Generates features and label for training.\n\n  Args:\n    file_pattern: List of paths or patterns of input tfrecord files.\n    data_accessor: DataAccessor for converting input to RecordBatch.\n    schema: schema of the input data.\n    batch_size: representing the number of consecutive elements of returned\n      dataset to combine in a single batch\n\n  Returns:\n    A dataset that contains (features, indices) tuple where features is a\n      dictionary of Tensors, and indices is a single Tensor of label indices.\n  \"\"\"\n  return data_accessor.tf_dataset_factory(\n      file_pattern,\n      tfxio.TensorFlowDatasetOptions(\n          batch_size=batch_size, label_key=_LABEL_KEY),\n      schema=schema).repeat()\n\n\ndef _make_keras_model() -&gt; tf.keras.Model:\n  \"\"\"Creates a DNN Keras model for classifying penguin data.\n\n  Returns:\n    A Keras Model.\n  \"\"\"\n  # The model below is built with Functional API, please refer to\n  # https://www.tensorflow.org/guide/keras/overview for all API options.\n  inputs = [keras.layers.Input(shape=(1,), name=f) for f in _FEATURE_KEYS]\n  d = keras.layers.concatenate(inputs)\n  for _ in range(2):\n    d = keras.layers.Dense(8, activation='relu')(d)\n  outputs = keras.layers.Dense(3)(d)\n\n  model = keras.Model(inputs=inputs, outputs=outputs)\n  model.compile(\n      optimizer=keras.optimizers.Adam(1e-2),\n      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n      metrics=[keras.metrics.SparseCategoricalAccuracy()])\n\n  model.summary(print_fn=logging.info)\n  return model\n\n\n# TFX Trainer will call this function.\ndef run_fn(fn_args: tfx.components.FnArgs):\n  \"\"\"Train the model based on given args.\n\n  Args:\n    fn_args: Holds args used to train the model as name/value pairs.\n  \"\"\"\n\n  # This schema is usually either an output of SchemaGen or a manually-curated\n  # version provided by pipeline author. A schema can also derived from TFT\n  # graph if a Transform component is used. In the case when either is missing,\n  # `schema_from_feature_spec` could be used to generate schema from very simple\n  # feature_spec, but the schema returned would be very primitive.\n  schema = schema_utils.schema_from_feature_spec(_FEATURE_SPEC)\n\n  train_dataset = _input_fn(\n      fn_args.train_files,\n      fn_args.data_accessor,\n      schema,\n      batch_size=_TRAIN_BATCH_SIZE)\n  eval_dataset = _input_fn(\n      fn_args.eval_files,\n      fn_args.data_accessor,\n      schema,\n      batch_size=_EVAL_BATCH_SIZE)\n\n  model = _make_keras_model()\n  model.fit(\n      train_dataset,\n      steps_per_epoch=fn_args.train_steps,\n      validation_data=eval_dataset,\n      validation_steps=fn_args.eval_steps)\n\n  # The result of the training should be saved in `fn_args.serving_model_dir`\n  # directory.\n  model.save(fn_args.serving_model_dir, save_format='tf')\n</pre> %%writefile {_trainer_module_file}  # Copied from https://www.tensorflow.org/tfx/tutorials/tfx/penguin_simple  from typing import List from absl import logging import tensorflow as tf from tensorflow import keras from tensorflow_transform.tf_metadata import schema_utils  from tfx import v1 as tfx from tfx_bsl.public import tfxio  from tensorflow_metadata.proto.v0 import schema_pb2  _FEATURE_KEYS = [     'culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g' ] _LABEL_KEY = 'species'  _TRAIN_BATCH_SIZE = 20 _EVAL_BATCH_SIZE = 10  # Since we're not generating or creating a schema, we will instead create # a feature spec.  Since there are a fairly small number of features this is # manageable for this dataset. _FEATURE_SPEC = {     **{         feature: tf.io.FixedLenFeature(shape=[1], dtype=tf.float32)            for feature in _FEATURE_KEYS        },     _LABEL_KEY: tf.io.FixedLenFeature(shape=[1], dtype=tf.int64) }   def _input_fn(file_pattern: List[str],               data_accessor: tfx.components.DataAccessor,               schema: schema_pb2.Schema,               batch_size: int) -&gt; tf.data.Dataset:   \"\"\"Generates features and label for training.    Args:     file_pattern: List of paths or patterns of input tfrecord files.     data_accessor: DataAccessor for converting input to RecordBatch.     schema: schema of the input data.     batch_size: representing the number of consecutive elements of returned       dataset to combine in a single batch    Returns:     A dataset that contains (features, indices) tuple where features is a       dictionary of Tensors, and indices is a single Tensor of label indices.   \"\"\"   return data_accessor.tf_dataset_factory(       file_pattern,       tfxio.TensorFlowDatasetOptions(           batch_size=batch_size, label_key=_LABEL_KEY),       schema=schema).repeat()   def _make_keras_model() -&gt; tf.keras.Model:   \"\"\"Creates a DNN Keras model for classifying penguin data.    Returns:     A Keras Model.   \"\"\"   # The model below is built with Functional API, please refer to   # https://www.tensorflow.org/guide/keras/overview for all API options.   inputs = [keras.layers.Input(shape=(1,), name=f) for f in _FEATURE_KEYS]   d = keras.layers.concatenate(inputs)   for _ in range(2):     d = keras.layers.Dense(8, activation='relu')(d)   outputs = keras.layers.Dense(3)(d)    model = keras.Model(inputs=inputs, outputs=outputs)   model.compile(       optimizer=keras.optimizers.Adam(1e-2),       loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),       metrics=[keras.metrics.SparseCategoricalAccuracy()])    model.summary(print_fn=logging.info)   return model   # TFX Trainer will call this function. def run_fn(fn_args: tfx.components.FnArgs):   \"\"\"Train the model based on given args.    Args:     fn_args: Holds args used to train the model as name/value pairs.   \"\"\"    # This schema is usually either an output of SchemaGen or a manually-curated   # version provided by pipeline author. A schema can also derived from TFT   # graph if a Transform component is used. In the case when either is missing,   # `schema_from_feature_spec` could be used to generate schema from very simple   # feature_spec, but the schema returned would be very primitive.   schema = schema_utils.schema_from_feature_spec(_FEATURE_SPEC)    train_dataset = _input_fn(       fn_args.train_files,       fn_args.data_accessor,       schema,       batch_size=_TRAIN_BATCH_SIZE)   eval_dataset = _input_fn(       fn_args.eval_files,       fn_args.data_accessor,       schema,       batch_size=_EVAL_BATCH_SIZE)    model = _make_keras_model()   model.fit(       train_dataset,       steps_per_epoch=fn_args.train_steps,       validation_data=eval_dataset,       validation_steps=fn_args.eval_steps)    # The result of the training should be saved in `fn_args.serving_model_dir`   # directory.   model.save(fn_args.serving_model_dir, save_format='tf') <p>Copy the module file to GCS which can be accessed from the pipeline components. Because model training happens on GCP, we need to upload this model definition.</p> <p>Otherwise, you might want to build a container image including the module file and use the image to run the pipeline.</p> In\u00a0[\u00a0]: Copied! <pre>!gsutil cp {_trainer_module_file} {MODULE_ROOT}/\n</pre> !gsutil cp {_trainer_module_file} {MODULE_ROOT}/ In\u00a0[\u00a0]: Copied! <pre>from typing import List, Optional\n\ndef _create_pipeline(pipeline_name: str, pipeline_root: str, query: str,\n                     module_file: str, serving_model_dir: str,\n                     beam_pipeline_args: Optional[List[str]],\n                     ) -&gt; tfx.dsl.Pipeline:\n  \"\"\"Creates a TFX pipeline using BigQuery.\"\"\"\n\n  # NEW: Query data in BigQuery as a data source.\n  example_gen = tfx.extensions.google_cloud_big_query.BigQueryExampleGen(\n      query=query)\n\n  # Uses user-provided Python function that trains a model.\n  trainer = tfx.components.Trainer(\n      module_file=module_file,\n      examples=example_gen.outputs['examples'],\n      train_args=tfx.proto.TrainArgs(num_steps=100),\n      eval_args=tfx.proto.EvalArgs(num_steps=5))\n\n  # Pushes the model to a file destination.\n  pusher = tfx.components.Pusher(\n      model=trainer.outputs['model'],\n      push_destination=tfx.proto.PushDestination(\n          filesystem=tfx.proto.PushDestination.Filesystem(\n              base_directory=serving_model_dir)))\n\n  components = [\n      example_gen,\n      trainer,\n      pusher,\n  ]\n\n  return tfx.dsl.Pipeline(\n      pipeline_name=pipeline_name,\n      pipeline_root=pipeline_root,\n      components=components,\n      # NEW: `beam_pipeline_args` is required to use BigQueryExampleGen.\n      beam_pipeline_args=beam_pipeline_args)\n</pre> from typing import List, Optional  def _create_pipeline(pipeline_name: str, pipeline_root: str, query: str,                      module_file: str, serving_model_dir: str,                      beam_pipeline_args: Optional[List[str]],                      ) -&gt; tfx.dsl.Pipeline:   \"\"\"Creates a TFX pipeline using BigQuery.\"\"\"    # NEW: Query data in BigQuery as a data source.   example_gen = tfx.extensions.google_cloud_big_query.BigQueryExampleGen(       query=query)    # Uses user-provided Python function that trains a model.   trainer = tfx.components.Trainer(       module_file=module_file,       examples=example_gen.outputs['examples'],       train_args=tfx.proto.TrainArgs(num_steps=100),       eval_args=tfx.proto.EvalArgs(num_steps=5))    # Pushes the model to a file destination.   pusher = tfx.components.Pusher(       model=trainer.outputs['model'],       push_destination=tfx.proto.PushDestination(           filesystem=tfx.proto.PushDestination.Filesystem(               base_directory=serving_model_dir)))    components = [       example_gen,       trainer,       pusher,   ]    return tfx.dsl.Pipeline(       pipeline_name=pipeline_name,       pipeline_root=pipeline_root,       components=components,       # NEW: `beam_pipeline_args` is required to use BigQueryExampleGen.       beam_pipeline_args=beam_pipeline_args) <p>We also need to pass <code>beam_pipeline_args</code> for the BigQueryExampleGen. It includes configs like the name of the GCP project and the temporary storage for the BigQuery execution.</p> In\u00a0[\u00a0]: Copied! <pre># docs_infra: no_execute\nimport os\n\n# We need to pass some GCP related configs to BigQuery. This is currently done\n# using `beam_pipeline_args` parameter.\nBIG_QUERY_WITH_DIRECT_RUNNER_BEAM_PIPELINE_ARGS = [\n   '--project=' + GOOGLE_CLOUD_PROJECT,\n   '--temp_location=' + os.path.join('gs://', GCS_BUCKET_NAME, 'tmp'),\n   ]\n\nPIPELINE_DEFINITION_FILE = PIPELINE_NAME + '_pipeline.json'\n\nrunner = tfx.orchestration.experimental.KubeflowV2DagRunner(\n    config=tfx.orchestration.experimental.KubeflowV2DagRunnerConfig(),\n    output_filename=PIPELINE_DEFINITION_FILE)\n_ = runner.run(\n    _create_pipeline(\n        pipeline_name=PIPELINE_NAME,\n        pipeline_root=PIPELINE_ROOT,\n        query=QUERY,\n        module_file=os.path.join(MODULE_ROOT, _trainer_module_file),\n        serving_model_dir=SERVING_MODEL_DIR,\n        beam_pipeline_args=BIG_QUERY_WITH_DIRECT_RUNNER_BEAM_PIPELINE_ARGS))\n</pre> # docs_infra: no_execute import os  # We need to pass some GCP related configs to BigQuery. This is currently done # using `beam_pipeline_args` parameter. BIG_QUERY_WITH_DIRECT_RUNNER_BEAM_PIPELINE_ARGS = [    '--project=' + GOOGLE_CLOUD_PROJECT,    '--temp_location=' + os.path.join('gs://', GCS_BUCKET_NAME, 'tmp'),    ]  PIPELINE_DEFINITION_FILE = PIPELINE_NAME + '_pipeline.json'  runner = tfx.orchestration.experimental.KubeflowV2DagRunner(     config=tfx.orchestration.experimental.KubeflowV2DagRunnerConfig(),     output_filename=PIPELINE_DEFINITION_FILE) _ = runner.run(     _create_pipeline(         pipeline_name=PIPELINE_NAME,         pipeline_root=PIPELINE_ROOT,         query=QUERY,         module_file=os.path.join(MODULE_ROOT, _trainer_module_file),         serving_model_dir=SERVING_MODEL_DIR,         beam_pipeline_args=BIG_QUERY_WITH_DIRECT_RUNNER_BEAM_PIPELINE_ARGS)) <p>The generated definition file can be submitted using kfp client.</p> In\u00a0[\u00a0]: Copied! <pre># docs_infra: no_execute\nfrom google.cloud import aiplatform\nfrom google.cloud.aiplatform import pipeline_jobs\nimport logging\nlogging.getLogger().setLevel(logging.INFO)\n\naiplatform.init(project=GOOGLE_CLOUD_PROJECT, location=GOOGLE_CLOUD_REGION)\n\njob = pipeline_jobs.PipelineJob(template_path=PIPELINE_DEFINITION_FILE,\n                                display_name=PIPELINE_NAME)\njob.submit()\n</pre> # docs_infra: no_execute from google.cloud import aiplatform from google.cloud.aiplatform import pipeline_jobs import logging logging.getLogger().setLevel(logging.INFO)  aiplatform.init(project=GOOGLE_CLOUD_PROJECT, location=GOOGLE_CLOUD_REGION)  job = pipeline_jobs.PipelineJob(template_path=PIPELINE_DEFINITION_FILE,                                 display_name=PIPELINE_NAME) job.submit() <p>Now you can visit the link in the output above or visit 'Vertex AI &gt; Pipelines' in Google Cloud Console to see the progress.</p>"},{"location":"tutorials/tfx/gcp/vertex_pipelines_bq/#copyright-2021-the-tensorflow-authors","title":"Copyright 2021 The TensorFlow Authors.\u00b6","text":""},{"location":"tutorials/tfx/gcp/vertex_pipelines_bq/#reading-data-from-bigquery-with-tfx-and-vertex-pipelines","title":"Reading data from BigQuery with TFX and Vertex Pipelines\u00b6","text":""},{"location":"tutorials/tfx/gcp/vertex_pipelines_bq/#set-up","title":"Set up\u00b6","text":"<p>If you have completed Simple TFX Pipeline for Vertex Pipelines Tutorial, you will have a working GCP project and a GCS bucket and that is all we need for this tutorial. Please read the preliminary tutorial first if you missed it.</p>"},{"location":"tutorials/tfx/gcp/vertex_pipelines_bq/#install-python-packages","title":"Install python packages\u00b6","text":""},{"location":"tutorials/tfx/gcp/vertex_pipelines_bq/#did-you-restart-the-runtime","title":"Did you restart the runtime?\u00b6","text":"<p>If you are using Google Colab, the first time that you run the cell above, you must restart the runtime by clicking above \"RESTART RUNTIME\" button or using \"Runtime &gt; Restart runtime ...\" menu. This is because of the way that Colab loads packages.</p>"},{"location":"tutorials/tfx/gcp/vertex_pipelines_bq/#login-in-to-google-for-this-notebook","title":"Login in to Google for this notebook\u00b6","text":"<p>If you are running this notebook on Colab, authenticate with your user account:</p>"},{"location":"tutorials/tfx/gcp/vertex_pipelines_bq/#set-up-variables","title":"Set up variables\u00b6","text":"<p>We will set up some variables used to customize the pipelines below. Following information is required:</p> <ul> <li>GCP Project id and number. See Identifying your project id and number.</li> <li>GCP Region to run pipelines. For more information about the regions that Vertex Pipelines is available in, see the Vertex AI locations guide.</li> <li>Google Cloud Storage Bucket to store pipeline outputs.</li> </ul> <p>Enter required values in the cell below before running it.</p>"},{"location":"tutorials/tfx/gcp/vertex_pipelines_bq/#create-a-pipeline","title":"Create a pipeline\u00b6","text":"<p>TFX pipelines are defined using Python APIs as we did in Simple TFX Pipeline for Vertex Pipelines Tutorial. We previously used <code>CsvExampleGen</code> which reads data from a CSV file. In this tutorial, we will use <code>BigQueryExampleGen</code> component which reads data from BigQuery.</p>"},{"location":"tutorials/tfx/gcp/vertex_pipelines_bq/#prepare-bigquery-query","title":"Prepare BigQuery query\u00b6","text":"<p>We will use the same Palmer Penguins dataset. However, we will read it from a BigQuery table <code>tfx-oss-public.palmer_penguins.palmer_penguins</code> which is populated using the same CSV file.</p> <p>If you are using Google Colab, you can examine the content of the BigQuery table directly.</p>"},{"location":"tutorials/tfx/gcp/vertex_pipelines_bq/#write-model-code","title":"Write model code.\u00b6","text":"<p>We will use the same model code as in the Simple TFX Pipeline Tutorial.</p>"},{"location":"tutorials/tfx/gcp/vertex_pipelines_bq/#write-a-pipeline-definition","title":"Write a pipeline definition\u00b6","text":"<p>We will define a function to create a TFX pipeline. We need to use <code>BigQueryExampleGen</code> which takes <code>query</code> as an argument. One more change from the previous tutorial is that we need to pass <code>beam_pipeline_args</code> which is passed to components when they are executed. We will use <code>beam_pipeline_args</code> to pass additional parameters to BigQuery.</p>"},{"location":"tutorials/tfx/gcp/vertex_pipelines_bq/#run-the-pipeline-on-vertex-pipelines","title":"Run the pipeline on Vertex Pipelines.\u00b6","text":"<p>We will use Vertex Pipelines to run the pipeline as we did in Simple TFX Pipeline for Vertex Pipelines Tutorial.</p>"},{"location":"tutorials/tfx/gcp/vertex_pipelines_simple/","title":"Simple TFX Pipeline for Vertex Pipelines","text":"In\u00a0[\u00a0]: Copied! <pre>#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n</pre> #@title Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # https://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. <p>Note: We recommend running this tutorial in a Colab notebook, with no setup required!  Just click \"Run in Google Colab\".</p>        View on TensorFlow.org             Run in Google Colab             View source on GitHub             Download notebook      <p>This notebook-based tutorial will create a simple TFX pipeline and run it using Google Cloud Vertex Pipelines.  This notebook is based on the TFX pipeline we built in Simple TFX Pipeline Tutorial. If you are not familiar with TFX and you have not read that tutorial yet, you should read it before proceeding with this notebook.</p> <p>Google Cloud Vertex Pipelines helps you to automate, monitor, and govern your ML systems by orchestrating your ML workflow in a serverless manner. You can define your ML pipelines using Python with TFX, and then execute your pipelines on Google Cloud. See Vertex Pipelines introduction to learn more about Vertex Pipelines.</p> <p>This notebook is intended to be run on Google Colab or on AI Platform Notebooks. If you are not using one of these, you can simply click \"Run in Google Colab\" button above.</p> <p>We will install required Python packages including TFX and KFP to author ML pipelines and submit jobs to Vertex Pipelines.</p> In\u00a0[\u00a0]: Copied! <pre># Use the latest version of pip.\n!pip install --upgrade pip\n!pip install --upgrade \"tfx[kfp]&lt;2\"\n</pre> # Use the latest version of pip. !pip install --upgrade pip !pip install --upgrade \"tfx[kfp]&lt;2\" <p>If you are not on Colab, you can restart runtime with following cell.</p> In\u00a0[\u00a0]: Copied! <pre># docs_infra: no_execute\nimport sys\nif not 'google.colab' in sys.modules:\n  # Automatically restart kernel after installs\n  import IPython\n  app = IPython.Application.instance()\n  app.kernel.do_shutdown(True)\n</pre> # docs_infra: no_execute import sys if not 'google.colab' in sys.modules:   # Automatically restart kernel after installs   import IPython   app = IPython.Application.instance()   app.kernel.do_shutdown(True) In\u00a0[\u00a0]: Copied! <pre>import sys\nif 'google.colab' in sys.modules:\n  from google.colab import auth\n  auth.authenticate_user()\n</pre> import sys if 'google.colab' in sys.modules:   from google.colab import auth   auth.authenticate_user() <p>If you are on AI Platform Notebooks, authenticate with Google Cloud before running the next section, by running</p> <pre>gcloud auth login\n</pre> <p>in the Terminal window (which you can open via File &gt; New in the menu). You only need to do this once per notebook instance.</p> <p>Check the package versions.</p> In\u00a0[\u00a0]: Copied! <pre>import tensorflow as tf\nprint('TensorFlow version: {}'.format(tf.__version__))\nfrom tfx import v1 as tfx\nprint('TFX version: {}'.format(tfx.__version__))\nimport kfp\nprint('KFP version: {}'.format(kfp.__version__))\n</pre> import tensorflow as tf print('TensorFlow version: {}'.format(tf.__version__)) from tfx import v1 as tfx print('TFX version: {}'.format(tfx.__version__)) import kfp print('KFP version: {}'.format(kfp.__version__)) In\u00a0[\u00a0]: Copied! <pre>GOOGLE_CLOUD_PROJECT = ''     # &lt;--- ENTER THIS\nGOOGLE_CLOUD_REGION = ''      # &lt;--- ENTER THIS\nGCS_BUCKET_NAME = ''          # &lt;--- ENTER THIS\n\nif not (GOOGLE_CLOUD_PROJECT and GOOGLE_CLOUD_REGION and GCS_BUCKET_NAME):\n    from absl import logging\n    logging.error('Please set all required parameters.')\n</pre> GOOGLE_CLOUD_PROJECT = ''     # &lt;--- ENTER THIS GOOGLE_CLOUD_REGION = ''      # &lt;--- ENTER THIS GCS_BUCKET_NAME = ''          # &lt;--- ENTER THIS  if not (GOOGLE_CLOUD_PROJECT and GOOGLE_CLOUD_REGION and GCS_BUCKET_NAME):     from absl import logging     logging.error('Please set all required parameters.') <p>Set <code>gcloud</code> to use your project.</p> In\u00a0[\u00a0]: Copied! <pre>!gcloud config set project {GOOGLE_CLOUD_PROJECT}\n</pre> !gcloud config set project {GOOGLE_CLOUD_PROJECT} In\u00a0[\u00a0]: Copied! <pre>PIPELINE_NAME = 'penguin-vertex-pipelines'\n\n# Path to various pipeline artifact.\nPIPELINE_ROOT = 'gs://{}/pipeline_root/{}'.format(\n    GCS_BUCKET_NAME, PIPELINE_NAME)\n\n# Paths for users' Python module.\nMODULE_ROOT = 'gs://{}/pipeline_module/{}'.format(\n    GCS_BUCKET_NAME, PIPELINE_NAME)\n\n# Paths for input data.\nDATA_ROOT = 'gs://{}/data/{}'.format(GCS_BUCKET_NAME, PIPELINE_NAME)\n\n# This is the path where your model will be pushed for serving.\nSERVING_MODEL_DIR = 'gs://{}/serving_model/{}'.format(\n    GCS_BUCKET_NAME, PIPELINE_NAME)\n\nprint('PIPELINE_ROOT: {}'.format(PIPELINE_ROOT))\n</pre> PIPELINE_NAME = 'penguin-vertex-pipelines'  # Path to various pipeline artifact. PIPELINE_ROOT = 'gs://{}/pipeline_root/{}'.format(     GCS_BUCKET_NAME, PIPELINE_NAME)  # Paths for users' Python module. MODULE_ROOT = 'gs://{}/pipeline_module/{}'.format(     GCS_BUCKET_NAME, PIPELINE_NAME)  # Paths for input data. DATA_ROOT = 'gs://{}/data/{}'.format(GCS_BUCKET_NAME, PIPELINE_NAME)  # This is the path where your model will be pushed for serving. SERVING_MODEL_DIR = 'gs://{}/serving_model/{}'.format(     GCS_BUCKET_NAME, PIPELINE_NAME)  print('PIPELINE_ROOT: {}'.format(PIPELINE_ROOT)) <p>We need to make our own copy of the dataset. Because TFX ExampleGen reads inputs from a directory, we need to create a directory and copy dataset to it on GCS.</p> In\u00a0[\u00a0]: Copied! <pre>!gsutil cp gs://download.tensorflow.org/data/palmer_penguins/penguins_processed.csv {DATA_ROOT}/\n</pre> !gsutil cp gs://download.tensorflow.org/data/palmer_penguins/penguins_processed.csv {DATA_ROOT}/ <p>Take a quick look at the CSV file.</p> In\u00a0[\u00a0]: Copied! <pre>!gsutil cat {DATA_ROOT}/penguins_processed.csv | head\n</pre> !gsutil cat {DATA_ROOT}/penguins_processed.csv | head In\u00a0[\u00a0]: Copied! <pre>_trainer_module_file = 'penguin_trainer.py'\n</pre> _trainer_module_file = 'penguin_trainer.py' In\u00a0[\u00a0]: Copied! <pre>%%writefile {_trainer_module_file}\n\n# Copied from https://www.tensorflow.org/tfx/tutorials/tfx/penguin_simple\n\nfrom typing import List\nfrom absl import logging\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow_transform.tf_metadata import schema_utils\n\n\nfrom tfx import v1 as tfx\nfrom tfx_bsl.public import tfxio\n\nfrom tensorflow_metadata.proto.v0 import schema_pb2\n\n_FEATURE_KEYS = [\n    'culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g'\n]\n_LABEL_KEY = 'species'\n\n_TRAIN_BATCH_SIZE = 20\n_EVAL_BATCH_SIZE = 10\n\n# Since we're not generating or creating a schema, we will instead create\n# a feature spec.  Since there are a fairly small number of features this is\n# manageable for this dataset.\n_FEATURE_SPEC = {\n    **{\n        feature: tf.io.FixedLenFeature(shape=[1], dtype=tf.float32)\n           for feature in _FEATURE_KEYS\n       },\n    _LABEL_KEY: tf.io.FixedLenFeature(shape=[1], dtype=tf.int64)\n}\n\n\ndef _input_fn(file_pattern: List[str],\n              data_accessor: tfx.components.DataAccessor,\n              schema: schema_pb2.Schema,\n              batch_size: int) -&gt; tf.data.Dataset:\n  \"\"\"Generates features and label for training.\n\n  Args:\n    file_pattern: List of paths or patterns of input tfrecord files.\n    data_accessor: DataAccessor for converting input to RecordBatch.\n    schema: schema of the input data.\n    batch_size: representing the number of consecutive elements of returned\n      dataset to combine in a single batch\n\n  Returns:\n    A dataset that contains (features, indices) tuple where features is a\n      dictionary of Tensors, and indices is a single Tensor of label indices.\n  \"\"\"\n  return data_accessor.tf_dataset_factory(\n      file_pattern,\n      tfxio.TensorFlowDatasetOptions(\n          batch_size=batch_size, label_key=_LABEL_KEY),\n      schema=schema).repeat()\n\n\ndef _make_keras_model() -&gt; tf.keras.Model:\n  \"\"\"Creates a DNN Keras model for classifying penguin data.\n\n  Returns:\n    A Keras Model.\n  \"\"\"\n  # The model below is built with Functional API, please refer to\n  # https://www.tensorflow.org/guide/keras/overview for all API options.\n  inputs = [keras.layers.Input(shape=(1,), name=f) for f in _FEATURE_KEYS]\n  d = keras.layers.concatenate(inputs)\n  for _ in range(2):\n    d = keras.layers.Dense(8, activation='relu')(d)\n  outputs = keras.layers.Dense(3)(d)\n\n  model = keras.Model(inputs=inputs, outputs=outputs)\n  model.compile(\n      optimizer=keras.optimizers.Adam(1e-2),\n      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n      metrics=[keras.metrics.SparseCategoricalAccuracy()])\n\n  model.summary(print_fn=logging.info)\n  return model\n\n\n# TFX Trainer will call this function.\ndef run_fn(fn_args: tfx.components.FnArgs):\n  \"\"\"Train the model based on given args.\n\n  Args:\n    fn_args: Holds args used to train the model as name/value pairs.\n  \"\"\"\n\n  # This schema is usually either an output of SchemaGen or a manually-curated\n  # version provided by pipeline author. A schema can also derived from TFT\n  # graph if a Transform component is used. In the case when either is missing,\n  # `schema_from_feature_spec` could be used to generate schema from very simple\n  # feature_spec, but the schema returned would be very primitive.\n  schema = schema_utils.schema_from_feature_spec(_FEATURE_SPEC)\n\n  train_dataset = _input_fn(\n      fn_args.train_files,\n      fn_args.data_accessor,\n      schema,\n      batch_size=_TRAIN_BATCH_SIZE)\n  eval_dataset = _input_fn(\n      fn_args.eval_files,\n      fn_args.data_accessor,\n      schema,\n      batch_size=_EVAL_BATCH_SIZE)\n\n  model = _make_keras_model()\n  model.fit(\n      train_dataset,\n      steps_per_epoch=fn_args.train_steps,\n      validation_data=eval_dataset,\n      validation_steps=fn_args.eval_steps)\n\n  # The result of the training should be saved in `fn_args.serving_model_dir`\n  # directory.\n  model.save(fn_args.serving_model_dir, save_format='tf')\n</pre> %%writefile {_trainer_module_file}  # Copied from https://www.tensorflow.org/tfx/tutorials/tfx/penguin_simple  from typing import List from absl import logging import tensorflow as tf from tensorflow import keras from tensorflow_transform.tf_metadata import schema_utils   from tfx import v1 as tfx from tfx_bsl.public import tfxio  from tensorflow_metadata.proto.v0 import schema_pb2  _FEATURE_KEYS = [     'culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g' ] _LABEL_KEY = 'species'  _TRAIN_BATCH_SIZE = 20 _EVAL_BATCH_SIZE = 10  # Since we're not generating or creating a schema, we will instead create # a feature spec.  Since there are a fairly small number of features this is # manageable for this dataset. _FEATURE_SPEC = {     **{         feature: tf.io.FixedLenFeature(shape=[1], dtype=tf.float32)            for feature in _FEATURE_KEYS        },     _LABEL_KEY: tf.io.FixedLenFeature(shape=[1], dtype=tf.int64) }   def _input_fn(file_pattern: List[str],               data_accessor: tfx.components.DataAccessor,               schema: schema_pb2.Schema,               batch_size: int) -&gt; tf.data.Dataset:   \"\"\"Generates features and label for training.    Args:     file_pattern: List of paths or patterns of input tfrecord files.     data_accessor: DataAccessor for converting input to RecordBatch.     schema: schema of the input data.     batch_size: representing the number of consecutive elements of returned       dataset to combine in a single batch    Returns:     A dataset that contains (features, indices) tuple where features is a       dictionary of Tensors, and indices is a single Tensor of label indices.   \"\"\"   return data_accessor.tf_dataset_factory(       file_pattern,       tfxio.TensorFlowDatasetOptions(           batch_size=batch_size, label_key=_LABEL_KEY),       schema=schema).repeat()   def _make_keras_model() -&gt; tf.keras.Model:   \"\"\"Creates a DNN Keras model for classifying penguin data.    Returns:     A Keras Model.   \"\"\"   # The model below is built with Functional API, please refer to   # https://www.tensorflow.org/guide/keras/overview for all API options.   inputs = [keras.layers.Input(shape=(1,), name=f) for f in _FEATURE_KEYS]   d = keras.layers.concatenate(inputs)   for _ in range(2):     d = keras.layers.Dense(8, activation='relu')(d)   outputs = keras.layers.Dense(3)(d)    model = keras.Model(inputs=inputs, outputs=outputs)   model.compile(       optimizer=keras.optimizers.Adam(1e-2),       loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),       metrics=[keras.metrics.SparseCategoricalAccuracy()])    model.summary(print_fn=logging.info)   return model   # TFX Trainer will call this function. def run_fn(fn_args: tfx.components.FnArgs):   \"\"\"Train the model based on given args.    Args:     fn_args: Holds args used to train the model as name/value pairs.   \"\"\"    # This schema is usually either an output of SchemaGen or a manually-curated   # version provided by pipeline author. A schema can also derived from TFT   # graph if a Transform component is used. In the case when either is missing,   # `schema_from_feature_spec` could be used to generate schema from very simple   # feature_spec, but the schema returned would be very primitive.   schema = schema_utils.schema_from_feature_spec(_FEATURE_SPEC)    train_dataset = _input_fn(       fn_args.train_files,       fn_args.data_accessor,       schema,       batch_size=_TRAIN_BATCH_SIZE)   eval_dataset = _input_fn(       fn_args.eval_files,       fn_args.data_accessor,       schema,       batch_size=_EVAL_BATCH_SIZE)    model = _make_keras_model()   model.fit(       train_dataset,       steps_per_epoch=fn_args.train_steps,       validation_data=eval_dataset,       validation_steps=fn_args.eval_steps)    # The result of the training should be saved in `fn_args.serving_model_dir`   # directory.   model.save(fn_args.serving_model_dir, save_format='tf') <p>Copy the module file to GCS which can be accessed from the pipeline components. Because model training happens on GCP, we need to upload this model definition.</p> <p>Otherwise, you might want to build a container image including the module file and use the image to run the pipeline.</p> In\u00a0[\u00a0]: Copied! <pre>!gsutil cp {_trainer_module_file} {MODULE_ROOT}/\n</pre> !gsutil cp {_trainer_module_file} {MODULE_ROOT}/ In\u00a0[\u00a0]: Copied! <pre># Copied from https://www.tensorflow.org/tfx/tutorials/tfx/penguin_simple and\n# slightly modified because we don't need `metadata_path` argument.\n\ndef _create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str,\n                     module_file: str, serving_model_dir: str,\n                     ) -&gt; tfx.dsl.Pipeline:\n  \"\"\"Creates a three component penguin pipeline with TFX.\"\"\"\n  # Brings data into the pipeline.\n  example_gen = tfx.components.CsvExampleGen(input_base=data_root)\n\n  # Uses user-provided Python function that trains a model.\n  trainer = tfx.components.Trainer(\n      module_file=module_file,\n      examples=example_gen.outputs['examples'],\n      train_args=tfx.proto.TrainArgs(num_steps=100),\n      eval_args=tfx.proto.EvalArgs(num_steps=5))\n\n  # Pushes the model to a filesystem destination.\n  pusher = tfx.components.Pusher(\n      model=trainer.outputs['model'],\n      push_destination=tfx.proto.PushDestination(\n          filesystem=tfx.proto.PushDestination.Filesystem(\n              base_directory=serving_model_dir)))\n\n  # Following three components will be included in the pipeline.\n  components = [\n      example_gen,\n      trainer,\n      pusher,\n  ]\n\n  return tfx.dsl.Pipeline(\n      pipeline_name=pipeline_name,\n      pipeline_root=pipeline_root,\n      components=components)\n</pre> # Copied from https://www.tensorflow.org/tfx/tutorials/tfx/penguin_simple and # slightly modified because we don't need `metadata_path` argument.  def _create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str,                      module_file: str, serving_model_dir: str,                      ) -&gt; tfx.dsl.Pipeline:   \"\"\"Creates a three component penguin pipeline with TFX.\"\"\"   # Brings data into the pipeline.   example_gen = tfx.components.CsvExampleGen(input_base=data_root)    # Uses user-provided Python function that trains a model.   trainer = tfx.components.Trainer(       module_file=module_file,       examples=example_gen.outputs['examples'],       train_args=tfx.proto.TrainArgs(num_steps=100),       eval_args=tfx.proto.EvalArgs(num_steps=5))    # Pushes the model to a filesystem destination.   pusher = tfx.components.Pusher(       model=trainer.outputs['model'],       push_destination=tfx.proto.PushDestination(           filesystem=tfx.proto.PushDestination.Filesystem(               base_directory=serving_model_dir)))    # Following three components will be included in the pipeline.   components = [       example_gen,       trainer,       pusher,   ]    return tfx.dsl.Pipeline(       pipeline_name=pipeline_name,       pipeline_root=pipeline_root,       components=components) <p>We need to define a runner to actually run the pipeline. You will compile your pipeline into our pipeline definition format using TFX APIs.</p> In\u00a0[\u00a0]: Copied! <pre># docs_infra: no_execute\nimport os\n\nPIPELINE_DEFINITION_FILE = PIPELINE_NAME + '_pipeline.json'\n\nrunner = tfx.orchestration.experimental.KubeflowV2DagRunner(\n    config=tfx.orchestration.experimental.KubeflowV2DagRunnerConfig(),\n    output_filename=PIPELINE_DEFINITION_FILE)\n# Following function will write the pipeline definition to PIPELINE_DEFINITION_FILE.\n_ = runner.run(\n    _create_pipeline(\n        pipeline_name=PIPELINE_NAME,\n        pipeline_root=PIPELINE_ROOT,\n        data_root=DATA_ROOT,\n        module_file=os.path.join(MODULE_ROOT, _trainer_module_file),\n        serving_model_dir=SERVING_MODEL_DIR))\n</pre> # docs_infra: no_execute import os  PIPELINE_DEFINITION_FILE = PIPELINE_NAME + '_pipeline.json'  runner = tfx.orchestration.experimental.KubeflowV2DagRunner(     config=tfx.orchestration.experimental.KubeflowV2DagRunnerConfig(),     output_filename=PIPELINE_DEFINITION_FILE) # Following function will write the pipeline definition to PIPELINE_DEFINITION_FILE. _ = runner.run(     _create_pipeline(         pipeline_name=PIPELINE_NAME,         pipeline_root=PIPELINE_ROOT,         data_root=DATA_ROOT,         module_file=os.path.join(MODULE_ROOT, _trainer_module_file),         serving_model_dir=SERVING_MODEL_DIR)) <p>The generated definition file can be submitted using kfp client.</p> In\u00a0[\u00a0]: Copied! <pre># docs_infra: no_execute\nfrom google.cloud import aiplatform\nfrom google.cloud.aiplatform import pipeline_jobs\nimport logging\nlogging.getLogger().setLevel(logging.INFO)\n\naiplatform.init(project=GOOGLE_CLOUD_PROJECT, location=GOOGLE_CLOUD_REGION)\n\njob = pipeline_jobs.PipelineJob(template_path=PIPELINE_DEFINITION_FILE,\n                                display_name=PIPELINE_NAME)\njob.submit()\n</pre> # docs_infra: no_execute from google.cloud import aiplatform from google.cloud.aiplatform import pipeline_jobs import logging logging.getLogger().setLevel(logging.INFO)  aiplatform.init(project=GOOGLE_CLOUD_PROJECT, location=GOOGLE_CLOUD_REGION)  job = pipeline_jobs.PipelineJob(template_path=PIPELINE_DEFINITION_FILE,                                 display_name=PIPELINE_NAME) job.submit() <p>Now you can visit the link in the output above or visit 'Vertex AI &gt; Pipelines' in Google Cloud Console to see the progress.</p>"},{"location":"tutorials/tfx/gcp/vertex_pipelines_simple/#copyright-2021-the-tensorflow-authors","title":"Copyright 2021 The TensorFlow Authors.\u00b6","text":""},{"location":"tutorials/tfx/gcp/vertex_pipelines_simple/#simple-tfx-pipeline-for-vertex-pipelines","title":"Simple TFX Pipeline for Vertex Pipelines\u00b6","text":""},{"location":"tutorials/tfx/gcp/vertex_pipelines_simple/#set-up","title":"Set up\u00b6","text":"<p>Before you run this notebook, ensure that you have following:</p> <ul> <li>A Google Cloud Platform project.</li> <li>A Google Cloud Storage bucket. See the guide for creating buckets.</li> <li>Enable Vertex AI and Cloud Storage API.</li> </ul> <p>Please see Vertex documentation to configure your GCP project further.</p>"},{"location":"tutorials/tfx/gcp/vertex_pipelines_simple/#install-python-packages","title":"Install python packages\u00b6","text":""},{"location":"tutorials/tfx/gcp/vertex_pipelines_simple/#did-you-restart-the-runtime","title":"Did you restart the runtime?\u00b6","text":"<p>If you are using Google Colab, the first time that you run the cell above, you must restart the runtime by clicking above \"RESTART RUNTIME\" button or using \"Runtime &gt; Restart runtime ...\" menu. This is because of the way that Colab loads packages.</p>"},{"location":"tutorials/tfx/gcp/vertex_pipelines_simple/#login-in-to-google-for-this-notebook","title":"Login in to Google for this notebook\u00b6","text":"<p>If you are running this notebook on Colab, authenticate with your user account:</p>"},{"location":"tutorials/tfx/gcp/vertex_pipelines_simple/#set-up-variables","title":"Set up variables\u00b6","text":"<p>We will set up some variables used to customize the pipelines below. Following information is required:</p> <ul> <li>GCP Project id. See Identifying your project id.</li> <li>GCP Region to run pipelines. For more information about the regions that Vertex Pipelines is available in, see the Vertex AI locations guide.</li> <li>Google Cloud Storage Bucket to store pipeline outputs.</li> </ul> <p>Enter required values in the cell below before running it.</p>"},{"location":"tutorials/tfx/gcp/vertex_pipelines_simple/#prepare-example-data","title":"Prepare example data\u00b6","text":"<p>We will use the same Palmer Penguins dataset as Simple TFX Pipeline Tutorial.</p> <p>There are four numeric features in this dataset which were already normalized to have range [0,1]. We will build a classification model which predicts the <code>species</code> of penguins.</p>"},{"location":"tutorials/tfx/gcp/vertex_pipelines_simple/#create-a-pipeline","title":"Create a pipeline\u00b6","text":"<p>TFX pipelines are defined using Python APIs. We will define a pipeline which consists of three components, CsvExampleGen, Trainer and Pusher. The pipeline and model definition is almost the same as Simple TFX Pipeline Tutorial.</p> <p>The only difference is that we don't need to set <code>metadata_connection_config</code> which is used to locate ML Metadata database. Because Vertex Pipelines uses a managed metadata service, users don't need to care of it, and we don't need to specify the parameter.</p> <p>Before actually define the pipeline, we need to write a model code for the Trainer component first.</p>"},{"location":"tutorials/tfx/gcp/vertex_pipelines_simple/#write-model-code","title":"Write model code.\u00b6","text":"<p>We will use the same model code as in the Simple TFX Pipeline Tutorial.</p>"},{"location":"tutorials/tfx/gcp/vertex_pipelines_simple/#write-a-pipeline-definition","title":"Write a pipeline definition\u00b6","text":"<p>We will define a function to create a TFX pipeline.</p>"},{"location":"tutorials/tfx/gcp/vertex_pipelines_simple/#run-the-pipeline-on-vertex-pipelines","title":"Run the pipeline on Vertex Pipelines.\u00b6","text":"<p>We used <code>LocalDagRunner</code> which runs on local environment in Simple TFX Pipeline Tutorial. TFX provides multiple orchestrators to run your pipeline. In this tutorial we will use the Vertex Pipelines together with the Kubeflow V2 dag runner.</p>"},{"location":"tutorials/tfx/gcp/vertex_pipelines_vertex_training/","title":"Vertex AI Training and Serving with TFX and Vertex Pipelines","text":"In\u00a0[\u00a0]: Copied! <pre>#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n</pre> #@title Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # https://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. <p>Note: We recommend running this tutorial in a Colab notebook, with no setup required!  Just click \"Run in Google Colab\".</p>        View on TensorFlow.org             Run in Google Colab             View source on GitHub             Download notebook      <p>This notebook-based tutorial will create and run a TFX pipeline which trains an ML model using Vertex AI Training service and publishes it to Vertex AI for serving.</p> <p>This notebook is based on the TFX pipeline we built in Simple TFX Pipeline for Vertex Pipelines Tutorial. If you have not read that tutorial yet, you should read it before proceeding with this notebook.</p> <p>You can train models on Vertex AI using AutoML, or use custom training. In custom training, you can select many different machine types to power your training jobs, enable distributed training, use hyperparameter tuning, and accelerate with GPUs.</p> <p>You can also serve prediction requests by deploying the trained model to Vertex AI Models and creating an endpoint.</p> <p>In this tutorial, we will use Vertex AI Training with custom jobs to train a model in a TFX pipeline. We will also deploy the model to serve prediction request using Vertex AI.</p> <p>This notebook is intended to be run on Google Colab or on AI Platform Notebooks. If you are not using one of these, you can simply click \"Run in Google Colab\" button above.</p> <p>We will install required Python packages including TFX and KFP to author ML pipelines and submit jobs to Vertex Pipelines.</p> In\u00a0[\u00a0]: Copied! <pre># Use the latest version of pip.\n!pip install --upgrade pip\n!pip install --upgrade \"tfx[kfp]&lt;2\"\n</pre> # Use the latest version of pip. !pip install --upgrade pip !pip install --upgrade \"tfx[kfp]&lt;2\" <p>If you are not on Colab, you can restart runtime with following cell.</p> In\u00a0[\u00a0]: Copied! <pre># docs_infra: no_execute\nimport sys\nif not 'google.colab' in sys.modules:\n  # Automatically restart kernel after installs\n  import IPython\n  app = IPython.Application.instance()\n  app.kernel.do_shutdown(True)\n</pre> # docs_infra: no_execute import sys if not 'google.colab' in sys.modules:   # Automatically restart kernel after installs   import IPython   app = IPython.Application.instance()   app.kernel.do_shutdown(True) In\u00a0[\u00a0]: Copied! <pre>import sys\nif 'google.colab' in sys.modules:\n  from google.colab import auth\n  auth.authenticate_user()\n</pre> import sys if 'google.colab' in sys.modules:   from google.colab import auth   auth.authenticate_user() <p>If you are on AI Platform Notebooks, authenticate with Google Cloud before running the next section, by running</p> <pre>gcloud auth login\n</pre> <p>in the Terminal window (which you can open via File &gt; New in the menu). You only need to do this once per notebook instance.</p> <p>Check the package versions.</p> In\u00a0[\u00a0]: Copied! <pre>import tensorflow as tf\nprint('TensorFlow version: {}'.format(tf.__version__))\nfrom tfx import v1 as tfx\nprint('TFX version: {}'.format(tfx.__version__))\nimport kfp\nprint('KFP version: {}'.format(kfp.__version__))\n</pre> import tensorflow as tf print('TensorFlow version: {}'.format(tf.__version__)) from tfx import v1 as tfx print('TFX version: {}'.format(tfx.__version__)) import kfp print('KFP version: {}'.format(kfp.__version__)) In\u00a0[\u00a0]: Copied! <pre>GOOGLE_CLOUD_PROJECT = ''     # &lt;--- ENTER THIS\nGOOGLE_CLOUD_REGION = ''      # &lt;--- ENTER THIS\nGCS_BUCKET_NAME = ''          # &lt;--- ENTER THIS\n\nif not (GOOGLE_CLOUD_PROJECT and GOOGLE_CLOUD_REGION and GCS_BUCKET_NAME):\n    from absl import logging\n    logging.error('Please set all required parameters.')\n</pre> GOOGLE_CLOUD_PROJECT = ''     # &lt;--- ENTER THIS GOOGLE_CLOUD_REGION = ''      # &lt;--- ENTER THIS GCS_BUCKET_NAME = ''          # &lt;--- ENTER THIS  if not (GOOGLE_CLOUD_PROJECT and GOOGLE_CLOUD_REGION and GCS_BUCKET_NAME):     from absl import logging     logging.error('Please set all required parameters.') <p>Set <code>gcloud</code> to use your project.</p> In\u00a0[\u00a0]: Copied! <pre>!gcloud config set project {GOOGLE_CLOUD_PROJECT}\n</pre> !gcloud config set project {GOOGLE_CLOUD_PROJECT} In\u00a0[\u00a0]: Copied! <pre>PIPELINE_NAME = 'penguin-vertex-training'\n\n# Path to various pipeline artifact.\nPIPELINE_ROOT = 'gs://{}/pipeline_root/{}'.format(GCS_BUCKET_NAME, PIPELINE_NAME)\n\n# Paths for users' Python module.\nMODULE_ROOT = 'gs://{}/pipeline_module/{}'.format(GCS_BUCKET_NAME, PIPELINE_NAME)\n\n# Paths for users' data.\nDATA_ROOT = 'gs://{}/data/{}'.format(GCS_BUCKET_NAME, PIPELINE_NAME)\n\n# Name of Vertex AI Endpoint.\nENDPOINT_NAME = 'prediction-' + PIPELINE_NAME\n\nprint('PIPELINE_ROOT: {}'.format(PIPELINE_ROOT))\n</pre> PIPELINE_NAME = 'penguin-vertex-training'  # Path to various pipeline artifact. PIPELINE_ROOT = 'gs://{}/pipeline_root/{}'.format(GCS_BUCKET_NAME, PIPELINE_NAME)  # Paths for users' Python module. MODULE_ROOT = 'gs://{}/pipeline_module/{}'.format(GCS_BUCKET_NAME, PIPELINE_NAME)  # Paths for users' data. DATA_ROOT = 'gs://{}/data/{}'.format(GCS_BUCKET_NAME, PIPELINE_NAME)  # Name of Vertex AI Endpoint. ENDPOINT_NAME = 'prediction-' + PIPELINE_NAME  print('PIPELINE_ROOT: {}'.format(PIPELINE_ROOT)) <p>We need to make our own copy of the dataset. Because TFX ExampleGen reads inputs from a directory, we need to create a directory and copy dataset to it on GCS.</p> In\u00a0[\u00a0]: Copied! <pre>!gsutil cp gs://download.tensorflow.org/data/palmer_penguins/penguins_processed.csv {DATA_ROOT}/\n</pre> !gsutil cp gs://download.tensorflow.org/data/palmer_penguins/penguins_processed.csv {DATA_ROOT}/ <p>Take a quick look at the CSV file.</p> In\u00a0[\u00a0]: Copied! <pre>!gsutil cat {DATA_ROOT}/penguins_processed.csv | head\n</pre> !gsutil cat {DATA_ROOT}/penguins_processed.csv | head In\u00a0[\u00a0]: Copied! <pre>_trainer_module_file = 'penguin_trainer.py'\n</pre> _trainer_module_file = 'penguin_trainer.py' In\u00a0[\u00a0]: Copied! <pre>%%writefile {_trainer_module_file}\n\n# Copied from https://www.tensorflow.org/tfx/tutorials/tfx/penguin_simple and\n# slightly modified run_fn() to add distribution_strategy.\n\nfrom typing import List\nfrom absl import logging\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow_metadata.proto.v0 import schema_pb2\nfrom tensorflow_transform.tf_metadata import schema_utils\n\nfrom tfx import v1 as tfx\nfrom tfx_bsl.public import tfxio\n\n_FEATURE_KEYS = [\n    'culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g'\n]\n_LABEL_KEY = 'species'\n\n_TRAIN_BATCH_SIZE = 20\n_EVAL_BATCH_SIZE = 10\n\n# Since we're not generating or creating a schema, we will instead create\n# a feature spec.  Since there are a fairly small number of features this is\n# manageable for this dataset.\n_FEATURE_SPEC = {\n    **{\n        feature: tf.io.FixedLenFeature(shape=[1], dtype=tf.float32)\n        for feature in _FEATURE_KEYS\n    }, _LABEL_KEY: tf.io.FixedLenFeature(shape=[1], dtype=tf.int64)\n}\n\n\ndef _input_fn(file_pattern: List[str],\n              data_accessor: tfx.components.DataAccessor,\n              schema: schema_pb2.Schema,\n              batch_size: int) -&gt; tf.data.Dataset:\n  \"\"\"Generates features and label for training.\n\n  Args:\n    file_pattern: List of paths or patterns of input tfrecord files.\n    data_accessor: DataAccessor for converting input to RecordBatch.\n    schema: schema of the input data.\n    batch_size: representing the number of consecutive elements of returned\n      dataset to combine in a single batch\n\n  Returns:\n    A dataset that contains (features, indices) tuple where features is a\n      dictionary of Tensors, and indices is a single Tensor of label indices.\n  \"\"\"\n  return data_accessor.tf_dataset_factory(\n      file_pattern,\n      tfxio.TensorFlowDatasetOptions(\n          batch_size=batch_size, label_key=_LABEL_KEY),\n      schema=schema).repeat()\n\n\ndef _make_keras_model() -&gt; tf.keras.Model:\n  \"\"\"Creates a DNN Keras model for classifying penguin data.\n\n  Returns:\n    A Keras Model.\n  \"\"\"\n  # The model below is built with Functional API, please refer to\n  # https://www.tensorflow.org/guide/keras/overview for all API options.\n  inputs = [keras.layers.Input(shape=(1,), name=f) for f in _FEATURE_KEYS]\n  d = keras.layers.concatenate(inputs)\n  for _ in range(2):\n    d = keras.layers.Dense(8, activation='relu')(d)\n  outputs = keras.layers.Dense(3)(d)\n\n  model = keras.Model(inputs=inputs, outputs=outputs)\n  model.compile(\n      optimizer=keras.optimizers.Adam(1e-2),\n      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n      metrics=[keras.metrics.SparseCategoricalAccuracy()])\n\n  model.summary(print_fn=logging.info)\n  return model\n\n\n# NEW: Read `use_gpu` from the custom_config of the Trainer.\n#      if it uses GPU, enable MirroredStrategy.\ndef _get_distribution_strategy(fn_args: tfx.components.FnArgs):\n  if fn_args.custom_config.get('use_gpu', False):\n    logging.info('Using MirroredStrategy with one GPU.')\n    return tf.distribute.MirroredStrategy(devices=['device:GPU:0'])\n  return None\n\n\n# TFX Trainer will call this function.\ndef run_fn(fn_args: tfx.components.FnArgs):\n  \"\"\"Train the model based on given args.\n\n  Args:\n    fn_args: Holds args used to train the model as name/value pairs.\n  \"\"\"\n\n  # This schema is usually either an output of SchemaGen or a manually-curated\n  # version provided by pipeline author. A schema can also derived from TFT\n  # graph if a Transform component is used. In the case when either is missing,\n  # `schema_from_feature_spec` could be used to generate schema from very simple\n  # feature_spec, but the schema returned would be very primitive.\n  schema = schema_utils.schema_from_feature_spec(_FEATURE_SPEC)\n\n  train_dataset = _input_fn(\n      fn_args.train_files,\n      fn_args.data_accessor,\n      schema,\n      batch_size=_TRAIN_BATCH_SIZE)\n  eval_dataset = _input_fn(\n      fn_args.eval_files,\n      fn_args.data_accessor,\n      schema,\n      batch_size=_EVAL_BATCH_SIZE)\n\n  # NEW: If we have a distribution strategy, build a model in a strategy scope.\n  strategy = _get_distribution_strategy(fn_args)\n  if strategy is None:\n    model = _make_keras_model()\n  else:\n    with strategy.scope():\n      model = _make_keras_model()\n\n  model.fit(\n      train_dataset,\n      steps_per_epoch=fn_args.train_steps,\n      validation_data=eval_dataset,\n      validation_steps=fn_args.eval_steps)\n\n  # The result of the training should be saved in `fn_args.serving_model_dir`\n  # directory.\n  model.save(fn_args.serving_model_dir, save_format='tf')\n</pre> %%writefile {_trainer_module_file}  # Copied from https://www.tensorflow.org/tfx/tutorials/tfx/penguin_simple and # slightly modified run_fn() to add distribution_strategy.  from typing import List from absl import logging import tensorflow as tf from tensorflow import keras from tensorflow_metadata.proto.v0 import schema_pb2 from tensorflow_transform.tf_metadata import schema_utils  from tfx import v1 as tfx from tfx_bsl.public import tfxio  _FEATURE_KEYS = [     'culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g' ] _LABEL_KEY = 'species'  _TRAIN_BATCH_SIZE = 20 _EVAL_BATCH_SIZE = 10  # Since we're not generating or creating a schema, we will instead create # a feature spec.  Since there are a fairly small number of features this is # manageable for this dataset. _FEATURE_SPEC = {     **{         feature: tf.io.FixedLenFeature(shape=[1], dtype=tf.float32)         for feature in _FEATURE_KEYS     }, _LABEL_KEY: tf.io.FixedLenFeature(shape=[1], dtype=tf.int64) }   def _input_fn(file_pattern: List[str],               data_accessor: tfx.components.DataAccessor,               schema: schema_pb2.Schema,               batch_size: int) -&gt; tf.data.Dataset:   \"\"\"Generates features and label for training.    Args:     file_pattern: List of paths or patterns of input tfrecord files.     data_accessor: DataAccessor for converting input to RecordBatch.     schema: schema of the input data.     batch_size: representing the number of consecutive elements of returned       dataset to combine in a single batch    Returns:     A dataset that contains (features, indices) tuple where features is a       dictionary of Tensors, and indices is a single Tensor of label indices.   \"\"\"   return data_accessor.tf_dataset_factory(       file_pattern,       tfxio.TensorFlowDatasetOptions(           batch_size=batch_size, label_key=_LABEL_KEY),       schema=schema).repeat()   def _make_keras_model() -&gt; tf.keras.Model:   \"\"\"Creates a DNN Keras model for classifying penguin data.    Returns:     A Keras Model.   \"\"\"   # The model below is built with Functional API, please refer to   # https://www.tensorflow.org/guide/keras/overview for all API options.   inputs = [keras.layers.Input(shape=(1,), name=f) for f in _FEATURE_KEYS]   d = keras.layers.concatenate(inputs)   for _ in range(2):     d = keras.layers.Dense(8, activation='relu')(d)   outputs = keras.layers.Dense(3)(d)    model = keras.Model(inputs=inputs, outputs=outputs)   model.compile(       optimizer=keras.optimizers.Adam(1e-2),       loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),       metrics=[keras.metrics.SparseCategoricalAccuracy()])    model.summary(print_fn=logging.info)   return model   # NEW: Read `use_gpu` from the custom_config of the Trainer. #      if it uses GPU, enable MirroredStrategy. def _get_distribution_strategy(fn_args: tfx.components.FnArgs):   if fn_args.custom_config.get('use_gpu', False):     logging.info('Using MirroredStrategy with one GPU.')     return tf.distribute.MirroredStrategy(devices=['device:GPU:0'])   return None   # TFX Trainer will call this function. def run_fn(fn_args: tfx.components.FnArgs):   \"\"\"Train the model based on given args.    Args:     fn_args: Holds args used to train the model as name/value pairs.   \"\"\"    # This schema is usually either an output of SchemaGen or a manually-curated   # version provided by pipeline author. A schema can also derived from TFT   # graph if a Transform component is used. In the case when either is missing,   # `schema_from_feature_spec` could be used to generate schema from very simple   # feature_spec, but the schema returned would be very primitive.   schema = schema_utils.schema_from_feature_spec(_FEATURE_SPEC)    train_dataset = _input_fn(       fn_args.train_files,       fn_args.data_accessor,       schema,       batch_size=_TRAIN_BATCH_SIZE)   eval_dataset = _input_fn(       fn_args.eval_files,       fn_args.data_accessor,       schema,       batch_size=_EVAL_BATCH_SIZE)    # NEW: If we have a distribution strategy, build a model in a strategy scope.   strategy = _get_distribution_strategy(fn_args)   if strategy is None:     model = _make_keras_model()   else:     with strategy.scope():       model = _make_keras_model()    model.fit(       train_dataset,       steps_per_epoch=fn_args.train_steps,       validation_data=eval_dataset,       validation_steps=fn_args.eval_steps)    # The result of the training should be saved in `fn_args.serving_model_dir`   # directory.   model.save(fn_args.serving_model_dir, save_format='tf') <p>Copy the module file to GCS which can be accessed from the pipeline components.</p> <p>Otherwise, you might want to build a container image including the module file and use the image to run the pipeline and AI Platform Training jobs.</p> In\u00a0[\u00a0]: Copied! <pre>!gsutil cp {_trainer_module_file} {MODULE_ROOT}/\n</pre> !gsutil cp {_trainer_module_file} {MODULE_ROOT}/ In\u00a0[\u00a0]: Copied! <pre>def _create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str,\n                     module_file: str, endpoint_name: str, project_id: str,\n                     region: str, use_gpu: bool) -&gt; tfx.dsl.Pipeline:\n  \"\"\"Implements the penguin pipeline with TFX.\"\"\"\n  # Brings data into the pipeline or otherwise joins/converts training data.\n  example_gen = tfx.components.CsvExampleGen(input_base=data_root)\n\n  # NEW: Configuration for Vertex AI Training.\n  # This dictionary will be passed as `CustomJobSpec`.\n  vertex_job_spec = {\n      'project': project_id,\n      'worker_pool_specs': [{\n          'machine_spec': {\n              'machine_type': 'n1-standard-4',\n          },\n          'replica_count': 1,\n          'container_spec': {\n              'image_uri': 'gcr.io/tfx-oss-public/tfx:{}'.format(tfx.__version__),\n          },\n      }],\n  }\n  if use_gpu:\n    # See https://cloud.google.com/vertex-ai/docs/reference/rest/v1/MachineSpec#acceleratortype\n    # for available machine types.\n    vertex_job_spec['worker_pool_specs'][0]['machine_spec'].update({\n        'accelerator_type': 'NVIDIA_TESLA_K80',\n        'accelerator_count': 1\n    })\n\n  # Trains a model using Vertex AI Training.\n  # NEW: We need to specify a Trainer for GCP with related configs.\n  trainer = tfx.extensions.google_cloud_ai_platform.Trainer(\n      module_file=module_file,\n      examples=example_gen.outputs['examples'],\n      train_args=tfx.proto.TrainArgs(num_steps=100),\n      eval_args=tfx.proto.EvalArgs(num_steps=5),\n      custom_config={\n          tfx.extensions.google_cloud_ai_platform.ENABLE_VERTEX_KEY:\n              True,\n          tfx.extensions.google_cloud_ai_platform.VERTEX_REGION_KEY:\n              region,\n          tfx.extensions.google_cloud_ai_platform.TRAINING_ARGS_KEY:\n              vertex_job_spec,\n          'use_gpu':\n              use_gpu,\n      })\n\n  # NEW: Configuration for pusher.\n  vertex_serving_spec = {\n      'project_id': project_id,\n      'endpoint_name': endpoint_name,\n      # Remaining argument is passed to aiplatform.Model.deploy()\n      # See https://cloud.google.com/vertex-ai/docs/predictions/deploy-model-api#deploy_the_model\n      # for the detail.\n      #\n      # Machine type is the compute resource to serve prediction requests.\n      # See https://cloud.google.com/vertex-ai/docs/predictions/configure-compute#machine-types\n      # for available machine types and acccerators.\n      'machine_type': 'n1-standard-4',\n  }\n\n  # Vertex AI provides pre-built containers with various configurations for\n  # serving.\n  # See https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers\n  # for available container images.\n  serving_image = 'us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-6:latest'\n  if use_gpu:\n    vertex_serving_spec.update({\n        'accelerator_type': 'NVIDIA_TESLA_K80',\n        'accelerator_count': 1\n    })\n    serving_image = 'us-docker.pkg.dev/vertex-ai/prediction/tf2-gpu.2-6:latest'\n\n  # NEW: Pushes the model to Vertex AI.\n  pusher = tfx.extensions.google_cloud_ai_platform.Pusher(\n      model=trainer.outputs['model'],\n      custom_config={\n          tfx.extensions.google_cloud_ai_platform.ENABLE_VERTEX_KEY:\n              True,\n          tfx.extensions.google_cloud_ai_platform.VERTEX_REGION_KEY:\n              region,\n          tfx.extensions.google_cloud_ai_platform.VERTEX_CONTAINER_IMAGE_URI_KEY:\n              serving_image,\n          tfx.extensions.google_cloud_ai_platform.SERVING_ARGS_KEY:\n            vertex_serving_spec,\n      })\n\n  components = [\n      example_gen,\n      trainer,\n      pusher,\n  ]\n\n  return tfx.dsl.Pipeline(\n      pipeline_name=pipeline_name,\n      pipeline_root=pipeline_root,\n      components=components)\n</pre> def _create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str,                      module_file: str, endpoint_name: str, project_id: str,                      region: str, use_gpu: bool) -&gt; tfx.dsl.Pipeline:   \"\"\"Implements the penguin pipeline with TFX.\"\"\"   # Brings data into the pipeline or otherwise joins/converts training data.   example_gen = tfx.components.CsvExampleGen(input_base=data_root)    # NEW: Configuration for Vertex AI Training.   # This dictionary will be passed as `CustomJobSpec`.   vertex_job_spec = {       'project': project_id,       'worker_pool_specs': [{           'machine_spec': {               'machine_type': 'n1-standard-4',           },           'replica_count': 1,           'container_spec': {               'image_uri': 'gcr.io/tfx-oss-public/tfx:{}'.format(tfx.__version__),           },       }],   }   if use_gpu:     # See https://cloud.google.com/vertex-ai/docs/reference/rest/v1/MachineSpec#acceleratortype     # for available machine types.     vertex_job_spec['worker_pool_specs'][0]['machine_spec'].update({         'accelerator_type': 'NVIDIA_TESLA_K80',         'accelerator_count': 1     })    # Trains a model using Vertex AI Training.   # NEW: We need to specify a Trainer for GCP with related configs.   trainer = tfx.extensions.google_cloud_ai_platform.Trainer(       module_file=module_file,       examples=example_gen.outputs['examples'],       train_args=tfx.proto.TrainArgs(num_steps=100),       eval_args=tfx.proto.EvalArgs(num_steps=5),       custom_config={           tfx.extensions.google_cloud_ai_platform.ENABLE_VERTEX_KEY:               True,           tfx.extensions.google_cloud_ai_platform.VERTEX_REGION_KEY:               region,           tfx.extensions.google_cloud_ai_platform.TRAINING_ARGS_KEY:               vertex_job_spec,           'use_gpu':               use_gpu,       })    # NEW: Configuration for pusher.   vertex_serving_spec = {       'project_id': project_id,       'endpoint_name': endpoint_name,       # Remaining argument is passed to aiplatform.Model.deploy()       # See https://cloud.google.com/vertex-ai/docs/predictions/deploy-model-api#deploy_the_model       # for the detail.       #       # Machine type is the compute resource to serve prediction requests.       # See https://cloud.google.com/vertex-ai/docs/predictions/configure-compute#machine-types       # for available machine types and acccerators.       'machine_type': 'n1-standard-4',   }    # Vertex AI provides pre-built containers with various configurations for   # serving.   # See https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers   # for available container images.   serving_image = 'us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-6:latest'   if use_gpu:     vertex_serving_spec.update({         'accelerator_type': 'NVIDIA_TESLA_K80',         'accelerator_count': 1     })     serving_image = 'us-docker.pkg.dev/vertex-ai/prediction/tf2-gpu.2-6:latest'    # NEW: Pushes the model to Vertex AI.   pusher = tfx.extensions.google_cloud_ai_platform.Pusher(       model=trainer.outputs['model'],       custom_config={           tfx.extensions.google_cloud_ai_platform.ENABLE_VERTEX_KEY:               True,           tfx.extensions.google_cloud_ai_platform.VERTEX_REGION_KEY:               region,           tfx.extensions.google_cloud_ai_platform.VERTEX_CONTAINER_IMAGE_URI_KEY:               serving_image,           tfx.extensions.google_cloud_ai_platform.SERVING_ARGS_KEY:             vertex_serving_spec,       })    components = [       example_gen,       trainer,       pusher,   ]    return tfx.dsl.Pipeline(       pipeline_name=pipeline_name,       pipeline_root=pipeline_root,       components=components) In\u00a0[\u00a0]: Copied! <pre># docs_infra: no_execute\nimport os\n\nPIPELINE_DEFINITION_FILE = PIPELINE_NAME + '_pipeline.json'\n\nrunner = tfx.orchestration.experimental.KubeflowV2DagRunner(\n    config=tfx.orchestration.experimental.KubeflowV2DagRunnerConfig(),\n    output_filename=PIPELINE_DEFINITION_FILE)\n_ = runner.run(\n    _create_pipeline(\n        pipeline_name=PIPELINE_NAME,\n        pipeline_root=PIPELINE_ROOT,\n        data_root=DATA_ROOT,\n        module_file=os.path.join(MODULE_ROOT, _trainer_module_file),\n        endpoint_name=ENDPOINT_NAME,\n        project_id=GOOGLE_CLOUD_PROJECT,\n        region=GOOGLE_CLOUD_REGION,\n        # We will use CPUs only for now.\n        use_gpu=False))\n</pre> # docs_infra: no_execute import os  PIPELINE_DEFINITION_FILE = PIPELINE_NAME + '_pipeline.json'  runner = tfx.orchestration.experimental.KubeflowV2DagRunner(     config=tfx.orchestration.experimental.KubeflowV2DagRunnerConfig(),     output_filename=PIPELINE_DEFINITION_FILE) _ = runner.run(     _create_pipeline(         pipeline_name=PIPELINE_NAME,         pipeline_root=PIPELINE_ROOT,         data_root=DATA_ROOT,         module_file=os.path.join(MODULE_ROOT, _trainer_module_file),         endpoint_name=ENDPOINT_NAME,         project_id=GOOGLE_CLOUD_PROJECT,         region=GOOGLE_CLOUD_REGION,         # We will use CPUs only for now.         use_gpu=False)) <p>The generated definition file can be submitted using Google Cloud aiplatform client in <code>google-cloud-aiplatform</code> package.</p> In\u00a0[\u00a0]: Copied! <pre># docs_infra: no_execute\nfrom google.cloud import aiplatform\nfrom google.cloud.aiplatform import pipeline_jobs\nimport logging\nlogging.getLogger().setLevel(logging.INFO)\n\naiplatform.init(project=GOOGLE_CLOUD_PROJECT, location=GOOGLE_CLOUD_REGION)\n\njob = pipeline_jobs.PipelineJob(template_path=PIPELINE_DEFINITION_FILE,\n                                display_name=PIPELINE_NAME)\njob.submit()\n</pre> # docs_infra: no_execute from google.cloud import aiplatform from google.cloud.aiplatform import pipeline_jobs import logging logging.getLogger().setLevel(logging.INFO)  aiplatform.init(project=GOOGLE_CLOUD_PROJECT, location=GOOGLE_CLOUD_REGION)  job = pipeline_jobs.PipelineJob(template_path=PIPELINE_DEFINITION_FILE,                                 display_name=PIPELINE_NAME) job.submit() <p>Now you can visit the link in the output above or visit 'Vertex AI &gt; Pipelines' in Google Cloud Console to see the progress.</p> In\u00a0[\u00a0]: Copied! <pre>ENDPOINT_ID=''     # &lt;--- ENTER THIS\nif not ENDPOINT_ID:\n    from absl import logging\n    logging.error('Please set the endpoint id.')\n</pre> ENDPOINT_ID=''     # &lt;--- ENTER THIS if not ENDPOINT_ID:     from absl import logging     logging.error('Please set the endpoint id.') <p>We use the same aiplatform client to send a request to the endpoint. We will send a prediction request for Penguin species classification. The input is the four features that we used, and the model will return three values, because our model outputs one value for each species.</p> <p>For example, the following specific example has the largest value at index '2' and will print '2'.</p> In\u00a0[\u00a0]: Copied! <pre># docs_infra: no_execute\nimport numpy as np\n\n# The AI Platform services require regional API endpoints.\nclient_options = {\n    'api_endpoint': GOOGLE_CLOUD_REGION + '-aiplatform.googleapis.com'\n    }\n# Initialize client that will be used to create and send requests.\nclient = aiplatform.gapic.PredictionServiceClient(client_options=client_options)\n\n# Set data values for the prediction request.\n# Our model expects 4 feature inputs and produces 3 output values for each\n# species. Note that the output is logit value rather than probabilities.\n# See the model code to understand input / output structure.\ninstances = [{\n    'culmen_length_mm':[0.71],\n    'culmen_depth_mm':[0.38],\n    'flipper_length_mm':[0.98],\n    'body_mass_g': [0.78],\n}]\n\nendpoint = client.endpoint_path(\n    project=GOOGLE_CLOUD_PROJECT,\n    location=GOOGLE_CLOUD_REGION,\n    endpoint=ENDPOINT_ID,\n)\n# Send a prediction request and get response.\nresponse = client.predict(endpoint=endpoint, instances=instances)\n\n# Uses argmax to find the index of the maximum value.\nprint('species:', np.argmax(response.predictions[0]))\n</pre> # docs_infra: no_execute import numpy as np  # The AI Platform services require regional API endpoints. client_options = {     'api_endpoint': GOOGLE_CLOUD_REGION + '-aiplatform.googleapis.com'     } # Initialize client that will be used to create and send requests. client = aiplatform.gapic.PredictionServiceClient(client_options=client_options)  # Set data values for the prediction request. # Our model expects 4 feature inputs and produces 3 output values for each # species. Note that the output is logit value rather than probabilities. # See the model code to understand input / output structure. instances = [{     'culmen_length_mm':[0.71],     'culmen_depth_mm':[0.38],     'flipper_length_mm':[0.98],     'body_mass_g': [0.78], }]  endpoint = client.endpoint_path(     project=GOOGLE_CLOUD_PROJECT,     location=GOOGLE_CLOUD_REGION,     endpoint=ENDPOINT_ID, ) # Send a prediction request and get response. response = client.predict(endpoint=endpoint, instances=instances)  # Uses argmax to find the index of the maximum value. print('species:', np.argmax(response.predictions[0])) <p>For detailed information about online prediction, please visit the Endpoints page in <code>Google Cloud Console</code>. you can find a guide on sending sample requests and links to more resources.</p> <p>Vertex AI supports training using various machine types including support for GPUs. See Machine spec reference for available options.</p> <p>We already defined our pipeline to support GPU training. All we need to do is setting <code>use_gpu</code> flag to True. Then a pipeline will be created with a machine spec including one NVIDIA_TESLA_K80 and our model training code will use <code>tf.distribute.MirroredStrategy</code>.</p> <p>Note that <code>use_gpu</code> flag is not a part of the Vertex or TFX API. It is just used to control the training code in this tutorial.</p> In\u00a0[\u00a0]: Copied! <pre># docs_infra: no_execute\nrunner.run(\n    _create_pipeline(\n        pipeline_name=PIPELINE_NAME,\n        pipeline_root=PIPELINE_ROOT,\n        data_root=DATA_ROOT,\n        module_file=os.path.join(MODULE_ROOT, _trainer_module_file),\n        endpoint_name=ENDPOINT_NAME,\n        project_id=GOOGLE_CLOUD_PROJECT,\n        region=GOOGLE_CLOUD_REGION,\n        # Updated: Use GPUs. We will use a NVIDIA_TESLA_K80 and \n        # the model code will use tf.distribute.MirroredStrategy.\n        use_gpu=True))\n\njob = pipeline_jobs.PipelineJob(template_path=PIPELINE_DEFINITION_FILE,\n                                display_name=PIPELINE_NAME)\njob.submit()\n</pre> # docs_infra: no_execute runner.run(     _create_pipeline(         pipeline_name=PIPELINE_NAME,         pipeline_root=PIPELINE_ROOT,         data_root=DATA_ROOT,         module_file=os.path.join(MODULE_ROOT, _trainer_module_file),         endpoint_name=ENDPOINT_NAME,         project_id=GOOGLE_CLOUD_PROJECT,         region=GOOGLE_CLOUD_REGION,         # Updated: Use GPUs. We will use a NVIDIA_TESLA_K80 and          # the model code will use tf.distribute.MirroredStrategy.         use_gpu=True))  job = pipeline_jobs.PipelineJob(template_path=PIPELINE_DEFINITION_FILE,                                 display_name=PIPELINE_NAME) job.submit() <p>Now you can visit the link in the output above or visit 'Vertex AI &gt; Pipelines' in Google Cloud Console to see the progress.</p>"},{"location":"tutorials/tfx/gcp/vertex_pipelines_vertex_training/#copyright-2021-the-tensorflow-authors","title":"Copyright 2021 The TensorFlow Authors.\u00b6","text":""},{"location":"tutorials/tfx/gcp/vertex_pipelines_vertex_training/#vertex-ai-training-and-serving-with-tfx-and-vertex-pipelines","title":"Vertex AI Training and Serving with TFX and Vertex Pipelines\u00b6","text":""},{"location":"tutorials/tfx/gcp/vertex_pipelines_vertex_training/#set-up","title":"Set up\u00b6","text":"<p>If you have completed Simple TFX Pipeline for Vertex Pipelines Tutorial, you will have a working GCP project and a GCS bucket and that is all we need for this tutorial. Please read the preliminary tutorial first if you missed it.</p>"},{"location":"tutorials/tfx/gcp/vertex_pipelines_vertex_training/#install-python-packages","title":"Install python packages\u00b6","text":""},{"location":"tutorials/tfx/gcp/vertex_pipelines_vertex_training/#did-you-restart-the-runtime","title":"Did you restart the runtime?\u00b6","text":"<p>If you are using Google Colab, the first time that you run the cell above, you must restart the runtime by clicking above \"RESTART RUNTIME\" button or using \"Runtime &gt; Restart runtime ...\" menu. This is because of the way that Colab loads packages.</p>"},{"location":"tutorials/tfx/gcp/vertex_pipelines_vertex_training/#login-in-to-google-for-this-notebook","title":"Login in to Google for this notebook\u00b6","text":"<p>If you are running this notebook on Colab, authenticate with your user account:</p>"},{"location":"tutorials/tfx/gcp/vertex_pipelines_vertex_training/#set-up-variables","title":"Set up variables\u00b6","text":"<p>We will set up some variables used to customize the pipelines below. Following information is required:</p> <ul> <li>GCP Project id. See Identifying your project id.</li> <li>GCP Region to run pipelines. For more information about the regions that Vertex Pipelines is available in, see the Vertex AI locations guide.</li> <li>Google Cloud Storage Bucket to store pipeline outputs.</li> </ul> <p>Enter required values in the cell below before running it.</p>"},{"location":"tutorials/tfx/gcp/vertex_pipelines_vertex_training/#prepare-example-data","title":"Prepare example data\u00b6","text":"<p>We will use the same Palmer Penguins dataset as Simple TFX Pipeline Tutorial.</p> <p>There are four numeric features in this dataset which were already normalized to have range [0,1]. We will build a classification model which predicts the <code>species</code> of penguins.</p>"},{"location":"tutorials/tfx/gcp/vertex_pipelines_vertex_training/#create-a-pipeline","title":"Create a pipeline\u00b6","text":"<p>Our pipeline will be very similar to the pipeline we created in Simple TFX Pipeline for Vertex Pipelines Tutorial. The pipeline will consists of three components, CsvExampleGen, Trainer and Pusher. But we will use a special Trainer and Pusher component. The Trainer component will move training workloads to Vertex AI, and the Pusher component will publish the trained ML model to Vertex AI instead of a filesystem.</p> <p>TFX provides a special <code>Trainer</code> to submit training jobs to Vertex AI Training service. All we have to do is use <code>Trainer</code> in the extension module instead of the standard <code>Trainer</code> component along with some required GCP parameters.</p> <p>In this tutorial, we will run Vertex AI Training jobs only using CPUs first and then with a GPU.</p> <p>TFX also provides a special <code>Pusher</code> to upload the model to Vertex AI Models. <code>Pusher</code> will create Vertex AI Endpoint resource to serve online perdictions, too. See Vertex AI documentation to learn more about online predictions provided by Vertex AI.</p>"},{"location":"tutorials/tfx/gcp/vertex_pipelines_vertex_training/#write-model-code","title":"Write model code.\u00b6","text":"<p>The model itself is almost similar to the model in Simple TFX Pipeline Tutorial.</p> <p>We will add <code>_get_distribution_strategy()</code> function which creates a TensorFlow distribution strategy and it is used in <code>run_fn</code> to use MirroredStrategy if GPU is available.</p>"},{"location":"tutorials/tfx/gcp/vertex_pipelines_vertex_training/#write-a-pipeline-definition","title":"Write a pipeline definition\u00b6","text":"<p>We will define a function to create a TFX pipeline. It has the same three Components as in Simple TFX Pipeline Tutorial, but we use a <code>Trainer</code> and <code>Pusher</code> component in the GCP extension module.</p> <p><code>tfx.extensions.google_cloud_ai_platform.Trainer</code> behaves like a regular <code>Trainer</code>, but it just moves the computation for the model training to cloud. It launches a custom job in Vertex AI Training service and the trainer component in the orchestration system will just wait until the Vertex AI Training job completes.</p> <p><code>tfx.extensions.google_cloud_ai_platform.Pusher</code>  creates a Vertex AI Model and a Vertex AI Endpoint using the trained model.</p>"},{"location":"tutorials/tfx/gcp/vertex_pipelines_vertex_training/#run-the-pipeline-on-vertex-pipelines","title":"Run the pipeline on Vertex Pipelines.\u00b6","text":"<p>We will use Vertex Pipelines to run the pipeline as we did in Simple TFX Pipeline for Vertex Pipelines Tutorial.</p>"},{"location":"tutorials/tfx/gcp/vertex_pipelines_vertex_training/#test-with-a-prediction-request","title":"Test with a prediction request\u00b6","text":"<p>Once the pipeline completes, you will find a deployed model at the one of the endpoints in 'Vertex AI &gt; Endpoints'. We need to know the id of the endpoint to send a prediction request to the new endpoint. This is different from the endpoint name we entered above. You can find the id at the Endpoints page in <code>Google Cloud Console</code>, it looks like a very long number.</p> <p>Set ENDPOINT_ID below before running it.</p>"},{"location":"tutorials/tfx/gcp/vertex_pipelines_vertex_training/#run-the-pipeline-using-a-gpu","title":"Run the pipeline using a GPU\u00b6","text":""},{"location":"tutorials/tfx/gcp/vertex_pipelines_vertex_training/#cleaning-up","title":"Cleaning up\u00b6","text":"<p>You have created a Vertex AI Model and Endpoint in this tutorial. Please delete these resources to avoid any unwanted charges by going to Endpoints and undeploying the model from the endpoint first. Then you can delete the endpoint and the model separately.</p>"},{"location":"tutorials/transform/census/","title":"Preprocessing data with TensorFlow Transform","text":"<p>Note: We recommend running this tutorial in a Colab notebook, with no setup required!  Just click \"Run in Google Colab\".</p>        View on TensorFlow.org             Run in Google Colab             View source on GitHub             Download notebook      In\u00a0[\u00a0]: Copied! <pre>#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n</pre> #@title Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # https://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. In\u00a0[\u00a0]: Copied! <pre>!pip install tensorflow-transform\n</pre> !pip install tensorflow-transform In\u00a0[\u00a0]: Copied! <pre># This cell is only necessary because packages were installed while python was\n# running. It avoids the need to restart the runtime when running in Colab.\nimport pkg_resources\nimport importlib\n\nimportlib.reload(pkg_resources)\n</pre> # This cell is only necessary because packages were installed while python was # running. It avoids the need to restart the runtime when running in Colab. import pkg_resources import importlib  importlib.reload(pkg_resources) In\u00a0[\u00a0]: Copied! <pre>import math\nimport os\nimport pprint\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nprint('TF: {}'.format(tf.__version__))\n\nimport apache_beam as beam\nprint('Beam: {}'.format(beam.__version__))\n\nimport tensorflow_transform as tft\nimport tensorflow_transform.beam as tft_beam\nfrom tensorflow_transform.keras_lib import tf_keras\nprint('Transform: {}'.format(tft.__version__))\n\nfrom tfx_bsl.public import tfxio\nfrom tfx_bsl.coders.example_coder import RecordBatchToExamplesEncoder\n</pre> import math import os import pprint  import pandas as pd import matplotlib.pyplot as plt  import tensorflow as tf print('TF: {}'.format(tf.__version__))  import apache_beam as beam print('Beam: {}'.format(beam.__version__))  import tensorflow_transform as tft import tensorflow_transform.beam as tft_beam from tensorflow_transform.keras_lib import tf_keras print('Transform: {}'.format(tft.__version__))  from tfx_bsl.public import tfxio from tfx_bsl.coders.example_coder import RecordBatchToExamplesEncoder <p>Next download the data files:</p> In\u00a0[\u00a0]: Copied! <pre>!wget https://storage.googleapis.com/artifacts.tfx-oss-public.appspot.com/datasets/census/adult.data\n!wget https://storage.googleapis.com/artifacts.tfx-oss-public.appspot.com/datasets/census/adult.test\n\ntrain_path = './adult.data'\ntest_path = './adult.test'\n</pre> !wget https://storage.googleapis.com/artifacts.tfx-oss-public.appspot.com/datasets/census/adult.data !wget https://storage.googleapis.com/artifacts.tfx-oss-public.appspot.com/datasets/census/adult.test  train_path = './adult.data' test_path = './adult.test' In\u00a0[\u00a0]: Copied! <pre>CATEGORICAL_FEATURE_KEYS = [\n    'workclass',\n    'education',\n    'marital-status',\n    'occupation',\n    'relationship',\n    'race',\n    'sex',\n    'native-country',\n]\n\nNUMERIC_FEATURE_KEYS = [\n    'age',\n    'capital-gain',\n    'capital-loss',\n    'hours-per-week',\n    'education-num'\n]\n\nORDERED_CSV_COLUMNS = [\n    'age', 'workclass', 'fnlwgt', 'education', 'education-num',\n    'marital-status', 'occupation', 'relationship', 'race', 'sex',\n    'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'label'\n]\n\nLABEL_KEY = 'label'\n</pre> CATEGORICAL_FEATURE_KEYS = [     'workclass',     'education',     'marital-status',     'occupation',     'relationship',     'race',     'sex',     'native-country', ]  NUMERIC_FEATURE_KEYS = [     'age',     'capital-gain',     'capital-loss',     'hours-per-week',     'education-num' ]  ORDERED_CSV_COLUMNS = [     'age', 'workclass', 'fnlwgt', 'education', 'education-num',     'marital-status', 'occupation', 'relationship', 'race', 'sex',     'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'label' ]  LABEL_KEY = 'label' <p>Here's a quick preview of the data:</p> In\u00a0[\u00a0]: Copied! <pre>pandas_train = pd.read_csv(train_path, header=None, names=ORDERED_CSV_COLUMNS)\n\npandas_train.head(5)\n</pre> pandas_train = pd.read_csv(train_path, header=None, names=ORDERED_CSV_COLUMNS)  pandas_train.head(5) In\u00a0[\u00a0]: Copied! <pre>one_row = dict(pandas_train.loc[0])\n</pre> one_row = dict(pandas_train.loc[0]) In\u00a0[\u00a0]: Copied! <pre>COLUMN_DEFAULTS = [\n  '' if isinstance(v, str) else 0.0\n  for v in  dict(pandas_train.loc[1]).values()]\n</pre> COLUMN_DEFAULTS = [   '' if isinstance(v, str) else 0.0   for v in  dict(pandas_train.loc[1]).values()] <p>The test data has 1 header line that needs to be skipped, and a trailing \".\" at the end of each line.</p> In\u00a0[\u00a0]: Copied! <pre>pandas_test = pd.read_csv(test_path, header=1, names=ORDERED_CSV_COLUMNS)\n\npandas_test.head(5)\n</pre> pandas_test = pd.read_csv(test_path, header=1, names=ORDERED_CSV_COLUMNS)  pandas_test.head(5) In\u00a0[\u00a0]: Copied! <pre>testing = os.getenv(\"WEB_TEST_BROWSER\", False)\nif testing:\n  pandas_train = pandas_train.loc[:1]\n  pandas_test = pandas_test.loc[:1]\n</pre> testing = os.getenv(\"WEB_TEST_BROWSER\", False) if testing:   pandas_train = pandas_train.loc[:1]   pandas_test = pandas_test.loc[:1] In\u00a0[\u00a0]: Copied! <pre>RAW_DATA_FEATURE_SPEC = dict(\n    [(name, tf.io.FixedLenFeature([], tf.string))\n     for name in CATEGORICAL_FEATURE_KEYS] +\n    [(name, tf.io.FixedLenFeature([], tf.float32))\n     for name in NUMERIC_FEATURE_KEYS] + \n    [(LABEL_KEY, tf.io.FixedLenFeature([], tf.string))]\n)\n\nSCHEMA = tft.DatasetMetadata.from_feature_spec(RAW_DATA_FEATURE_SPEC).schema\n</pre> RAW_DATA_FEATURE_SPEC = dict(     [(name, tf.io.FixedLenFeature([], tf.string))      for name in CATEGORICAL_FEATURE_KEYS] +     [(name, tf.io.FixedLenFeature([], tf.float32))      for name in NUMERIC_FEATURE_KEYS] +      [(LABEL_KEY, tf.io.FixedLenFeature([], tf.string))] )  SCHEMA = tft.DatasetMetadata.from_feature_spec(RAW_DATA_FEATURE_SPEC).schema <p>This tutorial needs to convert examples from the dataset to and from <code>tf.train.Example</code> protos in a few places.</p> <p>The hidden <code>encode_example</code> function below converts a dictionary of features forom the dataset to a <code>tf.train.Example</code>.</p> In\u00a0[\u00a0]: Copied! <pre>#@title\ndef encode_example(input_features):\n  input_features = dict(input_features)\n  output_features = {}\n  \n  for key in CATEGORICAL_FEATURE_KEYS:\n    value = input_features[key]\n    feature = tf.train.Feature(\n        bytes_list=tf.train.BytesList(value=[value.strip().encode()]))\n    output_features[key] = feature \n\n  for key in NUMERIC_FEATURE_KEYS:\n    value = input_features[key]\n    feature = tf.train.Feature(\n        float_list=tf.train.FloatList(value=[value]))\n    output_features[key] = feature \n\n  label_value = input_features.get(LABEL_KEY, None)\n  if label_value is not None:\n    output_features[LABEL_KEY]  = tf.train.Feature(\n        bytes_list = tf.train.BytesList(value=[label_value.strip().encode()]))\n\n  example = tf.train.Example(\n      features = tf.train.Features(feature=output_features)\n  )\n  return example\n</pre> #@title def encode_example(input_features):   input_features = dict(input_features)   output_features = {}      for key in CATEGORICAL_FEATURE_KEYS:     value = input_features[key]     feature = tf.train.Feature(         bytes_list=tf.train.BytesList(value=[value.strip().encode()]))     output_features[key] = feature     for key in NUMERIC_FEATURE_KEYS:     value = input_features[key]     feature = tf.train.Feature(         float_list=tf.train.FloatList(value=[value]))     output_features[key] = feature     label_value = input_features.get(LABEL_KEY, None)   if label_value is not None:     output_features[LABEL_KEY]  = tf.train.Feature(         bytes_list = tf.train.BytesList(value=[label_value.strip().encode()]))    example = tf.train.Example(       features = tf.train.Features(feature=output_features)   )   return example <p>Now you can convert dataset examples into <code>Example</code> protos:</p> In\u00a0[\u00a0]: Copied! <pre>tf_example = encode_example(pandas_train.loc[0])\ntf_example.features.feature['age']\n</pre> tf_example = encode_example(pandas_train.loc[0]) tf_example.features.feature['age'] In\u00a0[\u00a0]: Copied! <pre>serialized_example_batch = tf.constant([\n  encode_example(pandas_train.loc[i]).SerializeToString()\n  for i in range(3)\n])\n\nserialized_example_batch\n</pre> serialized_example_batch = tf.constant([   encode_example(pandas_train.loc[i]).SerializeToString()   for i in range(3) ])  serialized_example_batch <p>You can also convert batches of serialized Example protos back into a dictionary of tensors:</p> In\u00a0[\u00a0]: Copied! <pre>decoded_tensors = tf.io.parse_example(\n    serialized_example_batch,\n    features=RAW_DATA_FEATURE_SPEC\n)\n</pre> decoded_tensors = tf.io.parse_example(     serialized_example_batch,     features=RAW_DATA_FEATURE_SPEC ) <p>In some cases the label will not be passed in, so the encode function is written so that the label is optional:</p> In\u00a0[\u00a0]: Copied! <pre>features_dict = dict(pandas_train.loc[0])\nfeatures_dict.pop(LABEL_KEY)\n\nLABEL_KEY in features_dict\n</pre> features_dict = dict(pandas_train.loc[0]) features_dict.pop(LABEL_KEY)  LABEL_KEY in features_dict <p>When creating an <code>Example</code> proto it will simply not contain the label key.</p> In\u00a0[\u00a0]: Copied! <pre>no_label_example = encode_example(features_dict)\n\nLABEL_KEY in no_label_example.features.feature.keys()\n</pre> no_label_example = encode_example(features_dict)  LABEL_KEY in no_label_example.features.feature.keys() In\u00a0[\u00a0]: Copied! <pre>NUM_OOV_BUCKETS = 1\n\nEPOCH_SPLITS = 10\nTRAIN_NUM_EPOCHS = 2*EPOCH_SPLITS\nNUM_TRAIN_INSTANCES = len(pandas_train)\nNUM_TEST_INSTANCES = len(pandas_test)\n\nBATCH_SIZE = 128\n\nSTEPS_PER_TRAIN_EPOCH = tf.math.ceil(NUM_TRAIN_INSTANCES/BATCH_SIZE/EPOCH_SPLITS)\nEVALUATION_STEPS = tf.math.ceil(NUM_TEST_INSTANCES/BATCH_SIZE)\n\n# Names of temp files\nTRANSFORMED_TRAIN_DATA_FILEBASE = 'train_transformed'\nTRANSFORMED_TEST_DATA_FILEBASE = 'test_transformed'\nEXPORTED_MODEL_DIR = 'exported_model_dir'\n</pre> NUM_OOV_BUCKETS = 1  EPOCH_SPLITS = 10 TRAIN_NUM_EPOCHS = 2*EPOCH_SPLITS NUM_TRAIN_INSTANCES = len(pandas_train) NUM_TEST_INSTANCES = len(pandas_test)  BATCH_SIZE = 128  STEPS_PER_TRAIN_EPOCH = tf.math.ceil(NUM_TRAIN_INSTANCES/BATCH_SIZE/EPOCH_SPLITS) EVALUATION_STEPS = tf.math.ceil(NUM_TEST_INSTANCES/BATCH_SIZE)  # Names of temp files TRANSFORMED_TRAIN_DATA_FILEBASE = 'train_transformed' TRANSFORMED_TEST_DATA_FILEBASE = 'test_transformed' EXPORTED_MODEL_DIR = 'exported_model_dir' In\u00a0[\u00a0]: Copied! <pre>if testing:\n  TRAIN_NUM_EPOCHS = 1\n</pre> if testing:   TRAIN_NUM_EPOCHS = 1 <p>Here is a <code>preprocessing_fn</code> for this dataset. It does several things:</p> <ol> <li>Using <code>tft.scale_to_0_1</code>, it scales the numeric features to the <code>[0,1]</code> range.</li> <li>Using <code>tft.compute_and_apply_vocabulary</code>, it computes a vocabulary for each of the categorical features, and returns the integer IDs for each input as an <code>tf.int64</code>. This applies both to string and integer categorical-inputs.</li> <li>It applies some manual transformations to the data using standard TensorFlow operations. Here these operations are applied to the label but could transform the features as well. The TensorFlow operations do several things:</li> </ol> <ul> <li>They build a lookup table for the label (the <code>tf.init_scope</code> ensures that the table is only created the first time the function is called).</li> <li>They normalize the text of the label.</li> <li>They convert the label to a one-hot.</li> </ul> In\u00a0[\u00a0]: Copied! <pre>def preprocessing_fn(inputs):\n  \"\"\"Preprocess input columns into transformed columns.\"\"\"\n  # Since we are modifying some features and leaving others unchanged, we\n  # start by setting `outputs` to a copy of `inputs.\n  outputs = inputs.copy()\n\n  # Scale numeric columns to have range [0, 1].\n  for key in NUMERIC_FEATURE_KEYS:\n    outputs[key] = tft.scale_to_0_1(inputs[key])\n\n  # For all categorical columns except the label column, we generate a\n  # vocabulary but do not modify the feature.  This vocabulary is instead\n  # used in the trainer, by means of a feature column, to convert the feature\n  # from a string to an integer id.\n  for key in CATEGORICAL_FEATURE_KEYS:\n    outputs[key] = tft.compute_and_apply_vocabulary(\n        tf.strings.strip(inputs[key]),\n        num_oov_buckets=NUM_OOV_BUCKETS,\n        vocab_filename=key)\n\n  # For the label column we provide the mapping from string to index.\n  table_keys = ['&gt;50K', '&lt;=50K']\n  with tf.init_scope():\n    initializer = tf.lookup.KeyValueTensorInitializer(\n        keys=table_keys,\n        values=tf.cast(tf.range(len(table_keys)), tf.int64),\n        key_dtype=tf.string,\n        value_dtype=tf.int64)\n    table = tf.lookup.StaticHashTable(initializer, default_value=-1)\n\n  # Remove trailing periods for test data when the data is read with tf.data.\n  # label_str  = tf.sparse.to_dense(inputs[LABEL_KEY])\n  label_str = inputs[LABEL_KEY]\n  label_str = tf.strings.regex_replace(label_str, r'\\.$', '')\n  label_str = tf.strings.strip(label_str)\n  data_labels = table.lookup(label_str)\n  transformed_label = tf.one_hot(\n      indices=data_labels, depth=len(table_keys), on_value=1.0, off_value=0.0)\n  outputs[LABEL_KEY] = tf.reshape(transformed_label, [-1, len(table_keys)])\n\n  return outputs\n</pre> def preprocessing_fn(inputs):   \"\"\"Preprocess input columns into transformed columns.\"\"\"   # Since we are modifying some features and leaving others unchanged, we   # start by setting `outputs` to a copy of `inputs.   outputs = inputs.copy()    # Scale numeric columns to have range [0, 1].   for key in NUMERIC_FEATURE_KEYS:     outputs[key] = tft.scale_to_0_1(inputs[key])    # For all categorical columns except the label column, we generate a   # vocabulary but do not modify the feature.  This vocabulary is instead   # used in the trainer, by means of a feature column, to convert the feature   # from a string to an integer id.   for key in CATEGORICAL_FEATURE_KEYS:     outputs[key] = tft.compute_and_apply_vocabulary(         tf.strings.strip(inputs[key]),         num_oov_buckets=NUM_OOV_BUCKETS,         vocab_filename=key)    # For the label column we provide the mapping from string to index.   table_keys = ['&gt;50K', '&lt;=50K']   with tf.init_scope():     initializer = tf.lookup.KeyValueTensorInitializer(         keys=table_keys,         values=tf.cast(tf.range(len(table_keys)), tf.int64),         key_dtype=tf.string,         value_dtype=tf.int64)     table = tf.lookup.StaticHashTable(initializer, default_value=-1)    # Remove trailing periods for test data when the data is read with tf.data.   # label_str  = tf.sparse.to_dense(inputs[LABEL_KEY])   label_str = inputs[LABEL_KEY]   label_str = tf.strings.regex_replace(label_str, r'\\.$', '')   label_str = tf.strings.strip(label_str)   data_labels = table.lookup(label_str)   transformed_label = tf.one_hot(       indices=data_labels, depth=len(table_keys), on_value=1.0, off_value=0.0)   outputs[LABEL_KEY] = tf.reshape(transformed_label, [-1, len(table_keys)])    return outputs In\u00a0[\u00a0]: Copied! <pre>def transform_data(train_data_file, test_data_file, working_dir):\n  \"\"\"Transform the data and write out as a TFRecord of Example protos.\n\n  Read in the data using the CSV reader, and transform it using a\n  preprocessing pipeline that scales numeric data and converts categorical data\n  from strings to int64 values indices, by creating a vocabulary for each\n  category.\n\n  Args:\n    train_data_file: File containing training data\n    test_data_file: File containing test data\n    working_dir: Directory to write transformed data and metadata to\n  \"\"\"\n\n  # The \"with\" block will create a pipeline, and run that pipeline at the exit\n  # of the block.\n  with beam.Pipeline() as pipeline:\n    with tft_beam.Context(temp_dir=tempfile.mkdtemp()):\n      # Create a TFXIO to read the census data with the schema. To do this we\n      # need to list all columns in order since the schema doesn't specify the\n      # order of columns in the csv.\n      # We first read CSV files and use BeamRecordCsvTFXIO whose .BeamSource()\n      # accepts a PCollection[bytes] because we need to patch the records first\n      # (see \"FixCommasTrainData\" below). Otherwise, tfxio.CsvTFXIO can be used\n      # to both read the CSV files and parse them to TFT inputs:\n      # csv_tfxio = tfxio.CsvTFXIO(...)\n      # raw_data = (pipeline | 'ToRecordBatches' &gt;&gt; csv_tfxio.BeamSource())\n      train_csv_tfxio = tfxio.CsvTFXIO(\n          file_pattern=train_data_file,\n          telemetry_descriptors=[],\n          column_names=ORDERED_CSV_COLUMNS,\n          schema=SCHEMA)\n\n      # Read in raw data and convert using CSV TFXIO.\n      raw_data = (\n          pipeline |\n          'ReadTrainCsv' &gt;&gt; train_csv_tfxio.BeamSource())\n\n      # Combine data and schema into a dataset tuple.  Note that we already used\n      # the schema to read the CSV data, but we also need it to interpret\n      # raw_data.\n      cfg = train_csv_tfxio.TensorAdapterConfig()\n      raw_dataset = (raw_data, cfg)\n\n      # The TFXIO output format is chosen for improved performance.\n      transformed_dataset, transform_fn = (\n          raw_dataset | tft_beam.AnalyzeAndTransformDataset(\n              preprocessing_fn, output_record_batches=True))\n\n      # Transformed metadata is not necessary for encoding.\n      transformed_data, _ = transformed_dataset\n\n      # Extract transformed RecordBatches, encode and write them to the given\n      # directory.\n      coder = RecordBatchToExamplesEncoder()\n      _ = (\n          transformed_data\n          | 'EncodeTrainData' &gt;&gt;\n          beam.FlatMapTuple(lambda batch, _: coder.encode(batch))\n          | 'WriteTrainData' &gt;&gt; beam.io.WriteToTFRecord(\n              os.path.join(working_dir, TRANSFORMED_TRAIN_DATA_FILEBASE)))\n\n      # Now apply transform function to test data.  In this case we remove the\n      # trailing period at the end of each line, and also ignore the header line\n      # that is present in the test data file.\n      test_csv_tfxio = tfxio.CsvTFXIO(\n          file_pattern=test_data_file,\n          skip_header_lines=1,\n          telemetry_descriptors=[],\n          column_names=ORDERED_CSV_COLUMNS,\n          schema=SCHEMA)\n      raw_test_data = (\n          pipeline\n          | 'ReadTestCsv' &gt;&gt; test_csv_tfxio.BeamSource())\n\n      raw_test_dataset = (raw_test_data, test_csv_tfxio.TensorAdapterConfig())\n\n      # The TFXIO output format is chosen for improved performance.\n      transformed_test_dataset = (\n          (raw_test_dataset, transform_fn)\n          | tft_beam.TransformDataset(output_record_batches=True))\n\n      # Transformed metadata is not necessary for encoding.\n      transformed_test_data, _ = transformed_test_dataset\n\n      # Extract transformed RecordBatches, encode and write them to the given\n      # directory.\n      _ = (\n          transformed_test_data\n          | 'EncodeTestData' &gt;&gt;\n          beam.FlatMapTuple(lambda batch, _: coder.encode(batch))\n          | 'WriteTestData' &gt;&gt; beam.io.WriteToTFRecord(\n              os.path.join(working_dir, TRANSFORMED_TEST_DATA_FILEBASE)))\n\n      # Will write a SavedModel and metadata to working_dir, which can then\n      # be read by the tft.TFTransformOutput class.\n      _ = (\n          transform_fn\n          | 'WriteTransformFn' &gt;&gt; tft_beam.WriteTransformFn(working_dir))\n</pre> def transform_data(train_data_file, test_data_file, working_dir):   \"\"\"Transform the data and write out as a TFRecord of Example protos.    Read in the data using the CSV reader, and transform it using a   preprocessing pipeline that scales numeric data and converts categorical data   from strings to int64 values indices, by creating a vocabulary for each   category.    Args:     train_data_file: File containing training data     test_data_file: File containing test data     working_dir: Directory to write transformed data and metadata to   \"\"\"    # The \"with\" block will create a pipeline, and run that pipeline at the exit   # of the block.   with beam.Pipeline() as pipeline:     with tft_beam.Context(temp_dir=tempfile.mkdtemp()):       # Create a TFXIO to read the census data with the schema. To do this we       # need to list all columns in order since the schema doesn't specify the       # order of columns in the csv.       # We first read CSV files and use BeamRecordCsvTFXIO whose .BeamSource()       # accepts a PCollection[bytes] because we need to patch the records first       # (see \"FixCommasTrainData\" below). Otherwise, tfxio.CsvTFXIO can be used       # to both read the CSV files and parse them to TFT inputs:       # csv_tfxio = tfxio.CsvTFXIO(...)       # raw_data = (pipeline | 'ToRecordBatches' &gt;&gt; csv_tfxio.BeamSource())       train_csv_tfxio = tfxio.CsvTFXIO(           file_pattern=train_data_file,           telemetry_descriptors=[],           column_names=ORDERED_CSV_COLUMNS,           schema=SCHEMA)        # Read in raw data and convert using CSV TFXIO.       raw_data = (           pipeline |           'ReadTrainCsv' &gt;&gt; train_csv_tfxio.BeamSource())        # Combine data and schema into a dataset tuple.  Note that we already used       # the schema to read the CSV data, but we also need it to interpret       # raw_data.       cfg = train_csv_tfxio.TensorAdapterConfig()       raw_dataset = (raw_data, cfg)        # The TFXIO output format is chosen for improved performance.       transformed_dataset, transform_fn = (           raw_dataset | tft_beam.AnalyzeAndTransformDataset(               preprocessing_fn, output_record_batches=True))        # Transformed metadata is not necessary for encoding.       transformed_data, _ = transformed_dataset        # Extract transformed RecordBatches, encode and write them to the given       # directory.       coder = RecordBatchToExamplesEncoder()       _ = (           transformed_data           | 'EncodeTrainData' &gt;&gt;           beam.FlatMapTuple(lambda batch, _: coder.encode(batch))           | 'WriteTrainData' &gt;&gt; beam.io.WriteToTFRecord(               os.path.join(working_dir, TRANSFORMED_TRAIN_DATA_FILEBASE)))        # Now apply transform function to test data.  In this case we remove the       # trailing period at the end of each line, and also ignore the header line       # that is present in the test data file.       test_csv_tfxio = tfxio.CsvTFXIO(           file_pattern=test_data_file,           skip_header_lines=1,           telemetry_descriptors=[],           column_names=ORDERED_CSV_COLUMNS,           schema=SCHEMA)       raw_test_data = (           pipeline           | 'ReadTestCsv' &gt;&gt; test_csv_tfxio.BeamSource())        raw_test_dataset = (raw_test_data, test_csv_tfxio.TensorAdapterConfig())        # The TFXIO output format is chosen for improved performance.       transformed_test_dataset = (           (raw_test_dataset, transform_fn)           | tft_beam.TransformDataset(output_record_batches=True))        # Transformed metadata is not necessary for encoding.       transformed_test_data, _ = transformed_test_dataset        # Extract transformed RecordBatches, encode and write them to the given       # directory.       _ = (           transformed_test_data           | 'EncodeTestData' &gt;&gt;           beam.FlatMapTuple(lambda batch, _: coder.encode(batch))           | 'WriteTestData' &gt;&gt; beam.io.WriteToTFRecord(               os.path.join(working_dir, TRANSFORMED_TEST_DATA_FILEBASE)))        # Will write a SavedModel and metadata to working_dir, which can then       # be read by the tft.TFTransformOutput class.       _ = (           transform_fn           | 'WriteTransformFn' &gt;&gt; tft_beam.WriteTransformFn(working_dir)) <p>Run the pipeline:</p> In\u00a0[\u00a0]: Copied! <pre>import tempfile\nimport pathlib\n\noutput_dir = os.path.join(tempfile.mkdtemp(), 'keras')\n\n\ntransform_data(train_path, test_path, output_dir)\n</pre> import tempfile import pathlib  output_dir = os.path.join(tempfile.mkdtemp(), 'keras')   transform_data(train_path, test_path, output_dir) <p>Wrap up the output directory as a <code>tft.TFTransformOutput</code>:</p> In\u00a0[\u00a0]: Copied! <pre>tf_transform_output = tft.TFTransformOutput(output_dir)\n</pre> tf_transform_output = tft.TFTransformOutput(output_dir) In\u00a0[\u00a0]: Copied! <pre>tf_transform_output.transformed_feature_spec()\n</pre> tf_transform_output.transformed_feature_spec() <p>If you look in the directory you'll see it contains three things:</p> <ol> <li>The <code>train_transformed</code> and <code>test_transformed</code> data files</li> <li>The <code>transform_fn</code> directory (a <code>tf.saved_model</code>)</li> <li>The <code>transformed_metadata</code></li> </ol> <p>The followning sections show how to use these artifacts to train a model.</p> In\u00a0[\u00a0]: Copied! <pre>!ls -l {output_dir}\n</pre> !ls -l {output_dir} <p>Running the pipeline in the previous section created <code>TFRecord</code> files containing the transformed data.</p> <p>The following code uses <code>tf.data.experimental.make_batched_features_dataset</code> and <code>tft.TFTransformOutput.transformed_feature_spec</code> to read these data files as a <code>tf.data.Dataset</code>:</p> In\u00a0[\u00a0]: Copied! <pre>def _make_training_input_fn(tf_transform_output, train_file_pattern,\n                            batch_size):\n  \"\"\"An input function reading from transformed data, converting to model input.\n\n  Args:\n    tf_transform_output: Wrapper around output of tf.Transform.\n    transformed_examples: Base filename of examples.\n    batch_size: Batch size.\n\n  Returns:\n    The input data for training or eval, in the form of k.\n  \"\"\"\n  def input_fn():\n    return tf.data.experimental.make_batched_features_dataset(\n        file_pattern=train_file_pattern,\n        batch_size=batch_size,\n        features=tf_transform_output.transformed_feature_spec(),\n        reader=tf.data.TFRecordDataset,\n        label_key=LABEL_KEY,\n        shuffle=True)\n\n  return input_fn\n</pre> def _make_training_input_fn(tf_transform_output, train_file_pattern,                             batch_size):   \"\"\"An input function reading from transformed data, converting to model input.    Args:     tf_transform_output: Wrapper around output of tf.Transform.     transformed_examples: Base filename of examples.     batch_size: Batch size.    Returns:     The input data for training or eval, in the form of k.   \"\"\"   def input_fn():     return tf.data.experimental.make_batched_features_dataset(         file_pattern=train_file_pattern,         batch_size=batch_size,         features=tf_transform_output.transformed_feature_spec(),         reader=tf.data.TFRecordDataset,         label_key=LABEL_KEY,         shuffle=True)    return input_fn In\u00a0[\u00a0]: Copied! <pre>train_file_pattern = pathlib.Path(output_dir)/f'{TRANSFORMED_TRAIN_DATA_FILEBASE}*'\n\ninput_fn = _make_training_input_fn(\n    tf_transform_output=tf_transform_output,\n    train_file_pattern = str(train_file_pattern),\n    batch_size = 10\n)\n</pre> train_file_pattern = pathlib.Path(output_dir)/f'{TRANSFORMED_TRAIN_DATA_FILEBASE}*'  input_fn = _make_training_input_fn(     tf_transform_output=tf_transform_output,     train_file_pattern = str(train_file_pattern),     batch_size = 10 ) <p>Below you can see a transformed sample of the data. Note how the numeric columns like <code>education-num</code> and <code>hourd-per-week</code> are converted to floats with a range of [0,1], and the string columns have been converted to IDs:</p> In\u00a0[\u00a0]: Copied! <pre>for example, label in input_fn().take(1):\n  break\n\npd.DataFrame(example)\n</pre> for example, label in input_fn().take(1):   break  pd.DataFrame(example) In\u00a0[\u00a0]: Copied! <pre>label\n</pre> label <p>Build the model</p> In\u00a0[\u00a0]: Copied! <pre>def build_keras_model(working_dir):\n  inputs = build_keras_inputs(working_dir)\n\n  encoded_inputs = encode_inputs(inputs)\n\n  stacked_inputs = tf.concat(tf.nest.flatten(encoded_inputs), axis=1)\n  output = tf_keras.layers.Dense(100, activation='relu')(stacked_inputs)\n  output = tf_keras.layers.Dense(50, activation='relu')(output)\n  output = tf_keras.layers.Dense(2)(output)\n  model = tf_keras.Model(inputs=inputs, outputs=output)\n\n  return model\n</pre> def build_keras_model(working_dir):   inputs = build_keras_inputs(working_dir)    encoded_inputs = encode_inputs(inputs)    stacked_inputs = tf.concat(tf.nest.flatten(encoded_inputs), axis=1)   output = tf_keras.layers.Dense(100, activation='relu')(stacked_inputs)   output = tf_keras.layers.Dense(50, activation='relu')(output)   output = tf_keras.layers.Dense(2)(output)   model = tf_keras.Model(inputs=inputs, outputs=output)    return model In\u00a0[\u00a0]: Copied! <pre>def build_keras_inputs(working_dir):\n  tf_transform_output = tft.TFTransformOutput(working_dir)\n\n  feature_spec = tf_transform_output.transformed_feature_spec().copy()\n  feature_spec.pop(LABEL_KEY)\n\n  # Build the `keras.Input` objects.\n  inputs = {}\n  for key, spec in feature_spec.items():\n    if isinstance(spec, tf.io.VarLenFeature):\n      inputs[key] = tf_keras.layers.Input(\n          shape=[None], name=key, dtype=spec.dtype, sparse=True)\n    elif isinstance(spec, tf.io.FixedLenFeature):\n      inputs[key] = tf_keras.layers.Input(\n          shape=spec.shape, name=key, dtype=spec.dtype)\n    else:\n      raise ValueError('Spec type is not supported: ', key, spec)\n\n  return inputs\n</pre> def build_keras_inputs(working_dir):   tf_transform_output = tft.TFTransformOutput(working_dir)    feature_spec = tf_transform_output.transformed_feature_spec().copy()   feature_spec.pop(LABEL_KEY)    # Build the `keras.Input` objects.   inputs = {}   for key, spec in feature_spec.items():     if isinstance(spec, tf.io.VarLenFeature):       inputs[key] = tf_keras.layers.Input(           shape=[None], name=key, dtype=spec.dtype, sparse=True)     elif isinstance(spec, tf.io.FixedLenFeature):       inputs[key] = tf_keras.layers.Input(           shape=spec.shape, name=key, dtype=spec.dtype)     else:       raise ValueError('Spec type is not supported: ', key, spec)    return inputs In\u00a0[\u00a0]: Copied! <pre>def encode_inputs(inputs):\n  encoded_inputs = {}\n  for key in inputs:\n    feature = tf.expand_dims(inputs[key], -1)\n    if key in CATEGORICAL_FEATURE_KEYS:\n      num_buckets = tf_transform_output.num_buckets_for_transformed_feature(key)\n      encoding_layer = (\n          tf_keras.layers.CategoryEncoding(\n              num_tokens=num_buckets, output_mode='binary', sparse=False))\n      encoded_inputs[key] = encoding_layer(feature)\n    else:\n      encoded_inputs[key] = feature\n  \n  return encoded_inputs\n</pre> def encode_inputs(inputs):   encoded_inputs = {}   for key in inputs:     feature = tf.expand_dims(inputs[key], -1)     if key in CATEGORICAL_FEATURE_KEYS:       num_buckets = tf_transform_output.num_buckets_for_transformed_feature(key)       encoding_layer = (           tf_keras.layers.CategoryEncoding(               num_tokens=num_buckets, output_mode='binary', sparse=False))       encoded_inputs[key] = encoding_layer(feature)     else:       encoded_inputs[key] = feature      return encoded_inputs In\u00a0[\u00a0]: Copied! <pre>model = build_keras_model(output_dir)\n\ntf_keras.utils.plot_model(model,rankdir='LR', show_shapes=True)\n</pre> model = build_keras_model(output_dir)  tf_keras.utils.plot_model(model,rankdir='LR', show_shapes=True) <p>Build the datasets</p> In\u00a0[\u00a0]: Copied! <pre>def get_dataset(working_dir, filebase):\n  tf_transform_output = tft.TFTransformOutput(working_dir)\n\n  data_path_pattern = os.path.join(\n      working_dir,\n      filebase + '*')\n  \n  input_fn = _make_training_input_fn(\n      tf_transform_output,\n      data_path_pattern,\n      batch_size=BATCH_SIZE)\n  \n  dataset = input_fn()\n\n  return dataset\n</pre> def get_dataset(working_dir, filebase):   tf_transform_output = tft.TFTransformOutput(working_dir)    data_path_pattern = os.path.join(       working_dir,       filebase + '*')      input_fn = _make_training_input_fn(       tf_transform_output,       data_path_pattern,       batch_size=BATCH_SIZE)      dataset = input_fn()    return dataset <p>Train and evaluate the model:</p> In\u00a0[\u00a0]: Copied! <pre>def train_and_evaluate(\n    model,\n    working_dir):\n  \"\"\"Train the model on training data and evaluate on test data.\n\n  Args:\n    working_dir: The location of the Transform output.\n    num_train_instances: Number of instances in train set\n    num_test_instances: Number of instances in test set\n\n  Returns:\n    The results from the estimator's 'evaluate' method\n  \"\"\"\n  train_dataset = get_dataset(working_dir, TRANSFORMED_TRAIN_DATA_FILEBASE)\n  validation_dataset = get_dataset(working_dir, TRANSFORMED_TEST_DATA_FILEBASE)\n\n  model = build_keras_model(working_dir)\n\n  history = train_model(model, train_dataset, validation_dataset)\n\n  metric_values = model.evaluate(validation_dataset,\n                                 steps=EVALUATION_STEPS,\n                                 return_dict=True)\n  return model, history, metric_values\n</pre> def train_and_evaluate(     model,     working_dir):   \"\"\"Train the model on training data and evaluate on test data.    Args:     working_dir: The location of the Transform output.     num_train_instances: Number of instances in train set     num_test_instances: Number of instances in test set    Returns:     The results from the estimator's 'evaluate' method   \"\"\"   train_dataset = get_dataset(working_dir, TRANSFORMED_TRAIN_DATA_FILEBASE)   validation_dataset = get_dataset(working_dir, TRANSFORMED_TEST_DATA_FILEBASE)    model = build_keras_model(working_dir)    history = train_model(model, train_dataset, validation_dataset)    metric_values = model.evaluate(validation_dataset,                                  steps=EVALUATION_STEPS,                                  return_dict=True)   return model, history, metric_values In\u00a0[\u00a0]: Copied! <pre>def train_model(model, train_dataset, validation_dataset):\n  model.compile(optimizer='adam',\n                loss=tf.losses.CategoricalCrossentropy(from_logits=True),\n                metrics=['accuracy'])\n\n  history = model.fit(train_dataset, validation_data=validation_dataset,\n      epochs=TRAIN_NUM_EPOCHS,\n      steps_per_epoch=STEPS_PER_TRAIN_EPOCH,\n      validation_steps=EVALUATION_STEPS)\n  return history\n</pre> def train_model(model, train_dataset, validation_dataset):   model.compile(optimizer='adam',                 loss=tf.losses.CategoricalCrossentropy(from_logits=True),                 metrics=['accuracy'])    history = model.fit(train_dataset, validation_data=validation_dataset,       epochs=TRAIN_NUM_EPOCHS,       steps_per_epoch=STEPS_PER_TRAIN_EPOCH,       validation_steps=EVALUATION_STEPS)   return history In\u00a0[\u00a0]: Copied! <pre>model, history, metric_values = train_and_evaluate(model, output_dir)\n</pre> model, history, metric_values = train_and_evaluate(model, output_dir) In\u00a0[\u00a0]: Copied! <pre>plt.plot(history.history['loss'], label='Train')\nplt.plot(history.history['val_loss'], label='Eval')\nplt.ylim(0,max(plt.ylim()))\nplt.legend()\nplt.title('Loss');\n</pre> plt.plot(history.history['loss'], label='Train') plt.plot(history.history['val_loss'], label='Eval') plt.ylim(0,max(plt.ylim())) plt.legend() plt.title('Loss'); <p>Here's a function to load new, unprocessed batches from a source file:</p> In\u00a0[\u00a0]: Copied! <pre>def read_csv(file_name, batch_size):\n  return tf.data.experimental.make_csv_dataset(\n        file_pattern=file_name,\n        batch_size=batch_size,\n        column_names=ORDERED_CSV_COLUMNS,\n        column_defaults=COLUMN_DEFAULTS,\n        prefetch_buffer_size=0,\n        ignore_errors=True)\n</pre> def read_csv(file_name, batch_size):   return tf.data.experimental.make_csv_dataset(         file_pattern=file_name,         batch_size=batch_size,         column_names=ORDERED_CSV_COLUMNS,         column_defaults=COLUMN_DEFAULTS,         prefetch_buffer_size=0,         ignore_errors=True) In\u00a0[\u00a0]: Copied! <pre>for ex in read_csv(test_path, batch_size=5):\n  break\n\npd.DataFrame(ex)\n</pre> for ex in read_csv(test_path, batch_size=5):   break  pd.DataFrame(ex) <p>Load the <code>tft.TransformFeaturesLayer</code> to transform this data with the <code>preprocessing_fn</code>:</p> In\u00a0[\u00a0]: Copied! <pre>ex2 = ex.copy()\nex2.pop('fnlwgt')\n\ntft_layer = tf_transform_output.transform_features_layer()\nt_ex = tft_layer(ex2)\n\nlabel = t_ex.pop(LABEL_KEY)\npd.DataFrame(t_ex)\n</pre> ex2 = ex.copy() ex2.pop('fnlwgt')  tft_layer = tf_transform_output.transform_features_layer() t_ex = tft_layer(ex2)  label = t_ex.pop(LABEL_KEY) pd.DataFrame(t_ex) <p>The <code>tft_layer</code> is smart enough to still execute the transformation if only a subset of features are passed in. For example, if you only pass in two features, you'll get just the transformed versions of those features back:</p> In\u00a0[\u00a0]: Copied! <pre>ex2 = pd.DataFrame(ex)[['education', 'hours-per-week']]\nex2\n</pre> ex2 = pd.DataFrame(ex)[['education', 'hours-per-week']] ex2 In\u00a0[\u00a0]: Copied! <pre>pd.DataFrame(tft_layer(dict(ex2)))\n</pre> pd.DataFrame(tft_layer(dict(ex2))) <p>Here's a more robust version that drops features that are not in the feature-spec, and returns a <code>(features, label)</code> pair if the label is in the provided features:</p> In\u00a0[\u00a0]: Copied! <pre>class Transform(tf.Module):\n  def __init__(self, working_dir):\n    self.working_dir = working_dir\n    self.tf_transform_output = tft.TFTransformOutput(working_dir)\n    self.tft_layer = tf_transform_output.transform_features_layer()\n  \n  @tf.function\n  def __call__(self, features):\n    raw_features = {}\n\n    for key, val in features.items():\n      # Skip unused keys\n      if key not in RAW_DATA_FEATURE_SPEC:\n        continue\n\n      raw_features[key] = val\n\n    # Apply the `preprocessing_fn`.\n    transformed_features = tft_layer(raw_features)\n    \n    if LABEL_KEY in transformed_features:\n      # Pop the label and return a (features, labels) pair.\n      data_labels = transformed_features.pop(LABEL_KEY)\n      return (transformed_features, data_labels)\n    else:\n      return transformed_features\n</pre> class Transform(tf.Module):   def __init__(self, working_dir):     self.working_dir = working_dir     self.tf_transform_output = tft.TFTransformOutput(working_dir)     self.tft_layer = tf_transform_output.transform_features_layer()      @tf.function   def __call__(self, features):     raw_features = {}      for key, val in features.items():       # Skip unused keys       if key not in RAW_DATA_FEATURE_SPEC:         continue        raw_features[key] = val      # Apply the `preprocessing_fn`.     transformed_features = tft_layer(raw_features)          if LABEL_KEY in transformed_features:       # Pop the label and return a (features, labels) pair.       data_labels = transformed_features.pop(LABEL_KEY)       return (transformed_features, data_labels)     else:       return transformed_features  In\u00a0[\u00a0]: Copied! <pre>transform = Transform(output_dir)\n</pre> transform = Transform(output_dir) In\u00a0[\u00a0]: Copied! <pre>t_ex, t_label = transform(ex)\n</pre> t_ex, t_label = transform(ex) In\u00a0[\u00a0]: Copied! <pre>pd.DataFrame(t_ex)\n</pre> pd.DataFrame(t_ex) <p>Now you can use <code>Dataset.map</code> to apply that transformation, on the fly to new data:</p> In\u00a0[\u00a0]: Copied! <pre>model.evaluate(\n    read_csv(test_path, batch_size=5).map(transform),\n    steps=EVALUATION_STEPS,\n    return_dict=True\n)\n</pre> model.evaluate(     read_csv(test_path, batch_size=5).map(transform),     steps=EVALUATION_STEPS,     return_dict=True ) In\u00a0[\u00a0]: Copied! <pre>class ServingModel(tf.Module):\n  def __init__(self, model, working_dir):\n    self.model = model\n    self.working_dir = working_dir\n    self.transform = Transform(working_dir)\n\n  @tf.function(input_signature=[tf.TensorSpec(shape=[None], dtype=tf.string)])\n  def __call__(self, serialized_tf_examples):\n    # parse the tf.train.Example\n    feature_spec = RAW_DATA_FEATURE_SPEC.copy()\n    feature_spec.pop(LABEL_KEY)\n    parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)\n    # Apply the `preprocessing_fn`\n    transformed_features = self.transform(parsed_features)\n    # Run the model\n    outputs = self.model(transformed_features)\n    # Format the output\n    classes_names = tf.constant([['0', '1']])\n    classes = tf.tile(classes_names, [tf.shape(outputs)[0], 1])\n    return {'classes': classes, 'scores': outputs}\n\n  def export(self, output_dir):\n    # Increment the directory number. This is required in order to make this\n    # model servable with model_server.\n    save_model_dir = pathlib.Path(output_dir)/'model'\n    number_dirs = [int(p.name) for p in save_model_dir.glob('*')\n                  if p.name.isdigit()]\n    id = max([0] + number_dirs)+1\n    save_model_dir = save_model_dir/str(id)\n\n    # Set the signature to make it visible for serving.\n    concrete_serving_fn = self.__call__.get_concrete_function()\n    signatures = {'serving_default': concrete_serving_fn}\n\n    # Export the model.\n    tf.saved_model.save(\n        self,\n        str(save_model_dir),\n        signatures=signatures)\n    \n    return save_model_dir\n</pre> class ServingModel(tf.Module):   def __init__(self, model, working_dir):     self.model = model     self.working_dir = working_dir     self.transform = Transform(working_dir)    @tf.function(input_signature=[tf.TensorSpec(shape=[None], dtype=tf.string)])   def __call__(self, serialized_tf_examples):     # parse the tf.train.Example     feature_spec = RAW_DATA_FEATURE_SPEC.copy()     feature_spec.pop(LABEL_KEY)     parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)     # Apply the `preprocessing_fn`     transformed_features = self.transform(parsed_features)     # Run the model     outputs = self.model(transformed_features)     # Format the output     classes_names = tf.constant([['0', '1']])     classes = tf.tile(classes_names, [tf.shape(outputs)[0], 1])     return {'classes': classes, 'scores': outputs}    def export(self, output_dir):     # Increment the directory number. This is required in order to make this     # model servable with model_server.     save_model_dir = pathlib.Path(output_dir)/'model'     number_dirs = [int(p.name) for p in save_model_dir.glob('*')                   if p.name.isdigit()]     id = max([0] + number_dirs)+1     save_model_dir = save_model_dir/str(id)      # Set the signature to make it visible for serving.     concrete_serving_fn = self.__call__.get_concrete_function()     signatures = {'serving_default': concrete_serving_fn}      # Export the model.     tf.saved_model.save(         self,         str(save_model_dir),         signatures=signatures)          return save_model_dir <p>Build the model and test-run it on the batch of serialized examples:</p> In\u00a0[\u00a0]: Copied! <pre>serving_model = ServingModel(model, output_dir)\n\nserving_model(serialized_example_batch)\n</pre> serving_model = ServingModel(model, output_dir)  serving_model(serialized_example_batch) <p>Export the model as a SavedModel:</p> In\u00a0[\u00a0]: Copied! <pre>saved_model_dir = serving_model.export(output_dir)\nsaved_model_dir\n</pre> saved_model_dir = serving_model.export(output_dir) saved_model_dir <p>Reload the model and test it on the same batch of examples:</p> In\u00a0[\u00a0]: Copied! <pre>reloaded = tf.saved_model.load(str(saved_model_dir))\nrun_model = reloaded.signatures['serving_default']\n</pre> reloaded = tf.saved_model.load(str(saved_model_dir)) run_model = reloaded.signatures['serving_default'] In\u00a0[\u00a0]: Copied! <pre>run_model(serialized_example_batch)\n</pre> run_model(serialized_example_batch) In\u00a0[\u00a0]: Copied! <pre>def _make_training_input_fn(tf_transform_output, transformed_examples,\n                            batch_size):\n  \"\"\"Creates an input function reading from transformed data.\n\n  Args:\n    tf_transform_output: Wrapper around output of tf.Transform.\n    transformed_examples: Base filename of examples.\n    batch_size: Batch size.\n\n  Returns:\n    The input function for training or eval.\n  \"\"\"\n  def input_fn():\n    \"\"\"Input function for training and eval.\"\"\"\n    dataset = tf.data.experimental.make_batched_features_dataset(\n        file_pattern=transformed_examples,\n        batch_size=batch_size,\n        features=tf_transform_output.transformed_feature_spec(),\n        reader=tf.data.TFRecordDataset,\n        shuffle=True)\n\n    transformed_features = tf.compat.v1.data.make_one_shot_iterator(\n        dataset).get_next()\n\n    # Extract features and label from the transformed tensors.\n    transformed_labels = tf.where(\n        tf.equal(transformed_features.pop(LABEL_KEY), 1))\n\n    return transformed_features, transformed_labels[:,1]\n\n  return input_fn\n</pre> def _make_training_input_fn(tf_transform_output, transformed_examples,                             batch_size):   \"\"\"Creates an input function reading from transformed data.    Args:     tf_transform_output: Wrapper around output of tf.Transform.     transformed_examples: Base filename of examples.     batch_size: Batch size.    Returns:     The input function for training or eval.   \"\"\"   def input_fn():     \"\"\"Input function for training and eval.\"\"\"     dataset = tf.data.experimental.make_batched_features_dataset(         file_pattern=transformed_examples,         batch_size=batch_size,         features=tf_transform_output.transformed_feature_spec(),         reader=tf.data.TFRecordDataset,         shuffle=True)      transformed_features = tf.compat.v1.data.make_one_shot_iterator(         dataset).get_next()      # Extract features and label from the transformed tensors.     transformed_labels = tf.where(         tf.equal(transformed_features.pop(LABEL_KEY), 1))      return transformed_features, transformed_labels[:,1]    return input_fn In\u00a0[\u00a0]: Copied! <pre>def _make_serving_input_fn(tf_transform_output):\n  \"\"\"Creates an input function reading from raw data.\n\n  Args:\n    tf_transform_output: Wrapper around output of tf.Transform.\n\n  Returns:\n    The serving input function.\n  \"\"\"\n  raw_feature_spec = RAW_DATA_FEATURE_SPEC.copy()\n  # Remove label since it is not available during serving.\n  raw_feature_spec.pop(LABEL_KEY)\n\n  def serving_input_fn():\n    \"\"\"Input function for serving.\"\"\"\n    # Get raw features by generating the basic serving input_fn and calling it.\n    # Here we generate an input_fn that expects a parsed Example proto to be fed\n    # to the model at serving time.  See also\n    # tf.estimator.export.build_raw_serving_input_receiver_fn.\n    raw_input_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(\n        raw_feature_spec, default_batch_size=None)\n    serving_input_receiver = raw_input_fn()\n\n    # Apply the transform function that was used to generate the materialized\n    # data.\n    raw_features = serving_input_receiver.features\n    transformed_features = tf_transform_output.transform_raw_features(\n        raw_features)\n\n    return tf.estimator.export.ServingInputReceiver(\n        transformed_features, serving_input_receiver.receiver_tensors)\n\n  return serving_input_fn\n</pre> def _make_serving_input_fn(tf_transform_output):   \"\"\"Creates an input function reading from raw data.    Args:     tf_transform_output: Wrapper around output of tf.Transform.    Returns:     The serving input function.   \"\"\"   raw_feature_spec = RAW_DATA_FEATURE_SPEC.copy()   # Remove label since it is not available during serving.   raw_feature_spec.pop(LABEL_KEY)    def serving_input_fn():     \"\"\"Input function for serving.\"\"\"     # Get raw features by generating the basic serving input_fn and calling it.     # Here we generate an input_fn that expects a parsed Example proto to be fed     # to the model at serving time.  See also     # tf.estimator.export.build_raw_serving_input_receiver_fn.     raw_input_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(         raw_feature_spec, default_batch_size=None)     serving_input_receiver = raw_input_fn()      # Apply the transform function that was used to generate the materialized     # data.     raw_features = serving_input_receiver.features     transformed_features = tf_transform_output.transform_raw_features(         raw_features)      return tf.estimator.export.ServingInputReceiver(         transformed_features, serving_input_receiver.receiver_tensors)    return serving_input_fn In\u00a0[\u00a0]: Copied! <pre>def get_feature_columns(tf_transform_output):\n  \"\"\"Returns the FeatureColumns for the model.\n\n  Args:\n    tf_transform_output: A `TFTransformOutput` object.\n\n  Returns:\n    A list of FeatureColumns.\n  \"\"\"\n  # Wrap scalars as real valued columns.\n  real_valued_columns = [tf.feature_column.numeric_column(key, shape=())\n                         for key in NUMERIC_FEATURE_KEYS]\n\n  # Wrap categorical columns.\n  one_hot_columns = [\n      tf.feature_column.indicator_column(\n          tf.feature_column.categorical_column_with_identity(\n              key=key,\n              num_buckets=(NUM_OOV_BUCKETS +\n                  tf_transform_output.vocabulary_size_by_name(\n                      vocab_filename=key))))\n      for key in CATEGORICAL_FEATURE_KEYS]\n\n  return real_valued_columns + one_hot_columns\n</pre> def get_feature_columns(tf_transform_output):   \"\"\"Returns the FeatureColumns for the model.    Args:     tf_transform_output: A `TFTransformOutput` object.    Returns:     A list of FeatureColumns.   \"\"\"   # Wrap scalars as real valued columns.   real_valued_columns = [tf.feature_column.numeric_column(key, shape=())                          for key in NUMERIC_FEATURE_KEYS]    # Wrap categorical columns.   one_hot_columns = [       tf.feature_column.indicator_column(           tf.feature_column.categorical_column_with_identity(               key=key,               num_buckets=(NUM_OOV_BUCKETS +                   tf_transform_output.vocabulary_size_by_name(                       vocab_filename=key))))       for key in CATEGORICAL_FEATURE_KEYS]    return real_valued_columns + one_hot_columns In\u00a0[\u00a0]: Copied! <pre>def train_and_evaluate(working_dir, num_train_instances=NUM_TRAIN_INSTANCES,\n                       num_test_instances=NUM_TEST_INSTANCES):\n  \"\"\"Train the model on training data and evaluate on test data.\n\n  Args:\n    working_dir: Directory to read transformed data and metadata from and to\n        write exported model to.\n    num_train_instances: Number of instances in train set\n    num_test_instances: Number of instances in test set\n\n  Returns:\n    The results from the estimator's 'evaluate' method\n  \"\"\"\n  tf_transform_output = tft.TFTransformOutput(working_dir)\n\n  run_config = tf.estimator.RunConfig()\n\n  estimator = tf.estimator.LinearClassifier(\n      feature_columns=get_feature_columns(tf_transform_output),\n      config=run_config,\n      loss_reduction=tf.losses.Reduction.SUM)\n\n  # Fit the model using the default optimizer.\n  train_input_fn = _make_training_input_fn(\n      tf_transform_output,\n      os.path.join(working_dir, TRANSFORMED_TRAIN_DATA_FILEBASE + '*'),\n      batch_size=BATCH_SIZE)\n  estimator.train(\n      input_fn=train_input_fn,\n      max_steps=TRAIN_NUM_EPOCHS * num_train_instances / BATCH_SIZE)\n\n  # Evaluate model on test dataset.\n  eval_input_fn = _make_training_input_fn(\n      tf_transform_output,\n      os.path.join(working_dir, TRANSFORMED_TEST_DATA_FILEBASE + '*'),\n      batch_size=1)\n\n  # Export the model.\n  serving_input_fn = _make_serving_input_fn(tf_transform_output)\n  exported_model_dir = os.path.join(working_dir, EXPORTED_MODEL_DIR)\n  estimator.export_saved_model(exported_model_dir, serving_input_fn)\n\n  return estimator.evaluate(input_fn=eval_input_fn, steps=num_test_instances)\n</pre> def train_and_evaluate(working_dir, num_train_instances=NUM_TRAIN_INSTANCES,                        num_test_instances=NUM_TEST_INSTANCES):   \"\"\"Train the model on training data and evaluate on test data.    Args:     working_dir: Directory to read transformed data and metadata from and to         write exported model to.     num_train_instances: Number of instances in train set     num_test_instances: Number of instances in test set    Returns:     The results from the estimator's 'evaluate' method   \"\"\"   tf_transform_output = tft.TFTransformOutput(working_dir)    run_config = tf.estimator.RunConfig()    estimator = tf.estimator.LinearClassifier(       feature_columns=get_feature_columns(tf_transform_output),       config=run_config,       loss_reduction=tf.losses.Reduction.SUM)    # Fit the model using the default optimizer.   train_input_fn = _make_training_input_fn(       tf_transform_output,       os.path.join(working_dir, TRANSFORMED_TRAIN_DATA_FILEBASE + '*'),       batch_size=BATCH_SIZE)   estimator.train(       input_fn=train_input_fn,       max_steps=TRAIN_NUM_EPOCHS * num_train_instances / BATCH_SIZE)    # Evaluate model on test dataset.   eval_input_fn = _make_training_input_fn(       tf_transform_output,       os.path.join(working_dir, TRANSFORMED_TEST_DATA_FILEBASE + '*'),       batch_size=1)    # Export the model.   serving_input_fn = _make_serving_input_fn(tf_transform_output)   exported_model_dir = os.path.join(working_dir, EXPORTED_MODEL_DIR)   estimator.export_saved_model(exported_model_dir, serving_input_fn)    return estimator.evaluate(input_fn=eval_input_fn, steps=num_test_instances) In\u00a0[\u00a0]: Copied! <pre>import tempfile\ntemp = temp = os.path.join(tempfile.mkdtemp(),'estimator')\n\ntransform_data(train_path, test_path, temp)\nresults = train_and_evaluate(temp)\n</pre> import tempfile temp = temp = os.path.join(tempfile.mkdtemp(),'estimator')  transform_data(train_path, test_path, temp) results = train_and_evaluate(temp) In\u00a0[\u00a0]: Copied! <pre>pprint.pprint(results)\n</pre> pprint.pprint(results) <p></p>"},{"location":"tutorials/transform/census/#copyright-2020-the-tensorflow-authors","title":"Copyright 2020 The TensorFlow Authors.\u00b6","text":""},{"location":"tutorials/transform/census/#preprocessing-data-with-tensorflow-transform","title":"Preprocessing data with TensorFlow Transform\u00b6","text":"<p>The Feature Engineering Component of TensorFlow Extended (TFX)</p> <p>This example colab notebook provides a somewhat more advanced example of how TensorFlow Transform (<code>tf.Transform</code>) can be used to preprocess data using exactly the same code for both training a model and serving inferences in production.</p> <p>TensorFlow Transform is a library for preprocessing input data for TensorFlow, including creating features that require a full pass over the training dataset.  For example, using TensorFlow Transform you could:</p> <ul> <li>Normalize an input value by using the mean and standard deviation</li> <li>Convert strings to integers by generating a vocabulary over all of the input values</li> <li>Convert floats to integers by assigning them to buckets, based on the observed data distribution</li> </ul> <p>TensorFlow has built-in support for manipulations on a single example or a batch of examples. <code>tf.Transform</code> extends these capabilities to support full passes over the entire training dataset.</p> <p>The output of <code>tf.Transform</code> is exported as a TensorFlow graph which you can use for both training and serving. Using the same graph for both training and serving can prevent skew, since the same transformations are applied in both stages.</p> <p>Key Point: In order to understand <code>tf.Transform</code> and how it works with Apache Beam, you'll need to know a little bit about Apache Beam itself.  The Beam Programming Guide is a great place to start.</p>"},{"location":"tutorials/transform/census/#what-were-doing-in-this-example","title":"What we're doing in this example\u00b6","text":"<p>In this example we'll be processing a widely used dataset containing census data, and training a model to do classification.  Along the way we'll be transforming the data using <code>tf.Transform</code>.</p> <p>Key Point: As a modeler and developer, think about how this data is used and the potential benefits and harm a model's predictions can cause. A model like this could reinforce societal biases and disparities. Is a feature relevant to the problem you want to solve or will it introduce bias? For more information, read about ML fairness.</p> <p>Note: TensorFlow Model Analysis is a powerful tool for understanding how well your model predicts for various segments of your data, including understanding how your model may reinforce societal biases and disparities.</p>"},{"location":"tutorials/transform/census/#install-tensorflow-transform","title":"Install TensorFlow Transform\u00b6","text":""},{"location":"tutorials/transform/census/#imports-and-globals","title":"Imports and globals\u00b6","text":"<p>First import the stuff we need.</p>"},{"location":"tutorials/transform/census/#name-our-columns","title":"Name our columns\u00b6","text":"<p>We'll create some handy lists for referencing the columns in our dataset.</p>"},{"location":"tutorials/transform/census/#define-our-features-and-schema","title":"Define our features and schema\u00b6","text":"<p>Let's define a schema based on what types the columns are in our input.  Among other things this will help with importing them correctly.</p>"},{"location":"tutorials/transform/census/#optional-encode-and-decode-tftrainexample-protos","title":"[Optional] Encode and decode tf.train.Example protos\u00b6","text":""},{"location":"tutorials/transform/census/#setting-hyperparameters-and-basic-housekeeping","title":"Setting hyperparameters and basic housekeeping\u00b6","text":"<p>Constants and hyperparameters used for training.</p>"},{"location":"tutorials/transform/census/#preprocessing-with-tftransform","title":"Preprocessing with <code>tf.Transform</code>\u00b6","text":""},{"location":"tutorials/transform/census/#create-a-tftransform-preprocessing_fn","title":"Create a <code>tf.Transform</code> preprocessing_fn\u00b6","text":"<p>The preprocessing function is the most important concept of tf.Transform. A preprocessing function is where the transformation of the dataset really happens. It accepts and returns a dictionary of tensors, where a tensor means a <code>Tensor</code> or <code>SparseTensor</code>. There are two main groups of API calls that typically form the heart of a preprocessing function:</p> <ol> <li>TensorFlow Ops: Any function that accepts and returns tensors, which usually means TensorFlow ops. These add TensorFlow operations to the graph that transforms raw data into transformed data one feature vector at a time.  These will run for every example, during both training and serving.</li> <li>Tensorflow Transform Analyzers/Mappers: Any of the analyzers/mappers provided by tf.Transform. These also accept and return tensors, and typically contain a combination of Tensorflow ops and Beam computation, but unlike TensorFlow ops they only run in the Beam pipeline during analysis requiring a full pass over the entire training dataset. The Beam computation runs only once, (prior to training, during analysis), and typically make a full pass over the entire training dataset. They create <code>tf.constant</code> tensors, which are added to your graph. For example, <code>tft.min</code> computes the minimum of a tensor over the training dataset.</li> </ol> <p>Caution: When you apply your preprocessing function to serving inferences, the constants that were created by analyzers during training do not change.  If your data has trend or seasonality components, plan accordingly.</p>"},{"location":"tutorials/transform/census/#syntax","title":"Syntax\u00b6","text":"<p>You're almost ready to put everything together and use Apache Beam to run it.</p> <p>Apache Beam uses a special syntax to define and invoke transforms.  For example, in this line:</p> <pre><code>result = pass_this | 'name this step' &gt;&gt; to_this_call\n</code></pre> <p>The method <code>to_this_call</code> is being invoked and passed the object called <code>pass_this</code>, and this operation will be referred to as <code>name this step</code> in a stack trace.  The result of the call to <code>to_this_call</code> is returned in <code>result</code>.  You will often see stages of a pipeline chained together like this:</p> <pre><code>result = apache_beam.Pipeline() | 'first step' &gt;&gt; do_this_first() | 'second step' &gt;&gt; do_this_last()\n</code></pre> <p>and since that started with a new pipeline, you can continue like this:</p> <pre><code>next_result = result | 'doing more stuff' &gt;&gt; another_function()\n</code></pre>"},{"location":"tutorials/transform/census/#transform-the-data","title":"Transform the data\u00b6","text":"<p>Now we're ready to start transforming our data in an Apache Beam pipeline.</p> <ol> <li>Read in the data using the <code>tfxio.CsvTFXIO</code> CSV reader (to process lines of text in a pipeline use <code>tfxio.BeamRecordCsvTFXIO</code> instead).</li> <li>Analyse and transform the data using the <code>preprocessing_fn</code> defined above.</li> <li>Write out the result as a <code>TFRecord</code> of <code>Example</code> protos, which we will use for training a model later</li> </ol>"},{"location":"tutorials/transform/census/#using-our-preprocessed-data-to-train-a-model-using-tf_keras","title":"Using our preprocessed data to train a model using tf_keras\u00b6","text":"<p>To show how <code>tf.Transform</code> enables us to use the same code for both training and serving, and thus prevent skew, we're going to train a model.  To train our model and prepare our trained model for production we need to create input functions.  The main difference between our training input function and our serving input function is that training data contains the labels, and production data does not.  The arguments and returns are also somewhat different.</p>"},{"location":"tutorials/transform/census/#create-an-input-function-for-training","title":"Create an input function for training\u00b6","text":""},{"location":"tutorials/transform/census/#train-evaluate-the-model","title":"Train, Evaluate the model\u00b6","text":""},{"location":"tutorials/transform/census/#transform-new-data","title":"Transform new data\u00b6","text":"<p>In the previous section the training process used the hard-copies of the transformed data that were generated by <code>tft_beam.AnalyzeAndTransformDataset</code> in the <code>transform_dataset</code> function.</p> <p>For operating on new data you'll need to load final version of the <code>preprocessing_fn</code> that was saved by <code>tft_beam.WriteTransformFn</code>.</p> <p>The <code>TFTransformOutput.transform_features_layer</code> method loads the <code>preprocessing_fn</code> SavedModel from the output directory.</p>"},{"location":"tutorials/transform/census/#export-the-model","title":"Export the model\u00b6","text":"<p>So you have a trained model, and a method to apply the <code>preprocessing_fn</code> to new data. Assemble them into a new model that accepts serialized <code>tf.train.Example</code> protos as input.</p>"},{"location":"tutorials/transform/census/#what-we-did","title":"What we did\u00b6","text":"<p>In this example we used <code>tf.Transform</code> to preprocess a dataset of census data, and train a model with the cleaned and transformed data.  We also created an input function that we could use when we deploy our trained model in a production environment to perform inference.  By using the same code for both training and inference we avoid any issues with data skew.  Along the way we learned about creating an Apache Beam transform to perform the transformation that we needed for cleaning the data. We also saw how to use this transformed data to train a model using <code>tf_keras</code>.  This is just a small piece of what TensorFlow Transform can do!  We encourage you to dive into <code>tf.Transform</code> and discover what it can do for you.</p>"},{"location":"tutorials/transform/census/#optional-using-our-preprocessed-data-to-train-a-model-using-tfestimator","title":"[Optional] Using our preprocessed data to train a model using tf.estimator\u00b6","text":"<p>Warning: Estimators are not recommended for new code.  Estimators run <code>v1.Session</code>-style code which is more difficult to write correctly, and can behave unexpectedly, especially when combined with TF 2 code. Estimators do fall under our compatibility guarantees, but will receive no fixes other than security vulnerabilities. See the migration guide for details.</p>"},{"location":"tutorials/transform/census/#create-an-input-function-for-training","title":"Create an input function for training\u00b6","text":""},{"location":"tutorials/transform/census/#create-an-input-function-for-serving","title":"Create an input function for serving\u00b6","text":"<p>Let's create an input function that we could use in production, and prepare our trained model for serving.</p>"},{"location":"tutorials/transform/census/#wrap-our-input-data-in-featurecolumns","title":"Wrap our input data in FeatureColumns\u00b6","text":"<p>Our model will expect our data in TensorFlow FeatureColumns.</p>"},{"location":"tutorials/transform/census/#train-evaluate-and-export-our-model","title":"Train, Evaluate, and Export our model\u00b6","text":""},{"location":"tutorials/transform/census/#put-it-all-together","title":"Put it all together\u00b6","text":"<p>We've created all the stuff we need to preprocess our census data, train a model, and prepare it for serving.  So far we've just been getting things ready.  It's time to start running!</p> <p>Note: Scroll the output from this cell to see the whole process.  The results will be at the bottom.</p>"},{"location":"tutorials/transform/data_preprocessing_with_cloud/","title":"Data preprocessing for ML with Google Cloud","text":"<p>This tutorial shows you how to use TensorFlow Transform (the <code>tf.Transform</code> library) to implement data preprocessing for machine learning (ML). The <code>tf.Transform</code> library for TensorFlow lets you define both instance-level and full-pass data transformations through data preprocessing pipelines. These pipelines are efficiently executed with Apache Beam and they create as byproducts a TensorFlow graph to apply the same transformations during prediction as when the model is served.</p> <p>This tutorial provides an end-to-end example using Dataflow as a runner for Apache Beam. It assumes that you're familiar with BigQuery, Dataflow, Vertex AI, and the TensorFlow Keras API. It also assumes that you have some experience using Jupyter Notebooks, such as with Vertex AI Workbench.</p> <p>This tutorial also assumes that you're familiar with the concepts of preprocessing types, challenges, and options on Google Cloud, as described in Data preprocessing for ML: options and recommendations.</p>"},{"location":"tutorials/transform/data_preprocessing_with_cloud/#objectives","title":"Objectives","text":"<ul> <li>Implement the Apache Beam pipeline using the <code>tf.Transform</code> library.</li> <li>Run the pipeline in Dataflow.</li> <li>Implement the TensorFlow model using the <code>tf.Transform</code>     library.</li> <li>Train and use the model for predictions.</li> </ul>"},{"location":"tutorials/transform/data_preprocessing_with_cloud/#costs","title":"Costs","text":"<p>This tutorial uses the following billable components of Google Cloud:</p> <ul> <li>Vertex AI</li> <li>Cloud Storage</li> <li>BigQuery</li> <li>Dataflow</li> </ul> <p>To estimate the cost to run this tutorial, please refer to pricing calculator.</p>"},{"location":"tutorials/transform/data_preprocessing_with_cloud/#before-you-begin","title":"Before you begin","text":"<ol> <li> <p>In the Google Cloud console, on the project selector page, select or   create a Google Cloud project.</p> <p>Note</p> <p>If you don't plan to keep the resources that you create in this procedure, create a project instead of selecting an existing project. After you finish these steps, you can delete the project, removing all resources associated with the project.</p> </li> </ol> <p>Go to project selector</p> <ol> <li> <p>Make sure that billing is enabled for your Cloud project. Learn how to   check if billing is enabled on a project.</p> </li> <li> <p>Enable the Dataflow, Vertex AI, and Notebooks APIs.</p> </li> </ol> <p>Enable the APIs</p>"},{"location":"tutorials/transform/data_preprocessing_with_cloud/#jupyter-notebooks-for-this-solution","title":"Jupyter notebooks for this solution","text":"<p>The following Jupyter notebooks show the implementation example:</p> <ul> <li>Notebook 1     covers data preprocessing. Details are provided in the     Implementing the Apache Beam pipeline     section later.</li> <li>Notebook 2     covers model training. Details are provided in the     Implementing the TensorFlow model     section later.</li> </ul> <p>In the following sections, you clone these notebooks, and then you execute the notebooks to learn how the implementation example works.</p>"},{"location":"tutorials/transform/data_preprocessing_with_cloud/#launch-a-user-managed-notebooks-instance","title":"Launch a user-managed notebooks instance","text":"<ol> <li> <p>In the Google Cloud console, go to the Vertex AI Workbench page.</p> <p>Go to Workbench</p> </li> <li> <p>On the User-managed notebooks tab, click +New notebook.</p> </li> <li>Select TensorFlow Enterprise 2.8 (with LTS) without GPUs for the     instance type.</li> <li>Click Create.</li> </ol> <p>After you create the notebook, wait for the proxy to JupyterLab to finish initializing. When it's ready, Open JupyterLab is displayed next to the notebook name.</p>"},{"location":"tutorials/transform/data_preprocessing_with_cloud/#clone-the-notebook","title":"Clone the notebook","text":"<ol> <li> <p>On the User-managed notebooks tab, next to the notebook name, click     Open JupyterLab. The JupyterLab interface opens in a new tab.</p> <p>If the JupyterLab displays a Build Recommended dialog, click Cancel to reject the suggested build. 1.  On the Launcher tab, click Terminal. 1.  In the terminal window, clone the notebook:</p> <pre><code>git clone https://github.com/GoogleCloudPlatform/training-data-analyst\n</code></pre> </li> </ol>"},{"location":"tutorials/transform/data_preprocessing_with_cloud/#implement-the-apache-beam-pipeline","title":"Implement the Apache Beam pipeline","text":"<p>This section and the next section Run the pipeline in Dataflow provide an overview and context for Notebook 1. The notebook provides a practical example to describe how to use the <code>tf.Transform</code> library to preprocess data. This example uses the Natality dataset, which is used to predict baby weights based on various inputs. The data is stored in the public natality table in BigQuery.</p>"},{"location":"tutorials/transform/data_preprocessing_with_cloud/#run-notebook-1","title":"Run Notebook 1","text":"<ol> <li> <p>In the JupyterLab interface, click File &gt; Open from path, and     then enter the following path:</p> <pre><code>training-data-analyst/blogs/babyweight_tft/babyweight_tft_keras_01.ipynb\n</code></pre> </li> <li> <p>Click Edit &gt; Clear all outputs.</p> </li> <li> <p>In the Install required packages section, execute the first cell to     run the <code>pip install apache-beam</code> command.</p> <p>The last part of the output is the following:</p> <pre><code>Successfully installed ...\n</code></pre> <p>You can ignore dependency errors in the output. You don't need to restart the kernel yet.</p> </li> <li> <p>Execute the second cell to run the <code>pip install tensorflow-transform</code>command. The last part of the output is the following:</p> <pre><code>Successfully installed ...\nNote: you may need to restart the kernel to use updated packages.\n</code></pre> <p>You can ignore dependency errors in the output.</p> </li> <li> <p>Click Kernel &gt; Restart Kernel.</p> </li> <li>Execute the cells in the Confirm the installed packages and Create     setup.py to install packages to Dataflow containers sections.</li> <li>In the Set global flags section, next to <code>PROJECT</code> and <code>BUCKET</code>, replace     <code>your-project</code> with your Cloud project ID, and     then execute the cell.</li> <li>Execute all of the remaining cells through the last cell in the notebook.     For information about what to do in each cell, see the instructions in the     notebook.</li> </ol>"},{"location":"tutorials/transform/data_preprocessing_with_cloud/#overview-of-the-pipeline","title":"Overview of the pipeline","text":"<p>In the notebook example, Dataflow runs the <code>tf.Transform</code> pipeline at scale to prepare the data and produce the transformation artifacts. Later sections in this document describe the functions that perform each step in the pipeline. The overall pipeline steps are as follows:</p> <ol> <li>Read training data from BigQuery.</li> <li>Analyze and transform training data using the <code>tf.Transform</code> library.</li> <li>Write transformed training data to Cloud Storage in the     TFRecord     format.</li> <li>Read evaluation data from BigQuery.</li> <li>Transform evaluation data using the <code>transform_fn</code> graph produced by step 2.</li> <li>Write transformed training data to Cloud Storage in the     TFRecord format.</li> <li>Write transformation artifacts to Cloud Storage that will be used     later for creating and exporting the model.</li> </ol> <p>The following example shows the Python code for the overall pipeline. The sections that follow provide explanations and code listings for each step.</p> <pre><code>def run_transformation_pipeline(args):\n\n    pipeline_options = beam.pipeline.PipelineOptions(flags=[], **args)\n\n    runner = args['runner']\n    data_size = args['data_size']\n    transformed_data_location = args['transformed_data_location']\n    transform_artefact_location = args['transform_artefact_location']\n    temporary_dir = args['temporary_dir']\n    debug = args['debug']\n\n    # Instantiate the pipeline\n    with beam.Pipeline(runner, options=pipeline_options) as pipeline:\n        with impl.Context(temporary_dir):\n\n            # Preprocess train data\n            step = 'train'\n            # Read raw train data from BigQuery\n            raw_train_dataset = read_from_bq(pipeline, step, data_size)\n            # Analyze and transform raw_train_dataset\n            transformed_train_dataset, transform_fn = analyze_and_transform(raw_train_dataset, step)\n            # Write transformed train data to sink as tfrecords\n            write_tfrecords(transformed_train_dataset, transformed_data_location, step)\n\n            # Preprocess evaluation data\n            step = 'eval'\n            # Read raw eval data from BigQuery\n            raw_eval_dataset = read_from_bq(pipeline, step, data_size)\n            # Transform eval data based on produced transform_fn\n            transformed_eval_dataset = transform(raw_eval_dataset, transform_fn, step)\n            # Write transformed eval data to sink as tfrecords\n            write_tfrecords(transformed_eval_dataset, transformed_data_location, step)\n\n            # Write transformation artefacts\n            write_transform_artefacts(transform_fn, transform_artefact_location)\n\n            # (Optional) for debugging, write transformed data as text\n            step = 'debug'\n            # Write transformed train data as text if debug enabled\n            if debug == True:\n                write_text(transformed_train_dataset, transformed_data_location, step)\n</code></pre>"},{"location":"tutorials/transform/data_preprocessing_with_cloud/#read-raw-training-data-from-bigquery","title":"Read raw training data from BigQuery","text":"<p>The first step is to read the raw training data from BigQuery using the <code>read_from_bq</code> function. This function returns a <code>raw_dataset</code> object that is extracted from BigQuery. You pass a <code>data_size</code> value and pass a <code>step</code> value of <code>train</code> or <code>eval</code>. The BigQuery source query is constructed using the <code>get_source_query</code> function, as shown in the following example:</p> <pre><code>def read_from_bq(pipeline, step, data_size):\n\n    source_query = get_source_query(step, data_size)\n    raw_data = (\n        pipeline\n        | '{} - Read Data from BigQuery'.format(step) &gt;&gt; beam.io.Read(\n                           beam.io.BigQuerySource(query=source_query, use_standard_sql=True))\n        | '{} - Clean up Data'.format(step) &gt;&gt; beam.Map(prep_bq_row)\n    )\n\n    raw_metadata = create_raw_metadata()\n    raw_dataset = (raw_data, raw_metadata)\n    return raw_dataset\n</code></pre> <p>Before you perform the <code>tf.Transform</code> preprocessing, you might need to perform typical Apache Beam-based processing, including Map, Filter, Group, and Window processing. In the example, the code cleans the records read from BigQuery using the <code>beam.Map(prep_bq_row)</code> method, where <code>prep_bq_row</code> is a custom function. This custom function converts the numeric code for a categorical feature into human-readable labels.</p> <p>In addition, to use the <code>tf.Transform</code> library to analyze and transform the <code>raw_data</code> object extracted from BigQuery, you need to create a <code>raw_dataset</code> object, which is a tuple of <code>raw_data</code> and <code>raw_metadata</code> objects. The <code>raw_metadata</code> object is created using the <code>create_raw_metadata</code> function, as follows:</p> <pre><code>CATEGORICAL_FEATURE_NAMES = ['is_male', 'mother_race']\nNUMERIC_FEATURE_NAMES = ['mother_age', 'plurality', 'gestation_weeks']\nTARGET_FEATURE_NAME = 'weight_pounds'\n\ndef create_raw_metadata():\n\n    feature_spec = dict(\n        [(name, tf.io.FixedLenFeature([], tf.string)) for name in CATEGORICAL_FEATURE_NAMES] +\n        [(name, tf.io.FixedLenFeature([], tf.float32)) for name in NUMERIC_FEATURE_NAMES] +\n        [(TARGET_FEATURE_NAME, tf.io.FixedLenFeature([], tf.float32))])\n\n    raw_metadata = dataset_metadata.DatasetMetadata(\n        schema_utils.schema_from_feature_spec(feature_spec))\n\n    return raw_metadata\n</code></pre> <p>When you execute the cell in the notebook that immediately follows the cell that defines this method, the content of the <code>raw_metadata.schema</code> object is displayed. It includes the following columns:</p> <ul> <li><code>gestation_weeks</code> (type: <code>FLOAT</code>)</li> <li><code>is_male</code> (type: <code>BYTES</code>)</li> <li><code>mother_age</code> (type: <code>FLOAT</code>)</li> <li><code>mother_race</code> (type: <code>BYTES</code>)</li> <li><code>plurality</code> (type: <code>FLOAT</code>)</li> <li><code>weight_pounds</code> (type: <code>FLOAT</code>)</li> </ul>"},{"location":"tutorials/transform/data_preprocessing_with_cloud/#transform-raw-training-data","title":"Transform raw training data","text":"<p>Imagine that you want to apply typical preprocessing transformations to the input raw features of the training data in order to prepare it for ML. These transformations include both full-pass and instance-level operations, as shown in the following table:</p> Input feature Transformation Stats needed Type Output feature <code>weight_pound</code> None None NA <code>weight_pound</code> <code>mother_age</code> Normalize mean, var Full-pass <code>mother_age_normalized</code> <code>mother_age</code> Equal size bucketization quantiles Full-pass <code>mother_age_bucketized</code> <code>mother_age</code> Compute the log None Instance-level <code>mother_age_log</code> <code>plurality</code> Indicate if it is single or multiple babies None Instance-level <code>is_multiple</code> <code>is_multiple</code> Convert nominal values to numerical index vocab Full-pass <code>is_multiple_index</code> <code>gestation_weeks</code> Scale between 0 and 1 min, max Full-pass <code>gestation_weeks_scaled</code> <code>mother_race</code> Convert nominal values to numerical index vocab Full-pass <code>mother_race_index</code> <code>is_male</code> Convert nominal values to numerical index vocab Full-pass <code>is_male_index</code> <p>These transformations are implemented in a <code>preprocess_fn</code> function, which expects a dictionary of tensors (<code>input_features</code>) and returns a dictionary of processed features (<code>output_features</code>).</p> <p>The following code shows the implementation of the <code>preprocess_fn</code> function, using the <code>tf.Transform</code> full-pass transformation APIs (prefixed with <code>tft.</code>), and TensorFlow (prefixed with <code>tf.</code>) instance-level operations:</p> <pre><code>def preprocess_fn(input_features):\n\n    output_features = {}\n\n    # target feature\n    output_features['weight_pounds'] = input_features['weight_pounds']\n\n    # normalization\n    output_features['mother_age_normalized'] = tft.scale_to_z_score(input_features['mother_age'])\n\n    # scaling\n    output_features['gestation_weeks_scaled'] =  tft.scale_to_0_1(input_features['gestation_weeks'])\n\n    # bucketization based on quantiles\n    output_features['mother_age_bucketized'] = tft.bucketize(input_features['mother_age'], num_buckets=5)\n\n    # you can compute new features based on custom formulas\n    output_features['mother_age_log'] = tf.math.log(input_features['mother_age'])\n\n    # or create flags/indicators\n    is_multiple = tf.as_string(input_features['plurality'] &gt; tf.constant(1.0))\n\n    # convert categorical features to indexed vocab\n    output_features['mother_race_index'] = tft.compute_and_apply_vocabulary(input_features['mother_race'], vocab_filename='mother_race')\n    output_features['is_male_index'] = tft.compute_and_apply_vocabulary(input_features['is_male'], vocab_filename='is_male')\n    output_features['is_multiple_index'] = tft.compute_and_apply_vocabulary(is_multiple, vocab_filename='is_multiple')\n\n    return output_features\n</code></pre> <p>The <code>tf.Transform</code> framework has several other transformations in addition to those in the preceding example, including those listed in the following table:</p> Transformation Applies to Description <code>scale_by_min_max</code> Numeric features Scales a numerical column into the range [<code>output_min</code>, <code>output_max</code>] <code>scale_to_0_1</code> Numeric features Returns a column which is the input column scaled to have range [<code>0</code>,<code>1</code>] <code>scale_to_z_score</code> Numeric features Returns a standardized column with mean 0 and variance 1 <code>tfidf</code> Text features Maps the terms in x to their term frequency * inverse document frequency <code>compute_and_apply_vocabulary</code> Categorical features Generates a vocabulary for a categorical feature and maps it to an integer with this vocab <code>ngrams</code> Text features Creates a <code>SparseTensor</code> of n-grams <code>hash_strings</code> Categorical features Hashes strings into buckets <code>pca</code> Numeric features Computes PCA on the dataset using biased covariance <code>bucketize</code> Numeric features Returns an equal-sized (quantiles-based) bucketized column, with a bucket index assigned to each input <p>In order to apply the transformations implemented in the <code>preprocess_fn</code> function to the <code>raw_train_dataset</code> object produced in the previous step of the pipeline, you use the <code>AnalyzeAndTransformDataset</code> method. This method expects the <code>raw_dataset</code> object as input, applies the <code>preprocess_fn</code> function, and it produces the <code>transformed_dataset</code> object and the <code>transform_fn</code> graph. The following code illustrates this processing:</p> <pre><code>def analyze_and_transform(raw_dataset, step):\n\n    transformed_dataset, transform_fn = (\n        raw_dataset\n        | '{} - Analyze &amp; Transform'.format(step) &gt;&gt; tft_beam.AnalyzeAndTransformDataset(\n            preprocess_fn, output_record_batches=True)\n    )\n\n    return transformed_dataset, transform_fn\n</code></pre> <p>The transformations are applied on the raw data in two phases: the analyze phase and the transform phase. Figure 3 later in this document shows how the <code>AnalyzeAndTransformDataset</code> method is decomposed to the <code>AnalyzeDataset</code> method and the <code>TransformDataset</code> method.</p>"},{"location":"tutorials/transform/data_preprocessing_with_cloud/#the-analyze-phase","title":"The analyze phase","text":"<p>In the analyze phase, the raw training data is analyzed in a full-pass process to compute the statistics that are needed for the transformations. This includes computing the mean, variance, minimum, maximum, quantiles, and vocabulary. The analyze process expects a raw dataset (raw data plus raw metadata), and it produces two outputs:</p> <ul> <li><code>transform_fn</code>: a TensorFlow graph that contains the     computed stats from the analyze phase and the transformation logic (which     uses the stats) as instance-level operations. As discussed later in     Save the graph,     the <code>transform_fn</code> graph is saved to be attached to the model <code>serving_fn</code>     function. This makes it possible to apply the same transformation to the     online prediction data points.</li> <li><code>transform_metadata</code>: an object that describes the expected schema of     the data after transformation.</li> </ul> <p>The analyze phase is illustrated in the following diagram, figure 1:</p> <p>Figure 1: The <code>tf.Transform</code> analyze phase. </p> <p>The <code>tf.Transform</code> analyzers include <code>min</code>, <code>max</code>, <code>sum</code>, <code>size</code>, <code>mean</code>, <code>var</code>, <code>covariance</code>, <code>quantiles</code>, <code>vocabulary</code>, and <code>pca</code>.</p>"},{"location":"tutorials/transform/data_preprocessing_with_cloud/#the-transform-phase","title":"The transform phase","text":"<p>In the transform phase, the <code>transform_fn</code> graph that's produced by the analyze phase is used to transform the raw training data in an instance-level process in order to produce the transformed training data. The transformed training data is paired with the transformed metadata (produced by the analyze phase) to produce the <code>transformed_train_dataset</code> dataset.</p> <p>The transform phase is illustrated in the following diagram, figure 2:</p> <p>Figure 2: The <code>tf.Transform</code> transform phase. </p> <p>To preprocess the features, you call the required <code>tensorflow_transform</code> transformations (imported as <code>tft</code> in the code) in your implementation of the <code>preprocess_fn</code> function. For example, when you call the <code>tft.scale_to_z_score</code> operations, the <code>tf.Transform</code> library translates this function call into mean and variance analyzers, computes the stats in the analyze phase, and then applies these stats to normalize the numeric feature in the transform phase. This is all done automatically by calling the <code>AnalyzeAndTransformDataset(preprocess_fn)</code> method.</p> <p>The <code>transformed_metadata.schema</code> entity produced by this call includes the following columns:</p> <ul> <li><code>gestation_weeks_scaled</code> (type: <code>FLOAT</code>)</li> <li><code>is_male_index</code> (type: <code>INT</code>, is_categorical: <code>True</code>)</li> <li><code>is_multiple_index</code> (type: <code>INT</code>, is_categorical: <code>True</code>)</li> <li><code>mother_age_bucketized</code> (type: <code>INT</code>, is_categorical: <code>True</code>)</li> <li><code>mother_age_log</code> (type: <code>FLOAT</code>)</li> <li><code>mother_age_normalized</code> (type: <code>FLOAT</code>)</li> <li><code>mother_race_index</code> (type: <code>INT</code>, is_categorical: <code>True</code>)</li> <li><code>weight_pounds</code> (type: <code>FLOAT</code>)</li> </ul> <p>As explained in Preprocessing operations in the first part of this series, the feature transformation converts categorical features to a numeric representation. After the transformation, the categorical features are represented by integer values. In the <code>transformed_metadata.schema</code> entity, the <code>is_categorical</code> flag for <code>INT</code> type columns indicates whether the column represents a categorical feature or a true numeric feature.</p>"},{"location":"tutorials/transform/data_preprocessing_with_cloud/#write-transformed-training-data","title":"Write transformed training data","text":"<p>After the training data is preprocessed with the <code>preprocess_fn</code> function through the analyze and transform phases, you can write the data to a sink to be used for training the TensorFlow model. When you execute the Apache Beam pipeline using Dataflow, the sink is Cloud Storage. Otherwise, the sink is the local disk. Although you can write the data as a CSV file of fixed-width formatted files, the recommended file format for TensorFlow datasets is the TFRecord format. This is a simple record-oriented binary format that consists of <code>tf.train.Example</code> protocol buffer messages.</p> <p>Each <code>tf.train.Example</code> record contains one or more features. These are converted into tensors when they are fed to the model for training. The following code writes the transformed dataset to TFRecord files in the specified location:</p> <pre><code>def write_tfrecords(transformed_dataset, location, step):\n    from tfx_bsl.coders import example_coder\n\n    transformed_data, transformed_metadata = transformed_dataset\n    (\n        transformed_data\n        | '{} - Encode Transformed Data'.format(step) &gt;&gt; beam.FlatMapTuple(\n                            lambda batch, _: example_coder.RecordBatchToExamples(batch))\n        | '{} - Write Transformed Data'.format(step) &gt;&gt; beam.io.WriteToTFRecord(\n                            file_path_prefix=os.path.join(location,'{}'.format(step)),\n                            file_name_suffix='.tfrecords')\n    )\n</code></pre>"},{"location":"tutorials/transform/data_preprocessing_with_cloud/#read-transform-and-write-evaluation-data","title":"Read, transform, and write evaluation data","text":"<p>After you transform the training data and produce the <code>transform_fn</code> graph, you can use it to transform the evaluation data. First, you read and clean the evaluation data from BigQuery using the <code>read_from_bq</code> function described earlier in Read raw training data from BigQuery, and passing a value of <code>eval</code> for the <code>step</code> parameter. Then, you use the following code to transform the raw evaluation dataset (<code>raw_dataset</code>) to the expected transformed format (<code>transformed_dataset</code>):</p> <pre><code>def transform(raw_dataset, transform_fn, step):\n\n    transformed_dataset = (\n        (raw_dataset, transform_fn)\n        | '{} - Transform'.format(step) &gt;&gt; tft_beam.TransformDataset(output_record_batches=True)\n    )\n\n    return transformed_dataset\n</code></pre> <p>When you transform the evaluation data, only instance-level operations apply, using both the logic in the <code>transform_fn</code> graph and the statistics computed from the analyze phase in the training data. In other words, you don't analyze the evaluation data in a full-pass fashion to compute new statistics, like the mean and the variance for z-score normalization of numeric features in evaluation data. Instead, you use the computed statistics from the training data to transform the evaluation data in an instance-level fashion.</p> <p>Therefore, you use the <code>AnalyzeAndTransform</code> method in the context of training data to compute the statistics and transform the data. At the same time, you use the <code>TransformDataset</code> method in the context of transforming evaluation data to only transform the data using the statistics computed on the training data.</p> <p>You then write the data to a sink (Cloud Storage or local disk, depending on the runner) in the TFRecord format for evaluating the TensorFlow model during the training process. To do this, you use the <code>write_tfrecords</code> function that's discussed in Write transformed training data. The following diagram, figure 3, shows how the <code>transform_fn</code> graph that's produced in the analyze phase of the training data is used to transform the evaluation data.</p> <p>Figure 3: Transforming evaluation data using the <code>transform_fn</code> graph. </p>"},{"location":"tutorials/transform/data_preprocessing_with_cloud/#save-the-graph","title":"Save the graph","text":"<p>A final step in the <code>tf.Transform</code> preprocessing pipeline is to store the artifacts, which includes the <code>transform_fn</code> graph that's produced by the analyze phase on the training data. The code for storing the artifacts is shown in the following <code>write_transform_artefacts</code> function:</p> <pre><code>def write_transform_artefacts(transform_fn, location):\n\n    (\n        transform_fn\n        | 'Write Transform Artifacts' &gt;&gt; transform_fn_io.WriteTransformFn(location)\n    )\n</code></pre> <p>These artifacts will be used later for model training and exporting for serving. The following artifacts are also produced, as shown in the next section:</p> <ul> <li><code>saved_model.pb</code>: represents the TensorFlow graph     that includes the transformation logic (the <code>transform_fn</code> graph), which is     to be attached to the model serving interface to transform the raw data     points to the transformed format.</li> <li><code>variables</code>: includes the statistics computed during the analyze     phase of the training data, and is used in the transformation logic in the     <code>saved_model.pb</code> artifact.</li> <li><code>assets</code>: includes vocabulary files, one for each categorical feature     processed with the <code>compute_and_apply_vocabulary</code> method, to be used     during serving to convert an input raw nominal value to a numerical index.</li> <li><code>transformed_metadata</code>: a directory that contains the <code>schema.json</code> file     that describes the schema of the transformed data.</li> </ul>"},{"location":"tutorials/transform/data_preprocessing_with_cloud/#run-the-pipeline-in-dataflow","title":"Run the pipeline in Dataflow","text":"<p>After you define the <code>tf.Transform</code> pipeline, you run the pipeline using Dataflow. The following diagram, figure 4, shows the Dataflow execution graph of the <code>tf.Transform</code> pipeline described in the example.</p> <p>Figure 4: Dataflow execution graph of the <code>tf.Transform</code> pipeline. </p> <p>After you execute the Dataflow pipeline to preprocess the training and evaluation data, you can explore the produced objects in Cloud Storage by executing the last cell in the notebook. The code snippets in this section show the results, where <code>YOUR_BUCKET_NAME</code> is the name of your Cloud Storage bucket.</p> <p>The transformed training and evaluation data in TFRecord format are stored at the following location:</p> <pre><code>gs://YOUR_BUCKET_NAME/babyweight_tft/transformed\n</code></pre> <p>The transform artifacts are produced at the following location:</p> <pre><code>gs://YOUR_BUCKET_NAME/babyweight_tft/transform\n</code></pre> <p>The following list is the output of the pipeline, showing the produced data objects and artifacts:</p> <pre><code>transformed data:\ngs://YOUR_BUCKET_NAME/babyweight_tft/transformed/eval-00000-of-00001.tfrecords\ngs://YOUR_BUCKET_NAME/babyweight_tft/transformed/train-00000-of-00002.tfrecords\ngs://YOUR_BUCKET_NAME/babyweight_tft/transformed/train-00001-of-00002.tfrecords\n\ntransformed metadata:\ngs://YOUR_BUCKET_NAME/babyweight_tft/transform/transformed_metadata/\ngs://YOUR_BUCKET_NAME/babyweight_tft/transform/transformed_metadata/asset_map\ngs://YOUR_BUCKET_NAME/babyweight_tft/transform/transformed_metadata/schema.pbtxt\n\ntransform artefact:\ngs://YOUR_BUCKET_NAME/babyweight_tft/transform/transform_fn/\ngs://YOUR_BUCKET_NAME/babyweight_tft/transform/transform_fn/saved_model.pb\ngs://YOUR_BUCKET_NAME/babyweight_tft/transform/transform_fn/assets/\ngs://YOUR_BUCKET_NAME/babyweight_tft/transform/transform_fn/variables/\n\ntransform assets:\ngs://YOUR_BUCKET_NAME/babyweight_tft/transform/transform_fn/assets/\ngs://YOUR_BUCKET_NAME/babyweight_tft/transform/transform_fn/assets/is_male\ngs://YOUR_BUCKET_NAME/babyweight_tft/transform/transform_fn/assets/is_multiple\ngs://YOUR_BUCKET_NAME/babyweight_tft/transform/transform_fn/assets/mother_race\n</code></pre>"},{"location":"tutorials/transform/data_preprocessing_with_cloud/#implement-the-tensorflow-model","title":"Implement the TensorFlow model","text":"<p>This section and the next section, Train and use the model for predictions, provide an overview and context for Notebook 2. The notebook provides an example ML model to predict baby weights. In this example, a TensorFlow model is implemented using the Keras API. The model uses the data and artifacts that are produced by the <code>tf.Transform</code> preprocessing pipeline explained earlier.</p>"},{"location":"tutorials/transform/data_preprocessing_with_cloud/#run-notebook-2","title":"Run Notebook 2","text":"<ol> <li> <p>In the JupyterLab interface, click File &gt; Open from path, and then     enter the following path:</p> <pre><code>training-data-analyst/blogs/babyweight_tft/babyweight_tft_keras_02.ipynb\n</code></pre> </li> <li> <p>Click Edit &gt; Clear all outputs.</p> </li> <li> <p>In the Install required packages section, execute the first cell to run     the <code>pip install tensorflow-transform</code> command.</p> <p>The last part of the output is the following:</p> <pre><code>Successfully installed ...\nNote: you may need to restart the kernel to use updated packages.\n</code></pre> <p>You can ignore dependency errors in the output.</p> </li> <li> <p>In the Kernel menu, select Restart Kernel.</p> </li> <li>Execute the cells in the Confirm the installed packages and Create     setup.py to install packages to Dataflow containers sections.</li> <li>In the Set global flags section, next to <code>PROJECT</code> and <code>BUCKET</code>, replace     <code>your-project</code> with your Cloud project ID, and     then execute the cell.</li> <li>Execute all of the remaining cells through the last cell in the notebook.     For information about what to do in each cell, see the instructions in the     notebook.</li> </ol>"},{"location":"tutorials/transform/data_preprocessing_with_cloud/#overview-of-the-model-creation","title":"Overview of the model creation","text":"<p>The steps for creating the model are as follows:</p> <ol> <li>Create feature columns using the schema information that is stored in the     <code>transformed_metadata</code> directory.</li> <li>Create the wide and deep model with the Keras API using the feature     columns as input to the model.</li> <li>Create the <code>tfrecords_input_fn</code> function to read and parse the training     and evaluation data using the transform artifacts.</li> <li>Train and evaluate the model.</li> <li>Export the trained model by defining a <code>serving_fn</code> function that has the     <code>transform_fn</code> graph attached to it.</li> <li>Inspect the exported model using the     <code>saved_model_cli</code>     tool.</li> <li>Use the exported model for prediction.</li> </ol> <p>This document doesn't explain how to build the model, so it doesn't discuss in detail how the model was built or trained. However, the following sections show how the information stored in the <code>transform_metadata</code> directory\u2014which is produced by the <code>tf.Transform</code> process\u2014is used to create the feature columns of the model. The document also shows how the <code>transform_fn</code> graph\u2014which is also produced by <code>tf.Transform</code> process\u2014is used in the <code>serving_fn</code> function when the model is exported for serving.</p>"},{"location":"tutorials/transform/data_preprocessing_with_cloud/#use-the-generated-transform-artifacts-in-model-training","title":"Use the generated transform artifacts in model training","text":"<p>When you train the TensorFlow model, you use the transformed <code>train</code> and <code>eval</code> objects produced in the previous data processing step. These objects are stored as sharded files in the TFRecord format. The schema information in the <code>transformed_metadata</code> directory generated in the previous step can be useful in parsing the data (<code>tf.train.Example</code> objects) to feed into the model for training and evaluation.</p>"},{"location":"tutorials/transform/data_preprocessing_with_cloud/#parse-the-data","title":"Parse the data","text":"<p>Because you read files in the TFRecord format to feed the model with training and evaluation data, you need to parse each <code>tf.train.Example</code> object in the files to create a dictionary of features (tensors). This ensures that the features are mapped to the model input layer using the feature columns, which act as the model training and evaluation interface. To parse the data, you use the <code>TFTransformOutput</code> object that is created from the artifacts generated in the previous step:</p> <ol> <li> <p>Create a <code>TFTransformOutput</code> object from the artifacts that are generated     and saved in the previous preprocessing step, as described in the     Save the graph     section:</p> <pre><code>tf_transform_output = tft.TFTransformOutput(TRANSFORM_ARTEFACTS_DIR)\n</code></pre> </li> <li> <p>Extract a <code>feature_spec</code> object from the <code>TFTransformOutput</code> object:</p> <pre><code>tf_transform_output.transformed_feature_spec()\n</code></pre> </li> <li> <p>Use the <code>feature_spec</code> object to specify the features contained in the     <code>tf.train.Example</code> object as in the <code>tfrecords_input_fn</code> function:</p> <pre><code>def tfrecords_input_fn(files_name_pattern, batch_size=512):\n\n    tf_transform_output = tft.TFTransformOutput(TRANSFORM_ARTEFACTS_DIR)\n    TARGET_FEATURE_NAME = 'weight_pounds'\n\n    batched_dataset = tf.data.experimental.make_batched_features_dataset(\n        file_pattern=files_name_pattern,\n        batch_size=batch_size,\n        features=tf_transform_output.transformed_feature_spec(),\n        reader=tf.data.TFRecordDataset,\n        label_key=TARGET_FEATURE_NAME,\n        shuffle=True).prefetch(tf.data.experimental.AUTOTUNE)\n\n    return batched_dataset\n</code></pre> </li> </ol>"},{"location":"tutorials/transform/data_preprocessing_with_cloud/#create-the-feature-columns","title":"Create the feature columns","text":"<p>The pipeline produces the schema information in the <code>transformed_metadata</code> directory that describes the schema of the transformed data that is expected by the model for training and evaluation. The schema contains the feature name and data type, such as the following:</p> <ul> <li><code>gestation_weeks_scaled</code> (type: <code>FLOAT</code>)</li> <li><code>is_male_index</code> (type: <code>INT</code>, is_categorical: <code>True</code>)</li> <li><code>is_multiple_index</code> (type: <code>INT</code>, is_categorical: <code>True</code>)</li> <li><code>mother_age_bucketized</code> (type: <code>INT</code>, is_categorical: <code>True</code>)</li> <li><code>mother_age_log</code> (type: <code>FLOAT</code>)</li> <li><code>mother_age_normalized</code> (type: <code>FLOAT</code>)</li> <li><code>mother_race_index</code> (type: <code>INT</code>, is_categorical: <code>True</code>)</li> <li><code>weight_pounds</code> (type: <code>FLOAT</code>)</li> </ul> <p>To see this information, use the following commands:</p> <pre><code>transformed_metadata = tft.TFTransformOutput(TRANSFORM_ARTEFACTS_DIR).transformed_metadata\ntransformed_metadata.schema\n</code></pre> <p>The following code shows how you use the feature name to create feature columns:</p> <pre><code>def create_wide_and_deep_feature_columns():\n\n    deep_feature_columns = []\n    wide_feature_columns = []\n    inputs = {}\n    categorical_columns = {}\n\n    # Select features you've checked from the metadata\n    # Categorical features are associated with the vocabulary size (starting from 0)\n    numeric_features = ['mother_age_log', 'mother_age_normalized', 'gestation_weeks_scaled']\n    categorical_features = [('is_male_index', 1), ('is_multiple_index', 1),\n                            ('mother_age_bucketized', 4), ('mother_race_index', 10)]\n\n    for feature in numeric_features:\n        deep_feature_columns.append(tf.feature_column.numeric_column(feature))\n        inputs[feature] = layers.Input(shape=(), name=feature, dtype='float32')\n\n    for feature, vocab_size in categorical_features:\n        categorical_columns[feature] = (\n            tf.feature_column.categorical_column_with_identity(feature, num_buckets=vocab_size+1))\n        wide_feature_columns.append(tf.feature_column.indicator_column(categorical_columns[feature]))\n        inputs[feature] = layers.Input(shape=(), name=feature, dtype='int64')\n\n    mother_race_X_mother_age_bucketized = tf.feature_column.crossed_column(\n        [categorical_columns['mother_age_bucketized'],\n         categorical_columns['mother_race_index']],  55)\n    wide_feature_columns.append(tf.feature_column.indicator_column(mother_race_X_mother_age_bucketized))\n\n    mother_race_X_mother_age_bucketized_embedded = tf.feature_column.embedding_column(\n        mother_race_X_mother_age_bucketized, 5)\n    deep_feature_columns.append(mother_race_X_mother_age_bucketized_embedded)\n\n    return wide_feature_columns, deep_feature_columns, inputs\n</code></pre> <p>The code creates a <code>tf.feature_column.numeric_column</code> column for numeric features, and a <code>tf.feature_column.categorical_column_with_identity</code> column for categorical features.</p> <p>You can also create extended feature columns, as described in Option C: TensorFlow in the first part of this series. In the example used for this series, a new feature is created, <code>mother_race_X_mother_age_bucketized</code>, by crossing the <code>mother_race</code> and <code>mother_age_bucketized</code> features using the <code>tf.feature_column.crossed_column</code> feature column. Low-dimensional, dense representation of this crossed feature is created using the <code>tf.feature_column.embedding_column</code> feature column.</p> <p>The following diagram, figure 5, shows the transformed data and how the transformed metadata is used to define and train the TensorFlow model:</p> <p>Figure 5: Training the TensorFlow model with the transformed data. </p>"},{"location":"tutorials/transform/data_preprocessing_with_cloud/#export-the-model-for-serving-prediction","title":"Export the model for serving prediction","text":"<p>After you train the TensorFlow model with the Keras API, you export the trained model as a SavedModel object, so that it can serve new data points for prediction. When you export the model, you have to define its interface\u2014that is, the input features schema that is expected during serving. This input features schema is defined in the <code>serving_fn</code> function, as shown in the following code:</p> <pre><code>def export_serving_model(model, output_dir):\n\n    tf_transform_output = tft.TFTransformOutput(TRANSFORM_ARTEFACTS_DIR)\n    # The layer has to be saved to the model for Keras tracking purposes.\n    model.tft_layer = tf_transform_output.transform_features_layer()\n\n    @tf.function\n    def serveing_fn(uid, is_male, mother_race, mother_age, plurality, gestation_weeks):\n        features = {\n            'is_male': is_male,\n            'mother_race': mother_race,\n            'mother_age': mother_age,\n            'plurality': plurality,\n            'gestation_weeks': gestation_weeks\n        }\n        transformed_features = model.tft_layer(features)\n        outputs = model(transformed_features)\n        # The prediction results have multiple elements in general.\n        # But we need only the first element in our case.\n        outputs = tf.map_fn(lambda item: item[0], outputs)\n\n        return {'uid': uid, 'weight': outputs}\n\n    concrete_serving_fn = serveing_fn.get_concrete_function(\n        tf.TensorSpec(shape=[None], dtype=tf.string, name='uid'),\n        tf.TensorSpec(shape=[None], dtype=tf.string, name='is_male'),\n        tf.TensorSpec(shape=[None], dtype=tf.string, name='mother_race'),\n        tf.TensorSpec(shape=[None], dtype=tf.float32, name='mother_age'),\n        tf.TensorSpec(shape=[None], dtype=tf.float32, name='plurality'),\n        tf.TensorSpec(shape=[None], dtype=tf.float32, name='gestation_weeks')\n    )\n    signatures = {'serving_default': concrete_serving_fn}\n\n    model.save(output_dir, save_format='tf', signatures=signatures)\n</code></pre> <p>During serving, the model expects the data points in their raw form (that is, raw features before transformations). Therefore, the <code>serving_fn</code> function receives the raw features and stores them in a <code>features</code> object as a Python dictionary. However, as discussed earlier, the trained model expects the data points in the transformed schema. To convert the raw features into the <code>transformed_features</code> objects that are expected by the model interface, you apply the saved <code>transform_fn</code> graph to the <code>features</code> object with the following steps:</p> <ol> <li> <p>Create the <code>TFTransformOutput</code> object from the artifacts generated and     saved in the previous preprocessing step:</p> <pre><code>tf_transform_output = tft.TFTransformOutput(TRANSFORM_ARTEFACTS_DIR)\n</code></pre> </li> <li> <p>Create a <code>TransformFeaturesLayer</code> object from the <code>TFTransformOutput</code>     object:</p> <pre><code>model.tft_layer = tf_transform_output.transform_features_layer()\n</code></pre> </li> <li> <p>Apply the <code>transform_fn</code> graph using the <code>TransformFeaturesLayer</code> object:</p> <pre><code>transformed_features = model.tft_layer(features)\n</code></pre> </li> </ol> <p>The following diagram, figure 6, illustrates the final step of exporting a model for serving:</p> <p>Figure 6: Exporting the model for serving with the <code>transform_fn</code> graph attached. </p>"},{"location":"tutorials/transform/data_preprocessing_with_cloud/#train-and-use-the-model-for-predictions","title":"Train and use the model for predictions","text":"<p>You can train the model locally by executing the cells of the notebook. For examples of how to package the code and train your model at scale using Vertex AI Training, see the samples and guides in the Google Cloud cloudml-samples GitHub repository.</p> <p>When you inspect the exported SavedModel object using the <code>saved_model_cli</code> tool, you see that the <code>inputs</code> elements of the signature definition <code>signature_def</code> include the raw features, as shown in the following example:</p> <pre><code>signature_def['serving_default']:\n  The given SavedModel SignatureDef contains the following input(s):\n    inputs['gestation_weeks'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (-1)\n        name: serving_default_gestation_weeks:0\n    inputs['is_male'] tensor_info:\n        dtype: DT_STRING\n        shape: (-1)\n        name: serving_default_is_male:0\n    inputs['mother_age'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (-1)\n        name: serving_default_mother_age:0\n    inputs['mother_race'] tensor_info:\n        dtype: DT_STRING\n        shape: (-1)\n        name: serving_default_mother_race:0\n    inputs['plurality'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (-1)\n        name: serving_default_plurality:0\n    inputs['uid'] tensor_info:\n        dtype: DT_STRING\n        shape: (-1)\n        name: serving_default_uid:0\n  The given SavedModel SignatureDef contains the following output(s):\n    outputs['uid'] tensor_info:\n        dtype: DT_STRING\n        shape: (-1)\n        name: StatefulPartitionedCall_6:0\n    outputs['weight'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (-1)\n        name: StatefulPartitionedCall_6:1\n  Method name is: tensorflow/serving/predict\n</code></pre> <p>The remaining cells of the notebook show you how to use the exported model for a local prediction, and how to deploy the model as a microservice using Vertex AI Prediction. It is important to highlight that the input (sample) data point is in the raw schema in both cases.</p>"},{"location":"tutorials/transform/data_preprocessing_with_cloud/#clean-up","title":"Clean up","text":"<p>To avoid incurring additional charges to your Google Cloud account for the resources used in this tutorial, delete the project that contains the resources.</p>"},{"location":"tutorials/transform/data_preprocessing_with_cloud/#delete-the-project","title":"Delete the project","text":"<p>Caution</p> <p>Deleting a project has the following effects:</p> <ul> <li>Everything in the project is deleted. If you used an existing project for   this tutorial, when you delete it, you also delete any other work you've done in the project.</li> <li>Custom project IDs are lost. When you created this project, you might have created a custom project ID that you want to use in the future. To preserve the URLs that use the project ID, such as an <code>appspot.com</code> URL, delete selected resources inside the project instead of deleting the whole project.</li> </ul> <p>If you plan to explore multiple tutorials and quickstarts, reusing projects can help you avoid exceeding project quota limits.</p> <ol> <li>In the Google Cloud console,    go to the Manage resources page.</li> </ol> <p>Go to Manage resources</p> <ol> <li>In the project list, select the project that you want to delete, and then    click Delete.</li> <li>In the dialog, type the project ID, and then click Shut down to delete    the project.</li> </ol>"},{"location":"tutorials/transform/data_preprocessing_with_cloud/#whats-next","title":"What's next","text":"<ul> <li>To learn about the concepts, challenges, and options of data     preprocessing for machine learning on Google Cloud, see the first     article in this series,     Data preprocessing for ML: options and recommendations.</li> <li>For more information about how to implement, package, and run a     tf.Transform pipeline on Dataflow, see the     Predicting income with Census Dataset     sample.</li> <li>Take the Coursera specialization on ML with     TensorFlow on Google Cloud.</li> <li>Learn about best practices for ML engineering in     Rules of ML.</li> <li>For more reference architectures, diagrams, and best practices, explore the     Cloud Architecture Center.</li> </ul>"},{"location":"tutorials/transform/simple/","title":"Preprocess data with TensorFlow Transform","text":"In\u00a0[\u00a0]: Copied! <pre>#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n</pre> #@title Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # https://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. <p>Note: We recommend running this tutorial in a Colab notebook, with no setup required!  Just click \"Run in Google Colab\".</p>        View on TensorFlow.org             Run in Google Colab             View source on GitHub             Download notebook      <p>This example colab notebook provides a very simple example of how TensorFlow Transform (<code>tf.Transform</code>) can be used to preprocess data using exactly the same code for both training a model and serving inferences in production.</p> <p>TensorFlow Transform is a library for preprocessing input data for TensorFlow, including creating features that require a full pass over the training dataset.  For example, using TensorFlow Transform you could:</p> <ul> <li>Normalize an input value by using the mean and standard deviation</li> <li>Convert strings to integers by generating a vocabulary over all of the input values</li> <li>Convert floats to integers by assigning them to buckets, based on the observed data distribution</li> </ul> <p>TensorFlow has built-in support for manipulations on a single example or a batch of examples. <code>tf.Transform</code> extends these capabilities to support full passes over the entire training dataset.</p> <p>The output of <code>tf.Transform</code> is exported as a TensorFlow graph which you can use for both training and serving. Using the same graph for both training and serving can prevent skew, since the same transformations are applied in both stages.</p> In\u00a0[\u00a0]: Copied! <pre>try:\n  import colab\n  !pip install --upgrade pip\nexcept:\n  pass\n</pre> try:   import colab   !pip install --upgrade pip except:   pass In\u00a0[\u00a0]: Copied! <pre>!pip install -q -U tensorflow_transform\n</pre> !pip install -q -U tensorflow_transform In\u00a0[\u00a0]: Copied! <pre># This cell is only necessary because packages were installed while python was\n# running. It avoids the need to restart the runtime when running in Colab.\nimport pkg_resources\nimport importlib\n\nimportlib.reload(pkg_resources)\n</pre> # This cell is only necessary because packages were installed while python was # running. It avoids the need to restart the runtime when running in Colab. import pkg_resources import importlib  importlib.reload(pkg_resources) In\u00a0[\u00a0]: Copied! <pre>import pathlib\nimport pprint\nimport tempfile\n\nimport tensorflow as tf\nimport tensorflow_transform as tft\n\nimport tensorflow_transform.beam as tft_beam\nfrom tensorflow_transform.tf_metadata import dataset_metadata\nfrom tensorflow_transform.tf_metadata import schema_utils\nfrom tensorflow_transform.keras_lib import tf_keras\n</pre> import pathlib import pprint import tempfile  import tensorflow as tf import tensorflow_transform as tft  import tensorflow_transform.beam as tft_beam from tensorflow_transform.tf_metadata import dataset_metadata from tensorflow_transform.tf_metadata import schema_utils from tensorflow_transform.keras_lib import tf_keras In\u00a0[\u00a0]: Copied! <pre>raw_data = [\n      {'x': 1, 'y': 1, 's': 'hello'},\n      {'x': 2, 'y': 2, 's': 'world'},\n      {'x': 3, 'y': 3, 's': 'hello'}\n  ]\n\nraw_data_metadata = dataset_metadata.DatasetMetadata(\n    schema_utils.schema_from_feature_spec({\n        'y': tf.io.FixedLenFeature([], tf.float32),\n        'x': tf.io.FixedLenFeature([], tf.float32),\n        's': tf.io.FixedLenFeature([], tf.string),\n    }))\n</pre> raw_data = [       {'x': 1, 'y': 1, 's': 'hello'},       {'x': 2, 'y': 2, 's': 'world'},       {'x': 3, 'y': 3, 's': 'hello'}   ]  raw_data_metadata = dataset_metadata.DatasetMetadata(     schema_utils.schema_from_feature_spec({         'y': tf.io.FixedLenFeature([], tf.float32),         'x': tf.io.FixedLenFeature([], tf.float32),         's': tf.io.FixedLenFeature([], tf.string),     })) <ol> <li>TensorFlow Ops: Any function that accepts and returns tensors, which usually means TensorFlow ops. These add TensorFlow operations to the graph that transforms raw data into transformed data one feature vector at a time.  These will run for every example, during both training and serving.</li> <li>Tensorflow Transform Analyzers/Mappers: Any of the analyzers/mappers provided by tf.Transform. These also accept and return tensors, and typically contain a combination of Tensorflow ops and Beam computation, but unlike TensorFlow ops they only run in the Beam pipeline during analysis requiring a full pass over the entire training dataset. The Beam computation runs only once, (prior to training, during analysis), and typically make a full pass over the entire training dataset. They create <code>tf.constant</code> tensors, which are added to your graph. For example, <code>tft.min</code> computes the minimum of a tensor over the training dataset.</li> </ol> <p>Caution: When you apply your preprocessing function to serving inferences, the constants that were created by analyzers during training do not change.  If your data has trend or seasonality components, plan accordingly.</p> <p>Note: The <code>preprocessing_fn</code> is not directly callable. This means that calling <code>preprocessing_fn(raw_data)</code> will not work. Instead, it must be passed to the Transform Beam API as shown in the following cells.</p> In\u00a0[\u00a0]: Copied! <pre>def preprocessing_fn(inputs):\n    \"\"\"Preprocess input columns into transformed columns.\"\"\"\n    x = inputs['x']\n    y = inputs['y']\n    s = inputs['s']\n    x_centered = x - tft.mean(x)\n    y_normalized = tft.scale_to_0_1(y)\n    s_integerized = tft.compute_and_apply_vocabulary(s)\n    x_centered_times_y_normalized = (x_centered * y_normalized)\n    return {\n        'x_centered': x_centered,\n        'y_normalized': y_normalized,\n        's_integerized': s_integerized,\n        'x_centered_times_y_normalized': x_centered_times_y_normalized,\n    }\n</pre> def preprocessing_fn(inputs):     \"\"\"Preprocess input columns into transformed columns.\"\"\"     x = inputs['x']     y = inputs['y']     s = inputs['s']     x_centered = x - tft.mean(x)     y_normalized = tft.scale_to_0_1(y)     s_integerized = tft.compute_and_apply_vocabulary(s)     x_centered_times_y_normalized = (x_centered * y_normalized)     return {         'x_centered': x_centered,         'y_normalized': y_normalized,         's_integerized': s_integerized,         'x_centered_times_y_normalized': x_centered_times_y_normalized,     } In\u00a0[\u00a0]: Copied! <pre>def main(output_dir):\n  # Ignore the warnings\n  with tft_beam.Context(temp_dir=tempfile.mkdtemp()):\n    transformed_dataset, transform_fn = (  # pylint: disable=unused-variable\n        (raw_data, raw_data_metadata) | tft_beam.AnalyzeAndTransformDataset(\n            preprocessing_fn))\n\n  transformed_data, transformed_metadata = transformed_dataset  # pylint: disable=unused-variable\n\n  # Save the transform_fn to the output_dir\n  _ = (\n      transform_fn\n      | 'WriteTransformFn' &gt;&gt; tft_beam.WriteTransformFn(output_dir))\n\n  return transformed_data, transformed_metadata\n</pre> def main(output_dir):   # Ignore the warnings   with tft_beam.Context(temp_dir=tempfile.mkdtemp()):     transformed_dataset, transform_fn = (  # pylint: disable=unused-variable         (raw_data, raw_data_metadata) | tft_beam.AnalyzeAndTransformDataset(             preprocessing_fn))    transformed_data, transformed_metadata = transformed_dataset  # pylint: disable=unused-variable    # Save the transform_fn to the output_dir   _ = (       transform_fn       | 'WriteTransformFn' &gt;&gt; tft_beam.WriteTransformFn(output_dir))    return transformed_data, transformed_metadata In\u00a0[\u00a0]: Copied! <pre>output_dir = pathlib.Path(tempfile.mkdtemp())\n\ntransformed_data, transformed_metadata = main(str(output_dir))\n\nprint('\\nRaw data:\\n{}\\n'.format(pprint.pformat(raw_data)))\nprint('Transformed data:\\n{}'.format(pprint.pformat(transformed_data)))\n</pre> output_dir = pathlib.Path(tempfile.mkdtemp())  transformed_data, transformed_metadata = main(str(output_dir))  print('\\nRaw data:\\n{}\\n'.format(pprint.pformat(raw_data))) print('Transformed data:\\n{}'.format(pprint.pformat(transformed_data))) In\u00a0[\u00a0]: Copied! <pre>!ls -l {output_dir}\n</pre> !ls -l {output_dir} <p>The <code>transform_fn/</code> directory contains a <code>tf.saved_model</code> implementing with all the constants tensorflow-transform analysis results built into the graph.</p> <p>It is possible to load this directly with <code>tf.saved_model.load</code>, but this not easy to use:</p> In\u00a0[\u00a0]: Copied! <pre>loaded = tf.saved_model.load(str(output_dir/'transform_fn'))\nloaded.signatures['serving_default']\n</pre> loaded = tf.saved_model.load(str(output_dir/'transform_fn')) loaded.signatures['serving_default'] <p>A better approach is to load it using <code>tft.TFTransformOutput</code>. The <code>TFTransformOutput.transform_features_layer</code> method returns a <code>tft.TransformFeaturesLayer</code> object that can be used to apply the transformation:</p> In\u00a0[\u00a0]: Copied! <pre>tf_transform_output = tft.TFTransformOutput(output_dir)\n\ntft_layer = tf_transform_output.transform_features_layer()\ntft_layer\n</pre> tf_transform_output = tft.TFTransformOutput(output_dir)  tft_layer = tf_transform_output.transform_features_layer() tft_layer <p>This <code>tft.TransformFeaturesLayer</code> expects a dictionary of batched features. So create a <code>Dict[str, tf.Tensor]</code> from the <code>List[Dict[str, Any]]</code> in <code>raw_data</code>:</p> In\u00a0[\u00a0]: Copied! <pre>raw_data_batch = {\n    's': tf.constant([ex['s'] for ex in raw_data]),\n    'x': tf.constant([ex['x'] for ex in raw_data], dtype=tf.float32),\n    'y': tf.constant([ex['y'] for ex in raw_data], dtype=tf.float32),\n}\n</pre> raw_data_batch = {     's': tf.constant([ex['s'] for ex in raw_data]),     'x': tf.constant([ex['x'] for ex in raw_data], dtype=tf.float32),     'y': tf.constant([ex['y'] for ex in raw_data], dtype=tf.float32), } <p>You can use the <code>tft.TransformFeaturesLayer</code> on its own:</p> In\u00a0[\u00a0]: Copied! <pre>transformed_batch = tft_layer(raw_data_batch)\n\n{key: value.numpy() for key, value in transformed_batch.items()}\n</pre> transformed_batch = tft_layer(raw_data_batch)  {key: value.numpy() for key, value in transformed_batch.items()} <p>A more typical use case would use <code>tf.Transform</code> to apply the transformation to the training and evaluation datasets (see the next tutorial for an example). Then, after training, before exporting the model attach the <code>tft.TransformFeaturesLayer</code> as the first layer so that you can export it as part of your <code>tf.saved_model</code>. For a concrete example, keep reading.</p> In\u00a0[\u00a0]: Copied! <pre>class StackDict(tf_keras.layers.Layer):\n  def call(self, inputs):\n    values = [\n        tf.cast(v, tf.float32)\n        for k,v in sorted(inputs.items(), key=lambda kv: kv[0])]\n    return tf.stack(values, axis=1)\n</pre> class StackDict(tf_keras.layers.Layer):   def call(self, inputs):     values = [         tf.cast(v, tf.float32)         for k,v in sorted(inputs.items(), key=lambda kv: kv[0])]     return tf.stack(values, axis=1) In\u00a0[\u00a0]: Copied! <pre>class TrainedModel(tf_keras.Model):\n  def __init__(self):\n    super().__init__(self)\n    self.concat = StackDict()\n    self.body = tf_keras.Sequential([\n        tf_keras.layers.Dense(64, activation='relu'),\n        tf_keras.layers.Dense(64, activation='relu'),\n        tf_keras.layers.Dense(10),\n    ])\n\n  def call(self, inputs, training=None):\n    x = self.concat(inputs)\n    return self.body(x, training)\n</pre> class TrainedModel(tf_keras.Model):   def __init__(self):     super().__init__(self)     self.concat = StackDict()     self.body = tf_keras.Sequential([         tf_keras.layers.Dense(64, activation='relu'),         tf_keras.layers.Dense(64, activation='relu'),         tf_keras.layers.Dense(10),     ])    def call(self, inputs, training=None):     x = self.concat(inputs)     return self.body(x, training) In\u00a0[\u00a0]: Copied! <pre>trained_model = TrainedModel()\n</pre> trained_model = TrainedModel() <p>Imagine we trained the model.</p> <pre><code>trained_model.compile(loss=..., optimizer='adam')\ntrained_model.fit(...)\n</code></pre> <p>This model runs on the transformed inputs</p> In\u00a0[\u00a0]: Copied! <pre>trained_model_output = trained_model(transformed_batch)\ntrained_model_output.shape\n</pre> trained_model_output = trained_model(transformed_batch) trained_model_output.shape In\u00a0[\u00a0]: Copied! <pre>class ExportModel(tf.Module):\n  def __init__(self, trained_model, input_transform):\n    self.trained_model = trained_model\n    self.input_transform = input_transform\n\n  @tf.function\n  def __call__(self, inputs, training=None):\n    x = self.input_transform(inputs)\n    return self.trained_model(x)\n</pre> class ExportModel(tf.Module):   def __init__(self, trained_model, input_transform):     self.trained_model = trained_model     self.input_transform = input_transform    @tf.function   def __call__(self, inputs, training=None):     x = self.input_transform(inputs)     return self.trained_model(x) In\u00a0[\u00a0]: Copied! <pre>export_model = ExportModel(trained_model=trained_model,\n                           input_transform=tft_layer)\n</pre> export_model = ExportModel(trained_model=trained_model,                            input_transform=tft_layer) <p>This combined model works on the raw data, and produces exactly the same results as calling the trained model directly:</p> In\u00a0[\u00a0]: Copied! <pre>export_model_output = export_model(raw_data_batch)\nexport_model_output.shape\n</pre> export_model_output = export_model(raw_data_batch) export_model_output.shape In\u00a0[\u00a0]: Copied! <pre>tf.reduce_max(abs(export_model_output - trained_model_output)).numpy()\n</pre> tf.reduce_max(abs(export_model_output - trained_model_output)).numpy() <p>This <code>export_model</code> includes the <code>tft.TransformFeaturesLayer</code> and is entirely self-contained. You can save it and restore it in another environment and still get exactly the same result:</p> In\u00a0[\u00a0]: Copied! <pre>import tempfile\nmodel_dir = tempfile.mkdtemp(suffix='tft')\n\ntf.saved_model.save(export_model, model_dir)\n</pre> import tempfile model_dir = tempfile.mkdtemp(suffix='tft')  tf.saved_model.save(export_model, model_dir) In\u00a0[\u00a0]: Copied! <pre>reloaded = tf.saved_model.load(model_dir)\n\nreloaded_model_output = reloaded(raw_data_batch)\nreloaded_model_output.shape\n</pre> reloaded = tf.saved_model.load(model_dir)  reloaded_model_output = reloaded(raw_data_batch) reloaded_model_output.shape In\u00a0[\u00a0]: Copied! <pre>tf.reduce_max(abs(export_model_output - reloaded_model_output)).numpy()\n</pre> tf.reduce_max(abs(export_model_output - reloaded_model_output)).numpy()"},{"location":"tutorials/transform/simple/#copyright-2021-the-tensorflow-authors","title":"Copyright 2021 The TensorFlow Authors.\u00b6","text":""},{"location":"tutorials/transform/simple/#preprocess-data-with-tensorflow-transform","title":"Preprocess data with TensorFlow Transform\u00b6","text":"<p>The Feature Engineering Component of TensorFlow Extended (TFX)</p>"},{"location":"tutorials/transform/simple/#upgrade-pip","title":"Upgrade Pip\u00b6","text":"<p>To avoid upgrading Pip in a system when running locally, check to make sure that we're running in Colab.  Local systems can of course be upgraded separately.</p>"},{"location":"tutorials/transform/simple/#install-tensorflow-transform","title":"Install TensorFlow Transform\u00b6","text":""},{"location":"tutorials/transform/simple/#imports","title":"Imports\u00b6","text":""},{"location":"tutorials/transform/simple/#data-create-some-dummy-data","title":"Data: Create some dummy data\u00b6","text":"<p>We'll create some simple dummy data for our simple example:</p> <ul> <li><code>raw_data</code> is the initial raw data that we're going to preprocess</li> <li><code>raw_data_metadata</code> contains the schema that tells us the types of each of the columns in <code>raw_data</code>.  In this case, it's very simple.</li> </ul>"},{"location":"tutorials/transform/simple/#transform-create-a-preprocessing-function","title":"Transform: Create a preprocessing function\u00b6","text":"<p>The preprocessing function is the most important concept of tf.Transform. A preprocessing function is where the transformation of the dataset really happens. It accepts and returns a dictionary of tensors, where a tensor means a <code>Tensor</code> or <code>SparseTensor</code>. There are two main groups of API calls that typically form the heart of a preprocessing function:</p>"},{"location":"tutorials/transform/simple/#syntax","title":"Syntax\u00b6","text":"<p>You're almost ready to put everything together and use Apache Beam to run it.</p> <p>Apache Beam uses a special syntax to define and invoke transforms.  For example, in this line:</p> <pre><code>result = pass_this | 'name this step' &gt;&gt; to_this_call\n</code></pre> <p>The method <code>to_this_call</code> is being invoked and passed the object called <code>pass_this</code>, and this operation will be referred to as <code>name this step</code> in a stack trace.  The result of the call to <code>to_this_call</code> is returned in <code>result</code>.  You will often see stages of a pipeline chained together like this:</p> <pre><code>result = apache_beam.Pipeline() | 'first step' &gt;&gt; do_this_first() | 'second step' &gt;&gt; do_this_last()\n</code></pre> <p>and since that started with a new pipeline, you can continue like this:</p> <pre><code>next_result = result | 'doing more stuff' &gt;&gt; another_function()\n</code></pre>"},{"location":"tutorials/transform/simple/#putting-it-all-together","title":"Putting it all together\u00b6","text":"<p>Now we're ready to transform our data. We'll use Apache Beam with a direct runner, and supply three inputs:</p> <ol> <li><code>raw_data</code> - The raw input data that we created above</li> <li><code>raw_data_metadata</code> - The schema for the raw data</li> <li><code>preprocessing_fn</code> - The function that we created to do our transformation</li> </ol>"},{"location":"tutorials/transform/simple/#is-this-the-right-answer","title":"Is this the right answer?\u00b6","text":"<p>Previously, we used <code>tf.Transform</code> to do this:</p> <pre><code>x_centered = x - tft.mean(x)\ny_normalized = tft.scale_to_0_1(y)\ns_integerized = tft.compute_and_apply_vocabulary(s)\nx_centered_times_y_normalized = (x_centered * y_normalized)\n</code></pre> <ul> <li>x_centered - With input of <code>[1, 2, 3]</code> the mean of x is 2, and we subtract it from x to center our x values at 0.  So our result of <code>[-1.0, 0.0, 1.0]</code> is correct.</li> <li>y_normalized - We wanted to scale our y values between 0 and 1.  Our input was <code>[1, 2, 3]</code> so our result of <code>[0.0, 0.5, 1.0]</code> is correct.</li> <li>s_integerized - We wanted to map our strings to indexes in a vocabulary, and there were only 2 words in our vocabulary (\"hello\" and \"world\").  So with input of <code>[\"hello\", \"world\", \"hello\"]</code> our result of <code>[0, 1, 0]</code> is correct. Since \"hello\" occurs most frequently in this data, it will be the first entry in the vocabulary.</li> <li>x_centered_times_y_normalized - We wanted to create a new feature by crossing <code>x_centered</code> and <code>y_normalized</code> using multiplication.  Note that this multiplies the results, not the original values, and our new result of <code>[-0.0, 0.0, 1.0]</code> is correct.</li> </ul>"},{"location":"tutorials/transform/simple/#use-the-resulting-transform_fn","title":"Use the resulting <code>transform_fn</code>\u00b6","text":""},{"location":"tutorials/transform/simple/#export","title":"Export\u00b6","text":""},{"location":"tutorials/transform/simple/#an-example-training-model","title":"An example training model\u00b6","text":"<p>Below is a model that:</p> <ol> <li>takes the transformed batch,</li> <li>stacks them all together into a simple <code>(batch, features)</code> matrix,</li> <li>runs them through a few dense layers, and</li> <li>produces 10 linear outputs.</li> </ol> <p>In a real use case you would apply a one-hot to the <code>s_integerized</code> feature.</p> <p>You could train this model on a dataset transformed by <code>tf.Transform</code>:</p>"},{"location":"tutorials/transform/simple/#an-example-export-wrapper","title":"An example export wrapper\u00b6","text":"<p>Imagine you've trained the above model and want to export it.</p> <p>You'll want to include the transform function in the exported model:</p>"}]}