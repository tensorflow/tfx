{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YtDTm6wbIbpy"
   },
   "source": [
    "##### Copyright 2024 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iwgnKVaUuozP"
   },
   "source": [
    "# Licensed under the Apache License, Version 2.0 (the \"License\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kBFkQLk1In7I"
   },
   "outputs": [],
   "source": [
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uf3QpfdiIl7O"
   },
   "source": [
    "Note: We recommend running this tutorial in a Colab notebook, with no setup required!  Just click \"Run in Google Colab\".\n",
    "\n",
    "<div class=\"buttons-wrapper\">\n",
    "  <a class=\"md-button\" target=\"_blank\" href=\n",
    "     \"https://www.tensorflow.org/tfx/tutorials/tfx/gpt2_finetuning_and_conversion\">\n",
    "    <div class=\"buttons-content\">\n",
    "      <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">\n",
    "      View on TensorFlow.org\n",
    "    </div>\n",
    "  </a>\n",
    "  <a class=\"md-button\" target=\"_blank\" href=\n",
    "     \"https://colab.research.google.com/github/tensorflow/tfx/blob/master/docs/tutorials/tfx/gpt2_finetuning_and_conversion.ipynb\">\n",
    "    <div class=\"buttons-content\">\n",
    "      <img src=\n",
    "\t   \"https://www.tensorflow.org/images/colab_logo_32px.png\">\n",
    "      Run in Google Colab\n",
    "    </div>\n",
    "  </a>\n",
    "  <a class=\"md-button\" target=\"_blank\" href=\n",
    "     \"https://github.com/tensorflow/tfx/tree/master/docs/tutorials/tfx/gpt2_finetuning_and_conversion.ipynb\">\n",
    "    <div class=\"buttons-content\">\n",
    "      <img width=\"32px\" src=\n",
    "\t   \"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">\n",
    "      View source on GitHub\n",
    "    </div>\n",
    "  </a>\n",
    "  <a class=\"md-button\" href=\n",
    "     \"https://storage.googleapis.com/tensorflow_docs/tfx/docs/tutorials/tfx/gpt2_finetuning_and_conversion.ipynb\">\n",
    "    <div class=\"buttons-content\">\n",
    "      <img src=\n",
    "\t   \"https://www.tensorflow.org/images/download_logo_32px.png\">\n",
    "      Download notebook\n",
    "    </div>\n",
    "  </a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HU9YYythm0dx"
   },
   "source": [
    "### Why is this pipeline useful?\n",
    "\n",
    "TFX pipelines provide a powerful and structured approach to building and managing machine learning workflows, particularly those involving large language models. They offer significant advantages over traditional Python code, including:\n",
    "\n",
    "1. Enhanced Reproducibility: TFX pipelines ensure consistent results by capturing all steps and dependencies, eliminating the inconsistencies often associated with manual workflows.\n",
    "\n",
    "2. Scalability and Modularity: TFX allows for breaking down complex workflows into manageable, reusable components, promoting code organization.\n",
    "\n",
    "3. Streamlined Fine-Tuning and Conversion: The pipeline structure streamlines the fine-tuning and conversion processes of large language models, significantly reducing manual effort and time.\n",
    "\n",
    "4. Comprehensive Lineage Tracking: Through metadata tracking, TFX pipelines provide a clear understanding of data and model provenance, making debugging, auditing, and performance analysis much easier and more efficient.\n",
    "\n",
    "By leveraging the benefits of TFX pipelines, organizations can effectively manage the complexity of large language model development and deployment, achieving greater efficiency and control over their machine learning processes.\n",
    "\n",
    "### Note\n",
    "*GPT-2 is used here only to demonstrate the end-to-end process; the techniques and tooling introduced in this codelab are potentially transferrable to other generative language models such as Google T5.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2WgJ8Z8gJB0s"
   },
   "source": [
    "## Before You Begin\n",
    "\n",
    "Colab offers different kinds of runtimes. Make sure to go to **Runtime -> Change runtime type** and choose the GPU Hardware Accelerator runtime since you will finetune the GPT-2 model.\n",
    "\n",
    "**This tutorial's interactive pipeline is designed to function seamlessly with free Colab GPUs. However, for users opting to run the pipeline using the LocalDagRunner orchestrator (code provided at the end of this tutorial), a more substantial amount of GPU memory is required. Therefore, Colab Pro or a local machine equipped with a higher-capacity GPU is recommended for this approach.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-sj3HvNcJEgC"
   },
   "source": [
    "## Set Up\n",
    "\n",
    "We first install required python packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "73c9sPckJFSi"
   },
   "source": [
    "### Upgrade Pip\n",
    "To avoid upgrading Pip in a system when running locally, check to make sure that we are running in Colab. Local systems can of course be upgraded separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "45pIxa6afWOf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  import colab\n",
    "  !pip install --upgrade pip\n",
    "\n",
    "except:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yIf40NdqJLAH"
   },
   "source": [
    "### Install TFX, Keras 3, KerasNLP and required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A6mBN4dzfct7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q tfx tensorflow-text more_itertools tensorflow_datasets\n",
    "!pip install -q --upgrade keras-nlp\n",
    "!pip install -q --upgrade keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KnyILJ-k3NAy"
   },
   "source": [
    "*Note: pip's dependency resolver errors can be ignored. The required packages for this tutorial works as expected.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V0tnFDm6JRq_",
    "tags": []
   },
   "source": [
    "### Did you restart the runtime?\n",
    "\n",
    "If you are using Google Colab, the first time that you run the cell above, you must restart the runtime by clicking above \"RESTART SESSION\" button or using `\"Runtime > Restart session\"` menu. This is because of the way that Colab loads packages.\n",
    "\n",
    "Let's check the TensorFlow, Keras, Keras-nlp and TFX library versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hf5FbRzcfpMg",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import tensorflow as tf\n",
    "print('TensorFlow version: {}'.format(tf.__version__))\n",
    "from tfx import v1 as tfx\n",
    "print('TFX version: {}'.format(tfx.__version__))\n",
    "import keras\n",
    "print('Keras version: {}'.format(keras.__version__))\n",
    "import keras_nlp\n",
    "print('Keras NLP version: {}'.format(keras_nlp.__version__))\n",
    "\n",
    "keras.mixed_precision.set_global_policy(\"mixed_float16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ng1a9cCAtepl"
   },
   "source": [
    "### Using TFX Interactive Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k7ikXCc7v7Rh"
   },
   "source": [
    "An interactive context is used to provide global context when running a TFX pipeline in a notebook without using a runner or orchestrator such as Apache Airflow or Kubeflow. This style of development is only useful when developing the code for a pipeline, and cannot currently be used to deploy a working pipeline to production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TEge2nYDfwaM",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n",
    "context = InteractiveContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GF6Kk3MLxxCC"
   },
   "source": [
    "## Pipeline Overview\n",
    "\n",
    "Below are the components that this pipeline follows.\n",
    "\n",
    "* Custom Artifacts are artifacts that we have created for this pipeline. **Artifacts** are data that is produced by a component or consumed by a component. Artifacts are stored in a system for managing the storage and versioning of artifacts called MLMD.\n",
    "\n",
    "* **Components** are defined as the implementation of an ML task that you can use as a step in your pipeline\n",
    "* Aside from artifacts, **Parameters** are passed into the components to specify an argument.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BIBO-ueGVVHa"
   },
   "source": [
    "## ExampleGen\n",
    "We create a custom ExampleGen component which we use to load a TensorFlow Datasets (TFDS) dataset. This uses a custom executor in a FileBasedExampleGen.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pgvIaoAmXFVp",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Text\n",
    "import tensorflow_datasets as tfds\n",
    "import apache_beam as beam\n",
    "import json\n",
    "from tfx.components.example_gen.base_example_gen_executor import BaseExampleGenExecutor\n",
    "from tfx.components.example_gen.component import FileBasedExampleGen\n",
    "from tfx.components.example_gen import utils\n",
    "from tfx.dsl.components.base import executor_spec\n",
    "import os\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cjd9Z6SpVRCE",
    "tags": []
   },
   "outputs": [],
   "source": [
    "@beam.ptransform_fn\n",
    "@beam.typehints.with_input_types(beam.Pipeline)\n",
    "@beam.typehints.with_output_types(tf.train.Example)\n",
    "def _TFDatasetToExample(\n",
    "    pipeline: beam.Pipeline,\n",
    "    exec_properties: Dict[str, Any],\n",
    "    split_pattern: str\n",
    "    ) -> beam.pvalue.PCollection:\n",
    "    \"\"\"Read a TensorFlow Dataset and create tf.Examples\"\"\"\n",
    "    custom_config = json.loads(exec_properties['custom_config'])\n",
    "    dataset_name = custom_config['dataset']\n",
    "    split_name = custom_config['split']\n",
    "\n",
    "    builder = tfds.builder(dataset_name)\n",
    "    builder.download_and_prepare()\n",
    "\n",
    "    return (pipeline\n",
    "            | 'MakeExamples' >> tfds.beam.ReadFromTFDS(builder, split=split_name)\n",
    "            | 'AsNumpy' >> beam.Map(tfds.as_numpy)\n",
    "            | 'ToDict' >> beam.Map(dict)\n",
    "            | 'ToTFExample' >> beam.Map(utils.dict_to_example)\n",
    "            )\n",
    "\n",
    "class TFDSExecutor(BaseExampleGenExecutor):\n",
    "  def GetInputSourceToExamplePTransform(self) -> beam.PTransform:\n",
    "    \"\"\"Returns PTransform for TF Dataset to TF examples.\"\"\"\n",
    "    return _TFDatasetToExample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2D159hAzJgK2"
   },
   "source": [
    "For this demonstration, we're using a subset of the IMDb reviews dataset, representing 20% of the total data. This allows for a more manageable training process. You can modify the \"custom_config\" settings to experiment with larger amounts of data, up to the full dataset, depending on your computational resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nNDu1ECBXuvI",
    "tags": []
   },
   "outputs": [],
   "source": [
    "example_gen = FileBasedExampleGen(\n",
    "    input_base='dummy',\n",
    "    custom_config={'dataset':'imdb_reviews', 'split':'train[:20%]'},\n",
    "    custom_executor_spec=executor_spec.BeamExecutorSpec(TFDSExecutor))\n",
    "context.run(example_gen, enable_cache=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "74JGpvIgJgK2"
   },
   "source": [
    "We've developed a handy utility for examining datasets composed of TFExamples. When used with the reviews dataset, this tool returns a clear dictionary containing both the text and the corresponding label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GA8VMXKogXxB",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def inspect_examples(component,\n",
    "                     channel_name='examples',\n",
    "                     split_name='train',\n",
    "                     num_examples=1):\n",
    "  # Get the URI of the output artifact, which is a directory\n",
    "  full_split_name = 'Split-{}'.format(split_name)\n",
    "  print('channel_name: {}, split_name: {} (\\\"{}\\\"), num_examples: {}\\n'.format(\n",
    "      channel_name, split_name, full_split_name, num_examples))\n",
    "  train_uri = os.path.join(\n",
    "      component.outputs[channel_name].get()[0].uri, full_split_name)\n",
    "  print('train_uri: {}'.format(train_uri))\n",
    "\n",
    "  # Get the list of files in this directory (all compressed TFRecord files)\n",
    "  tfrecord_filenames = [os.path.join(train_uri, name)\n",
    "                        for name in os.listdir(train_uri)]\n",
    "\n",
    "  # Create a `TFRecordDataset` to read these files\n",
    "  dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")\n",
    "\n",
    "  # Iterate over the records and print them\n",
    "  print()\n",
    "  for tfrecord in dataset.take(num_examples):\n",
    "    serialized_example = tfrecord.numpy()\n",
    "    example = tf.train.Example()\n",
    "    example.ParseFromString(serialized_example)\n",
    "    pp.pprint(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rcUvtz5egaIy",
    "tags": []
   },
   "outputs": [],
   "source": [
    "inspect_examples(example_gen, num_examples=1, split_name='eval')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gVmx7JHK8RkO"
   },
   "source": [
    "## StatisticsGen\n",
    "\n",
    "`StatisticsGen` component computes statistics over your dataset for data analysis, such as the number of examples, the number of features, and the data types of the features. It uses the [TensorFlow Data Validation](https://www.tensorflow.org/tfx/data_validation/get_started) library. `StatisticsGen` takes as input the dataset we just ingested using `ExampleGen`.\n",
    "\n",
    "*Note that the statistics generator is appropriate for tabular data, and therefore, text dataset for this LLM tutorial may not be the optimal dataset for the analysis with statistics generator.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TzeNGNEnyq_d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tfx.components import StatisticsGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xWWl7LeRKsXA",
    "tags": []
   },
   "outputs": [],
   "source": [
    "statistics_gen = tfx.components.StatisticsGen(\n",
    "    examples=example_gen.outputs['examples'], exclude_splits=['eval']\n",
    ")\n",
    "context.run(statistics_gen, enable_cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LnWKjMyIVVB7"
   },
   "outputs": [],
   "source": [
    "context.show(statistics_gen.outputs['statistics'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oqXFJyoO9O8-"
   },
   "source": [
    "## SchemaGen\n",
    "\n",
    "The `SchemaGen` component generates a schema based on your data statistics. (A schema defines the expected bounds, types, and properties of the features in your dataset.) It also uses the [TensorFlow Data Validation](https://www.tensorflow.org/tfx/data_validation/get_started) library.\n",
    "\n",
    "Note: The generated schema is best-effort and only tries to infer basic properties of the data. It is expected that you review and modify it as needed.\n",
    "\n",
    "`SchemaGen` will take as input the statistics that we generated with `StatisticsGen`, looking at the training split by default.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PpPFaV6tX5wQ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "schema_gen = tfx.components.SchemaGen(\n",
    "    statistics=statistics_gen.outputs['statistics'],\n",
    "    infer_feature_shape=False,\n",
    "    exclude_splits=['eval'],\n",
    ")\n",
    "context.run(schema_gen, enable_cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H6DNNUi3YAmo",
    "tags": []
   },
   "outputs": [],
   "source": [
    "context.show(schema_gen.outputs['schema'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GDdpADUb9VJR"
   },
   "source": [
    "## ExampleValidator\n",
    "\n",
    "The `ExampleValidator` component detects anomalies in your data, based on the expectations defined by the schema. It also uses the [TensorFlow Data Validation](https://www.tensorflow.org/tfx/data_validation/get_started) library.\n",
    "\n",
    "`ExampleValidator` will take as input the statistics from `StatisticsGen`, and the schema from `SchemaGen`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S_F5pLZ7YdZg"
   },
   "outputs": [],
   "source": [
    "example_validator = tfx.components.ExampleValidator(\n",
    "    statistics=statistics_gen.outputs['statistics'],\n",
    "    schema=schema_gen.outputs['schema'],\n",
    "    exclude_splits=['eval'],\n",
    ")\n",
    "context.run(example_validator, enable_cache=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DgiXSTRawolF"
   },
   "source": [
    "After `ExampleValidator` finishes running, we can visualize the anomalies as a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3eAHpc2UYfk_"
   },
   "outputs": [],
   "source": [
    "context.show(example_validator.outputs['anomalies'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7H6fecGTiFmN"
   },
   "source": [
    "## Transform\n",
    "\n",
    "For a structured and repeatable design of a TFX pipeline we will need a scalable approach to feature engineering. The `Transform` component performs feature engineering for both training and serving. It uses the [TensorFlow Transform](https://www.tensorflow.org/tfx/transform/get_started) library.\n",
    "\n",
    "\n",
    "The Transform component uses a module file to supply user code for the feature engineering what we want to do, so our first step is to create that module file. We will only be working with the summary field.\n",
    "\n",
    "**Note:**\n",
    "*The %%writefile {_movies_transform_module_file} cell magic below creates and writes the contents of that cell to a file on the notebook server where this notebook is running (for example, the Colab VM). When doing this outside of a notebook you would just create a Python file.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "22TBUtG9ME9N"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists(\"modules\"):\n",
    "  os.mkdir(\"modules\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "teaCGLgfnjw_"
   },
   "outputs": [],
   "source": [
    "_transform_module_file = 'modules/_transform_module.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rN6nRx3KnkpM"
   },
   "outputs": [],
   "source": [
    "%%writefile {_transform_module_file}\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def _fill_in_missing(x, default_value):\n",
    "  \"\"\"Replace missing values in a SparseTensor.\n",
    "\n",
    "  Fills in missing values of `x` with the default_value.\n",
    "\n",
    "  Args:\n",
    "    x: A `SparseTensor` of rank 2.  Its dense shape should have size at most 1\n",
    "      in the second dimension.\n",
    "    default_value: the value with which to replace the missing values.\n",
    "\n",
    "  Returns:\n",
    "    A rank 1 tensor where missing values of `x` have been filled in.\n",
    "  \"\"\"\n",
    "  if not isinstance(x, tf.sparse.SparseTensor):\n",
    "    return x\n",
    "  return tf.squeeze(\n",
    "      tf.sparse.to_dense(\n",
    "          tf.SparseTensor(x.indices, x.values, [x.dense_shape[0], 1]),\n",
    "          default_value),\n",
    "      axis=1)\n",
    "\n",
    "def preprocessing_fn(inputs):\n",
    "  outputs = {}\n",
    "  # outputs[\"summary\"] = _fill_in_missing(inputs[\"summary\"],\"\")\n",
    "  outputs[\"summary\"] = _fill_in_missing(inputs[\"text\"],\"\")\n",
    "  return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v-f5NaLTiFmO"
   },
   "outputs": [],
   "source": [
    "preprocessor = tfx.components.Transform(\n",
    "    examples=example_gen.outputs['examples'],\n",
    "    schema=schema_gen.outputs['schema'],\n",
    "    module_file=os.path.abspath(_transform_module_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MkjIuwHeiFmO"
   },
   "outputs": [],
   "source": [
    "context.run(preprocessor, enable_cache=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OH8OkaCwJgLF"
   },
   "source": [
    "Let's take a look at some of the transformed examples and check that they are indeed processed as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bt70Z16zJHy7"
   },
   "outputs": [],
   "source": [
    "def pprint_examples(artifact, n_examples=2):\n",
    "  print(\"artifact:\", artifact, \"\\n\")\n",
    "  uri = os.path.join(artifact.uri, \"Split-eval\")\n",
    "  print(\"uri:\", uri, \"\\n\")\n",
    "  tfrecord_filenames = [os.path.join(uri, name) for name in os.listdir(uri)]\n",
    "  print(\"tfrecord_filenames:\", tfrecord_filenames, \"\\n\")\n",
    "  dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")\n",
    "  for tfrecord in dataset.take(n_examples):\n",
    "    serialized_example = tfrecord.numpy()\n",
    "    example = tf.train.Example.FromString(serialized_example)\n",
    "    pp.pprint(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tg4I-TvXJIuO"
   },
   "outputs": [],
   "source": [
    "pprint_examples(preprocessor.outputs['transformed_examples'].get()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mJll-vDn_eJP"
   },
   "source": [
    "## Trainer\n",
    "\n",
    "Trainer component trains an ML model, and it requires a model definition code from users.\n",
    "\n",
    "The `run_fn` function in TFX's Trainer component is the entry point for training a machine learning model. It is a user-supplied function that takes in a set of arguments and returns a model artifact.\n",
    "\n",
    "The `run_fn` function is responsible for:\n",
    "\n",
    "*   Building the machine learning model.\n",
    "*   Training the model on the training data.\n",
    "*   Saving the trained model to the serving model directory.\n",
    "\n",
    "\n",
    "### Write model training code\n",
    "We will create a very simple fine-tuned model, with the preprocessing GPT-2 model. First, we need to create a module that contains the `run_fn` function for TFX Trainer because TFX Trainer expects the `run_fn` function to be defined in a module.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OQPtqKG5pmpn"
   },
   "outputs": [],
   "source": [
    "model_file = \"modules/model.py\"\n",
    "model_fn = \"modules.model.run_fn\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6drMNHJMAk7g"
   },
   "source": [
    "Now, we write the run_fn function:\n",
    "\n",
    "This run_fn function first gets the training data from the `fn_args.examples` argument. It then gets the schema of the training data from the `fn_args.schema` argument. Next, it loads finetuned GPT-2 model along with its preprocessor. The model is then trained on the training data using the model.train() method.\n",
    "Finally, the trained model weights are saved to the `fn_args.serving_model_dir` argument.\n",
    "\n",
    "\n",
    "Now, we are going to work with Keras NLP's GPT-2 Model! You can learn about the full GPT-2 model implementation in KerasNLP on [GitHub](https://github.com/keras-team/keras-nlp/tree/r0.5/keras_nlp/models/gpt2) or can read and interactively test the model on [Google IO2023 colab notebook](https://colab.research.google.com/github/tensorflow/codelabs/blob/main/KerasNLP/io2023_workshop.ipynb#scrollTo=81EZQ0D1R8LL ).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B-ME_d8i2sTB"
   },
   "outputs": [],
   "source": [
    "import keras_nlp\n",
    "import keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NnvkSqd6AB0q"
   },
   "source": [
    "*Note: To accommodate the limited resources of a free Colab GPU, we've adjusted the GPT-2 model's `sequence_length` parameter to `128` from its default `256`. This optimization enables efficient model training on the T4 GPU, facilitating faster fine-tuning while adhering to resource constraints.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N9yjLDqHoFb-"
   },
   "outputs": [],
   "source": [
    "%%writefile {model_file}\n",
    "\n",
    "import os\n",
    "import time\n",
    "from absl import logging\n",
    "import keras_nlp\n",
    "import more_itertools\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import tfx\n",
    "import tfx.components.trainer.fn_args_utils\n",
    "import gc\n",
    "\n",
    "\n",
    "_EPOCH = 1\n",
    "_BATCH_SIZE = 20\n",
    "_INITIAL_LEARNING_RATE = 5e-5\n",
    "_END_LEARNING_RATE = 0.0\n",
    "_SEQUENCE_LENGTH = 128 # default value is 256\n",
    "\n",
    "def _input_fn(file_pattern: str) -> list:\n",
    "  \"\"\"Retrieves training data and returns a list of articles for training.\n",
    "\n",
    "  For each row in the TFRecordDataset, generated in the previous ExampleGen\n",
    "  component, create a new tf.train.Example object and parse the TFRecord into\n",
    "  the example object. Articles, which are initially in bytes objects, are\n",
    "  decoded into a string.\n",
    "\n",
    "  Args:\n",
    "    file_pattern: Path to the TFRecord file of the training dataset.\n",
    "\n",
    "  Returns:\n",
    "    A list of training articles.\n",
    "\n",
    "  Raises:\n",
    "    FileNotFoundError: If TFRecord dataset is not found in the file_pattern\n",
    "    directory.\n",
    "  \"\"\"\n",
    "\n",
    "  if os.path.basename(file_pattern) == '*':\n",
    "    file_loc = os.path.dirname(file_pattern)\n",
    "\n",
    "  else:\n",
    "    raise FileNotFoundError(\n",
    "        f\"There is no file in the current directory: '{file_pattern}.\"\n",
    "    )\n",
    "\n",
    "  file_paths = [os.path.join(file_loc, name) for name in os.listdir(file_loc)]\n",
    "  train_articles = []\n",
    "  parsed_dataset = tf.data.TFRecordDataset(file_paths, compression_type=\"GZIP\")\n",
    "  for raw_record in parsed_dataset:\n",
    "    example = tf.train.Example()\n",
    "    example.ParseFromString(raw_record.numpy())\n",
    "    train_articles.append(\n",
    "        example.features.feature[\"summary\"].bytes_list.value[0].decode('utf-8')\n",
    "    )\n",
    "  return train_articles\n",
    "\n",
    "def run_fn(fn_args: tfx.components.trainer.fn_args_utils.FnArgs) -> None:\n",
    "  \"\"\"Trains the model and outputs the trained model to a the desired location given by FnArgs.\n",
    "\n",
    "  Args:\n",
    "    FnArgs :  Args to pass to user defined training/tuning function(s)\n",
    "  \"\"\"\n",
    "\n",
    "  train_articles =  pd.Series(_input_fn(\n",
    "          fn_args.train_files[0],\n",
    "      ))\n",
    "  tf_train_ds = tf.data.Dataset.from_tensor_slices(train_articles)\n",
    "\n",
    "  gpt2_preprocessor = keras_nlp.models.GPT2CausalLMPreprocessor.from_preset(\n",
    "      'gpt2_base_en',\n",
    "      sequence_length=_SEQUENCE_LENGTH,\n",
    "      add_end_token=True,\n",
    "  )\n",
    "  gpt2_lm = keras_nlp.models.GPT2CausalLM.from_preset(\n",
    "      'gpt2_base_en', preprocessor=gpt2_preprocessor\n",
    "  )\n",
    "\n",
    "  processed_ds = (\n",
    "      tf_train_ds\n",
    "      .batch(_BATCH_SIZE)\n",
    "      .cache()\n",
    "      .prefetch(tf.data.AUTOTUNE)\n",
    "  )\n",
    "\n",
    "  gpt2_lm.include_preprocessing = False\n",
    "\n",
    "  lr = tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "      5e-5,\n",
    "      decay_steps=processed_ds.cardinality() * _EPOCH,\n",
    "      end_learning_rate=0.0,\n",
    "  )\n",
    "  loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "  gpt2_lm.compile(\n",
    "      optimizer=keras.optimizers.Adam(lr),\n",
    "      loss=loss,\n",
    "      weighted_metrics=['accuracy'],\n",
    "  )\n",
    "\n",
    "  gpt2_lm.fit(processed_ds, epochs=_EPOCH)\n",
    "  if os.path.exists(fn_args.serving_model_dir):\n",
    "    os.rmdir(fn_args.serving_model_dir)\n",
    "  os.mkdir(fn_args.serving_model_dir)\n",
    "  gpt2_lm.save_weights(\n",
    "      filepath=os.path.join(fn_args.serving_model_dir, \"model_weights.weights.h5\")\n",
    "  )\n",
    "  del gpt2_lm, gpt2_preprocessor, processed_ds, tf_train_ds\n",
    "  gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bnbMFKqc5gfK"
   },
   "outputs": [],
   "source": [
    "trainer = tfx.components.Trainer(\n",
    "    run_fn=model_fn,\n",
    "    examples=preprocessor.outputs['transformed_examples'],\n",
    "    train_args=tfx.proto.TrainArgs(splits=['train']),\n",
    "    eval_args=tfx.proto.EvalArgs(splits=['train']),\n",
    "    schema=schema_gen.outputs['schema'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "COCqeu-8CyHN"
   },
   "outputs": [],
   "source": [
    "context.run(trainer, enable_cache=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "btljwhMwWeQ9"
   },
   "source": [
    "## Inference and Evaluation\n",
    "\n",
    "With our model fine-tuned, let's evaluate its performance by generating inferences. To capture and preserve these results, we'll create an EvaluationMetric artifact.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S79afpeeVkwc"
   },
   "outputs": [],
   "source": [
    "from tfx.types import artifact\n",
    "from tfx import types\n",
    "\n",
    "Property = artifact.Property\n",
    "PropertyType = artifact.PropertyType\n",
    "\n",
    "DURATION_PROPERTY = Property(type=PropertyType.FLOAT)\n",
    "EVAL_OUTPUT_PROPERTY = Property(type=PropertyType.STRING)\n",
    "\n",
    "class EvaluationMetric(types.Artifact):\n",
    "  \"\"\"Artifact that contains metrics for a model.\n",
    "\n",
    "  * Properties:\n",
    "\n",
    "     - 'model_prediction_time' : time it took for the model to make predictions\n",
    "     based on the input text.\n",
    "     - 'model_evaluation_output_path' : saves the path to the CSV file that\n",
    "     contains the model's prediction based on the testing inputs.\n",
    "  \"\"\"\n",
    "  TYPE_NAME = 'Evaluation_Metric'\n",
    "  PROPERTIES = {\n",
    "      'model_prediction_time': DURATION_PROPERTY,\n",
    "      'model_evaluation_output_path': EVAL_OUTPUT_PROPERTY,\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GQ3Wq2Ylb6JF"
   },
   "source": [
    "These helper functions contribute to the evaluation of a language model (LLM) by providing tools for calculating perplexity, a key metric reflecting the model's ability to predict the next word in a sequence, and by facilitating the extraction, preparation, and processing of evaluation data. The `input_fn` function retrieves training data from a specified TFRecord file, while the `trim_sentence` function ensures consistency by limiting sentence length. A lower perplexity score indicates higher prediction confidence and generally better model performance, making these functions essential for comprehensive evaluation within the LLM pipeline.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tkXaZlsg38jI"
   },
   "outputs": [],
   "source": [
    "\"\"\"This is an evaluation component for the LLM pipeline takes in a\n",
    "standard trainer artifact and outputs a custom evaluation artifact.\n",
    "It displays the evaluation output in the colab notebook.\n",
    "\"\"\"\n",
    "import os\n",
    "import time\n",
    "import keras_nlp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tfx.v1 as tfx\n",
    "\n",
    "def input_fn(file_pattern: str) -> list:\n",
    "  \"\"\"Retrieves training data and returns a list of articles for training.\n",
    "\n",
    "  Args:\n",
    "    file_pattern: Path to the TFRecord file of the training dataset.\n",
    "\n",
    "  Returns:\n",
    "    A list of test articles\n",
    "\n",
    "  Raises:\n",
    "    FileNotFoundError: If the file path does not exist.\n",
    "  \"\"\"\n",
    "  if os.path.exists(file_pattern):\n",
    "    file_paths = [os.path.join(file_pattern, name) for name in os.listdir(file_pattern)]\n",
    "    test_articles = []\n",
    "    parsed_dataset = tf.data.TFRecordDataset(file_paths, compression_type=\"GZIP\")\n",
    "    for raw_record in parsed_dataset:\n",
    "      example = tf.train.Example()\n",
    "      example.ParseFromString(raw_record.numpy())\n",
    "      test_articles.append(\n",
    "          example.features.feature[\"summary\"].bytes_list.value[0].decode('utf-8')\n",
    "      )\n",
    "    return test_articles\n",
    "  else:\n",
    "    raise FileNotFoundError(f'File path \"{file_pattern}\" does not exist.')\n",
    "\n",
    "def trim_sentence(sentence: str, max_words: int = 20):\n",
    "  \"\"\"Trims the sentence to include up to the given number of words.\n",
    "\n",
    "  Args:\n",
    "    sentence: The sentence to trim.\n",
    "    max_words: The maximum number of words to include in the trimmed sentence.\n",
    "\n",
    "  Returns:\n",
    "    The trimmed sentence.\n",
    "  \"\"\"\n",
    "  words = sentence.split(' ')\n",
    "  if len(words) <= max_words:\n",
    "    return sentence\n",
    "  return ' '.join(words[:max_words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ypRrAQMpfEFd"
   },
   "source": [
    "![perplexity.png](images/gpt2_fine_tuning_and_conversion/perplexity.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yo5fvOa9GmzL"
   },
   "source": [
    "One of the useful metrics for evaluating a Large Language Model is **Perplexity**. Perplexity is a measure of how well a language model predicts the next token in a sequence. It is calculated by taking the exponentiation of the average negative log-likelihood of the next token. A lower perplexity score indicates that the language model is better at predicting the next token.\n",
    "\n",
    "This is the *formula* for calculating perplexity.\n",
    "\n",
    " $\\text{Perplexity} = \\exp(-1 * $ Average Negative Log Likelihood $) =\n",
    "  \\exp\\left(-\\frac{1}{T} \\sum_{t=1}^T \\log p(w_t | w_{<t})\\right)$.\n",
    "\n",
    "\n",
    "In this colab notebook, we calculate perplexity using [keras_nlp's perplexity](https://keras.io/api/keras_nlp/metrics/perplexity/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kNfs9ZplgPAH"
   },
   "source": [
    "**Computing Perplexity for Base GPT-2 Model and Finetuned Model**\n",
    "\n",
    "The code below is the function which will be used later in the notebook for computing perplexity for the base GPT-2 model and the finetuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "27iA8w6-GlSz"
   },
   "outputs": [],
   "source": [
    "def calculate_perplexity(gpt2_model, gpt2_tokenizer, sentence) -> int:\n",
    "  \"\"\"Calculates perplexity of a model given a sentence.\n",
    "\n",
    "  Args:\n",
    "    gpt2_model: GPT-2 Language Model\n",
    "    gpt2_tokenizer: A GPT-2 tokenizer using Byte-Pair Encoding subword segmentation.\n",
    "    sentence: Sentence that the model's perplexity is calculated upon.\n",
    "\n",
    "  Returns:\n",
    "    A perplexity score.\n",
    "  \"\"\"\n",
    "  # gpt2_tokenizer([sentence])[0] produces a tensor containing an array of tokens that form the sentence.\n",
    "  tokens = gpt2_tokenizer([sentence])[0].numpy()\n",
    "  # decoded_sentences is an array containing sentences that increase by one token in size.\n",
    "  # e.g. if tokens for a sentence \"I love dogs\" are [\"I\", \"love\", \"dogs\"], then decoded_sentences = [\"I love\", \"I love dogs\"]\n",
    "  decoded_sentences = [gpt2_tokenizer.detokenize([tokens[:i]])[0].numpy() for i in range(1, len(tokens))]\n",
    "  predictions = gpt2_model.predict(decoded_sentences)\n",
    "  logits = [predictions[i - 1][i] for i in range(1, len(tokens))]\n",
    "  target = tokens[1:].reshape(len(tokens) - 1, 1)\n",
    "  perplexity = keras_nlp.metrics.Perplexity(from_logits=True)\n",
    "  perplexity.update_state(target, logits)\n",
    "  result = perplexity.result()\n",
    "  return result.numpy()\n",
    "\n",
    "def average_perplexity(gpt2_model, gpt2_tokenizer, sentences):\n",
    "  perplexity_lst = [calculate_perplexity(gpt2_model, gpt2_tokenizer, sent) for sent in sentences]\n",
    "  return np.mean(perplexity_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ELmkaY-ygbog"
   },
   "source": [
    "## Evaluator\n",
    "\n",
    "Having established the necessary helper functions for evaluation, we proceed to define the Evaluator component. This component facilitates model inference using both base and fine-tuned models, computes perplexity scores for all models, and measures inference time. The Evaluator's output provides comprehensive insights for a thorough comparison and assessment of each model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eb5fD5vzEQJ0"
   },
   "outputs": [],
   "source": [
    "@tfx.dsl.components.component\n",
    "def Evaluator(\n",
    "    examples: tfx.dsl.components.InputArtifact[\n",
    "        tfx.types.standard_artifacts.Examples\n",
    "    ],\n",
    "    trained_model: tfx.dsl.components.InputArtifact[\n",
    "        tfx.types.standard_artifacts.Model\n",
    "    ],\n",
    "    max_length: tfx.dsl.components.Parameter[int],\n",
    "    evaluation: tfx.dsl.components.OutputArtifact[EvaluationMetric],\n",
    ") -> None:\n",
    "  \"\"\"Makes inferences with base model, finetuned model, TFlite model, and quantized model.\n",
    "\n",
    "  Args:\n",
    "    examples: Standard TFX examples artifacts for retrieving test dataset.\n",
    "    trained_model: Standard TFX trained model artifact finetuned with imdb-reviews\n",
    "      dataset.\n",
    "    tflite_model: Unquantized TFLite model.\n",
    "    quantized_model: Quantized TFLite model.\n",
    "    max_length: Length of the text that the model generates given custom input\n",
    "      statements.\n",
    "    evaluation: An evaluation artifact that saves predicted outcomes of custom\n",
    "      inputs in a csv document and inference speed of the model.\n",
    "  \"\"\"\n",
    "  _TEST_SIZE = 10\n",
    "  _INPUT_LENGTH = 10\n",
    "  _SEQUENCE_LENGTH = 128\n",
    "\n",
    "  path = os.path.join(examples.uri, 'Split-eval')\n",
    "  test_data = input_fn(path)\n",
    "  evaluation_inputs = [\n",
    "      trim_sentence(article, max_words=_INPUT_LENGTH)\n",
    "      for article in test_data[:_TEST_SIZE]\n",
    "  ]\n",
    "  true_test = [\n",
    "      trim_sentence(article, max_words=max_length)\n",
    "      for article in test_data[:_TEST_SIZE]\n",
    "  ]\n",
    "\n",
    "  # Loading base model, making inference, and calculating perplexity on the base model.\n",
    "  gpt2_preprocessor = keras_nlp.models.GPT2CausalLMPreprocessor.from_preset(\n",
    "      'gpt2_base_en',\n",
    "      sequence_length=_SEQUENCE_LENGTH,\n",
    "      add_end_token=True,\n",
    "  )\n",
    "  gpt2_lm = keras_nlp.models.GPT2CausalLM.from_preset(\n",
    "      'gpt2_base_en', preprocessor=gpt2_preprocessor\n",
    "  )\n",
    "  gpt2_tokenizer = keras_nlp.models.GPT2Tokenizer.from_preset('gpt2_base_en')\n",
    "\n",
    "  base_average_perplexity = average_perplexity(\n",
    "      gpt2_lm, gpt2_tokenizer, true_test\n",
    "  )\n",
    "\n",
    "  start_base_model = time.time()\n",
    "  base_evaluation = [\n",
    "      gpt2_lm.generate(input, max_length)\n",
    "      for input in evaluation_inputs\n",
    "  ]\n",
    "  end_base_model = time.time()\n",
    "\n",
    "  # Loading finetuned model and making inferences with the finetuned model.\n",
    "  model_weights_path = os.path.join(\n",
    "      trained_model.uri, \"Format-Serving\", \"model_weights.weights.h5\"\n",
    "  )\n",
    "  gpt2_lm.load_weights(model_weights_path)\n",
    "\n",
    "  trained_model_average_perplexity = average_perplexity(\n",
    "      gpt2_lm, gpt2_tokenizer, true_test\n",
    "  )\n",
    "\n",
    "  start_trained = time.time()\n",
    "  trained_evaluation = [\n",
    "      gpt2_lm.generate(input, max_length)\n",
    "      for input in evaluation_inputs\n",
    "  ]\n",
    "  end_trained = time.time()\n",
    "\n",
    "  # Building an inference table.\n",
    "  inference_data = {\n",
    "      'input': evaluation_inputs,\n",
    "      'actual_test_output': true_test,\n",
    "      'base_model_prediction': base_evaluation,\n",
    "      'trained_model_prediction': trained_evaluation,\n",
    "  }\n",
    "\n",
    "  models = [\n",
    "      'Base Model',\n",
    "      'Finetuned Model',\n",
    "  ]\n",
    "  inference_time = [\n",
    "      (end_base_model - start_base_model),\n",
    "      (end_trained - start_trained),\n",
    "  ]\n",
    "  average_inference_time = [time / _TEST_SIZE for time in inference_time]\n",
    "  average_perplexity_lst = [\n",
    "      base_average_perplexity,\n",
    "      trained_model_average_perplexity,\n",
    "  ]\n",
    "  evaluation_data = {\n",
    "      'Model': models,\n",
    "      'Average Inference Time (sec)': average_inference_time,\n",
    "      'Average Perplexity': average_perplexity_lst,\n",
    "  }\n",
    "\n",
    "  # creating directory in examples artifact to save metric dataframes\n",
    "  metrics_path = os.path.join(evaluation.uri, 'metrics')\n",
    "  if not os.path.exists(metrics_path):\n",
    "      os.mkdir(metrics_path)\n",
    "\n",
    "  evaluation_df = pd.DataFrame(evaluation_data).set_index('Model').transpose()\n",
    "  evaluation_path = os.path.join(metrics_path, 'evaluation_output.csv')\n",
    "  evaluation_df.to_csv(evaluation_path)\n",
    "\n",
    "  inference_df = pd.DataFrame(inference_data)\n",
    "  inference_path = os.path.join(metrics_path, 'inference_output.csv')\n",
    "  inference_df.to_csv(inference_path)\n",
    "  evaluation.model_evaluation_output_path = inference_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UkC0RrleWP9O"
   },
   "outputs": [],
   "source": [
    "evaluator = Evaluator(examples = preprocessor.outputs['transformed_examples'],\n",
    "                      trained_model = trainer.outputs['model'],\n",
    "                      max_length = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KQQvbT96XXDT"
   },
   "outputs": [],
   "source": [
    "context.run(evaluator, enable_cache = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xVUIimCogdjZ"
   },
   "source": [
    "### Evaluator Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EPKArU8f3FpD"
   },
   "source": [
    "Once our evaluation component execution is completed, we will load the evaluation metrics from evaluator URI and display them.\n",
    "\n",
    "\n",
    "*Note:*\n",
    "\n",
    "**Perplexity Calculation:**\n",
    "*Perplexity is only one of many ways to evaluate LLMs. LLM evaluation is an [active research topic](https://arxiv.org/abs/2307.03109) and a comprehensive treatment is beyond the scope of this notebook.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NVv5F_Ok7Jss"
   },
   "outputs": [],
   "source": [
    "evaluation_path = os.path.join(evaluator.outputs['evaluation']._artifacts[0].uri, 'metrics')\n",
    "inference_df = pd.read_csv(os.path.join(evaluation_path, 'inference_output.csv'), index_col=0)\n",
    "evaluation_df = pd.read_csv(os.path.join(evaluation_path, 'evaluation_output.csv'), index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qndIFspM9ELf"
   },
   "source": [
    "The fine-tuned GPT-2 model exhibits a slight improvement in perplexity compared to the baseline model. Further training with more epochs or a larger dataset may yield more substantial perplexity reductions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XvtAnvrm6H-a"
   },
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "display.display(display.HTML(inference_df.to_html()))\n",
    "display.display(display.HTML(evaluation_df.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RiCy6OQ7J3C5"
   },
   "source": [
    "# Running the Entire Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AJmAdbO9AWpx"
   },
   "source": [
    "*Note: For running below section, a more substantial amount of GPU memory is required. Therefore, Colab Pro or a local machine equipped with a higher-capacity GPU is recommended for running below pipeline.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kvYtjmkFHSxu"
   },
   "source": [
    "TFX supports multiple orchestrators to run pipelines. In this tutorial we will use LocalDagRunner which is included in the TFX Python package and runs pipelines on local environment. We often call TFX pipelines \"DAGs\" which stands for directed acyclic graph.\n",
    "\n",
    "LocalDagRunner provides fast iterations for development and debugging. TFX also supports other orchestrators including Kubeflow Pipelines and Apache Airflow which are suitable for production use cases. See [TFX on Cloud AI Platform Pipelines](/tutorials/tfx/cloud-ai-platform-pipelines) or [TFX Airflow](/tutorials/tfx/airflow_workshop) Tutorial to learn more about other orchestration systems.\n",
    "\n",
    "Now we create a LocalDagRunner and pass a Pipeline object created from the function we already defined. The pipeline runs directly and you can see logs for the progress of the pipeline including ML model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4FQgyxOQLn22"
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "PIPELINE_NAME = \"tfx-llm-imdb-reviews\"\n",
    "model_fn = \"modules.model.run_fn\"\n",
    "_transform_module_file = \"modules/_transform_module.py\"\n",
    "\n",
    "# Output directory to store artifacts generated from the pipeline.\n",
    "PIPELINE_ROOT = os.path.join('pipelines', PIPELINE_NAME)\n",
    "# Path to a SQLite DB file to use as an MLMD storage.\n",
    "METADATA_PATH = os.path.join('metadata', PIPELINE_NAME, 'metadata.db')\n",
    "# Output directory where created models from the pipeline will be exported.\n",
    "SERVING_MODEL_DIR = os.path.join('serving_model', PIPELINE_NAME)\n",
    "\n",
    "from absl import logging\n",
    "logging.set_verbosity(logging.INFO)  # Set default logging level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tgTwBpN-pe3_"
   },
   "outputs": [],
   "source": [
    "def _create_pipeline(\n",
    "    pipeline_name: str,\n",
    "    pipeline_root: str,\n",
    "    model_fn: str,\n",
    "    serving_model_dir: str,\n",
    "    metadata_path: str,\n",
    ") -> tfx.dsl.Pipeline:\n",
    "  \"\"\"Creates a Pipeline for Fine-Tuning and Converting an Large Language Model with TFX.\"\"\"\n",
    "\n",
    "  example_gen = FileBasedExampleGen(\n",
    "    input_base='dummy',\n",
    "    custom_config={'dataset':'imdb_reviews', 'split':'train[:5%]'},\n",
    "    custom_executor_spec=executor_spec.BeamExecutorSpec(TFDSExecutor))\n",
    "\n",
    "  statistics_gen = tfx.components.StatisticsGen(\n",
    "      examples=example_gen.outputs['examples'], exclude_splits=['eval']\n",
    "  )\n",
    "\n",
    "  schema_gen = tfx.components.SchemaGen(\n",
    "      statistics=statistics_gen.outputs['statistics'],\n",
    "      infer_feature_shape=False,\n",
    "      exclude_splits=['eval'],\n",
    "  )\n",
    "\n",
    "  example_validator = tfx.components.ExampleValidator(\n",
    "      statistics=statistics_gen.outputs['statistics'],\n",
    "      schema=schema_gen.outputs['schema'],\n",
    "      exclude_splits=['eval'],\n",
    "  )\n",
    "\n",
    "  preprocessor = tfx.components.Transform(\n",
    "    examples=example_gen.outputs['examples'],\n",
    "    schema=schema_gen.outputs['schema'],\n",
    "    module_file= _transform_module_file,\n",
    "  )\n",
    "\n",
    "  trainer = tfx.components.Trainer(\n",
    "      run_fn=model_fn,\n",
    "      examples=preprocessor.outputs['transformed_examples'],\n",
    "      train_args=tfx.proto.TrainArgs(splits=['train']),\n",
    "      eval_args=tfx.proto.EvalArgs(splits=['train']),\n",
    "      schema=schema_gen.outputs['schema'],\n",
    "  )\n",
    "\n",
    "\n",
    "  evaluator = Evaluator(\n",
    "      examples=preprocessor.outputs['transformed_examples'],\n",
    "      trained_model=trainer.outputs['model'],\n",
    "      max_length=50,\n",
    "  )\n",
    "\n",
    "  # Following 7 components will be included in the pipeline.\n",
    "  components = [\n",
    "      example_gen,\n",
    "      statistics_gen,\n",
    "      schema_gen,\n",
    "      example_validator,\n",
    "      preprocessor,\n",
    "      trainer,\n",
    "      evaluator,\n",
    "  ]\n",
    "\n",
    "  return tfx.dsl.Pipeline(\n",
    "      pipeline_name=pipeline_name,\n",
    "      pipeline_root=pipeline_root,\n",
    "      metadata_connection_config=tfx.orchestration.metadata.sqlite_metadata_connection_config(\n",
    "          metadata_path\n",
    "      ),\n",
    "      components=components,\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DkgLXyZGJ9CO"
   },
   "outputs": [],
   "source": [
    "tfx.orchestration.LocalDagRunner().run(\n",
    "    _create_pipeline(\n",
    "        pipeline_name=PIPELINE_NAME,\n",
    "        pipeline_root=PIPELINE_ROOT,\n",
    "        model_fn=model_fn,\n",
    "        serving_model_dir=SERVING_MODEL_DIR,\n",
    "        metadata_path=METADATA_PATH,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mo3Z08xzHa4G"
   },
   "source": [
    "You should see INFO:absl:Component Evaluator is finished.\" at the end of the logs if the pipeline finished successfully because evaluator component is the last component of the pipeline."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "iwgnKVaUuozP"
   ],
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
