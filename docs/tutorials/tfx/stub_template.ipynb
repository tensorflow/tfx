{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "stub_template.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5rc1"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iamsusiep/tfx/blob/template/docs/tutorials/tfx/stub_template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ydsPE-aN1Alm"
      },
      "source": [
        "##### Copyright &copy; 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uCSByGH6C7zS"
      },
      "source": [
        "<font size=-1>Licensed under the Apache License, Version 2.0 (the \\\"License\\\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at [https://www.apache.org/licenses/LICENSE-2.0](https://www.apache.org/licenses/LICENSE-2.0)\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \\\"AS IS\\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the License for the specific language governing permissions and limitations under the License.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6TyrY7lV0oke"
      },
      "source": [
        "# TFX Template Pipeline Testing Framework using StubExecutor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iLYriYe10okf"
      },
      "source": [
        "## Introduction\n",
        "**You should complete `template.ipynb` tutorial up to *Step 6* in order to proceed this tutorial.**\n",
        "\n",
        "This document will provide instructions to test a TensorFlow Extended (TFX) pipeline\n",
        "using `BaseStubExecuctor`, which generates fake data using the golden test data. This is intended for users to replace executors they don't want to test so that they could save time from running the actual executors. Stub executor is provided with TFX Python package under `tfx.experimental.pipeline_testing.base_stub_executor`.\n",
        "\n",
        "Many of the instructions are Linux shell commands, which will run on an AI Platform Notebooks instance. Corresponding Jupyter Notebook code cells which invoke those commands using `!` are provided.\n",
        "\n",
        "This tutorial serves as an extension to `template.ipynb` tutorial, thus you will also use [Taxi Trips dataset](\n",
        "https://data.cityofchicago.org/Transportation/Taxi-Trips/wrvz-psew)\n",
        "released by the City of Chicago. We strongly encourage you to try modifying the components prior to utilizing stub executors.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jxPMeugQ0okg"
      },
      "source": [
        "## Step 1. Set up your environment.\n",
        "\n",
        "AI Platform Pipelines will prepare a development environment to build a pipeline, and a Kubeflow Pipeline cluster to run the newly built pipeline.\n",
        "\n",
        "**NOTE:** To select a particular TensorFlow version, or select a GPU instance, create a TensorFlow pre-installed instance in AI Platform Notebooks.\n",
        "\n",
        "**NOTE:** There might be some errors during package installation. For example: \n",
        "\n",
        ">\"ERROR: some-package 0.some_version.1 has requirement other-package!=2.0.,&lt;3,&gt;=1.15, but you'll have other-package 2.0.0 which is incompatible.\" Please ignore these errors at this moment.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyPsZPm9ujR5",
        "colab_type": "text"
      },
      "source": [
        "Install `tfx` from the source code.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjcVYp8UudKh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd\n",
        "!git clone https://github.com/tensorflow/tfx.git\n",
        "%cd tfx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ty-Ysf4duuNz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python setup.py bdist_wheel\n",
        "!python setup.py build\n",
        "!python setup.py install\n",
        "!pip install -e ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-am1yWXt0okh"
      },
      "source": [
        "Install `kfp`, and `skaffold`, and add installation path to the `PATH` environment variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XNiqq_kN0okj",
        "colab": {}
      },
      "source": [
        "# Install kfp Python packages.\n",
        "import sys\n",
        "!{sys.executable} -m pip install --user --upgrade -q kfp==0.5.1\n",
        "# Download skaffold and set it executable.\n",
        "!curl -Lo skaffold https://storage.googleapis.com/skaffold/releases/latest/skaffold-linux-amd64 && chmod +x skaffold && mv skaffold /home/jupyter/.local/bin/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "43ncix2Q0okm",
        "colab": {}
      },
      "source": [
        "# Set `PATH` to include user python binary directory and a directory containing `skaffold`.\n",
        "PATH=%env PATH\n",
        "%env PATH={PATH}:/home/jupyter/.local/bin"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hX1rqpbQ0okp"
      },
      "source": [
        "Let's check the versions of TFX."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XAIoKMNG0okq",
        "colab": {}
      },
      "source": [
        "!python3 -c \"import tfx; print('TFX version: {}'.format(tfx.__version__))\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_7JLpaXT0okv"
      },
      "source": [
        "In AI Platform Pipelines, TFX is running in a hosted Kubernetes environment using [Kubeflow Pipelines](https://www.kubeflow.org/docs/pipelines/overview/pipelines-overview/).\n",
        "\n",
        "Let's set some environment variables to use Kubeflow Pipelines.\n",
        "\n",
        "First, get your GCP project ID."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Hw3nsooU0okv",
        "colab": {}
      },
      "source": [
        "# Read GCP project id from env.\n",
        "shell_output=!gcloud config list --format 'value(core.project)' 2>/dev/null\n",
        "GOOGLE_CLOUD_PROJECT=shell_output[0]\n",
        "%env GOOGLE_CLOUD_PROJECT={GOOGLE_CLOUD_PROJECT}\n",
        "print(\"GCP project ID:\" + GOOGLE_CLOUD_PROJECT)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "A_6r4uzE0oky"
      },
      "source": [
        "We also need to access your KFP cluster. You can access it in your Google Cloud Console under \"AI Platform > Pipeline\" menu. The \"endpoint\" of the KFP cluster can be found from the URL of the Pipelines dashboard, or you can get it from the URL of the Getting Started page where you launched this notebook. Let's create an `ENDPOINT` environment variable and set it to the KFP cluster endpoint. **ENDPOINT should contain only the hostname part of the URL.** For example, if the URL of the KFP dashboard is `https://1e9deb537390ca22-dot-asia-east1.pipelines.googleusercontent.com/#/start`, ENDPOINT value becomes `1e9deb537390ca22-dot-asia-east1.pipelines.googleusercontent.com`.\n",
        "\n",
        ">**NOTE: You MUST set your ENDPOINT value below.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AzqEQORV0oky",
        "colab": {}
      },
      "source": [
        "# This refers to the KFP cluster endpoint\n",
        "ENDPOINT='' # Enter your ENDPOINT here.\n",
        "if not ENDPOINT:\n",
        "    from absl import logging\n",
        "    logging.error('Set your ENDPOINT in this cell.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "K6T-KXeA0ok3"
      },
      "source": [
        "Set the image name as `tfx-pipeline` under the current GCP project."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3ztxXOVD0ok4",
        "colab": {}
      },
      "source": [
        "# Docker image name for the pipeline image.\n",
        "CUSTOM_TFX_IMAGE='gcr.io/' + GOOGLE_CLOUD_PROJECT + '/tfx-stub-pipeline'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4DAp8WfuxqP",
        "colab_type": "text"
      },
      "source": [
        "## Create a Docker image for tfx packages\n",
        "Since tfx is installed using github, uploaded to GCP. By default, the docker image will be named `gcr.io/tfx-intern-sujip-gke/tensorflow:latest`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pxa8jVqBuvc-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DOCKER_IMAGE_REPO='gcr.io/' + GOOGLE_CLOUD_PROJECT + '/tensorflow'\n",
        "DOCKER_IMAGE_TAG='latest'\n",
        "%env DOCKER_IMAGE_REPO={DOCKER_IMAGE_REPO}\n",
        "%env DOCKER_IMAGE_TAG={DOCKER_IMAGE_TAG}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4go5x4Suwq9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!./tfx/tools/docker/build_docker_image.sh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dG_kCbaWu0ck",
        "colab_type": "text"
      },
      "source": [
        "Push docker image to GCP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgAQT7uYu0KY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DOCKER_IMAGE=\"{}:{}\".format(DOCKER_IMAGE_REPO, DOCKER_IMAGE_TAG)\n",
        "!gcloud docker -- push {DOCKER_IMAGE}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_-LlwHuzX_J",
        "colab_type": "text"
      },
      "source": [
        "# Step 2. Record the pipeline outputs in Google Cloud Storage\n",
        "\n",
        "Since this tutorial assumes that you have completed `template.ipynb`, \n",
        "Open a Terminal and run the following commands:\n",
        "\n",
        "> `$ gcloud container clusters get-credentials <cluster_name> --zone <compute_zone> --project {GOOGLE_CLOUD_PROJECT}`\n",
        "\n",
        "> `$ nohup kubectl port-forward deployment/metadata-grpc-deployment -n <namespace> {PORT}:8080 &`\n",
        "\n",
        "You should set `PORT` to an unused port and `HOST` is typically set to \"localhost\".\n",
        "\n",
        "After this step is complete"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5n21UHXz5_W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TESTDATA='testdata'\n",
        "PORT='' # Enter your PORT here.\n",
        "HOST='' # Enter your HOST here.\n",
        "# Enter name of the pipeline that you created and ran in template.ipynb\n",
        "# By default, template.ipynb creates pipeline named \"my_pipeline\"\n",
        "SAVED_PIPELINE_NAME='my_pipeline' \n",
        "RECORD_DIR=\"gs://{}-kubeflowpipelines-default/{}\".format(GOOGLE_CLOUD_PROJECT, TESTDATA)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPFkcew7zYwq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! python tfx/experimental/pipeline_testing/pipeline_recorder.py \\\n",
        "--output_dir={RECORD_DIR} \\\n",
        "--host={HOST} \\\n",
        "--port={POST} \\\n",
        "--pipeline_name={RECORD_PIPELINE}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TOsQbkky0ok7"
      },
      "source": [
        "And, it's done. We are ready to test a pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Cxlbi1QM0ok8"
      },
      "source": [
        "## Step 3. Copy the predefined template to your project directory.\n",
        "\n",
        "In this step, we will create a working pipeline project directory and files by copying additional files from a predefined template.\n",
        "\n",
        "You may give your pipeline a different name by changing the `PIPELINE_NAME` below. This will also become the name of the project directory where your files will be put.\n",
        "\n",
        "Note that this `PIPELINE_NAME` should be different from the `PIPELINE_NAME` that you used for the `template.ipynb` tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cIPlt-700ok-",
        "colab": {}
      },
      "source": [
        "PIPELINE_NAME=\"my_stub_pipeline\"\n",
        "import os\n",
        "PROJECT_DIR=os.path.join(os.path.expanduser(\"~\"),\"AIHub\",PIPELINE_NAME)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ozHIomcd0olB"
      },
      "source": [
        "TFX includes the `taxi` template with the TFX python package. If you are planning to solve a point-wise prediction problem, including classification and regresssion, this template could be used as a starting point.\n",
        "\n",
        "The `tfx template copy` CLI command copies predefined template files into your project directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VLXpTTjU0olD",
        "colab": {}
      },
      "source": [
        "!tfx template copy \\\n",
        "  --pipeline-name={PIPELINE_NAME} \\\n",
        "  --destination-path={PROJECT_DIR} \\\n",
        "  --model=taxi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yxOT19QS0olH"
      },
      "source": [
        "Change the working directory context in this notebook to the project directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6P-HljcU0olI",
        "colab": {}
      },
      "source": [
        "%cd {PROJECT_DIR}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1tEYUQxH0olO"
      },
      "source": [
        ">NOTE: Don't forget to change directory in `File Browser` on the left by clicking into the project directory once it is created."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IzT2PFrN0olQ"
      },
      "source": [
        "## Step 3. Browse your copied source files\n",
        "\n",
        "The TFX template provides basic scaffold files to build a pipeline, including Python source code, sample data, and Jupyter Notebooks to analyse the output of the pipeline. The `taxi` template uses the same *Chicago Taxi* dataset and ML model as the [Airflow Tutorial](https://www.tensorflow.org/tfx/tutorials/tfx/airflow_workshop).\n",
        "\n",
        "Here is brief introduction to each of the Python files.\n",
        "-   `pipeline` - This directory contains the definition of the pipeline\n",
        "    -   `configs.py` — defines common constants for pipeline runners\n",
        "    -   `pipeline.py` — defines TFX components and a pipeline\n",
        "-   `launcher` - This directory contains launcher for stub executors\n",
        "    -   `custom_stub_component_launcher.py` - defines component ids to replace with stub executors\n",
        "\n",
        "\n",
        "-   `models` - This directory contains ML model definitions.\n",
        "    -   `features.py`, `features_test.py` — defines features for the model\n",
        "    -   `preprocessing.py`, `preprocessing_test.py` — defines preprocessing\n",
        "        jobs using `tf::Transform`\n",
        "    -   `estimator` - This directory contains an Estimator based model.\n",
        "        -   `constants.py` — defines constants of the model\n",
        "        -   `model.py`, `model_test.py` — defines DNN model using TF estimator\n",
        "    -   `keras` - This directory contains a Keras based model.\n",
        "        -   `constants.py` — defines constants of the model\n",
        "        -   `model.py`, `model_test.py` — defines DNN model using Keras\n",
        "-   `beam_dag_runner.py`, `kubeflow_dag_runner.py` — define runners for each orchestration engine\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ROwHAsDK0olT"
      },
      "source": [
        "You might notice that there are some files with `_test.py` in their name. These are unit tests of the pipeline and it is recommended to add more unit tests as you implement your own pipelines.\n",
        "You can run unit tests by supplying the module name of test files with `-m` flag. You can usually get a module name by deleting `.py` extension and replacing `/` with `.`.  For example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "M0cMdE2Z0olU",
        "colab": {}
      },
      "source": [
        "!{sys.executable} -m models.features_test\n",
        "!{sys.executable} -m models.keras.model_test\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tO9Jhplo0olX"
      },
      "source": [
        "## Step 4. Create and run your TFX test pipeline\n",
        "\n",
        "Components in the TFX pipeline will generate outputs for each run as [ML Metadata Artifacts](https://www.tensorflow.org/tfx/guide/mlmd), and they need to be stored somewhere. You can use any storage which the KFP cluster can access, and for this example we will use Google Cloud Storage (GCS). A default GCS bucket should have been created automatically. Its name will be `<your-project-id>-kubeflowpipelines-default`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Zr-RjyPWTHdH"
      },
      "source": [
        "Let's upload our sample data to GCS bucket so that we can use it in our pipeline later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gW-dSHW-TSdc",
        "colab": {}
      },
      "source": [
        "!gsutil cp data/data.csv gs://{GOOGLE_CLOUD_PROJECT}-kubeflowpipelines-default/tfx-template/data/data.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9S-QVk3PjRwG",
        "colab_type": "text"
      },
      "source": [
        "**Double-click to open `kubeflow_dag_runner.py`**. Uncomment `supported_launcher_classes` argument of `KubeflowDagRunnerConfig` to be able to launch stub executors.\n",
        "\n",
        "**Double-click to change directory to `launcher` and double-click again to open `custom_stub_component_launcher.py`.**\n",
        "Make sure to set `test_data_dir` to the directory where KFP outputs are recorded, or `RECORD_DIR`. Customize the list of component ids to replace with stub executor. Make sure to save `custom_stub_component_launcher.py` after you edit it. (Tip: search for comments containing TODO)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wc54hDZu0ole"
      },
      "source": [
        "\n",
        "Let's create a TFX pipeline using the `tfx pipeline create` command.\n",
        "\n",
        ">Note: When creating a pipeline for KFP, we need a container image which will be used to run our pipeline. And `skaffold` will build the image for us. Because skaffold pulls base images from the docker hub, it will take 5~10 minutes when we build the image for the first time, but it will take much less time from the second build."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kOU7zQof0olf",
        "colab": {}
      },
      "source": [
        "!tfx pipeline create  \\\n",
        "--pipeline-path=kubeflow_dag_runner.py \\\n",
        "--endpoint={ENDPOINT} \\\n",
        "--build-target-image={CUSTOM_TFX_IMAGE} \\\n",
        "--build-base-image={DOCKER_IMAGE} \\\n",
        "--engine=kubeflow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QmA6___Y0olh"
      },
      "source": [
        "While creating a pipeline, `Dockerfile` and `build.yaml` will be generated to build a Docker image. Don't forget to add these files to the source control system (for example, git) along with other source files.\n",
        "\n",
        "A pipeline definition file for [argo](https://argoproj.github.io/argo/) will be generated, too. The name of this file is `${PIPELINE_NAME}.tar.gz`. For example, it will be `my_pipeline.tar.gz` if the name of your pipeline is `my_pipeline`. It is recommended NOT to include this pipeline definition file into source control, because it will be generated from other Python files and will be updated whenever you update the pipeline. For your convenience, this file is already listed in `.gitignore` which is generated automatically.\n",
        "\n",
        "NOTE: `kubeflow` will be automatically selected as an orchestration engine if `airflow` is not installed and `--engine` is not specified.\n",
        "\n",
        "Now start an execution run with the newly created pipeline using the `tfx run create` command."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cKSjVVsa0oli",
        "colab": {}
      },
      "source": [
        "!tfx run create --pipeline-name={PIPELINE_NAME} --endpoint={ENDPOINT} --engine=kubeflow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Pg0VxvUC0olk"
      },
      "source": [
        "Or, you can also run the pipeline in the KFP Dashboard.  The new execution run will be listed under Experiments in the KFP Dashboard.  Clicking into the experiment will allow you to monitor progress and visualize the artifacts created during the execution run."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nLN4ges90oll"
      },
      "source": [
        "However, we recommend visiting the KFP Dashboard. You can access the KFP Dashboard from the Cloud AI Platform Pipelines menu in Google Cloud Console. Once you visit the dashboard, you will be able to find the pipeline, and access a wealth of information about the pipeline.\n",
        "For example, you can find your runs under the *Experiments* menu, and when you open your execution run under Experiments you can find all your artifacts from the pipeline under *Artifacts* menu.\n",
        "\n",
        ">Note: If your pipeline run fails, you can see detailed logs for each TFX component in the Experiments tab in the KFP Dashboard.\n",
        "    \n",
        "One of the major sources of failure is permission related problems. Please make sure your KFP cluster has permissions to access Google Cloud APIs. This can be configured [when you create a KFP cluster in GCP](https://cloud.google.com/ai-platform/pipelines/docs/setting-up), or see [Troubleshooting document in GCP](https://cloud.google.com/ai-platform/pipelines/docs/troubleshooting)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeRyBBPFgzVM",
        "colab_type": "text"
      },
      "source": [
        "# Step 5. (Optional) Update the pipeline definition of the existing pipeline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bYH8Y2KB0olm"
      },
      "source": [
        "\n",
        "You may want to update the existing pipeline with modified pipeline definition. Use the `tfx pipeline update` command to update your pipeline, followed by the `tfx run create` command to create a new execution run of your updated pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VE-Pqvto0olm",
        "colab": {}
      },
      "source": [
        "# Update the pipeline\n",
        "!tfx pipeline update \\\n",
        "--pipeline-path=kubeflow_dag_runner.py \\\n",
        "--endpoint={ENDPOINT} \\\n",
        "--engine=kubeflow\n",
        "# You can run the pipeline the same way.\n",
        "!tfx run create --pipeline-name {PIPELINE_NAME} --endpoint={ENDPOINT} --engine=kubeflow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8q1ZYEHX0olo"
      },
      "source": [
        "### Check pipeline outputs\n",
        "\n",
        "Visit the KFP dashboard to find pipeline outputs in the page for your pipeline run. Click the *Experiments* tab on the left, and *All runs* in the Experiments page. You should be able to find the latest run under the name of your pipeline."
      ]
    }
  ]
}