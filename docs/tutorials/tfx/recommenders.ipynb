{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X80i_girFR2o"
      },
      "source": [
        "##### Copyright 2022 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "bB8gHCR3FVC0"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xu4gy3TAw4As"
      },
      "source": [
        "# Recommending Movies: Recommender Models in TFX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z17OmgavQfp4"
      },
      "source": [
        "Note: We recommend running this tutorial in a Colab notebook, with no setup\n",
        "required!  Just click \"Run in Google Colab\".\n",
        "\n",
        "\u003cdiv class=\"devsite-table-wrapper\"\u003e\u003ctable class=\"tfo-notebook-buttons\" align=\"left\"\u003e\n",
        "\u003ctd\u003e\u003ca target=\"_blank\" href=\"https://www.tensorflow.org/tfx/tutorials/tfx/recommenders\"\u003e\n",
        "\u003cimg src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" /\u003eView on TensorFlow.org\u003c/a\u003e\u003c/td\u003e\n",
        "\u003ctd\u003e\u003ca target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/tfx/blob/master/docs/tutorials/tfx/recommenders.ipynb\"\u003e\n",
        "\u003cimg src=\"https://www.tensorflow.org/images/colab_logo_32px.png\"\u003eRun in Google Colab\u003c/a\u003e\u003c/td\u003e\n",
        "\u003ctd\u003e\u003ca target=\"_blank\" href=\"https://github.com/tensorflow/tfx/tree/master/docs/tutorials/tfx/recommenders.ipynb\"\u003e\n",
        "\u003cimg width=32px src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\"\u003eView source on GitHub\u003c/a\u003e\u003c/td\u003e\n",
        "\u003ctd\u003e\u003ca target=\"_blank\" href=\"https://storage.googleapis.com/tensorflow_docs/tfx/docs/tutorials/tfx/recommenders.ipynb\"\u003e\n",
        "\u003cimg width=32px src=\"https://www.tensorflow.org/images/download_logo_32px.png\"\u003eDownload notebook\u003c/a\u003e\u003c/td\u003e\n",
        "\u003c/table\u003e\u003c/div\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCeYA79m1DEX"
      },
      "source": [
        "## TFRS Tutorial Ported to TFX\n",
        "This is a port of a basic TensorFlow Recommenders (TFRS) tutorial to TFX, which is designed to demonstrate how to use TFRS in a TFX pipeline.  It mirrors the [basic tutorial](https://www.tensorflow.org/recommenders/examples/basic_retrieval).\n",
        "\n",
        "For context, real-world recommender systems are often composed of two stages:\n",
        "\n",
        "1. The retrieval stage is responsible for selecting an initial set of hundreds of candidates from all possible candidates. The main objective of this model is to efficiently weed out all candidates that the user is not interested in. Because the retrieval model may be dealing with millions of candidates, it has to be computationally efficient.\n",
        "2. The ranking stage takes the outputs of the retrieval model and fine-tunes them to select the best possible handful of recommendations. Its task is to narrow down the set of items the user may be interested in to a shortlist of likely candidates.\n",
        "\n",
        "In this tutorial, we're going to focus on the first stage, retrieval. Retrieval models are often composed of two sub-models:\n",
        "\n",
        "1. A query model computing the query representation (normally a fixed-dimensionality embedding vector) using query features.\n",
        "2. A candidate model computing the candidate representation (an equally-sized vector) using the candidate features\n",
        "\n",
        "The outputs of the two models are then multiplied together to give a query-candidate affinity score, with higher scores expressing a better match between the candidate and the query.\n",
        "\n",
        "In this tutorial, we're going to build and train such a two-tower model using the Movielens dataset.\n",
        "\n",
        "We're going to:\n",
        "\n",
        "1. Ingest and inspect the MovieLens dataset.\n",
        "2. Implement a retrieval model.\n",
        "3. Train and export the model.\n",
        "4. Make predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7QZ3kkMQo48"
      },
      "source": [
        "## The dataset\n",
        "\n",
        "The Movielens dataset is a classic dataset from the [GroupLens](https://grouplens.org/datasets/movielens/) research group at the University of Minnesota. It contains a set of ratings given to movies by a set of users, and is a workhorse of recommender system research.\n",
        "\n",
        "The data can be treated in two ways:\n",
        "\n",
        "1. It can be interpreted as expressesing which movies the users watched (and rated), and which they did not. This is a form of implicit feedback, where users' watches tell us which things they prefer to see and which they'd rather not see.\n",
        "2. It can also be seen as expressesing how much the users liked the movies they did watch. This is a form of explicit feedback: given that a user watched a movie, we can tell roughly how much they liked by looking at the rating they have given.\n",
        "\n",
        "In this tutorial, we are focusing on a retrieval system: a model that predicts a set of movies from the catalogue that the user is likely to watch. Often, implicit data is more useful here, and so we are going to treat Movielens as an implicit system. This means that every movie a user watched is a positive example, and every movie they have not seen is an implicit negative example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sawo1x8kQk9b"
      },
      "source": [
        "## Imports\n",
        "\n",
        "\n",
        "Let's first get our imports out of the way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GtR3txiwrT9w"
      },
      "outputs": [],
      "source": [
        "!pip install -Uq tfx\n",
        "!pip install -Uq tensorflow-recommenders\n",
        "!pip install -Uq tensorflow-datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCnUpuaJGKa0"
      },
      "source": [
        "### Did you restart the runtime?\n",
        "If you are using Google Colab, the first time that you run the cell above, you must restart the runtime (Runtime \u003e Restart runtime ...). This is because of the way that Colab loads packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZGYDaF-m5wZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import absl\n",
        "import json\n",
        "import pprint\n",
        "import tempfile\n",
        "\n",
        "from typing import Any, Dict, List, Text\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_recommenders as tfrs\n",
        "import apache_beam as beam\n",
        "\n",
        "from absl import logging\n",
        "\n",
        "from tfx.components.example_gen.base_example_gen_executor import BaseExampleGenExecutor\n",
        "from tfx.components.example_gen.component import FileBasedExampleGen\n",
        "from tfx.components.example_gen import utils\n",
        "from tfx.dsl.components.base import executor_spec\n",
        "\n",
        "from tfx.types import artifact\n",
        "from tfx.types import artifact_utils\n",
        "from tfx.types import channel\n",
        "from tfx.types import standard_artifacts\n",
        "from tfx.types.standard_artifacts import Examples\n",
        "\n",
        "from tfx.dsl.component.experimental.annotations import InputArtifact\n",
        "from tfx.dsl.component.experimental.annotations import OutputArtifact\n",
        "from tfx.dsl.component.experimental.annotations import Parameter\n",
        "from tfx.dsl.component.experimental.decorators import component\n",
        "from tfx.types.experimental.simple_artifacts import Dataset\n",
        "\n",
        "from tfx import v1 as tfx\n",
        "from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n",
        "\n",
        "# Set up logging.\n",
        "tf.get_logger().propagate = False\n",
        "absl.logging.set_verbosity(absl.logging.INFO)\n",
        "pp = pprint.PrettyPrinter()\n",
        "\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"TFX version: {tfx.__version__}\")\n",
        "print(f\"TensorFlow Recommenders version: {tfrs.__version__}\")\n",
        "\n",
        "%load_ext tfx.orchestration.experimental.interactive.notebook_extensions.skip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Znn3FAHAsomB"
      },
      "source": [
        "## Create a TFDS ExampleGen\n",
        "\n",
        "We create a [custom ExampleGen component](https://www.tensorflow.org/tfx/guide/examplegen#custom_examplegen) which we use to load a TensorFlow Datasets (TFDS) dataset.  This uses a custom executor in a FileBasedExampleGen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcVgf7rLsv70"
      },
      "outputs": [],
      "source": [
        "@beam.ptransform_fn\n",
        "@beam.typehints.with_input_types(beam.Pipeline)\n",
        "@beam.typehints.with_output_types(tf.train.Example)\n",
        "def _TFDatasetToExample(  # pylint: disable=invalid-name\n",
        "    pipeline: beam.Pipeline,\n",
        "    exec_properties: Dict[str, Any],\n",
        "    split_pattern: str\n",
        "    ) -\u003e beam.pvalue.PCollection:\n",
        "    \"\"\"Read a TensorFlow Dataset and create tf.Examples\"\"\"\n",
        "    custom_config = json.loads(exec_properties['custom_config'])\n",
        "    dataset_name = custom_config['dataset']\n",
        "    split_name = custom_config['split']\n",
        "\n",
        "    builder = tfds.builder(dataset_name)\n",
        "    builder.download_and_prepare()\n",
        "\n",
        "    return (pipeline\n",
        "            | 'MakeExamples' \u003e\u003e tfds.beam.ReadFromTFDS(builder, split=split_name)\n",
        "            | 'AsNumpy' \u003e\u003e beam.Map(tfds.as_numpy)\n",
        "            | 'ToDict' \u003e\u003e beam.Map(dict)\n",
        "            | 'ToTFExample' \u003e\u003e beam.Map(utils.dict_to_example)\n",
        "            )\n",
        "\n",
        "class TFDSExecutor(BaseExampleGenExecutor):\n",
        "  def GetInputSourceToExamplePTransform(self) -\u003e beam.PTransform:\n",
        "    \"\"\"Returns PTransform for TF Dataset to TF examples.\"\"\"\n",
        "    return _TFDatasetToExample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srH6ChVCtR55"
      },
      "source": [
        "## Init TFX Pipeline Context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sM-46D40tW_V"
      },
      "outputs": [],
      "source": [
        "context = InteractiveContext()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PAqjR4a1RR4"
      },
      "source": [
        "## Preparing the dataset\n",
        "\n",
        "We will use our custom executor in a `FileBasedExampleGen` to load our datasets from TFDS.  Since we have two datasets, we will create two `ExampleGen` components."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aaQhqcLGP0jL"
      },
      "outputs": [],
      "source": [
        "# Ratings data.\n",
        "ratings_example_gen = FileBasedExampleGen(\n",
        "    input_base='dummy',\n",
        "    custom_config={'dataset':'movielens/100k-ratings', 'split':'train'},\n",
        "    custom_executor_spec=executor_spec.ExecutorClassSpec(TFDSExecutor))\n",
        "context.run(ratings_example_gen, enable_cache=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlUFANrRvKDW"
      },
      "outputs": [],
      "source": [
        "# Features of all the available movies.\n",
        "movies_example_gen = FileBasedExampleGen(\n",
        "    input_base='dummy',\n",
        "    custom_config={'dataset':'movielens/100k-movies', 'split':'train'},\n",
        "    custom_executor_spec=executor_spec.ExecutorClassSpec(TFDSExecutor))\n",
        "context.run(movies_example_gen, enable_cache=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRHorm8W1yf3"
      },
      "source": [
        "## Create `inspect_examples` utility\n",
        "We create a convenience utility to inspect datasets of TF.Examples. The ratings dataset returns a dictionary of movie id, user id, the assigned rating, timestamp, movie information, and user information:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1-KQV2ynMdh"
      },
      "outputs": [],
      "source": [
        "def inspect_examples(component,\n",
        "                     channel_name='examples',\n",
        "                     split_name='train',\n",
        "                     num_examples=1):\n",
        "  # Get the URI of the output artifact, which is a directory\n",
        "  full_split_name = 'Split-{}'.format(split_name)\n",
        "  print('channel_name: {}, split_name: {} (\\\"{}\\\"), num_examples: {}\\n'.format(\n",
        "      channel_name, split_name, full_split_name, num_examples))\n",
        "  train_uri = os.path.join(\n",
        "      component.outputs[channel_name].get()[0].uri, full_split_name)\n",
        "\n",
        "  # Get the list of files in this directory (all compressed TFRecord files)\n",
        "  tfrecord_filenames = [os.path.join(train_uri, name)\n",
        "                        for name in os.listdir(train_uri)]\n",
        "\n",
        "  # Create a `TFRecordDataset` to read these files\n",
        "  dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")\n",
        "\n",
        "  # Iterate over the records and print them\n",
        "  for tfrecord in dataset.take(num_examples):\n",
        "    serialized_example = tfrecord.numpy()\n",
        "    example = tf.train.Example()\n",
        "    example.ParseFromString(serialized_example)\n",
        "    pp.pprint(example)\n",
        "\n",
        "inspect_examples(ratings_example_gen)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGLGCjSt_q96"
      },
      "source": [
        "The movies dataset contains the movie id, movie title, and data on what genres it belongs to. Note that the genres are encoded with integer labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHLsIHhw_x1d"
      },
      "outputs": [],
      "source": [
        "inspect_examples(movies_example_gen)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iu4XSa_G1nyN"
      },
      "source": [
        "## ExampleGen did the split\n",
        "When we ingested the movie lens dataset, our `ExampleGen` component split the data into `train` and `eval` splits.  They are actually named `Split-train` and `Split-eval`. By default the split is 66% training, 34% evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nk7Y7hWGKH9e"
      },
      "source": [
        "## Generate statistics for movies and ratings\n",
        "\n",
        "For a TFX pipeline we need to generate statistics for the dataset.  We do that by using a [StatisticsGen component](https://www.tensorflow.org/tfx/guide/statsgen). These will be used by the [SchemaGen component](https://www.tensorflow.org/tfx/guide/schemagen) below when we generate a schema for our dataset.  This is good practice anyway, because it's important to examine and analyze your data on an ongoing basis.  Since we have two datasets we will create two StatisticsGen components."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7-ZI8IsKT2P"
      },
      "outputs": [],
      "source": [
        "movies_stats_gen = tfx.components.StatisticsGen(\n",
        "    examples=movies_example_gen.outputs['examples'])\n",
        "context.run(movies_stats_gen, enable_cache=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlKLrrgnKzIe"
      },
      "outputs": [],
      "source": [
        "context.show(movies_stats_gen.outputs['statistics'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmTThijxKmhA"
      },
      "outputs": [],
      "source": [
        "ratings_stats_gen = tfx.components.StatisticsGen(\n",
        "    examples=ratings_example_gen.outputs['examples'])\n",
        "context.run(ratings_stats_gen, enable_cache=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UoRcgChqK62O"
      },
      "outputs": [],
      "source": [
        "context.show(ratings_stats_gen.outputs['statistics'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_Lj0OBwLElb"
      },
      "source": [
        "## Create schemas for movies and ratings\n",
        "\n",
        "For a TFX pipeline we need to generate a data schema from our dataset.  We do that by using a [SchemaGen component](https://www.tensorflow.org/tfx/guide/schemagen). This will be used by the [Transform component](https://www.tensorflow.org/tfx/guide/transform) below to do our feature engineering in a way that is highly scalable to large datasets, and avoids training/serving skew.  Since we have two datasets we will create two SchemaGen components."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vL85CAcILJiw"
      },
      "outputs": [],
      "source": [
        "movies_schema_gen = tfx.components.SchemaGen(\n",
        "    statistics=movies_stats_gen.outputs['statistics'],\n",
        "    infer_feature_shape=False)\n",
        "context.run(movies_schema_gen, enable_cache=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9eMtN1U1Lha1"
      },
      "outputs": [],
      "source": [
        "context.show(movies_schema_gen.outputs['schema'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-DkVUOeLmvX"
      },
      "outputs": [],
      "source": [
        "ratings_schema_gen = tfx.components.SchemaGen(\n",
        "    statistics=ratings_stats_gen.outputs['statistics'],\n",
        "    infer_feature_shape=False)\n",
        "context.run(ratings_schema_gen, enable_cache=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SxD9oAhZLt_Z"
      },
      "outputs": [],
      "source": [
        "context.show(ratings_schema_gen.outputs['schema'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EN648jrYLxmH"
      },
      "source": [
        "## Feature Engineering using Transform\n",
        "\n",
        "For a structured and repeatable design of a TFX pipeline we will need a scalable approach to feature engineering.  This allows us to handle the large datasets which are usually part of many recommender systems, and it also avoids training/serving skew.  We will do that using the [Transform component](https://www.tensorflow.org/tfx/guide/transform).\n",
        "\n",
        "The Transform component uses a module file to supply user code for the feature engineering what we want to do, so our first step is to create that module file. Since we have two datasets, we will create two of these module files and two Transform components.\n",
        "\n",
        "One of the things that our recommender needs is vocabularies for the `user_id` and `movie_title` fields.  In the [basic_retrieval tutorial](https://www.tensorflow.org/recommenders/examples/basic_retrieval) those are created with inline Numpy, but here we will use Transform.\n",
        "\n",
        "Note: The `%%writefile {_movies_transform_module_file}` cell magic below creates and writes the contents of that cell to a file on the notebook server where this notebook is running (for example, the Colab VM).  When doing this outside of a notebook you would just create a Python file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Oqzx5mSI8zm"
      },
      "outputs": [],
      "source": [
        "_movies_transform_module_file = 'movies_transform_module.py'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKROCiPo_5LJ"
      },
      "outputs": [],
      "source": [
        "%%writefile {_movies_transform_module_file}\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_transform as tft\n",
        "\n",
        "def preprocessing_fn(inputs):\n",
        "  # We only want the movie title\n",
        "  return {'movie_title':inputs['movie_title']}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQcQBN9SIzIa"
      },
      "outputs": [],
      "source": [
        "movies_transform = tfx.components.Transform(\n",
        "    examples=movies_example_gen.outputs['examples'],\n",
        "    schema=movies_schema_gen.outputs['schema'],\n",
        "    module_file=os.path.abspath(_movies_transform_module_file))\n",
        "context.run(movies_transform, enable_cache=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5oai0TlNWlv"
      },
      "outputs": [],
      "source": [
        "context.show(movies_transform.outputs['post_transform_schema'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWahQqCiBXqA"
      },
      "outputs": [],
      "source": [
        "inspect_examples(movies_transform, channel_name='transformed_examples')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4PmR-a8O-mD"
      },
      "outputs": [],
      "source": [
        "_ratings_transform_module_file = 'ratings_transform_module.py'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWXuBqivPDuK"
      },
      "outputs": [],
      "source": [
        "%%writefile {_ratings_transform_module_file}\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_transform as tft\n",
        "import pdb\n",
        "\n",
        "NUM_OOV_BUCKETS = 1\n",
        "\n",
        "def preprocessing_fn(inputs):\n",
        "  # We only want the user ID and the movie title, but we also need vocabularies\n",
        "  # for both of them.  The vocabularies aren't features, they're only used by\n",
        "  # the lookup.\n",
        "  outputs = {}\n",
        "  outputs['user_id'] = tft.sparse_tensor_to_dense_with_shape(inputs['user_id'], [None, 1], '-1')\n",
        "  outputs['movie_title'] = tft.sparse_tensor_to_dense_with_shape(inputs['movie_title'], [None, 1], '-1')\n",
        "\n",
        "  tft.compute_and_apply_vocabulary(\n",
        "      inputs['user_id'],\n",
        "      num_oov_buckets=NUM_OOV_BUCKETS,\n",
        "      vocab_filename='user_id_vocab')\n",
        "\n",
        "  tft.compute_and_apply_vocabulary(\n",
        "      inputs['movie_title'],\n",
        "      num_oov_buckets=NUM_OOV_BUCKETS,\n",
        "      vocab_filename='movie_title_vocab')\n",
        "\n",
        "  return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4NgpBOkPXsj"
      },
      "outputs": [],
      "source": [
        "ratings_transform = tfx.components.Transform(\n",
        "    examples=ratings_example_gen.outputs['examples'],\n",
        "    schema=ratings_schema_gen.outputs['schema'],\n",
        "    module_file=os.path.abspath(_ratings_transform_module_file))\n",
        "context.run(ratings_transform, enable_cache=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9Vqby34Dvzd"
      },
      "outputs": [],
      "source": [
        "context.show(ratings_transform.outputs['post_transform_schema'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_ec39jiaMG-"
      },
      "outputs": [],
      "source": [
        "inspect_examples(ratings_transform, channel_name='transformed_examples')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCi-seR86qqa"
      },
      "source": [
        "## Implementing a model in TFX\n",
        "\n",
        "In the [basic_retrieval](https://www.tensorflow.org/recommenders/examples/basic_retrieval) tutorial the model was created inline in the Python runtime. In a TFX pipeline, the model, metric, and loss are defined and trained in the module file for a [pipeline component called Trainer](https://www.tensorflow.org/tfx/guide/trainer). This makes the model, metric, and loss part of a repeatable process which can be automated and monitored.\n",
        "\n",
        "### TensorFlow Recommenders model architecture\n",
        "\n",
        "We are going to build a two-tower retrieval model. The concept of two-tower means we will have a query tower computing the user representation using user features, and another item tower computing the movie representation using the movie features. We can build each tower separately (in the `_build_user_model()` and `_build_movie_model()` methods below) and then combine them in the final model (as in the `MobieLensModel` class). `MovieLensModel` is a subclass of `tfrs.Model` base class, which streamlines building models: all we need to do is to set up the components in the `__init__` method, and implement the `compute_loss` method, taking in the raw features and returning a loss value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_mmYhjAJP4g"
      },
      "outputs": [],
      "source": [
        "# We're now going to create the module file for Trainer, which will include the\n",
        "# code above with some modifications for TFX.\n",
        "\n",
        "_trainer_module_file = 'trainer_module.py'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHQZJEhXP93N"
      },
      "outputs": [],
      "source": [
        "%%writefile {_trainer_module_file}\n",
        "\n",
        "from typing import Dict, List, Text\n",
        "\n",
        "import pdb\n",
        "\n",
        "import os\n",
        "import absl\n",
        "import datetime\n",
        "import glob\n",
        "import tensorflow as tf\n",
        "import tensorflow_transform as tft\n",
        "import tensorflow_recommenders as tfrs\n",
        "\n",
        "from absl import logging\n",
        "from tfx.types import artifact_utils\n",
        "\n",
        "from tfx import v1 as tfx\n",
        "from tfx_bsl.coders import example_coder\n",
        "from tfx_bsl.public import tfxio\n",
        "\n",
        "absl.logging.set_verbosity(absl.logging.INFO)\n",
        "\n",
        "EMBEDDING_DIMENSION = 32\n",
        "INPUT_FN_BATCH_SIZE = 1\n",
        "\n",
        "\n",
        "def extract_str_feature(dataset, feature_name):\n",
        "  np_dataset = []\n",
        "  for example in dataset:\n",
        "    np_example = example_coder.ExampleToNumpyDict(example.numpy())\n",
        "    np_dataset.append(np_example[feature_name][0].decode())\n",
        "  return tf.data.Dataset.from_tensor_slices(np_dataset)\n",
        "\n",
        "\n",
        "class MovielensModel(tfrs.Model):\n",
        "\n",
        "  def __init__(self, user_model, movie_model, tf_transform_output, movies_uri):\n",
        "    super().__init__()\n",
        "    self.movie_model: tf.keras.Model = movie_model\n",
        "    self.user_model: tf.keras.Model = user_model\n",
        "\n",
        "    movies_artifact = movies_uri.get()[0]\n",
        "    input_dir = artifact_utils.get_split_uri([movies_artifact], 'train')\n",
        "    movie_files = glob.glob(os.path.join(input_dir, '*'))\n",
        "    movies = tf.data.TFRecordDataset(movie_files, compression_type=\"GZIP\")\n",
        "    movies_dataset = extract_str_feature(movies, 'movie_title')\n",
        "\n",
        "    loss_metrics = tfrs.metrics.FactorizedTopK(\n",
        "        candidates=movies_dataset.batch(128).map(movie_model)\n",
        "        )\n",
        "\n",
        "    self.task: tf.keras.layers.Layer = tfrs.tasks.Retrieval(\n",
        "        metrics=loss_metrics\n",
        "        )\n",
        "\n",
        "\n",
        "  def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -\u003e tf.Tensor:\n",
        "    # We pick out the user features and pass them into the user model.\n",
        "    try:\n",
        "      user_embeddings = tf.squeeze(self.user_model(features['user_id']), axis=1)\n",
        "      # And pick out the movie features and pass them into the movie model,\n",
        "      # getting embeddings back.\n",
        "      positive_movie_embeddings = self.movie_model(features['movie_title'])\n",
        "\n",
        "      # The task computes the loss and the metrics.\n",
        "      _task = self.task(user_embeddings, positive_movie_embeddings)\n",
        "    except BaseException as err:\n",
        "      logging.error('######## ERROR IN compute_loss:\\n{}\\n###############'.format(err))\n",
        "\n",
        "    return _task\n",
        "\n",
        "\n",
        "# This function will apply the same transform operation to training data\n",
        "# and serving requests.\n",
        "def _apply_preprocessing(raw_features, tft_layer):\n",
        "  try:\n",
        "    transformed_features = tft_layer(raw_features)\n",
        "  except BaseException as err:\n",
        "    logging.error('######## ERROR IN _apply_preprocessing:\\n{}\\n###############'.format(err))\n",
        "\n",
        "  return transformed_features\n",
        "\n",
        "\n",
        "def _input_fn(file_pattern: List[Text],\n",
        "              data_accessor: tfx.components.DataAccessor,\n",
        "              tf_transform_output: tft.TFTransformOutput,\n",
        "              batch_size: int = 200) -\u003e tf.data.Dataset:\n",
        "  \"\"\"Generates features and label for tuning/training.\n",
        "\n",
        "  Args:\n",
        "    file_pattern: List of paths or patterns of input tfrecord files.\n",
        "    data_accessor: DataAccessor for converting input to RecordBatch.\n",
        "    tf_transform_output: A TFTransformOutput.\n",
        "    batch_size: representing the number of consecutive elements of returned\n",
        "      dataset to combine in a single batch\n",
        "\n",
        "  Returns:\n",
        "    A dataset that contains (features, indices) tuple where features is a\n",
        "      dictionary of Tensors, and indices is a single Tensor of label indices.\n",
        "  \"\"\"\n",
        "  try:\n",
        "    return data_accessor.tf_dataset_factory(\n",
        "      file_pattern,\n",
        "      tfxio.TensorFlowDatasetOptions(\n",
        "          batch_size=batch_size),\n",
        "      tf_transform_output.transformed_metadata.schema)\n",
        "  except BaseException as err:\n",
        "    logging.error('######## ERROR IN _input_fn:\\n{}\\n###############'.format(err))\n",
        "\n",
        "  return None\n",
        "\n",
        "\n",
        "def _get_serve_tf_examples_fn(model, tf_transform_output):\n",
        "  \"\"\"Returns a function that parses a serialized tf.Example and applies TFT.\"\"\"\n",
        "  try:\n",
        "    model.tft_layer = tf_transform_output.transform_features_layer()\n",
        "\n",
        "    @tf.function\n",
        "    def serve_tf_examples_fn(serialized_tf_examples):\n",
        "      \"\"\"Returns the output to be used in the serving signature.\"\"\"\n",
        "      try:\n",
        "        feature_spec = tf_transform_output.raw_feature_spec()\n",
        "        parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)\n",
        "        transformed_features = model.tft_layer(parsed_features)\n",
        "        result = model(transformed_features)\n",
        "      except BaseException as err:\n",
        "        logging.error('######## ERROR IN serve_tf_examples_fn:\\n{}\\n###############'.format(err))\n",
        "      return result\n",
        "  except BaseException as err:\n",
        "      logging.error('######## ERROR IN _get_serve_tf_examples_fn:\\n{}\\n###############'.format(err))\n",
        "\n",
        "  return serve_tf_examples_fn\n",
        "\n",
        "\n",
        "def _build_user_model(\n",
        "    tf_transform_output: tft.TFTransformOutput, # Specific to ratings\n",
        "    embedding_dimension: int = 32) -\u003e tf.keras.Model:\n",
        "  \"\"\"Creates a Keras model for the query tower.\n",
        "\n",
        "  Args:\n",
        "    tf_transform_output: [tft.TFTransformOutput], the results of Transform\n",
        "    embedding_dimension: [int], the dimensionality of the embedding space\n",
        "\n",
        "  Returns:\n",
        "    A keras Model.\n",
        "  \"\"\"\n",
        "  try:\n",
        "    unique_user_ids = tf_transform_output.vocabulary_by_name('user_id_vocab')\n",
        "    users_vocab_str = [b.decode() for b in unique_user_ids]\n",
        "\n",
        "    model = tf.keras.Sequential(\n",
        "        [\n",
        "         tf.keras.layers.StringLookup(\n",
        "             vocabulary=users_vocab_str, mask_token=None),\n",
        "         # We add an additional embedding to account for unknown tokens.\n",
        "         tf.keras.layers.Embedding(len(users_vocab_str) + 1, embedding_dimension)\n",
        "         ])\n",
        "  except BaseException as err:\n",
        "    logging.error('######## ERROR IN _build_user_model:\\n{}\\n###############'.format(err))\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "def _build_movie_model(\n",
        "    tf_transform_output: tft.TFTransformOutput, # Specific to movies\n",
        "    embedding_dimension: int = 32) -\u003e tf.keras.Model:\n",
        "  \"\"\"Creates a Keras model for the candidate tower.\n",
        "\n",
        "  Args:\n",
        "    tf_transform_output: [tft.TFTransformOutput], the results of Transform\n",
        "    embedding_dimension: [int], the dimensionality of the embedding space\n",
        "\n",
        "  Returns:\n",
        "    A keras Model.\n",
        "  \"\"\"\n",
        "  try:\n",
        "    unique_movie_titles = tf_transform_output.vocabulary_by_name('movie_title_vocab')\n",
        "    titles_vocab_str = [b.decode() for b in unique_movie_titles]\n",
        "\n",
        "    model = tf.keras.Sequential(\n",
        "        [\n",
        "         tf.keras.layers.StringLookup(\n",
        "             vocabulary=titles_vocab_str, mask_token=None),\n",
        "         # We add an additional embedding to account for unknown tokens.\n",
        "         tf.keras.layers.Embedding(len(titles_vocab_str) + 1, embedding_dimension)\n",
        "        ])\n",
        "  except BaseException as err:\n",
        "      logging.error('######## ERROR IN _build_movie_model:\\n{}\\n###############'.format(err))\n",
        "  return model\n",
        "\n",
        "\n",
        "# TFX Trainer will call this function.\n",
        "def run_fn(fn_args: tfx.components.FnArgs):\n",
        "  \"\"\"Train the model based on given args.\n",
        "\n",
        "  Args:\n",
        "    fn_args: Holds args used to train the model as name/value pairs.\n",
        "  \"\"\"\n",
        "  try:\n",
        "    tf_transform_output = tft.TFTransformOutput(fn_args.transform_output)\n",
        "\n",
        "    train_dataset = _input_fn(fn_args.train_files, fn_args.data_accessor,\n",
        "                              tf_transform_output, INPUT_FN_BATCH_SIZE)\n",
        "    eval_dataset = _input_fn(fn_args.eval_files, fn_args.data_accessor,\n",
        "                            tf_transform_output, INPUT_FN_BATCH_SIZE)\n",
        "\n",
        "    model = MovielensModel(\n",
        "        _build_user_model(tf_transform_output, EMBEDDING_DIMENSION),\n",
        "        _build_movie_model(tf_transform_output, EMBEDDING_DIMENSION),\n",
        "        tf_transform_output,\n",
        "        fn_args.custom_config['movies']\n",
        "        )\n",
        "\n",
        "    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
        "        log_dir=fn_args.model_run_dir, update_freq='batch')\n",
        "\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1))\n",
        "  except BaseException as err:\n",
        "    logging.error('######## ERROR IN run_fn before fit:\\n{}\\n###############'.format(err))\n",
        "\n",
        "  try:\n",
        "    model.fit(\n",
        "        train_dataset,\n",
        "        epochs=fn_args.custom_config['epochs'],\n",
        "        steps_per_epoch=fn_args.train_steps,\n",
        "        validation_data=eval_dataset,\n",
        "        validation_steps=fn_args.eval_steps,\n",
        "        callbacks=[tensorboard_callback])\n",
        "  except BaseException as err:\n",
        "      logging.error('######## ERROR IN run_fn during fit:\\n{}\\n###############'.format(err))\n",
        "\n",
        "  try:\n",
        "    index = tfrs.layers.factorized_top_k.BruteForce(model.user_model)\n",
        "\n",
        "    movies_artifact = fn_args.custom_config['movies'].get()[0]\n",
        "    input_dir = artifact_utils.get_split_uri([movies_artifact], 'eval')\n",
        "    movie_files = glob.glob(os.path.join(input_dir, '*'))\n",
        "    movies = tf.data.TFRecordDataset(movie_files, compression_type=\"GZIP\")\n",
        "\n",
        "    movies_dataset = extract_str_feature(movies, 'movie_title')\n",
        "\n",
        "    index.index_from_dataset(\n",
        "      tf.data.Dataset.zip((\n",
        "          movies_dataset.batch(100),\n",
        "          movies_dataset.batch(100).map(model.movie_model))\n",
        "      )\n",
        "    )\n",
        "\n",
        "    # Run once so that we can get the right signatures into SavedModel\n",
        "    _, titles = index(tf.constant([\"42\"]))\n",
        "    print(f\"Recommendations for user 42: {titles[0, :3]}\")\n",
        "\n",
        "    signatures = {\n",
        "        'serving_default':\n",
        "            _get_serve_tf_examples_fn(index,\n",
        "                                      tf_transform_output).get_concrete_function(\n",
        "                                          tf.TensorSpec(\n",
        "                                              shape=[None],\n",
        "                                              dtype=tf.string,\n",
        "                                              name='examples')),\n",
        "    }\n",
        "    index.save(fn_args.serving_model_dir, save_format='tf', signatures=signatures)\n",
        "\n",
        "  except BaseException as err:\n",
        "      logging.error('######## ERROR IN run_fn during export:\\n{}\\n###############'.format(err))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDN_LJGlnRGo"
      },
      "source": [
        "## Training the model\n",
        "\n",
        "After defining the model, we can run the [Trainer component](https://www.tensorflow.org/tfx/guide/trainer) to do the model training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hsWC8UpVrngY",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "trainer = tfx.components.Trainer(\n",
        "    module_file=os.path.abspath(_trainer_module_file),\n",
        "    examples=ratings_transform.outputs['transformed_examples'],\n",
        "    transform_graph=ratings_transform.outputs['transform_graph'],\n",
        "    schema=ratings_transform.outputs['post_transform_schema'],\n",
        "    train_args=tfx.proto.TrainArgs(num_steps=500),\n",
        "    eval_args=tfx.proto.EvalArgs(num_steps=10),\n",
        "    custom_config={\n",
        "        'epochs':5,\n",
        "        'movies':movies_transform.outputs['transformed_examples'],\n",
        "        'movie_schema':movies_transform.outputs['post_transform_schema'],\n",
        "        'ratings':ratings_transform.outputs['transformed_examples'],\n",
        "        'ratings_schema':ratings_transform.outputs['post_transform_schema']\n",
        "        })\n",
        "\n",
        "context.run(trainer, enable_cache=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WXkKKZpRkaj"
      },
      "source": [
        "## Exporting the model\n",
        "\n",
        "After training the model, we can use the [Pusher component](https://www.tensorflow.org/tfx/guide/pusher) to export the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1hXlzwMTRkaj",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "_serving_model_dir = os.path.join(tempfile.mkdtemp(), 'serving_model/tfrs_retrieval')\n",
        "\n",
        "pusher = tfx.components.Pusher(\n",
        "    model=trainer.outputs['model'],\n",
        "    push_destination=tfx.proto.PushDestination(\n",
        "        filesystem=tfx.proto.PushDestination.Filesystem(\n",
        "            base_directory=_serving_model_dir)))\n",
        "context.run(pusher, enable_cache=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NB2v43NJU3Xf"
      },
      "source": [
        "## Make predictions\n",
        "\n",
        "Now that we have a model, we load it back and make predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MUwd9QoGRkaj",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "loaded = tf.saved_model.load(pusher.outputs['pushed_model'].get()[0].uri)\n",
        "scores, titles = loaded([\"42\"])\n",
        "\n",
        "print(f\"Recommendations: {titles[0][:3]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ipAubFuRkak"
      },
      "source": [
        "## Next step\n",
        "\n",
        "In this tutorial, you have learned how to implement a retrieval model with TensorFlow Recommenders and TFX. To expand on what is presented here, have a look at the [TFRS ranking with TFX](https://www.tensorflow.org/recommenders/examples/ranking_tfx) tutorial."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "X80i_girFR2o"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
