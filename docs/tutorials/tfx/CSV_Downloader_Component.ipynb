{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nl4XCJN9g8Bc"
      },
      "source": [
        "Copyright 2023 The TensorFlow Authors.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dIUc9Zh3hM6H"
      },
      "outputs": [],
      "source": [
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wU-hMBZVmyCo"
      },
      "source": [
        "# TFX Pipeline Tutorial for Large Language Model using CNN Daily Dataset\n",
        "\n",
        "\n",
        "In this codelab, we use  KerasNLP to load a pre-trained Large Language Model (LLM) - GPT-2 model - finetune it to a dataset. The dataset that is used in this demo is CNN daily dataset.  Note that GPT-2 is used here only to demonstrate the end-to-end process; the techniques and tooling introduced in this codelab are potentially transferrable to other generative language models such as Google T5."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJAp-HxKiKsE"
      },
      "source": [
        "\u003cdiv class=\"devsite-table-wrapper\"\u003e\u003ctable class=\"tfo-notebook-buttons\" align=\"left\"\u003e\n",
        "\u003ctd\u003e\u003ca target=\"_blank\" href=\"https://www.tensorflow.org/tfx/tutorials/tfx/penguin_simple\"\u003e\n",
        "\u003cimg src=\"https://www.tensorflow.org/images/tf_logo_32px.png\"/\u003eView on TensorFlow.org\u003c/a\u003e\u003c/td\u003e\n",
        "\u003ctd\u003e\u003ca target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/tfx/blob/master/docs/tutorials/tfx/penguin_simple.ipynb\"\u003e\n",
        "\u003cimg src=\"https://www.tensorflow.org/images/colab_logo_32px.png\"\u003eRun in Google Colab\u003c/a\u003e\u003c/td\u003e\n",
        "\u003ctd\u003e\u003ca target=\"_blank\" href=\"https://github.com/tensorflow/tfx/tree/master/docs/tutorials/tfx/penguin_simple.ipynb\"\u003e\n",
        "\u003cimg width=32px src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\"\u003eView source on GitHub\u003c/a\u003e\u003c/td\u003e\n",
        "\u003ctd\u003e\u003ca href=\"https://storage.googleapis.com/tensorflow_docs/tfx/docs/tutorials/tfx/penguin_simple.ipynb\"\u003e\u003cimg src=\"https://www.tensorflow.org/images/download_logo_32px.png\" /\u003eDownload notebook\u003c/a\u003e\u003c/td\u003e\n",
        "\u003c/table\u003e\u003c/div\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MK3ryPikKtj"
      },
      "source": [
        "# Before You Begin\n",
        "\n",
        "Colab offers different kinds of runtimes. Make sure to go to **Runtime -\u003e Change runtime** type and choose the GPU Hardware Accelerator runtime (which should have \u003e12G System RAM and ~15G GPU RAM) since you will finetune the GPT-2 model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMmMNdV1jZAS"
      },
      "source": [
        "# Set Up\n",
        "\n",
        "We first install the TFX Python package."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C23ItymvmVth"
      },
      "source": [
        "## Upgrade Pip\n",
        "To avoid upgrading Pip in a system when running locally, check to make sure that we are running in Colab. Local systems can of course be upgraded separately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfSG5IFamUq7"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  import colab\n",
        "  !pip install --upgrade pip\n",
        "except:\n",
        "  pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "te56mTWomdLq"
      },
      "source": [
        "## Install TFX\n",
        "\n",
        "TFX is currently experiencing issues with Python 3.10 in Colab.\n",
        "Therefore, simply running the command\n",
        "```\n",
        "!pip install -U tfx\n",
        "```\n",
        "to install tfx **will fail**. Hence, follow the code below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGlfiX4PmcjZ"
      },
      "outputs": [],
      "source": [
        "%%shell\n",
        "update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.8 3\n",
        "curl -O https://bootstrap.pypa.io/get-pip.py\n",
        "python get-pip.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nYHRZQjQEcS7"
      },
      "outputs": [],
      "source": [
        "# 1) TFX relies on an old version of google-api-core so we let google-auth float\n",
        "# for the install. We grep it out below:\n",
        "!grep -v google-auth /etc/requirements.core.in \u003e requirements.txt\n",
        "\n",
        "# 2) httplib2 should be included in /etc/requirements.core.in but it's not for\n",
        "# reasons. We ensure it's included:\n",
        "!grep httplib2 /etc/requirements.user.in \u003e\u003e requirements.txt\n",
        "\n",
        "# 3) google.colab package is not available as a wheel. We symlink that in so\n",
        "# it's on the sys.path of Python 3.8:\n",
        "!mkdir /usr/local/lib/python3.8/dist-packages/google\n",
        "!ln -s /usr/local/lib/python3.10/dist-packages/google/colab /usr/local/lib/python3.8/dist-packages/google/colab\n",
        "\n",
        "# Now with those pre-requisites out of the way:\n",
        "!pip install tfx==1.13.0 -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9cAnZCO4IEJ"
      },
      "source": [
        "## Did you restart the runtime?\n",
        "\n",
        "If you are using Google Colab, the first time that you run the cell above, you must restart the runtime by clicking above \"RESTART RUNTIME\" button or using \"Runtime \u003e Restart runtime ...\" menu. This is because of the way that Colab loads packages. Check the TensorFlow and TFX versions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZo6NOYQEcS7"
      },
      "source": [
        "# Imports\n",
        "Let's first get our imports out of the way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VDhX6vgUEcS7"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tfx.types import Channel\n",
        "from absl import logging\n",
        "from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVMFdYDtmgPX"
      },
      "source": [
        "## Uninstall shapely\n",
        "\n",
        "TODO(b/263441833) This is a temporal solution to avoid an ImportError. Ultimately, it should be handled by supporting a recent version of Bigquery, instead of uninstalling other extra dependencies.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4AKpWUiEcS7"
      },
      "outputs": [],
      "source": [
        "!pip uninstall shapely -y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fac1XkwrnXW6"
      },
      "source": [
        "Let's check the library versions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNwD6G4TXrlq"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "print('TensorFlow version: {}'.format(tf.__version__))\n",
        "from tfx import v1 as tfx\n",
        "print('TFX version: {}'.format(tfx.__version__))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnvgEYNwtMhJ"
      },
      "source": [
        "## Set up variables\n",
        "There are some variables used to define a pipeline. You can customize these variables as you want. By default all output from the pipeline will be generated under the current directory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVFcsQhWkbkw"
      },
      "source": [
        "# CSV Downloader\n",
        "In order to make the pipeline more efficient and possible for automation, it is useful to have a component that takes in a download link to the CSV file to be downloaded. Furthermore, one important goal of TFX production ML pipeline is to collect metadata containing information about the pipeline components, their executions, and resulting artifacts. In other words, the purpose of the metadata is to analyze the lineage of pipeline components and debug issues, and the CSV Downloader Component would help the users logging and tracking information about the source of the data and the preprocessing steps that the data have undergone before entering the pipeline. In this section, we declare a new artifact called CSVdoc and develop a custom component -- CSV Downloader -- which stores information about the dataset and downloads the CSV file in the CSVdoc artifact's URI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jc1JTbjjo0bd"
      },
      "outputs": [],
      "source": [
        "from tfx.types import artifact\n",
        "from tfx import types\n",
        "\n",
        "Property = artifact.Property\n",
        "PropertyType = artifact.PropertyType\n",
        "\n",
        "URL_PROPERTY = Property(type=PropertyType.STRING)\n",
        "PATH_PROPERTY = Property(type=PropertyType.STRING)\n",
        "\n",
        "class CsvDoc(types.Artifact):\n",
        "  \"\"\" Artifact that contains the CSV dataset.\n",
        "\n",
        "     - 'url' : saves the source of the original data.\n",
        "     - 'path': saves the path to the CSV file.\n",
        "  \"\"\"\n",
        "\n",
        "  TYPE_NAME = 'CsvDoc'\n",
        "  PROPERTIES = {\n",
        "      'url' : URL_PROPERTY,\n",
        "      'path': PATH_PROPERTY,\n",
        "  }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Qks2al5X1Us"
      },
      "outputs": [],
      "source": [
        "from absl import logging\n",
        "import requests\n",
        "import os\n",
        "import tfx.v1 as tfx\n",
        "from tfx.dsl.component.experimental.decorators import component\n",
        "\n",
        "@tfx.dsl.components.component\n",
        "def CsvDownloaderComponent(\n",
        "    url: tfx.dsl.components.Parameter[str],\n",
        "    file_name: tfx.dsl.components.Parameter[str],\n",
        "    saved_file: tfx.dsl.components.OutputArtifact[CsvDoc],\n",
        ") -\u003e None:\n",
        "  response = requests.get(url)\n",
        "  saved_file.url = url\n",
        "  if response.status_code == 200:\n",
        "    file_path = os.path.join(saved_file.uri, file_name)\n",
        "    saved_file.path = file_path\n",
        "    url_content = response.content\n",
        "    with open(file_path, 'wb') as csv_file:\n",
        "      csv_file.write(url_content)\n",
        "    logging.info(f\"CSV file saved successfully at {file_path}\")\n",
        "  else:\n",
        "    raise Exception(\"CSV file failed to be saved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3D3O4L6hYBBt"
      },
      "outputs": [],
      "source": [
        "downloader = CsvDownloaderComponent(\n",
        "  url = 'https://drive.google.com/uc?id=1YdZsJlRafqxiNSl0nHQkwR7rzrNlN9LI\u0026export=download', file_name ='testing_doc.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGm5cG6cYE10"
      },
      "outputs": [],
      "source": [
        "from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n",
        "context = InteractiveContext()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHpBtrduYG7U"
      },
      "outputs": [],
      "source": [
        "context.run(downloader, enable_cache = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8cIOkip4xUf"
      },
      "source": [
        "# CSV ExampleGen\n",
        "\n",
        "As a second component of the LLM pipeline, this component takes in a CSV artifact as its input and ouputs a CsvDoc artifact, a custom artifact created in the previous step. We apply NLTK Tokenizer in this step as well to tokenize a single long text into a list of tokenized sentences. For the purpose of MLOPs and parallelized design, users should be able to  input CSV files with different columns and file structures. Therefore, this component should be designed in a way that can take in any CSV files and create an example component. The current design is implemented in the way that the user defines a column in the dataset to extract the article contents.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0y_KcE8E40wo"
      },
      "source": [
        "## Imports\n",
        "Let's do all the necessary imports for our custom CSVExampleGen Component."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMIfpVLr3TC3"
      },
      "outputs": [],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SRubhiQN42bY"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "from absl import logging\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O10BV7Bu458v"
      },
      "source": [
        "In our case, we are performing next word prediction in a language model, so we only need the 'article' feature.\n",
        "\n",
        "To further prepare our training data, we are going to use a NLP package: NLTK. NLTK helps us download a sentence tokenizer 'punkt', which divides text into a list of sentences.\n",
        "\n",
        "We define a helper function to merge shorter sentences into longer ones, until they reach a pre-defined max_length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RIm2FH9K44J4"
      },
      "outputs": [],
      "source": [
        "import tfx.v1 as tfx\n",
        "from tfx.dsl.component.experimental.decorators import component\n",
        "from absl import logging\n",
        "import os\n",
        "import pandas as pd\n",
        "from nltk import tokenize\n",
        "\n",
        "def merge_sentences(sentences, max_length):\n",
        "    res = []\n",
        "    cur_len = 0\n",
        "    cur_sentences = []\n",
        "    for s in sentences:\n",
        "        if cur_len + len(s) \u003e max_length:\n",
        "            # If adding the next sentence exceeds `max_length`, we add the\n",
        "            # current sentences into collection\n",
        "            res.append(\" \".join(cur_sentences))\n",
        "            cur_len = len(s)\n",
        "            cur_sentences = [s]\n",
        "        else:\n",
        "            cur_len += len(s)\n",
        "            cur_sentences.append(s)\n",
        "    res.append(\" \".join(cur_sentences))\n",
        "    return res\n",
        "\n",
        "def create_csv(data_arr, save_dir):\n",
        "  column_value = 'preprocessed_text'\n",
        "  df = pd.DataFrame(data = data_arr, columns = [column_value])\n",
        "  df.to_csv(save_dir, index = False)\n",
        "\n",
        "def preprocess_and_save(csv_file_dir, column_name, save_dir):\n",
        "  with open(csv_file_dir, \"r\") as f:\n",
        "    df = pd.read_csv(f, delimiter = ',')\n",
        "  all_sentences = []\n",
        "  count = 0\n",
        "  num_articles_to_process = 500\n",
        "  max_length = 512\n",
        "  if column_name in df.columns:\n",
        "    logging.info(f\"{column_name} is found in the dataframe.\")\n",
        "    for index, row in df.iterrows():\n",
        "        article = row[column_name]\n",
        "        # Use NLTK tokenize to split articles into sentences\n",
        "        sentences = tokenize.sent_tokenize(str(article))\n",
        "        # Merge individual sentences into longer context\n",
        "        combined_res = merge_sentences(sentences, max_length)\n",
        "        # Add merged context into collection\n",
        "        all_sentences.extend(combined_res)\n",
        "        count += 1\n",
        "        if count \u003e= num_articles_to_process:\n",
        "          break\n",
        "    return create_csv(all_sentences, save_dir)\n",
        "  else:\n",
        "    raise Exception(f\"{column_name} is not found in the dataframe.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcNc8o3G49O0"
      },
      "source": [
        "Below is the code for custom CSV Example Generator Component for an LLM Model.This csv examplegen component takes a csv artifact as an input and generates train and eval examples for downstream components."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6chIUx8N47vh"
      },
      "outputs": [],
      "source": [
        "@tfx.dsl.components.component\n",
        "def ExampleGenComponent(\n",
        "    input_csv: tfx.dsl.components.InputArtifact[CsvDoc],\n",
        "    column_name : tfx.dsl.components.Parameter[str],\n",
        "    output_csv: tfx.dsl.components.OutputArtifact[CsvDoc]) -\u003e None:\n",
        "    input_csv_dir = input_csv.full_path\n",
        "    output_csv.full_path = f\"{output_csv.uri}/processed_data.csv\"\n",
        "    output_csv.url = input_csv.url\n",
        "    preprocess_and_save(input_csv_dir, column_name, output_csv.full_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZ2hJXP14_Ph"
      },
      "outputs": [],
      "source": [
        "# from customCSVexampleGenComponent import customCSVexampleGenComponent\n",
        "ExampleGenerator = ExampleGenComponent(input_csv = downloader.outputs['saved_file'], column_name = 'article')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M64XoTMD5BVQ"
      },
      "outputs": [],
      "source": [
        "context.run(ExampleGenerator, enable_cache = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3TONXEtllrY"
      },
      "source": [
        "# Trainer\n",
        "\n",
        "First, let's import all the necessary packages and libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y896sfqyll-9"
      },
      "outputs": [],
      "source": [
        "import keras_nlp\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtFt2I4Tln7N"
      },
      "source": [
        "KerasNLP provides a number of pre-trained models, such as Google Bert and GPT-2. You can see the list of models available in the KerasNLP repository.\n",
        "\n",
        "It's very easy to load the GPT-2 model as you can see below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GcTJwnf6lnDJ"
      },
      "outputs": [],
      "source": [
        "gpt2_tokenizer = keras_nlp.models.GPT2Tokenizer.from_preset(\"gpt2_base_en\")\n",
        "gpt2_preprocessor = keras_nlp.models.GPT2CausalLMPreprocessor.from_preset(\n",
        "    \"gpt2_base_en\",\n",
        "    sequence_length=256,\n",
        "    add_end_token=True,\n",
        ")\n",
        "gpt2_lm = keras_nlp.models.GPT2CausalLM.from_preset(\"gpt2_base_en\", preprocessor=gpt2_preprocessor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkAOLVj3lrcM"
      },
      "source": [
        "We will define a new artifact that represents a trained model. This artifact contains a directory to the saved model and the parameters used for training the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LRcGjuyKlrP9"
      },
      "outputs": [],
      "source": [
        "from tfx.types import artifact\n",
        "from tfx import types\n",
        "\n",
        "Property = artifact.Property\n",
        "PropertyType = artifact.PropertyType\n",
        "\n",
        "EPOCH_PROPERTY = Property(type=PropertyType.INT)\n",
        "\n",
        "class CustomModel(types.Artifact):\n",
        "  \"\"\" Artifact that contains the trained model.\n",
        "\n",
        "  * Properties:\n",
        "     - 'epoch' : saves the number of epochs it took to train the model.\n",
        "\n",
        "  \"\"\"\n",
        "  TYPE_NAME = 'Model'\n",
        "  PROPERTIES = {\n",
        "      'epoch' : EPOCH_PROPERTY,\n",
        "  }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPuZXoo8lumV"
      },
      "source": [
        "A dictionary, which maps signature keys to either tf.function instances with input signatures or concrete functions. Keys of such a dictionary may be arbitrary strings, but will typically be from the tf.saved_model.signature_constants module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_p8M9uYIlqAt"
      },
      "outputs": [],
      "source": [
        "import tfx.v1 as tfx\n",
        "from tfx.dsl.component.experimental.decorators import component\n",
        "\n",
        "\n",
        "@tfx.dsl.components.component\n",
        "def Trainer(\n",
        "    preprocessed_data : tfx.dsl.components.InputArtifact[CsvDoc],\n",
        "    trained_model : tfx.dsl.components.OutputArtifact[CustomModel],\n",
        "    num_epochs :tfx.dsl.components.Parameter[int]) -\u003e None:\n",
        "      with open(preprocessed_data.full_path, \"r\") as f:\n",
        "        df = pd.read_csv(f, delimiter = ',')\n",
        "      tf_train_ds = tf.data.Dataset.from_tensor_slices(df['preprocessed_text'])\n",
        "      processed_ds = tf_train_ds.map(gpt2_preprocessor, tf.data.AUTOTUNE).batch(20).cache().prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "      trained_model.epoch = num_epochs\n",
        "      gpt2_lm.include_preprocessing = False\n",
        "\n",
        "      lr = tf.keras.optimizers.schedules.PolynomialDecay(\n",
        "          5e-5,\n",
        "          decay_steps=processed_ds.cardinality() * num_epochs,\n",
        "          end_learning_rate=0.0,\n",
        "      )\n",
        "      loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "      gpt2_lm.compile(\n",
        "          optimizer=keras.optimizers.experimental.Adam(lr),\n",
        "          loss=loss,\n",
        "          weighted_metrics=[\"accuracy\"])\n",
        "\n",
        "      gpt2_lm.fit(processed_ds, epochs=num_epochs)\n",
        "      tf.saved_model.save(gpt2_lm, trained_model.uri)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kFQ61ABOlwao"
      },
      "outputs": [],
      "source": [
        "# from customCSVexampleGenComponent import customCSVexampleGenComponent\n",
        "custom_trainer = Trainer(preprocessed_data = ExampleGenerator.outputs['output_csv'] , num_epochs = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "If2pveDAlxm1"
      },
      "outputs": [],
      "source": [
        "context.run(custom_trainer, enable_cache = False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "CSV_Downloader_Component.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
