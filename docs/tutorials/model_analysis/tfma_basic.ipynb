{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdeKOEkv1Fe8"
      },
      "source": [
        "##### Copyright 2021 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "c2jyGuiG1gHr"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLsMb4vqY244"
      },
      "source": [
        "Note: You can run this example right now in a Jupyter-style notebook, no setup required!  Just click \"Run in Google Colab\"\n",
        "\n",
        "\u003cdiv class=\"devsite-table-wrapper\"\u003e\u003ctable class=\"tfo-notebook-buttons\" align=\"left\"\u003e\n",
        "\u003ctd\u003e\u003ca target=\"_blank\" href=\"https://www.tensorflow.org/tfx/tutorials/model_analysis/tfma_basic\"\u003e\n",
        "\u003cimg src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" /\u003eView on TensorFlow.org\u003c/a\u003e\u003c/td\u003e\n",
        "\u003ctd\u003e\u003ca target=\"_blank\" href=\"https://colab.sandbox.google.com/github/tensorflow/tfx/blob/master/docs/tutorials/model_analysis/tfma_basic.ipynb\"\u003e\n",
        "\u003cimg src=\"https://www.tensorflow.org/images/colab_logo_32px.png\"\u003eRun in Google Colab\u003c/a\u003e\u003c/td\u003e\n",
        "\u003ctd\u003e\u003ca target=\"_blank\" href=\"https://github.com/tensorflow/tfx/blob/master/docs/tutorials/model_analysis/tfma_basic.ipynb\"\u003e\n",
        "\u003cimg width=32px src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\"\u003eView source on GitHub\u003c/a\u003e\u003c/td\u003e\n",
        "\u003ctd\u003e\u003ca target=\"_blank\" href=\"https://storage.googleapis.com/tensorflow_docs/tfx/docs/tutorials/model_analysis/tfma_basic.ipynb\"\u003e\n",
        "\u003cimg width=32px src=\"https://www.tensorflow.org/images/download_logo_32px.png\"\u003eDownload notebook\u003c/a\u003e\u003c/td\u003e\n",
        "\u003c/table\u003e\u003c/div\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuSYVbwEYNHw"
      },
      "source": [
        "# TensorFlow Model Analysis\n",
        "***An Example of a Key Component of TensorFlow Extended (TFX)***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPt5BHTwy_0F"
      },
      "source": [
        "[TensorFlow Model Analysis (TFMA)](https://www.tensorflow.org/tfx/guide/tfma) is a library for performing model evaluation across different slices of data. TFMA performs its computations in a distributed manner over large amounts of data using [Apache Beam](https://beam.apache.org/documentation/programming-guide/).\n",
        "\n",
        "This example colab notebook illustrates how  TFMA can be used to investigate and visualize the performance of a model with respect to characteristics of the dataset.  We'll use a model that we trained previously, and now you get to play with the results! The model we trained was for the [Chicago Taxi Example](https://github.com/tensorflow/tfx/tree/master/tfx/examples/chicago_taxi_pipeline), which uses the [Taxi Trips dataset](https://data.cityofchicago.org/Transportation/Taxi-Trips/wrvz-psew) released by the City of Chicago. Explore the full dataset in the [BigQuery UI](https://bigquery.cloud.google.com/dataset/bigquery-public-data:chicago_taxi_trips).\n",
        "\n",
        "As a modeler and developer, think about how this data is used and the potential benefits and harm a model's predictions can cause. A model like this could reinforce societal biases and disparities. Is a feature relevant to the problem you want to solve or will it introduce bias? For more information, read about \u003ca target='_blank' href='https://developers.google.com/machine-learning/fairness-overview/'\u003eML fairness\u003c/a\u003e.\n",
        "\n",
        "Note: In order to understand TFMA and how it works with Apache Beam, you'll need to know a little bit about Apache Beam itself.  The \u003ca target='_blank' href='https://beam.apache.org/documentation/programming-guide/'\u003eBeam Programming Guide\u003c/a\u003e is a great place to start."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fnm6Mj3vTGLm"
      },
      "source": [
        "The columns in the dataset are:\n",
        "\u003ctable\u003e\n",
        "\u003ctr\u003e\u003ctd\u003epickup_community_area\u003c/td\u003e\u003ctd\u003efare\u003c/td\u003e\u003ctd\u003etrip_start_month\u003c/td\u003e\u003c/tr\u003e\n",
        "\n",
        "\u003ctr\u003e\u003ctd\u003etrip_start_hour\u003c/td\u003e\u003ctd\u003etrip_start_day\u003c/td\u003e\u003ctd\u003etrip_start_timestamp\u003c/td\u003e\u003c/tr\u003e\n",
        "\u003ctr\u003e\u003ctd\u003epickup_latitude\u003c/td\u003e\u003ctd\u003epickup_longitude\u003c/td\u003e\u003ctd\u003edropoff_latitude\u003c/td\u003e\u003c/tr\u003e\n",
        "\u003ctr\u003e\u003ctd\u003edropoff_longitude\u003c/td\u003e\u003ctd\u003etrip_miles\u003c/td\u003e\u003ctd\u003epickup_census_tract\u003c/td\u003e\u003c/tr\u003e\n",
        "\u003ctr\u003e\u003ctd\u003edropoff_census_tract\u003c/td\u003e\u003ctd\u003epayment_type\u003c/td\u003e\u003ctd\u003ecompany\u003c/td\u003e\u003c/tr\u003e\n",
        "\u003ctr\u003e\u003ctd\u003etrip_seconds\u003c/td\u003e\u003ctd\u003edropoff_community_area\u003c/td\u003e\u003ctd\u003etips\u003c/td\u003e\u003c/tr\u003e\n",
        "\u003c/table\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7-ouHFnWAsu"
      },
      "source": [
        "## Install Jupyter Extensions\n",
        "Note: If running in a local Jupyter notebook, then these Jupyter extensions must be installed in the environment before running Jupyter.\n",
        "\n",
        "```bash\n",
        "jupyter nbextension enable --py widgetsnbextension --sys-prefix \n",
        "jupyter nbextension install --py --symlink tensorflow_model_analysis --sys-prefix \n",
        "jupyter nbextension enable --py tensorflow_model_analysis --sys-prefix \n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZj-impiAD_l"
      },
      "source": [
        "## Install TensorFlow Model Analysis (TFMA)\n",
        "\n",
        "This will pull in all the dependencies, and will take a minute.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8X32Q_lIKYxH"
      },
      "outputs": [],
      "source": [
        "# Upgrade pip to the latest, and install TFMA.\n",
        "!pip install -U pip\n",
        "!pip install tensorflow-model-analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7121_u1LO5W"
      },
      "source": [
        "**Now you must restart the runtime before running the cells below.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SA2E343NAMRF"
      },
      "outputs": [],
      "source": [
        "# This setup was tested with TF 2.10 and TFMA 0.41 (using colab), but it should\n",
        "# also work with the latest release.\n",
        "import sys\n",
        "\n",
        "# Confirm that we're using Python 3\n",
        "assert sys.version_info.major==3, 'This notebook must be run using Python 3.'\n",
        "\n",
        "import tensorflow as tf\n",
        "print('TF version: {}'.format(tf.__version__))\n",
        "import apache_beam as beam\n",
        "print('Beam version: {}'.format(beam.__version__))\n",
        "import tensorflow_model_analysis as tfma\n",
        "print('TFMA version: {}'.format(tfma.__version__))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aD7n5eECydb"
      },
      "source": [
        "**NOTE: The output above should be clear of errors before proceeding. Re-run the install if you are still seeing errors. Also, make sure to restart the runtime/kernel before moving to the next step.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RptgLn2RYuK3"
      },
      "source": [
        "## Load The Files\n",
        "We'll download a tar file that has everything we need.  That includes:\n",
        "\n",
        "* Training and evaluation datasets\n",
        "* Data schema\n",
        "* Training and serving saved models (keras and estimator) and eval saved models (estimator)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4QXVIM7iglN"
      },
      "outputs": [],
      "source": [
        "# Download the tar file from GCP and extract it\n",
        "import io, os, tempfile\n",
        "TAR_NAME = 'saved_models-2.2'\n",
        "BASE_DIR = tempfile.mkdtemp()\n",
        "DATA_DIR = os.path.join(BASE_DIR, TAR_NAME, 'data')\n",
        "MODELS_DIR = os.path.join(BASE_DIR, TAR_NAME, 'models')\n",
        "SCHEMA = os.path.join(BASE_DIR, TAR_NAME, 'schema.pbtxt')\n",
        "OUTPUT_DIR = os.path.join(BASE_DIR, 'output')\n",
        "\n",
        "!curl -O https://storage.googleapis.com/artifacts.tfx-oss-public.appspot.com/datasets/{TAR_NAME}.tar\n",
        "!tar xf {TAR_NAME}.tar\n",
        "!mv {TAR_NAME} {BASE_DIR}\n",
        "!rm {TAR_NAME}.tar\n",
        "\n",
        "print(\"Here's what we downloaded:\")\n",
        "!ls -R {BASE_DIR}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xa7ZDV1MycO"
      },
      "source": [
        "## Parse the Schema\n",
        "\n",
        "Among the things we downloaded was a schema for our data that was created by [TensorFlow Data Validation](https://www.tensorflow.org/tfx/data_validation/get_started/).  Let's parse that now so that we can use it with TFMA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uW5eB4TPcwFw"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from google.protobuf import text_format\n",
        "from tensorflow.python.lib.io import file_io\n",
        "from tensorflow_metadata.proto.v0 import schema_pb2\n",
        "from tensorflow.core.example import example_pb2\n",
        "\n",
        "schema = schema_pb2.Schema()\n",
        "contents = file_io.read_file_to_string(SCHEMA)\n",
        "schema = text_format.Parse(contents, schema)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UP3yuJxfNXRL"
      },
      "source": [
        "## Use the Schema to Create TFRecords\n",
        "\n",
        "We need to give TFMA access to our dataset, so let's create a TFRecords file.  We can use our schema to create it, since it gives us the correct type for each feature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-wud3fPczl6"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "datafile = os.path.join(DATA_DIR, 'eval', 'data.csv')\n",
        "reader = csv.DictReader(open(datafile, 'r'))\n",
        "examples = []\n",
        "for line in reader:\n",
        "  example = example_pb2.Example()\n",
        "  for feature in schema.feature:\n",
        "    key = feature.name\n",
        "    if feature.type == schema_pb2.FLOAT:\n",
        "      example.features.feature[key].float_list.value[:] = (\n",
        "          [float(line[key])] if len(line[key]) \u003e 0 else [])\n",
        "    elif feature.type == schema_pb2.INT:\n",
        "      example.features.feature[key].int64_list.value[:] = (\n",
        "          [int(line[key])] if len(line[key]) \u003e 0 else [])\n",
        "    elif feature.type == schema_pb2.BYTES:\n",
        "      example.features.feature[key].bytes_list.value[:] = (\n",
        "          [line[key].encode('utf8')] if len(line[key]) \u003e 0 else [])\n",
        "  # Add a new column 'big_tipper' that indicates if tips was \u003e 20% of the fare. \n",
        "  # TODO(b/157064428): Remove after label transformation is supported for Keras.\n",
        "  big_tipper = float(line['tips']) \u003e float(line['fare']) * 0.2\n",
        "  example.features.feature['big_tipper'].float_list.value[:] = [big_tipper]\n",
        "  examples.append(example)\n",
        "\n",
        "tfrecord_file = os.path.join(BASE_DIR, 'train_data.rio')\n",
        "with tf.io.TFRecordWriter(tfrecord_file) as writer:\n",
        "  for example in examples:\n",
        "    writer.write(example.SerializeToString())\n",
        "\n",
        "!ls {tfrecord_file}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fp8Ub7GTXH3j"
      },
      "source": [
        "## Setup and Run TFMA\n",
        "\n",
        "TFMA supports a number of different model types including TF keras models, models based on generic TF2 signature APIs, as well TF estimator based models. The [get_started](https://www.tensorflow.org/tfx/model_analysis/get_started) guide has the full list of model types supported and any restrictions. For this example we are going to show how to configure a keras based model as well as an estimator based model that was saved as an [`EvalSavedModel`](https://www.tensorflow.org/tfx/model_analysis/eval_saved_model). See the [FAQ](https://www.tensorflow.org/tfx/model_analysis/faq) for examples of other configurations.\n",
        "\n",
        "TFMA provides support for calculating metrics that were used at training time (i.e. built-in metrics) as well metrics defined after the model was saved as part of the TFMA configuration settings. For our keras [setup](https://www.tensorflow.org/tfx/model_analysis/setup) we will demonstrate adding our metrics and plots manually as part of our configuration (see the [metrics](https://www.tensorflow.org/tfx/model_analysis/metrics) guide for information on the metrics and plots that are supported). For the estimator setup we will use the built-in metrics that were saved with the model. Our setups also include a number of slicing specs which are discussed in more detail in the following sections.\n",
        "\n",
        "After creating a [`tfma.EvalConfig`](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/EvalConfig) and [`tfma.EvalSharedModel`](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/EvalSharedModel) we can then run TFMA using [`tfma.run_model_analysis`](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/run_model_analysis). This will create a [`tfma.EvalResult`](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/EvalResult) which we can use later for rendering our metrics and plots."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgC7NdCatT8y"
      },
      "source": [
        "### Keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLJxcjpjfwkx"
      },
      "outputs": [],
      "source": [
        "import tensorflow_model_analysis as tfma\n",
        "\n",
        "# Setup tfma.EvalConfig settings\n",
        "keras_eval_config = text_format.Parse(\"\"\"\n",
        "  ## Model information\n",
        "  model_specs {\n",
        "    # For keras (and serving models) we need to add a `label_key`.\n",
        "    label_key: \"big_tipper\"\n",
        "  }\n",
        "\n",
        "  ## Post training metric information. These will be merged with any built-in\n",
        "  ## metrics from training.\n",
        "  metrics_specs {\n",
        "    metrics { class_name: \"ExampleCount\" }\n",
        "    metrics { class_name: \"AUC\" }\n",
        "    metrics { class_name: \"Precision\" }\n",
        "    metrics { class_name: \"Recall\" }\n",
        "    metrics { class_name: \"MeanPrediction\" }\n",
        "    metrics { class_name: \"Calibration\" }\n",
        "    metrics { class_name: \"CalibrationPlot\" }\n",
        "    metrics { class_name: \"ConfusionMatrixPlot\" }\n",
        "    # ... add additional metrics and plots ...\n",
        "  }\n",
        "\n",
        "  ## Slicing information\n",
        "  slicing_specs {}  # overall slice\n",
        "  slicing_specs {\n",
        "    feature_keys: [\"trip_start_hour\"]\n",
        "  }\n",
        "  slicing_specs {\n",
        "    feature_keys: [\"trip_start_day\"]\n",
        "  }\n",
        "  slicing_specs {\n",
        "    feature_values: {\n",
        "      key: \"trip_start_month\"\n",
        "      value: \"1\"\n",
        "    }\n",
        "  }\n",
        "\"\"\", tfma.EvalConfig())\n",
        "\n",
        "# Create a tfma.EvalSharedModel that points at our keras model.\n",
        "keras_model_path = os.path.join(MODELS_DIR, 'keras', '2')\n",
        "keras_eval_shared_model = tfma.default_eval_shared_model(\n",
        "    eval_saved_model_path=keras_model_path,\n",
        "    eval_config=keras_eval_config)\n",
        "\n",
        "keras_output_path = os.path.join(OUTPUT_DIR, 'keras')\n",
        "\n",
        "# Run TFMA\n",
        "keras_eval_result = tfma.run_model_analysis(\n",
        "    eval_shared_model=keras_eval_shared_model,\n",
        "    eval_config=keras_eval_config,\n",
        "    data_location=tfrecord_file,\n",
        "    output_path=keras_output_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMtoi_FpthQL"
      },
      "source": [
        "### Estimator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6MJg42JVtjjj"
      },
      "outputs": [],
      "source": [
        "import tensorflow_model_analysis as tfma\n",
        "\n",
        "# Setup tfma.EvalConfig settings\n",
        "estimator_eval_config = text_format.Parse(\"\"\"\n",
        "  ## Model information\n",
        "  model_specs {\n",
        "    # To use EvalSavedModel set `signature_name` to \"eval\".\n",
        "    signature_name: \"eval\"\n",
        "  }\n",
        "\n",
        "  ## Post training metric information. These will be merged with any built-in\n",
        "  ## metrics from training.\n",
        "  metrics_specs {\n",
        "    metrics { class_name: \"ConfusionMatrixPlot\" }\n",
        "    # ... add additional metrics and plots ...\n",
        "  }\n",
        "\n",
        "  ## Slicing information\n",
        "  slicing_specs {}  # overall slice\n",
        "  slicing_specs {\n",
        "    feature_keys: [\"trip_start_hour\"]\n",
        "  }\n",
        "  slicing_specs {\n",
        "    feature_keys: [\"trip_start_day\"]\n",
        "  }\n",
        "  slicing_specs {\n",
        "    feature_values: {\n",
        "      key: \"trip_start_month\"\n",
        "      value: \"1\"\n",
        "    }\n",
        "  }\n",
        "\"\"\", tfma.EvalConfig())\n",
        "\n",
        "# Create a tfma.EvalSharedModel that points at our eval saved model.\n",
        "estimator_base_model_path = os.path.join(\n",
        "    MODELS_DIR, 'estimator', 'eval_model_dir')\n",
        "estimator_model_path = os.path.join(\n",
        "    estimator_base_model_path, os.listdir(estimator_base_model_path)[0])\n",
        "estimator_eval_shared_model = tfma.default_eval_shared_model(\n",
        "    eval_saved_model_path=estimator_model_path,\n",
        "    eval_config=estimator_eval_config)\n",
        "\n",
        "estimator_output_path = os.path.join(OUTPUT_DIR, 'estimator')\n",
        "\n",
        "# Run TFMA\n",
        "estimator_eval_result = tfma.run_model_analysis(\n",
        "    eval_shared_model=estimator_eval_shared_model,\n",
        "    eval_config=estimator_eval_config,\n",
        "    data_location=tfrecord_file,\n",
        "    output_path=estimator_output_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0khNBC9FlEO"
      },
      "source": [
        "## Visualizing Metrics and Plots\n",
        "\n",
        "Now that we've run the evaluation, let's take a look at our visualizations using TFMA. For the following examples, we will visualize the results from running the evaluation on the keras model. To view the estimator based model update the `eval_result_path` to point at our `estimator_output_path` variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XFY0BqGtGkJ0"
      },
      "outputs": [],
      "source": [
        "eval_result_path = keras_output_path\n",
        "# eval_result_path = estimator_output_path\n",
        "\n",
        "eval_result = keras_eval_result\n",
        "# eval_result = estimator_eval_result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSl9qyTCbBKR"
      },
      "source": [
        "### Rendering Metrics\n",
        "\n",
        "TFMA provides dataframe APIs in [`tfma.experimental.dataframe`](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/experimental) to load the materalized output as [`Pandas DataFrames`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html). To view metrics you can use `metrics_as_dataframes(tfma.load_metrics(eval_path))`, which returns an object which potentially contains several DataFrames, one for each metric value type (`double_value`, `confusion_matrix_at_thresholds`,  `bytes_value`, and `array_value`). The specific DataFrames populated depends on the eval result. Here, we show the `double_value` DataFrame as an example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJ5_UMnWYmaE"
      },
      "outputs": [],
      "source": [
        "import tensorflow_model_analysis.experimental.dataframe as tfma_dataframe\n",
        "dfs = tfma_dataframe.metrics_as_dataframes(\n",
        "  tfma.load_metrics(eval_result_path))\n",
        "\n",
        "display(dfs.double_value.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QohnC7RAMu7F"
      },
      "source": [
        "Each of the DataFrames has a column multi-index with the top-level columns: `slices`, `metric_keys`, and `metric_values`. The exact columns of each group can change according to the payload. we can use `DataFrame.columns` API to inspect all the multi-index columns. For example, the slices columns are 'Overall', 'trip_start_day', 'trip_start_hour', and 'trip_start_month', which is configured by the `slicing_specs` in the `eval_config`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGscUL2KMyWn"
      },
      "outputs": [],
      "source": [
        "print(dfs.double_value.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJuxvGCpn4yF"
      },
      "source": [
        "### Auto pivoting\n",
        "The DataFrame is verbose by design so that there is no loss of information from the payload. However, sometimes, for direct consumption, we might want to organize the information in a more concise but lossy form: slices as rows and metrics as columns. TFMA provides an `auto_pivot` API for this purpose. The util pivots on all of the non-unique columns inside `metric_keys`, and condenses all the slices into one `stringified_slices` column by default."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NWDGhnEoK1HM"
      },
      "outputs": [],
      "source": [
        "tfma_dataframe.auto_pivot(dfs.double_value).head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQT-1Ckcnd_7"
      },
      "source": [
        "### Filtering slices\n",
        "Since the outputs are DataFrames, any native DataFrame APIs can be used to slice and dice the DataFrame. For example, if we are only interested in `trip_start_hour` of 1, 3, 5, 7 and not in `trip_start_day`, we can use DataFrame's `.loc` filtering logic. Again, we use the `auto_pivot` function to re-organize the DataFrame in the slice vs. metrics view after the filtering is performed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UOfquHOHK_uE"
      },
      "outputs": [],
      "source": [
        "df_double = dfs.double_value\n",
        "df_filtered = (df_double\n",
        "  .loc[df_double.slices.trip_start_hour.isin([1,3,5,7])]\n",
        ")\n",
        "display(tfma_dataframe.auto_pivot(df_filtered))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSnqI6Esb1XM"
      },
      "source": [
        "### Sorting by metric values\n",
        "We can also sort slices by metrics value. As an example, we show how to sort slices in the above DataFrame by ascending AUC, so that we can find poorly performing slices. This involves two steps: auto-pivoting so that slices are represented as rows and columns are metrics, and then sorting the pivoted DataFrame by the AUC column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVeZ9saBR8gX"
      },
      "outputs": [],
      "source": [
        "# Pivoted table sorted by AUC in ascending order.\n",
        "df_sorted = (\n",
        "    tfma_dataframe.auto_pivot(df_double)\n",
        "    .sort_values(by='auc', ascending=True)\n",
        "    )\n",
        "display(df_sorted.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8acksU33KMm"
      },
      "source": [
        "### Rendering Plots\n",
        "\n",
        "Any plots that were added to the `tfma.EvalConfig` as post training `metric_specs` can be displayed using [`tfma.view.render_plot`](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/view/render_plot).\n",
        "\n",
        "As with metrics, plots can be viewed by slice. Unlike metrics, only plots for a particular slice value can be displayed so the `tfma.SlicingSpec` must be used and it must specify both a slice feature name and value. If no slice is provided then the plots for the `Overall` slice is used.\n",
        "\n",
        "In the example below we are displaying the `CalibrationPlot` and `ConfusionMatrixPlot` plots that were computed for the `trip_start_hour:1` slice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4TCKjGw3S-a"
      },
      "outputs": [],
      "source": [
        "tfma.view.render_plot(\n",
        "    eval_result,\n",
        "    tfma.SlicingSpec(feature_values={'trip_start_hour': '1'}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "meRvFkKcPbux"
      },
      "source": [
        "## Tracking Model Performance Over Time\n",
        "\n",
        "Your training dataset will be used for training your model, and will hopefully be representative of your test dataset and the data that will be sent to your model in production.  However, while the data in inference requests may remain the same as your training data, in many cases it will start to change enough so that the performance of your model will change.\n",
        "\n",
        "That means that you need to monitor and measure your model's performance on an ongoing basis, so that you can be aware of and react to changes.  Let's take a look at how TFMA can help.\n",
        "\n",
        "Let's load 3 different model runs and use TFMA to see how they compare using [`render_time_series`](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/view/render_time_series)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJYUOjmFfuPy"
      },
      "outputs": [],
      "source": [
        "# Note this re-uses the EvalConfig from the keras setup.\n",
        "\n",
        "# Run eval on each saved model\n",
        "output_paths = []\n",
        "for i in range(3):\n",
        "  # Create a tfma.EvalSharedModel that points at our saved model.\n",
        "  eval_shared_model = tfma.default_eval_shared_model(\n",
        "      eval_saved_model_path=os.path.join(MODELS_DIR, 'keras', str(i)),\n",
        "      eval_config=keras_eval_config)\n",
        "\n",
        "  output_path = os.path.join(OUTPUT_DIR, 'time_series', str(i))\n",
        "  output_paths.append(output_path)\n",
        "\n",
        "  # Run TFMA\n",
        "  tfma.run_model_analysis(eval_shared_model=eval_shared_model,\n",
        "                          eval_config=keras_eval_config,\n",
        "                          data_location=tfrecord_file,\n",
        "                          output_path=output_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsO-gqCRK0ar"
      },
      "source": [
        "First, we'll imagine that we've trained and deployed our model yesterday, and now we want to see how it's doing on the new data coming in today.  The visualization will start by displaying AUC. From the UI you can:\n",
        "\n",
        "* Add other metrics using the \"Add metric series\" menu.\n",
        "* Close unwanted graphs by clicking on x\n",
        "* Hover over data points (the ends of line segments in the graph) to get more details\n",
        "\n",
        "Note: In the metric series charts the X axis is the model directory name of the model run that you're examining.  These names themselves are not meaningful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KjEws8T0cDm9"
      },
      "outputs": [],
      "source": [
        "eval_results_from_disk = tfma.load_eval_results(output_paths[:2])\n",
        "\n",
        "tfma.view.render_time_series(eval_results_from_disk)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQ7kZxESN9Bx"
      },
      "source": [
        "Now we'll imagine that another day has passed and we want to see how it's doing on the new data coming in today, compared to the previous two days:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjQmlXMmLwHf"
      },
      "outputs": [],
      "source": [
        "eval_results_from_disk = tfma.load_eval_results(output_paths)\n",
        "\n",
        "tfma.view.render_time_series(eval_results_from_disk)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1jpShgQxlVL"
      },
      "source": [
        "## Model Validation\n",
        "\n",
        "TFMA can be configured to evaluate multiple models at the same time. Typically this is done to compare a new model against a baseline (such as the currently serving model) to determine what the performance differences in metrics (e.g. AUC, etc) are relative to the baseline. When [thresholds](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/MetricThreshold) are configured, TFMA will produce a [`tfma.ValidationResult`](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/ValidationResult) record indicating whether the performance matches expecations.\n",
        "\n",
        "Let's re-configure our keras evaluation to compare two models: a candidate and a baseline. We will also validate the candidate's performance against the baseline by setting a [`tmfa.MetricThreshold`](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/MetricThreshold) on the AUC metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkatdR6Y1-4G"
      },
      "outputs": [],
      "source": [
        "# Setup tfma.EvalConfig setting\n",
        "eval_config_with_thresholds = text_format.Parse(\"\"\"\n",
        "  ## Model information\n",
        "  model_specs {\n",
        "    name: \"candidate\"\n",
        "    # For keras we need to add a `label_key`.\n",
        "    label_key: \"big_tipper\"\n",
        "  }\n",
        "  model_specs {\n",
        "    name: \"baseline\"\n",
        "    # For keras we need to add a `label_key`.\n",
        "    label_key: \"big_tipper\"\n",
        "    is_baseline: true\n",
        "  }\n",
        "\n",
        "  ## Post training metric information\n",
        "  metrics_specs {\n",
        "    metrics { class_name: \"ExampleCount\" }\n",
        "    metrics { class_name: \"BinaryAccuracy\" }\n",
        "    metrics { class_name: \"BinaryCrossentropy\" }\n",
        "    metrics {\n",
        "      class_name: \"AUC\"\n",
        "      threshold {\n",
        "        # Ensure that AUC is always \u003e 0.9\n",
        "        value_threshold {\n",
        "          lower_bound { value: 0.9 }\n",
        "        }\n",
        "        # Ensure that AUC does not drop by more than a small epsilon\n",
        "        # e.g. (candidate - baseline) \u003e -1e-10 or candidate \u003e baseline - 1e-10\n",
        "        change_threshold {\n",
        "          direction: HIGHER_IS_BETTER\n",
        "          absolute { value: -1e-10 }\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "    metrics { class_name: \"AUCPrecisionRecall\" }\n",
        "    metrics { class_name: \"Precision\" }\n",
        "    metrics { class_name: \"Recall\" }\n",
        "    metrics { class_name: \"MeanLabel\" }\n",
        "    metrics { class_name: \"MeanPrediction\" }\n",
        "    metrics { class_name: \"Calibration\" }\n",
        "    metrics { class_name: \"CalibrationPlot\" }\n",
        "    metrics { class_name: \"ConfusionMatrixPlot\" }\n",
        "    # ... add additional metrics and plots ...\n",
        "  }\n",
        "\n",
        "  ## Slicing information\n",
        "  slicing_specs {}  # overall slice\n",
        "  slicing_specs {\n",
        "    feature_keys: [\"trip_start_hour\"]\n",
        "  }\n",
        "  slicing_specs {\n",
        "    feature_keys: [\"trip_start_day\"]\n",
        "  }\n",
        "  slicing_specs {\n",
        "    feature_keys: [\"trip_start_month\"]\n",
        "  }\n",
        "  slicing_specs {\n",
        "    feature_keys: [\"trip_start_hour\", \"trip_start_day\"]\n",
        "  }\n",
        "\"\"\", tfma.EvalConfig())\n",
        "\n",
        "# Create tfma.EvalSharedModels that point at our keras models.\n",
        "candidate_model_path = os.path.join(MODELS_DIR, 'keras', '2')\n",
        "baseline_model_path = os.path.join(MODELS_DIR, 'keras', '1')\n",
        "eval_shared_models = [\n",
        "  tfma.default_eval_shared_model(\n",
        "      model_name=tfma.CANDIDATE_KEY,\n",
        "      eval_saved_model_path=candidate_model_path,\n",
        "      eval_config=eval_config_with_thresholds),\n",
        "  tfma.default_eval_shared_model(\n",
        "      model_name=tfma.BASELINE_KEY,\n",
        "      eval_saved_model_path=baseline_model_path,\n",
        "      eval_config=eval_config_with_thresholds),\n",
        "]\n",
        "\n",
        "validation_output_path = os.path.join(OUTPUT_DIR, 'validation')\n",
        "\n",
        "# Run TFMA\n",
        "eval_result_with_validation = tfma.run_model_analysis(\n",
        "    eval_shared_models,\n",
        "    eval_config=eval_config_with_thresholds,\n",
        "    data_location=tfrecord_file,\n",
        "    output_path=validation_output_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siF6npd3IfJq"
      },
      "source": [
        "When running evaluations with one or more models against a baseline, TFMA automatically adds diff metrics for all of the metrics computed during the evaluation. These metrics are named after the corresponding metric but with `_diff` appended to the metric name.\n",
        "\n",
        "Let's take a look at the metrics produced by our run:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGIw9TDuJ7wn"
      },
      "outputs": [],
      "source": [
        "tfma.view.render_time_series(eval_result_with_validation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIsehm_V4oKU"
      },
      "source": [
        "Now let's look at the output from our validation checks. To view the validation results we use [`tfma.load_validator_result`](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/load_validation_result). For our example, the validation fails because AUC is below the threshold."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48EdSTUW5eE1"
      },
      "outputs": [],
      "source": [
        "validation_result = tfma.load_validation_result(validation_output_path)\n",
        "print(validation_result.validation_ok)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tghWegsjhpkt"
      },
      "source": [
        "# Copyright \u0026copy; 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rSGJWC5biBiG"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvsmelXGasty"
      },
      "source": [
        "Note: This site provides applications using data that has been modified for use from its original source, www.cityofchicago.org, the official website of the City of Chicago. The City of Chicago makes no claims as to the content, accuracy, timeliness, or completeness of any of the data provided at this site. The data provided at this site is subject to change at any time. It is understood that the data provided at this site is being used at one’s own risk."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "tfma_basic.ipynb",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
